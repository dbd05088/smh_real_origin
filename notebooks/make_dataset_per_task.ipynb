{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow Memory: Make a dataset of each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import PIL.ImageEnhance\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read json file which follows below format. \n",
    "\n",
    "[{\"klass\": \"truck\", \"file_name\": \"test/truck/01.jpg\"}, ...]\n",
    "\n",
    "You should change the file name as below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('C:/smh/AI/code/rainbow-memory/dataset/train_json.json')\n",
    "test = pd.read_json('C:/smh/AI/code/rainbow-memory/dataset/test_json.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variables `rnd_seed` and `num_tasks` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 3 # random seed \n",
    "num_tasks = 5 # the number of tasks. \n",
    "np.random.seed(rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "klass = train.klass.unique()\n",
    "num_cls_per_task = len(klass) // num_tasks\n",
    "#print(num_cls_per_task) # cifar10이므로 10/5 = 2\n",
    "np.random.shuffle(klass)\n",
    "# enumerate : index 번호와 class를 tuple의 형태로 반환\n",
    "class2label = {cls_:idx for idx, cls_ in enumerate(klass)}  # house : 0, cat : 1 이런식으로 순서대로 배정해주는 것\n",
    "'''\n",
    "apply와 lambda\n",
    "lambda를 하면 대입을 해야하는 것을 대신해주게 된다. 여기서는 x에 class2label[x]를 넣어주는 역할을 한줄의 코드로 가능하게 만들어준다.\n",
    "원래 x에 0~9까지의 숫자가 들어 있었기 때문에, 각 class에 해당하는 애를 matching 시켜줌\n",
    "이를 바탕으로 label이라는 column을 새롭게 matching 시켜줌\n",
    "'''\n",
    "train[\"label\"] = train.klass.apply(lambda x: class2label[x])\n",
    "test[\"label\"] = test.klass.apply(lambda x: class2label[x])\n",
    "\n",
    "task_class = np.split(klass, num_tasks) # task별로 class를 나눠줌 (2개씩!!)\n",
    "\n",
    "# list comprehension\n",
    "'''\n",
    "[(변수를 활용할 방법) for (사용할 변수 이름) in (순회할 수 있는 값)]\n",
    "'''\n",
    "# task별로 쪼갠 것 (isin을 통해서 tc에 있는 애들은 true, 없으면 false를 반환하게 하고, train[true]인 애들끼리 묶어줌)\n",
    "# 이를 통해서 task별로 train data와 test data들을 묶음 단위로 쪼개줄 수 있다.\n",
    "task_train = [train[train.klass.isin(tc)] for tc in task_class]\n",
    "task_test = [test[test.klass.isin(tc)] for tc in task_class]\n",
    "#print(task_class)\n",
    "#print(task_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_frame: pd.DataFrame, dataset: str, transform=None):\n",
    "        self.data_frame = data_frame\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.data_frame.iloc[idx][\"file_name\"]\n",
    "        label = self.data_frame.iloc[idx].get(\"label\", -1)\n",
    "\n",
    "        img_path = os.path.join(\"dataset\", self.dataset, img_name)\n",
    "        image = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample[\"image\"] = image\n",
    "        sample[\"label\"] = label\n",
    "        sample[\"image_name\"] = img_name\n",
    "        return sample\n",
    "\n",
    "    def get_image_class(self, y):\n",
    "        return self.data_frame[self.data_frame[\"label\"] == y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, n_worker, train_list, test_list):\n",
    "        # Loader\n",
    "        train_loader = None\n",
    "        test_loader = None\n",
    "        if train_list is not None and len(train_list) > 0:\n",
    "            train_dataset = ImageDataset(\n",
    "                pd.DataFrame(train_list),\n",
    "                dataset=\"cifar10\",\n",
    "                transform=self.train_transform,\n",
    "            )\n",
    "            # drop last becasue of BatchNorm1D in IcarlNet\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                shuffle=True,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=n_worker,\n",
    "                drop_last=True,\n",
    "            )\n",
    "\n",
    "        if test_list is not None:\n",
    "            test_dataset = ImageDataset(\n",
    "                pd.DataFrame(test_list),\n",
    "                dataset=\"cifar10\",\n",
    "                transform=self.test_transform,\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, shuffle=False, batch_size=batch_size, num_workers=n_worker\n",
    "            )\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_datalist(args, exp_name: str, cur_iter: int) -> List:\n",
    "    if exp_name is None:\n",
    "        exp_name = args.exp_name\n",
    "\n",
    "    if exp_name in [\"joint\", \"blurry10\", \"blurry30\"]:\n",
    "        # merge over all tasks\n",
    "        tasks = list(range(args.n_tasks))\n",
    "    elif exp_name == \"disjoint\":\n",
    "        # merge current and all previous tasks\n",
    "        tasks = list(range(cur_iter + 1))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    datalist = []\n",
    "    for iter_ in tasks:\n",
    "        collection_name = \"{dataset}_test_rand{rnd}_cls{n_cls}_task{iter}\".format(\n",
    "            dataset=args.dataset, rnd=args.rnd_seed, n_cls=args.n_cls_a_task, iter=iter_\n",
    "        )\n",
    "        datalist += pd.read_json(\n",
    "            f\"collections/{args.dataset}/{collection_name}.json\"\n",
    "        ).to_dict(orient=\"records\")\n",
    "        logger.info(f\"[Test ] Get datalist from {collection_name}.json\")\n",
    "\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disjoint Task Boundaries Benchmark\n",
    "\n",
    "Configure disjoint dataset which does not share the classes of each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../collections/disjoint/cifar10_train_disjoint_rand3_cls2_task0.json\n",
      "../collections/disjoint/cifar10_train_disjoint_rand3_cls2_task1.json\n",
      "../collections/disjoint/cifar10_train_disjoint_rand3_cls2_task2.json\n",
      "../collections/disjoint/cifar10_train_disjoint_rand3_cls2_task3.json\n",
      "../collections/disjoint/cifar10_train_disjoint_rand3_cls2_task4.json\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"cifar10_train\" # Need to change the name of your dataset.\n",
    "root = Path('../collections/disjoint')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, train_task in enumerate(task_train):\n",
    "    file_name = origin_name\n",
    "    train_prefix = {'_disjoint':'', \n",
    "              '_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    " \n",
    "    for name, value in train_prefix.items():\n",
    "        file_name += name + str(value)\n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    train_task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blurry Task Boundaries Benchmark\n",
    "\n",
    "Configure blurry task dataset which can share the classes of each task\n",
    "\n",
    "There are two types of classes per each task as described in the paper. \n",
    "\n",
    "- **Major Classes** account for 90(70) percent of whole dataset of the corresponding dataset in blurry-10(30). \n",
    "- **Minor Classes** account for 10(30) percent of whole dataset of the corresponding dataset in blurry-10(30). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 버젼\n",
    "def make_blurry(major_ratio, num_labeled, num_classes):\n",
    "    # major_ratio = 0.9 # 0.9 for blurry-10, 0.7 for blurry-30.\n",
    "    # num_labeled = 4000 # cifar10 train total 50000\n",
    "    # num_classes = 10 # cifar10\n",
    "    major_classes = []\n",
    "    label_per_class = num_labeled // num_classes\n",
    "    \n",
    "    task_trainM = []\n",
    "    task_trainN = []\n",
    "    for t in task_train:\n",
    "        major_classes.append(list(t.klass.unique()))\n",
    "        sub_task_trainN = []\n",
    "\n",
    "        # sample 함수를 통해서 M%에 해당하는 data 추출\n",
    "        taskM = t.sample(n = int(len(t) * major_ratio), replace=False)\n",
    "        taskN = pd.concat([taskM, t]).drop_duplicates(keep=False)\n",
    "        taskN_size = len(taskN)\n",
    "\n",
    "        task_trainM.append(taskM)\n",
    "\n",
    "        # 각각의 task에서 M/5%씩 추출\n",
    "        for _ in range(len(task_train)-1):\n",
    "            sub_task_trainN.append(taskN.sample(n=taskN_size//(len(task_train)-1)))\n",
    "\n",
    "        task_trainN.append(sub_task_trainN)\n",
    "\n",
    "    task_mixed_train = []\n",
    "    for idx, task in enumerate(task_trainM):\n",
    "        other_task_samples = pd.DataFrame() \n",
    "        for j in range(len(task_trainM)):\n",
    "            if idx != j: \n",
    "                other_task_samples = pd.concat([other_task_samples, task_trainN[j].pop(0)])\n",
    "        mixed_task = pd.concat([task, other_task_samples])\n",
    "        task_mixed_train.append(mixed_task)\n",
    "    labeled_idx = []\n",
    "    total_data = []\n",
    "    for idx, data_per_task in enumerate(task_mixed_train):\n",
    "        major_class = major_classes[idx]\n",
    "        for mc in major_class:\n",
    "            index = list(np.where(data_per_task.klass == mc)[0])\n",
    "            # print(len(index))\n",
    "            index = np.random.choice(index, label_per_class, False) # choice 함수 : index에서 label_per_class만큼 choose하고 replace = False\n",
    "            labeled_data = []\n",
    "            # labeled data로 일부 추출\n",
    "            for i in index:\n",
    "                labeled_data.append(task_train[idx].iloc[i, :])\n",
    "            # unlabeled data는 전체 data\n",
    "            unlabeled_data = data_per_task\n",
    "            total_data.append((pd.DataFrame(labeled_data), unlabeled_data, task_test[idx]))\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-05258554fdc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-6e7e7c484c47>\u001b[0m in \u001b[0;36mget_dataloader\u001b[1;34m(batch_size, n_worker, train_list, test_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m             train_dataset = ImageDataset(\n\u001b[0;32m      7\u001b[0m                 \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "total_data = make_blurry(1.0, 4000, 10)\n",
    "print(len(total_data))\n",
    "print(len(total_data[0]))\n",
    "train_loader, test_loader = get_dataloader(128, 0, total_data[0][0], total_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_name = \"cifar10_train\" # Need to change the name of your dataset.\n",
    "root = Path('../my_collections/blurry')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, task in enumerate(task_mixed_train):\n",
    "    file_name = origin_name\n",
    "    prefix = {'_blurry':f'{int(round((1.0 - major_ratio)*100))}', \n",
    "              '_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    "    \n",
    "    for name, value in prefix.items():\n",
    "        file_name += name + str(value)\n",
    "\n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../collections/test/cifar10_test_rand3_cls2_task0.json\n",
      "../collections/test/cifar10_test_rand3_cls2_task1.json\n",
      "../collections/test/cifar10_test_rand3_cls2_task2.json\n",
      "../collections/test/cifar10_test_rand3_cls2_task3.json\n",
      "../collections/test/cifar10_test_rand3_cls2_task4.json\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"cifar10_test\" # Need to change the name of your dataset.\n",
    "task_test = [test[test.klass.isin(tc)] for tc in task_class]\n",
    "\n",
    "root = Path('../collections/test')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, task in enumerate(task_test):\n",
    "    file_name = origin_name\n",
    "    prefix = {'_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    "    for name, value in prefix.items():\n",
    "        file_name += name + str(value)\n",
    "        \n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
