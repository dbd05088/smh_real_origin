class2label {'dog': 0, 'deer': 1, 'automobile': 2, 'bird': 3, 'truck': 4, 'frog': 5, 'horse': 6, 'airplane': 7, 'cat': 8, 'ship': 9}
task_class [array(['dog', 'deer'], dtype=object), array(['automobile', 'bird'], dtype=object), array(['truck', 'frog'], dtype=object), array(['horse', 'airplane'], dtype=object), array(['cat', 'ship'], dtype=object)]
[INFO] main.py:214 > Set the device (cuda)
[INFO] main.py:259 > Using train-transforms Compose(
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=4)
    ToTensor()
    Normalize(mean=(0.4914, 0.482158, 0.4465231), std=(0.247032, 0.243485, 0.2615877))
)
[INFO] augment.py:18 > cifar10: autoaugmentation is applied
[INFO] main.py:282 > Using train-transforms [AutoAugment CIFAR10 Policy]
[INFO] main.py:292 > [1] Select a CIL method (rm)
[INFO] method_manager.py:48 > CIL Scenario: 
n_tasks: 5
n_init_cls: 10
n_cls_a_task: 2
total cls: 10
[INFO] main.py:298 > [2] Incrementally training 5 tasks

##################################################
# Task 0 iteration
##################################################

[INFO] main.py:308 > [2-1] Prepare a datalist for the current task
meta_pseudo_init
total : 5000  current step :  0
total : 5000  current step :  1
total : 5000  current step :  2
total : 5000  current step :  3
total : 5000  current step :  4
total : 5000  current step :  5
total : 5000  current step :  6
total : 5000  current step :  7
total : 5000  current step :  8
total : 5000  current step :  9
total : 5000  current step :  10
total : 5000  current step :  11
total : 5000  current step :  12
total : 5000  current step :  13
total : 5000  current step :  14
total : 5000  current step :  15
total : 5000  current step :  16
total : 5000  current step :  17
total : 5000  current step :  18
total : 5000  current step :  19
total : 5000  current step :  20
total : 5000  current step :  21
total : 5000  current step :  22
total : 5000  current step :  23
total : 5000  current step :  24
total : 5000  current step :  25
total : 5000  current step :  26
total : 5000  current step :  27
total : 5000  current step :  28
total : 5000  current step :  29
total : 5000  current step :  30
total : 5000  current step :  31
total : 5000  current step :  32
total : 5000  current step :  33
total : 5000  current step :  34
total : 5000  current step :  35
total : 5000  current step :  36
total : 5000  current step :  37
total : 5000  current step :  38
total : 5000  current step :  39
total : 5000  current step :  40
total : 5000  current step :  41
total : 5000  current step :  42
total : 5000  current step :  43
total : 5000  current step :  44
total : 5000  current step :  45
total : 5000  current step :  46
total : 5000  current step :  47
total : 5000  current step :  48
total : 5000  current step :  49
total : 5000  current step :  50
total : 5000  current step :  51
total : 5000  current step :  52
total : 5000  current step :  53
total : 5000  current step :  54
total : 5000  current step :  55
total : 5000  current step :  56
total : 5000  current step :  57
total : 5000  current step :  58
total : 5000  current step :  59
total : 5000  current step :  60
total : 5000  current step :  61
total : 5000  current step :  62
total : 5000  current step :  63
total : 5000  current step :  64
total : 5000  current step :  65
total : 5000  current step :  66
total : 5000  current step :  67
total : 5000  current step :  68
total : 5000  current step :  69
total : 5000  current step :  70
total : 5000  current step :  71
total : 5000  current step :  72
total : 5000  current step :  73
total : 5000  current step :  74
total : 5000  current step :  75
total : 5000  current step :  76
total : 5000  current step :  77
total : 5000  current step :  78
total : 5000  current step :  79
total : 5000  current step :  80
total : 5000  current step :  81
total : 5000  current step :  82
total : 5000  current step :  83
total : 5000  current step :  84
total : 5000  current step :  85
total : 5000  current step :  86
total : 5000  current step :  87
total : 5000  current step :  88
total : 5000  current step :  89
total : 5000  current step :  90
total : 5000  current step :  91
total : 5000  current step :  92
total : 5000  current step :  93
total : 5000  current step :  94
total : 5000  current step :  95
total : 5000  current step :  96
total : 5000  current step :  97
total : 5000  current step :  98
total : 5000  current step :  99
total : 5000  current step :  100
total : 5000  current step :  101
total : 5000  current step :  102
total : 5000  current step :  103
total : 5000  current step :  104
total : 5000  current step :  105
total : 5000  current step :  106
total : 5000  current step :  107
total : 5000  current step :  108
total : 5000  current step :  109
total : 5000  current step :  110
total : 5000  current step :  111
total : 5000  current step :  112
total : 5000  current step :  113
total : 5000  current step :  114
total : 5000  current step :  115
total : 5000  current step :  116
total : 5000  current step :  117
total : 5000  current step :  118
total : 5000  current step :  119
total : 5000  current step :  120
total : 5000  current step :  121
total : 5000  current step :  122
total : 5000  current step :  123
total : 5000  current step :  124
total : 5000  current step :  125
total : 5000  current step :  126
total : 5000  current step :  127
total : 5000  current step :  128
total : 5000  current step :  129
total : 5000  current step :  130
total : 5000  current step :  131
total : 5000  current step :  132
total : 5000  current step :  133
total : 5000  current step :  134
total : 5000  current step :  135
total : 5000  current step :  136
total : 5000  current step :  137
total : 5000  current step :  138
total : 5000  current step :  139
total : 5000  current step :  140
total : 5000  current step :  141
total : 5000  current step :  142
total : 5000  current step :  143
total : 5000  current step :  144
total : 5000  current step :  145
total : 5000  current step :  146
total : 5000  current step :  147
total : 5000  current step :  148
total : 5000  current step :  149
total : 5000  current step :  150
total : 5000  current step :  151
total : 5000  current step :  152
total : 5000  current step :  153
total : 5000  current step :  154
total : 5000  current step :  155
total : 5000  current step :  156
total : 5000  current step :  157
total : 5000  current step :  158
total : 5000  current step :  159
total : 5000  current step :  160
total : 5000  current step :  161
total : 5000  current step :  162
total : 5000  current step :  163
total : 5000  current step :  164
total : 5000  current step :  165
total : 5000  current step :  166
total : 5000  current step :  167
total : 5000  current step :  168
total : 5000  current step :  169
total : 5000  current step :  170
total : 5000  current step :  171
total : 5000  current step :  172
total : 5000  current step :  173
total : 5000  current step :  174
total : 5000  current step :  175
total : 5000  current step :  176
total : 5000  current step :  177
total : 5000  current step :  178
total : 5000  current step :  179
total : 5000  current step :  180
total : 5000  current step :  181
total : 5000  current step :  182
total : 5000  current step :  183
total : 5000  current step :  184
total : 5000  current step :  185
total : 5000  current step :  186
total : 5000  current step :  187
total : 5000  current step :  188
total : 5000  current step :  189
total : 5000  current step :  190
total : 5000  current step :  191
total : 5000  current step :  192
total : 5000  current step :  193
total : 5000  current step :  194
total : 5000  current step :  195
total : 5000  current step :  196
total : 5000  current step :  197
total : 5000  current step :  198
total : 5000  current step :  199
total : 5000  current step :  200
total : 5000  current step :  201
total : 5000  current step :  202
total : 5000  current step :  203
total : 5000  current step :  204
total : 5000  current step :  205
total : 5000  current step :  206
total : 5000  current step :  207
total : 5000  current step :  208
total : 5000  current step :  209
total : 5000  current step :  210
total : 5000  current step :  211
total : 5000  current step :  212
total : 5000  current step :  213
total : 5000  current step :  214
total : 5000  current step :  215
total : 5000  current step :  216
total : 5000  current step :  217
total : 5000  current step :  218
total : 5000  current step :  219
total : 5000  current step :  220
total : 5000  current step :  221
total : 5000  current step :  222
total : 5000  current step :  223
total : 5000  current step :  224
total : 5000  current step :  225
total : 5000  current step :  226
total : 5000  current step :  227
total : 5000  current step :  228
total : 5000  current step :  229
total : 5000  current step :  230
total : 5000  current step :  231
total : 5000  current step :  232
total : 5000  current step :  233
total : 5000  current step :  234
total : 5000  current step :  235
total : 5000  current step :  236
total : 5000  current step :  237
total : 5000  current step :  238
total : 5000  current step :  239
total : 5000  current step :  240
total : 5000  current step :  241
total : 5000  current step :  242
total : 5000  current step :  243
total : 5000  current step :  244
total : 5000  current step :  245
total : 5000  current step :  246
total : 5000  current step :  247
total : 5000  current step :  248
total : 5000  current step :  249
total : 5000  current step :  250
total : 5000  current step :  251
total : 5000  current step :  252
total : 5000  current step :  253
total : 5000  current step :  254
total : 5000  current step :  255
total : 5000  current step :  256
total : 5000  current step :  257
total : 5000  current step :  258
total : 5000  current step :  259
total : 5000  current step :  260
total : 5000  current step :  261
total : 5000  current step :  262
total : 5000  current step :  263
total : 5000  current step :  264
total : 5000  current step :  265
total : 5000  current step :  266
total : 5000  current step :  267
total : 5000  current step :  268
total : 5000  current step :  269
total : 5000  current step :  270
total : 5000  current step :  271
total : 5000  current step :  272
total : 5000  current step :  273
total : 5000  current step :  274
total : 5000  current step :  275
total : 5000  current step :  276
total : 5000  current step :  277
total : 5000  current step :  278
total : 5000  current step :  279
total : 5000  current step :  280
total : 5000  current step :  281
total : 5000  current step :  282
total : 5000  current step :  283
total : 5000  current step :  284
total : 5000  current step :  285
total : 5000  current step :  286
total : 5000  current step :  287
total : 5000  current step :  288
total : 5000  current step :  289
total : 5000  current step :  290
total : 5000  current step :  291
total : 5000  current step :  292
total : 5000  current step :  293
total : 5000  current step :  294
total : 5000  current step :  295
total : 5000  current step :  296
total : 5000  current step :  297
total : 5000  current step :  298
total : 5000  current step :  299
total : 5000  current step :  300
total : 5000  current step :  301
total : 5000  current step :  302
total : 5000  current step :  303
total : 5000  current step :  304
total : 5000  current step :  305
total : 5000  current step :  306
total : 5000  current step :  307
total : 5000  current step :  308
total : 5000  current step :  309
total : 5000  current step :  310
total : 5000  current step :  311
total : 5000  current step :  312
total : 5000  current step :  313
total : 5000  current step :  314
total : 5000  current step :  315
total : 5000  current step :  316
total : 5000  current step :  317
total : 5000  current step :  318
total : 5000  current step :  319
total : 5000  current step :  320
total : 5000  current step :  321
total : 5000  current step :  322
total : 5000  current step :  323
total : 5000  current step :  324
total : 5000  current step :  325
total : 5000  current step :  326
total : 5000  current step :  327
total : 5000  current step :  328
total : 5000  current step :  329
total : 5000  current step :  330
total : 5000  current step :  331
total : 5000  current step :  332
total : 5000  current step :  333
total : 5000  current step :  334
total : 5000  current step :  335
total : 5000  current step :  336
total : 5000  current step :  337
total : 5000  current step :  338
total : 5000  current step :  339
total : 5000  current step :  340
total : 5000  current step :  341
total : 5000  current step :  342
total : 5000  current step :  343
total : 5000  current step :  344
total : 5000  current step :  345
total : 5000  current step :  346
total : 5000  current step :  347
total : 5000  current step :  348
total : 5000  current step :  349
total : 5000  current step :  350
total : 5000  current step :  351
total : 5000  current step :  352
total : 5000  current step :  353
total : 5000  current step :  354
total : 5000  current step :  355
total : 5000  current step :  356
total : 5000  current step :  357
total : 5000  current step :  358
total : 5000  current step :  359
total : 5000  current step :  360
total : 5000  current step :  361
total : 5000  current step :  362
total : 5000  current step :  363
total : 5000  current step :  364
total : 5000  current step :  365
total : 5000  current step :  366
total : 5000  current step :  367
total : 5000  current step :  368
total : 5000  current step :  369
total : 5000  current step :  370
total : 5000  current step :  371
total : 5000  current step :  372
total : 5000  current step :  373
total : 5000  current step :  374
total : 5000  current step :  375
total : 5000  current step :  376
total : 5000  current step :  377
total : 5000  current step :  378
total : 5000  current step :  379
total : 5000  current step :  380
total : 5000  current step :  381
total : 5000  current step :  382
total : 5000  current step :  383
total : 5000  current step :  384
total : 5000  current step :  385
total : 5000  current step :  386
total : 5000  current step :  387
total : 5000  current step :  388
total : 5000  current step :  389
total : 5000  current step :  390
total : 5000  current step :  391
total : 5000  current step :  392
total : 5000  current step :  393
total : 5000  current step :  394
total : 5000  current step :  395
total : 5000  current step :  396
total : 5000  current step :  397
total : 5000  current step :  398
total : 5000  current step :  399
total : 5000  current step :  400
total : 5000  current step :  401
total : 5000  current step :  402
total : 5000  current step :  403
total : 5000  current step :  404
total : 5000  current step :  405
total : 5000  current step :  406
total : 5000  current step :  407
total : 5000  current step :  408
total : 5000  current step :  409
total : 5000  current step :  410
total : 5000  current step :  411
total : 5000  current step :  412
total : 5000  current step :  413
total : 5000  current step :  414
total : 5000  current step :  415
total : 5000  current step :  416
total : 5000  current step :  417
total : 5000  current step :  418
total : 5000  current step :  419
total : 5000  current step :  420
total : 5000  current step :  421
total : 5000  current step :  422
total : 5000  current step :  423
total : 5000  current step :  424
total : 5000  current step :  425
total : 5000  current step :  426
total : 5000  current step :  427
total : 5000  current step :  428
total : 5000  current step :  429
total : 5000  current step :  430
total : 5000  current step :  431
total : 5000  current step :  432
total : 5000  current step :  433
total : 5000  current step :  434
total : 5000  current step :  435
total : 5000  current step :  436
total : 5000  current step :  437
total : 5000  current step :  438
total : 5000  current step :  439
total : 5000  current step :  440
total : 5000  current step :  441
total : 5000  current step :  442
total : 5000  current step :  443
total : 5000  current step :  444
total : 5000  current step :  445
total : 5000  current step :  446
total : 5000  current step :  447
total : 5000  current step :  448
total : 5000  current step :  449
total : 5000  current step :  450
total : 5000  current step :  451
total : 5000  current step :  452
total : 5000  current step :  453
total : 5000  current step :  454
total : 5000  current step :  455
total : 5000  current step :  456
total : 5000  current step :  457
total : 5000  current step :  458
total : 5000  current step :  459
total : 5000  current step :  460
total : 5000  current step :  461
total : 5000  current step :  462
total : 5000  current step :  463
total : 5000  current step :  464
total : 5000  current step :  465
total : 5000  current step :  466
total : 5000  current step :  467
total : 5000  current step :  468
total : 5000  current step :  469
total : 5000  current step :  470
total : 5000  current step :  471
total : 5000  current step :  472
total : 5000  current step :  473
total : 5000  current step :  474
total : 5000  current step :  475
total : 5000  current step :  476
total : 5000  current step :  477
total : 5000  current step :  478
total : 5000  current step :  479
total : 5000  current step :  480
total : 5000  current step :  481
total : 5000  current step :  482
total : 5000  current step :  483
total : 5000  current step :  484
total : 5000  current step :  485
total : 5000  current step :  486
total : 5000  current step :  487
total : 5000  current step :  488
total : 5000  current step :  489
total : 5000  current step :  490
total : 5000  current step :  491
total : 5000  current step :  492
total : 5000  current step :  493
total : 5000  current step :  494
total : 5000  current step :  495
total : 5000  current step :  496
total : 5000  current step :  497
total : 5000  current step :  498
total : 5000  current step :  499
total : 5000  current step :  500
total : 5000  current step :  501
total : 5000  current step :  502
total : 5000  current step :  503
total : 5000  current step :  504
total : 5000  current step :  505
total : 5000  current step :  506
total : 5000  current step :  507
total : 5000  current step :  508
total : 5000  current step :  509
total : 5000  current step :  510
total : 5000  current step :  511
total : 5000  current step :  512
total : 5000  current step :  513
total : 5000  current step :  514
total : 5000  current step :  515
total : 5000  current step :  516
total : 5000  current step :  517
total : 5000  current step :  518
total : 5000  current step :  519
total : 5000  current step :  520
total : 5000  current step :  521
total : 5000  current step :  522
total : 5000  current step :  523
total : 5000  current step :  524
total : 5000  current step :  525
total : 5000  current step :  526
total : 5000  current step :  527
total : 5000  current step :  528
total : 5000  current step :  529
total : 5000  current step :  530
total : 5000  current step :  531
total : 5000  current step :  532
total : 5000  current step :  533
total : 5000  current step :  534
total : 5000  current step :  535
total : 5000  current step :  536
total : 5000  current step :  537
total : 5000  current step :  538
total : 5000  current step :  539
total : 5000  current step :  540
total : 5000  current step :  541
total : 5000  current step :  542
total : 5000  current step :  543
total : 5000  current step :  544
total : 5000  current step :  545
total : 5000  current step :  546
total : 5000  current step :  547
total : 5000  current step :  548
total : 5000  current step :  549
total : 5000  current step :  550
total : 5000  current step :  551
total : 5000  current step :  552
total : 5000  current step :  553
total : 5000  current step :  554
total : 5000  current step :  555
total : 5000  current step :  556
total : 5000  current step :  557
total : 5000  current step :  558
total : 5000  current step :  559
total : 5000  current step :  560
total : 5000  current step :  561
total : 5000  current step :  562
total : 5000  current step :  563
total : 5000  current step :  564
total : 5000  current step :  565
total : 5000  current step :  566
total : 5000  current step :  567
total : 5000  current step :  568
total : 5000  current step :  569
total : 5000  current step :  570
total : 5000  current step :  571
total : 5000  current step :  572
total : 5000  current step :  573
total : 5000  current step :  574
total : 5000  current step :  575
total : 5000  current step :  576
total : 5000  current step :  577
total : 5000  current step :  578
total : 5000  current step :  579
total : 5000  current step :  580
total : 5000  current step :  581
total : 5000  current step :  582
total : 5000  current step :  583
total : 5000  current step :  584
total : 5000  current step :  585
total : 5000  current step :  586
total : 5000  current step :  587
total : 5000  current step :  588
total : 5000  current step :  589
total : 5000  current step :  590
total : 5000  current step :  591
total : 5000  current step :  592
total : 5000  current step :  593
total : 5000  current step :  594
total : 5000  current step :  595
total : 5000  current step :  596
total : 5000  current step :  597
total : 5000  current step :  598
total : 5000  current step :  599
total : 5000  current step :  600
total : 5000  current step :  601
total : 5000  current step :  602
total : 5000  current step :  603
total : 5000  current step :  604
total : 5000  current step :  605
total : 5000  current step :  606
total : 5000  current step :  607
total : 5000  current step :  608
total : 5000  current step :  609
total : 5000  current step :  610
total : 5000  current step :  611
total : 5000  current step :  612
total : 5000  current step :  613
total : 5000  current step :  614
total : 5000  current step :  615
total : 5000  current step :  616
total : 5000  current step :  617
total : 5000  current step :  618
total : 5000  current step :  619
total : 5000  current step :  620
total : 5000  current step :  621
total : 5000  current step :  622
total : 5000  current step :  623
total : 5000  current step :  624
total : 5000  current step :  625
total : 5000  current step :  626
total : 5000  current step :  627
total : 5000  current step :  628
total : 5000  current step :  629
total : 5000  current step :  630
total : 5000  current step :  631
total : 5000  current step :  632
total : 5000  current step :  633
total : 5000  current step :  634
total : 5000  current step :  635
total : 5000  current step :  636
total : 5000  current step :  637
total : 5000  current step :  638
total : 5000  current step :  639
total : 5000  current step :  640
total : 5000  current step :  641
total : 5000  current step :  642
total : 5000  current step :  643
total : 5000  current step :  644
total : 5000  current step :  645
total : 5000  current step :  646
total : 5000  current step :  647
total : 5000  current step :  648
total : 5000  current step :  649
total : 5000  current step :  650
total : 5000  current step :  651
total : 5000  current step :  652
total : 5000  current step :  653
total : 5000  current step :  654
total : 5000  current step :  655
total : 5000  current step :  656
total : 5000  current step :  657
total : 5000  current step :  658
total : 5000  current step :  659
total : 5000  current step :  660
total : 5000  current step :  661
total : 5000  current step :  662
total : 5000  current step :  663
total : 5000  current step :  664
total : 5000  current step :  665
total : 5000  current step :  666
total : 5000  current step :  667
total : 5000  current step :  668
total : 5000  current step :  669
total : 5000  current step :  670
total : 5000  current step :  671
total : 5000  current step :  672
total : 5000  current step :  673
total : 5000  current step :  674
total : 5000  current step :  675
total : 5000  current step :  676
total : 5000  current step :  677
total : 5000  current step :  678
total : 5000  current step :  679
total : 5000  current step :  680
total : 5000  current step :  681
total : 5000  current step :  682
total : 5000  current step :  683
total : 5000  current step :  684
total : 5000  current step :  685
total : 5000  current step :  686
total : 5000  current step :  687
total : 5000  current step :  688
total : 5000  current step :  689
total : 5000  current step :  690
total : 5000  current step :  691
total : 5000  current step :  692
total : 5000  current step :  693
total : 5000  current step :  694
total : 5000  current step :  695
total : 5000  current step :  696
total : 5000  current step :  697
total : 5000  current step :  698
total : 5000  current step :  699
total : 5000  current step :  700
total : 5000  current step :  701
total : 5000  current step :  702
total : 5000  current step :  703
total : 5000  current step :  704
total : 5000  current step :  705
total : 5000  current step :  706
total : 5000  current step :  707
total : 5000  current step :  708
total : 5000  current step :  709
total : 5000  current step :  710
total : 5000  current step :  711
total : 5000  current step :  712
total : 5000  current step :  713
total : 5000  current step :  714
total : 5000  current step :  715
total : 5000  current step :  716
total : 5000  current step :  717
total : 5000  current step :  718
total : 5000  current step :  719
total : 5000  current step :  720
total : 5000  current step :  721
total : 5000  current step :  722
total : 5000  current step :  723
total : 5000  current step :  724
total : 5000  current step :  725
total : 5000  current step :  726
total : 5000  current step :  727
total : 5000  current step :  728
total : 5000  current step :  729
total : 5000  current step :  730
total : 5000  current step :  731
total : 5000  current step :  732
total : 5000  current step :  733
total : 5000  current step :  734
total : 5000  current step :  735
total : 5000  current step :  736
total : 5000  current step :  737
total : 5000  current step :  738
total : 5000  current step :  739
total : 5000  current step :  740
total : 5000  current step :  741
total : 5000  current step :  742
total : 5000  current step :  743
total : 5000  current step :  744
total : 5000  current step :  745
total : 5000  current step :  746
total : 5000  current step :  747
total : 5000  current step :  748
total : 5000  current step :  749
total : 5000  current step :  750
total : 5000  current step :  751
total : 5000  current step :  752
total : 5000  current step :  753
total : 5000  current step :  754
total : 5000  current step :  755
total : 5000  current step :  756
total : 5000  current step :  757
total : 5000  current step :  758
total : 5000  current step :  759
total : 5000  current step :  760
total : 5000  current step :  761
total : 5000  current step :  762
total : 5000  current step :  763
total : 5000  current step :  764
total : 5000  current step :  765
total : 5000  current step :  766
total : 5000  current step :  767
total : 5000  current step :  768
total : 5000  current step :  769
total : 5000  current step :  770
total : 5000  current step :  771
total : 5000  current step :  772
total : 5000  current step :  773
total : 5000  current step :  774
total : 5000  current step :  775
total : 5000  current step :  776
total : 5000  current step :  777
total : 5000  current step :  778
total : 5000  current step :  779
total : 5000  current step :  780
total : 5000  current step :  781
total : 5000  current step :  782
total : 5000  current step :  783
total : 5000  current step :  784
total : 5000  current step :  785
total : 5000  current step :  786
total : 5000  current step :  787
total : 5000  current step :  788
total : 5000  current step :  789
total : 5000  current step :  790
total : 5000  current step :  791
total : 5000  current step :  792
total : 5000  current step :  793
total : 5000  current step :  794
total : 5000  current step :  795
total : 5000  current step :  796
total : 5000  current step :  797
total : 5000  current step :  798
total : 5000  current step :  799
total : 5000  current step :  800
total : 5000  current step :  801
total : 5000  current step :  802
total : 5000  current step :  803
total : 5000  current step :  804
total : 5000  current step :  805
total : 5000  current step :  806
total : 5000  current step :  807
total : 5000  current step :  808
total : 5000  current step :  809
total : 5000  current step :  810
total : 5000  current step :  811
total : 5000  current step :  812
total : 5000  current step :  813
total : 5000  current step :  814
total : 5000  current step :  815
total : 5000  current step :  816
total : 5000  current step :  817
total : 5000  current step :  818
total : 5000  current step :  819
total : 5000  current step :  820
total : 5000  current step :  821
total : 5000  current step :  822
total : 5000  current step :  823
total : 5000  current step :  824
total : 5000  current step :  825
total : 5000  current step :  826
total : 5000  current step :  827
total : 5000  current step :  828
total : 5000  current step :  829
total : 5000  current step :  830
total : 5000  current step :  831
total : 5000  current step :  832
total : 5000  current step :  833
total : 5000  current step :  834
total : 5000  current step :  835
total : 5000  current step :  836
total : 5000  current step :  837
total : 5000  current step :  838
total : 5000  current step :  839
total : 5000  current step :  840
total : 5000  current step :  841
total : 5000  current step :  842
total : 5000  current step :  843
total : 5000  current step :  844
total : 5000  current step :  845
total : 5000  current step :  846
total : 5000  current step :  847
total : 5000  current step :  848
total : 5000  current step :  849
total : 5000  current step :  850
total : 5000  current step :  851
total : 5000  current step :  852
total : 5000  current step :  853
total : 5000  current step :  854
total : 5000  current step :  855
total : 5000  current step :  856
total : 5000  current step :  857
total : 5000  current step :  858
total : 5000  current step :  859
total : 5000  current step :  860
total : 5000  current step :  861
total : 5000  current step :  862
total : 5000  current step :  863
total : 5000  current step :  864
total : 5000  current step :  865
total : 5000  current step :  866
total : 5000  current step :  867
total : 5000  current step :  868
total : 5000  current step :  869
total : 5000  current step :  870
total : 5000  current step :  871
total : 5000  current step :  872
total : 5000  current step :  873
total : 5000  current step :  874
total : 5000  current step :  875
total : 5000  current step :  876
total : 5000  current step :  877
total : 5000  current step :  878
total : 5000  current step :  879
total : 5000  current step :  880
total : 5000  current step :  881
total : 5000  current step :  882
total : 5000  current step :  883
total : 5000  current step :  884
total : 5000  current step :  885
total : 5000  current step :  886
total : 5000  current step :  887
total : 5000  current step :  888
total : 5000  current step :  889
total : 5000  current step :  890
total : 5000  current step :  891
total : 5000  current step :  892
total : 5000  current step :  893
total : 5000  current step :  894
total : 5000  current step :  895
total : 5000  current step :  896
total : 5000  current step :  897
total : 5000  current step :  898
total : 5000  current step :  899
total : 5000  current step :  900
total : 5000  current step :  901
total : 5000  current step :  902
total : 5000  current step :  903
total : 5000  current step :  904
total : 5000  current step :  905
total : 5000  current step :  906
total : 5000  current step :  907
total : 5000  current step :  908
total : 5000  current step :  909
total : 5000  current step :  910
total : 5000  current step :  911
total : 5000  current step :  912
total : 5000  current step :  913
total : 5000  current step :  914
total : 5000  current step :  915
total : 5000  current step :  916
total : 5000  current step :  917
total : 5000  current step :  918
total : 5000  current step :  919
total : 5000  current step :  920
total : 5000  current step :  921
total : 5000  current step :  922
total : 5000  current step :  923
total : 5000  current step :  924
total : 5000  current step :  925
total : 5000  current step :  926
total : 5000  current step :  927
total : 5000  current step :  928
total : 5000  current step :  929
total : 5000  current step :  930
total : 5000  current step :  931
total : 5000  current step :  932
total : 5000  current step :  933
total : 5000  current step :  934
total : 5000  current step :  935
total : 5000  current step :  936
total : 5000  current step :  937
total : 5000  current step :  938
total : 5000  current step :  939
total : 5000  current step :  940
total : 5000  current step :  941
total : 5000  current step :  942
total : 5000  current step :  943
total : 5000  current step :  944
total : 5000  current step :  945
total : 5000  current step :  946
total : 5000  current step :  947
total : 5000  current step :  948
total : 5000  current step :  949
total : 5000  current step :  950
total : 5000  current step :  951
total : 5000  current step :  952
total : 5000  current step :  953
total : 5000  current step :  954
total : 5000  current step :  955
total : 5000  current step :  956
total : 5000  current step :  957
total : 5000  current step :  958
total : 5000  current step :  959
total : 5000  current step :  960
total : 5000  current step :  961
total : 5000  current step :  962
total : 5000  current step :  963
total : 5000  current step :  964
total : 5000  current step :  965
total : 5000  current step :  966
total : 5000  current step :  967
total : 5000  current step :  968
total : 5000  current step :  969
total : 5000  current step :  970
total : 5000  current step :  971
total : 5000  current step :  972
total : 5000  current step :  973
total : 5000  current step :  974
total : 5000  current step :  975
total : 5000  current step :  976
total : 5000  current step :  977
total : 5000  current step :  978
total : 5000  current step :  979
total : 5000  current step :  980
total : 5000  current step :  981
total : 5000  current step :  982
total : 5000  current step :  983
total : 5000  current step :  984
total : 5000  current step :  985
total : 5000  current step :  986
total : 5000  current step :  987
total : 5000  current step :  988
total : 5000  current step :  989
total : 5000  current step :  990
total : 5000  current step :  991
total : 5000  current step :  992
total : 5000  current step :  993
total : 5000  current step :  994
total : 5000  current step :  995
total : 5000  current step :  996
total : 5000  current step :  997
total : 5000  current step :  998
total : 5000  current step :  999
total : 5000  current step :  1000
total : 5000  current step :  1001
total : 5000  current step :  1002
total : 5000  current step :  1003
total : 5000  current step :  1004
total : 5000  current step :  1005
total : 5000  current step :  1006
total : 5000  current step :  1007
total : 5000  current step :  1008
total : 5000  current step :  1009
total : 5000  current step :  1010
total : 5000  current step :  1011
total : 5000  current step :  1012
total : 5000  current step :  1013
total : 5000  current step :  1014
total : 5000  current step :  1015
total : 5000  current step :  1016
total : 5000  current step :  1017
total : 5000  current step :  1018
total : 5000  current step :  1019
total : 5000  current step :  1020
total : 5000  current step :  1021
total : 5000  current step :  1022
total : 5000  current step :  1023
total : 5000  current step :  1024
total : 5000  current step :  1025
total : 5000  current step :  1026
total : 5000  current step :  1027
total : 5000  current step :  1028
total : 5000  current step :  1029
total : 5000  current step :  1030
total : 5000  current step :  1031
total : 5000  current step :  1032
total : 5000  current step :  1033
total : 5000  current step :  1034
total : 5000  current step :  1035
total : 5000  current step :  1036
total : 5000  current step :  1037
total : 5000  current step :  1038
total : 5000  current step :  1039
total : 5000  current step :  1040
total : 5000  current step :  1041
total : 5000  current step :  1042
total : 5000  current step :  1043
total : 5000  current step :  1044
total : 5000  current step :  1045
total : 5000  current step :  1046
total : 5000  current step :  1047
total : 5000  current step :  1048
total : 5000  current step :  1049
total : 5000  current step :  1050
total : 5000  current step :  1051
total : 5000  current step :  1052
total : 5000  current step :  1053
total : 5000  current step :  1054
total : 5000  current step :  1055
total : 5000  current step :  1056
total : 5000  current step :  1057
total : 5000  current step :  1058
total : 5000  current step :  1059
total : 5000  current step :  1060
total : 5000  current step :  1061
total : 5000  current step :  1062
total : 5000  current step :  1063
total : 5000  current step :  1064
total : 5000  current step :  1065
total : 5000  current step :  1066
total : 5000  current step :  1067
total : 5000  current step :  1068
total : 5000  current step :  1069
total : 5000  current step :  1070
total : 5000  current step :  1071
total : 5000  current step :  1072
total : 5000  current step :  1073
total : 5000  current step :  1074
total : 5000  current step :  1075
total : 5000  current step :  1076
total : 5000  current step :  1077
total : 5000  current step :  1078
total : 5000  current step :  1079
total : 5000  current step :  1080
total : 5000  current step :  1081
total : 5000  current step :  1082
total : 5000  current step :  1083
total : 5000  current step :  1084
total : 5000  current step :  1085
total : 5000  current step :  1086
total : 5000  current step :  1087
total : 5000  current step :  1088
total : 5000  current step :  1089
total : 5000  current step :  1090
total : 5000  current step :  1091
total : 5000  current step :  1092
total : 5000  current step :  1093
total : 5000  current step :  1094
total : 5000  current step :  1095
total : 5000  current step :  1096
total : 5000  current step :  1097
total : 5000  current step :  1098
total : 5000  current step :  1099
total : 5000  current step :  1100
total : 5000  current step :  1101
total : 5000  current step :  1102
total : 5000  current step :  1103
total : 5000  current step :  1104
total : 5000  current step :  1105
total : 5000  current step :  1106
total : 5000  current step :  1107
total : 5000  current step :  1108
total : 5000  current step :  1109
total : 5000  current step :  1110
total : 5000  current step :  1111
total : 5000  current step :  1112
total : 5000  current step :  1113
total : 5000  current step :  1114
total : 5000  current step :  1115
total : 5000  current step :  1116
total : 5000  current step :  1117
total : 5000  current step :  1118
total : 5000  current step :  1119
total : 5000  current step :  1120
total : 5000  current step :  1121
total : 5000  current step :  1122
total : 5000  current step :  1123
total : 5000  current step :  1124
total : 5000  current step :  1125
total : 5000  current step :  1126
total : 5000  current step :  1127
total : 5000  current step :  1128
total : 5000  current step :  1129
total : 5000  current step :  1130
total : 5000  current step :  1131
total : 5000  current step :  1132
total : 5000  current step :  1133
total : 5000  current step :  1134
total : 5000  current step :  1135
total : 5000  current step :  1136
total : 5000  current step :  1137
total : 5000  current step :  1138
total : 5000  current step :  1139
total : 5000  current step :  1140
total : 5000  current step :  1141
total : 5000  current step :  1142
total : 5000  current step :  1143
total : 5000  current step :  1144
total : 5000  current step :  1145
total : 5000  current step :  1146
total : 5000  current step :  1147
total : 5000  current step :  1148
total : 5000  current step :  1149
total : 5000  current step :  1150
total : 5000  current step :  1151
total : 5000  current step :  1152
total : 5000  current step :  1153
total : 5000  current step :  1154
total : 5000  current step :  1155
total : 5000  current step :  1156
total : 5000  current step :  1157
total : 5000  current step :  1158
total : 5000  current step :  1159
total : 5000  current step :  1160
total : 5000  current step :  1161
total : 5000  current step :  1162
total : 5000  current step :  1163
total : 5000  current step :  1164
total : 5000  current step :  1165
total : 5000  current step :  1166
total : 5000  current step :  1167
total : 5000  current step :  1168
total : 5000  current step :  1169
total : 5000  current step :  1170
total : 5000  current step :  1171
total : 5000  current step :  1172
total : 5000  current step :  1173
total : 5000  current step :  1174
total : 5000  current step :  1175
total : 5000  current step :  1176
total : 5000  current step :  1177
total : 5000  current step :  1178
total : 5000  current step :  1179
total : 5000  current step :  1180
total : 5000  current step :  1181
total : 5000  current step :  1182
total : 5000  current step :  1183
total : 5000  current step :  1184
total : 5000  current step :  1185
total : 5000  current step :  1186
total : 5000  current step :  1187
total : 5000  current step :  1188
total : 5000  current step :  1189
total : 5000  current step :  1190
total : 5000  current step :  1191
total : 5000  current step :  1192
total : 5000  current step :  1193
total : 5000  current step :  1194
total : 5000  current step :  1195
total : 5000  current step :  1196
total : 5000  current step :  1197
total : 5000  current step :  1198
total : 5000  current step :  1199
total : 5000  current step :  1200
total : 5000  current step :  1201
total : 5000  current step :  1202
total : 5000  current step :  1203
total : 5000  current step :  1204
total : 5000  current step :  1205
total : 5000  current step :  1206
total : 5000  current step :  1207
total : 5000  current step :  1208
total : 5000  current step :  1209
total : 5000  current step :  1210
total : 5000  current step :  1211
total : 5000  current step :  1212
total : 5000  current step :  1213
total : 5000  current step :  1214
total : 5000  current step :  1215
total : 5000  current step :  1216
total : 5000  current step :  1217
total : 5000  current step :  1218
total : 5000  current step :  1219
total : 5000  current step :  1220
total : 5000  current step :  1221
total : 5000  current step :  1222
total : 5000  current step :  1223
total : 5000  current step :  1224
total : 5000  current step :  1225
total : 5000  current step :  1226
total : 5000  current step :  1227
total : 5000  current step :  1228
total : 5000  current step :  1229
total : 5000  current step :  1230
total : 5000  current step :  1231
total : 5000  current step :  1232
total : 5000  current step :  1233
total : 5000  current step :  1234
total : 5000  current step :  1235
total : 5000  current step :  1236
total : 5000  current step :  1237
total : 5000  current step :  1238
total : 5000  current step :  1239
total : 5000  current step :  1240
total : 5000  current step :  1241
total : 5000  current step :  1242
total : 5000  current step :  1243
total : 5000  current step :  1244
total : 5000  current step :  1245
total : 5000  current step :  1246
total : 5000  current step :  1247
total : 5000  current step :  1248
total : 5000  current step :  1249
total : 5000  current step :  1250
total : 5000  current step :  1251
total : 5000  current step :  1252
total : 5000  current step :  1253
total : 5000  current step :  1254
total : 5000  current step :  1255
total : 5000  current step :  1256
total : 5000  current step :  1257
total : 5000  current step :  1258
total : 5000  current step :  1259
total : 5000  current step :  1260
total : 5000  current step :  1261
total : 5000  current step :  1262
total : 5000  current step :  1263
total : 5000  current step :  1264
total : 5000  current step :  1265
total : 5000  current step :  1266
total : 5000  current step :  1267
total : 5000  current step :  1268
total : 5000  current step :  1269
total : 5000  current step :  1270
total : 5000  current step :  1271
total : 5000  current step :  1272
total : 5000  current step :  1273
total : 5000  current step :  1274
total : 5000  current step :  1275
total : 5000  current step :  1276
total : 5000  current step :  1277
total : 5000  current step :  1278
total : 5000  current step :  1279
total : 5000  current step :  1280
total : 5000  current step :  1281
total : 5000  current step :  1282
total : 5000  current step :  1283
total : 5000  current step :  1284
total : 5000  current step :  1285
total : 5000  current step :  1286
total : 5000  current step :  1287
total : 5000  current step :  1288
total : 5000  current step :  1289
total : 5000  current step :  1290
total : 5000  current step :  1291
total : 5000  current step :  1292
total : 5000  current step :  1293
total : 5000  current step :  1294
total : 5000  current step :  1295
total : 5000  current step :  1296
total : 5000  current step :  1297
total : 5000  current step :  1298
total : 5000  current step :  1299
total : 5000  current step :  1300
total : 5000  current step :  1301
total : 5000  current step :  1302
total : 5000  current step :  1303
total : 5000  current step :  1304
total : 5000  current step :  1305
total : 5000  current step :  1306
total : 5000  current step :  1307
total : 5000  current step :  1308
total : 5000  current step :  1309
total : 5000  current step :  1310
total : 5000  current step :  1311
total : 5000  current step :  1312
total : 5000  current step :  1313
total : 5000  current step :  1314
total : 5000  current step :  1315
total : 5000  current step :  1316
total : 5000  current step :  1317
total : 5000  current step :  1318
total : 5000  current step :  1319
total : 5000  current step :  1320
total : 5000  current step :  1321
total : 5000  current step :  1322
total : 5000  current step :  1323
total : 5000  current step :  1324
total : 5000  current step :  1325
total : 5000  current step :  1326
total : 5000  current step :  1327
total : 5000  current step :  1328
total : 5000  current step :  1329
total : 5000  current step :  1330
total : 5000  current step :  1331
total : 5000  current step :  1332
total : 5000  current step :  1333
total : 5000  current step :  1334
total : 5000  current step :  1335
total : 5000  current step :  1336
total : 5000  current step :  1337
total : 5000  current step :  1338
total : 5000  current step :  1339
total : 5000  current step :  1340
total : 5000  current step :  1341
total : 5000  current step :  1342
total : 5000  current step :  1343
total : 5000  current step :  1344
total : 5000  current step :  1345
total : 5000  current step :  1346
total : 5000  current step :  1347
total : 5000  current step :  1348
total : 5000  current step :  1349
total : 5000  current step :  1350
total : 5000  current step :  1351
total : 5000  current step :  1352
total : 5000  current step :  1353
total : 5000  current step :  1354
total : 5000  current step :  1355
total : 5000  current step :  1356
total : 5000  current step :  1357
total : 5000  current step :  1358
total : 5000  current step :  1359
total : 5000  current step :  1360
total : 5000  current step :  1361
total : 5000  current step :  1362
total : 5000  current step :  1363
total : 5000  current step :  1364
total : 5000  current step :  1365
total : 5000  current step :  1366
total : 5000  current step :  1367
total : 5000  current step :  1368
total : 5000  current step :  1369
total : 5000  current step :  1370
total : 5000  current step :  1371
total : 5000  current step :  1372
total : 5000  current step :  1373
total : 5000  current step :  1374
total : 5000  current step :  1375
total : 5000  current step :  1376
total : 5000  current step :  1377
total : 5000  current step :  1378
total : 5000  current step :  1379
total : 5000  current step :  1380
total : 5000  current step :  1381
total : 5000  current step :  1382
total : 5000  current step :  1383
total : 5000  current step :  1384
total : 5000  current step :  1385
total : 5000  current step :  1386
total : 5000  current step :  1387
total : 5000  current step :  1388
total : 5000  current step :  1389
total : 5000  current step :  1390
total : 5000  current step :  1391
total : 5000  current step :  1392
total : 5000  current step :  1393
total : 5000  current step :  1394
total : 5000  current step :  1395
total : 5000  current step :  1396
total : 5000  current step :  1397
total : 5000  current step :  1398
total : 5000  current step :  1399
total : 5000  current step :  1400
total : 5000  current step :  1401
total : 5000  current step :  1402
total : 5000  current step :  1403
total : 5000  current step :  1404
total : 5000  current step :  1405
total : 5000  current step :  1406
total : 5000  current step :  1407
total : 5000  current step :  1408
total : 5000  current step :  1409
total : 5000  current step :  1410
total : 5000  current step :  1411
total : 5000  current step :  1412
total : 5000  current step :  1413
total : 5000  current step :  1414
total : 5000  current step :  1415
total : 5000  current step :  1416
total : 5000  current step :  1417
total : 5000  current step :  1418
total : 5000  current step :  1419
total : 5000  current step :  1420
total : 5000  current step :  1421
total : 5000  current step :  1422
total : 5000  current step :  1423
total : 5000  current step :  1424
total : 5000  current step :  1425
total : 5000  current step :  1426
total : 5000  current step :  1427
total : 5000  current step :  1428
total : 5000  current step :  1429
total : 5000  current step :  1430
total : 5000  current step :  1431
total : 5000  current step :  1432
total : 5000  current step :  1433
total : 5000  current step :  1434
total : 5000  current step :  1435
total : 5000  current step :  1436
total : 5000  current step :  1437
total : 5000  current step :  1438
total : 5000  current step :  1439
total : 5000  current step :  1440
total : 5000  current step :  1441
total : 5000  current step :  1442
total : 5000  current step :  1443
total : 5000  current step :  1444
total : 5000  current step :  1445
total : 5000  current step :  1446
total : 5000  current step :  1447
total : 5000  current step :  1448
total : 5000  current step :  1449
total : 5000  current step :  1450
total : 5000  current step :  1451
total : 5000  current step :  1452
total : 5000  current step :  1453
total : 5000  current step :  1454
total : 5000  current step :  1455
total : 5000  current step :  1456
total : 5000  current step :  1457
total : 5000  current step :  1458
total : 5000  current step :  1459
total : 5000  current step :  1460
total : 5000  current step :  1461
total : 5000  current step :  1462
total : 5000  current step :  1463
total : 5000  current step :  1464
total : 5000  current step :  1465
total : 5000  current step :  1466
total : 5000  current step :  1467
total : 5000  current step :  1468
total : 5000  current step :  1469
total : 5000  current step :  1470
total : 5000  current step :  1471
total : 5000  current step :  1472
total : 5000  current step :  1473
total : 5000  current step :  1474
total : 5000  current step :  1475
total : 5000  current step :  1476
total : 5000  current step :  1477
total : 5000  current step :  1478
total : 5000  current step :  1479
total : 5000  current step :  1480
total : 5000  current step :  1481
total : 5000  current step :  1482
total : 5000  current step :  1483
total : 5000  current step :  1484
total : 5000  current step :  1485
total : 5000  current step :  1486
total : 5000  current step :  1487
total : 5000  current step :  1488
total : 5000  current step :  1489
total : 5000  current step :  1490
total : 5000  current step :  1491
total : 5000  current step :  1492
total : 5000  current step :  1493
total : 5000  current step :  1494
total : 5000  current step :  1495
total : 5000  current step :  1496
total : 5000  current step :  1497
total : 5000  current step :  1498
total : 5000  current step :  1499
total : 5000  current step :  1500
total : 5000  current step :  1501
total : 5000  current step :  1502
total : 5000  current step :  1503
total : 5000  current step :  1504
total : 5000  current step :  1505
total : 5000  current step :  1506
total : 5000  current step :  1507
total : 5000  current step :  1508
total : 5000  current step :  1509
total : 5000  current step :  1510
total : 5000  current step :  1511
total : 5000  current step :  1512
total : 5000  current step :  1513
total : 5000  current step :  1514
total : 5000  current step :  1515
total : 5000  current step :  1516
total : 5000  current step :  1517
total : 5000  current step :  1518
total : 5000  current step :  1519
total : 5000  current step :  1520
total : 5000  current step :  1521
total : 5000  current step :  1522
total : 5000  current step :  1523
total : 5000  current step :  1524
total : 5000  current step :  1525
total : 5000  current step :  1526
total : 5000  current step :  1527
total : 5000  current step :  1528
total : 5000  current step :  1529
total : 5000  current step :  1530
total : 5000  current step :  1531
total : 5000  current step :  1532
total : 5000  current step :  1533
total : 5000  current step :  1534
total : 5000  current step :  1535
total : 5000  current step :  1536
total : 5000  current step :  1537
total : 5000  current step :  1538
total : 5000  current step :  1539
total : 5000  current step :  1540
total : 5000  current step :  1541
total : 5000  current step :  1542
total : 5000  current step :  1543
total : 5000  current step :  1544
total : 5000  current step :  1545
total : 5000  current step :  1546
total : 5000  current step :  1547
total : 5000  current step :  1548
total : 5000  current step :  1549
total : 5000  current step :  1550
total : 5000  current step :  1551
total : 5000  current step :  1552
total : 5000  current step :  1553
total : 5000  current step :  1554
total : 5000  current step :  1555
total : 5000  current step :  1556
total : 5000  current step :  1557
total : 5000  current step :  1558
total : 5000  current step :  1559
total : 5000  current step :  1560
total : 5000  current step :  1561
total : 5000  current step :  1562
total : 5000  current step :  1563
total : 5000  current step :  1564
total : 5000  current step :  1565
total : 5000  current step :  1566
total : 5000  current step :  1567
total : 5000  current step :  1568
total : 5000  current step :  1569
total : 5000  current step :  1570
total : 5000  current step :  1571
total : 5000  current step :  1572
total : 5000  current step :  1573
total : 5000  current step :  1574
total : 5000  current step :  1575
total : 5000  current step :  1576
total : 5000  current step :  1577
total : 5000  current step :  1578
total : 5000  current step :  1579
total : 5000  current step :  1580
total : 5000  current step :  1581
total : 5000  current step :  1582
total : 5000  current step :  1583
total : 5000  current step :  1584
total : 5000  current step :  1585
total : 5000  current step :  1586
total : 5000  current step :  1587
total : 5000  current step :  1588
total : 5000  current step :  1589
total : 5000  current step :  1590
total : 5000  current step :  1591
total : 5000  current step :  1592
total : 5000  current step :  1593
total : 5000  current step :  1594
total : 5000  current step :  1595
total : 5000  current step :  1596
total : 5000  current step :  1597
total : 5000  current step :  1598
total : 5000  current step :  1599
total : 5000  current step :  1600
total : 5000  current step :  1601
total : 5000  current step :  1602
total : 5000  current step :  1603
total : 5000  current step :  1604
total : 5000  current step :  1605
total : 5000  current step :  1606
total : 5000  current step :  1607
total : 5000  current step :  1608
total : 5000  current step :  1609
total : 5000  current step :  1610
total : 5000  current step :  1611
total : 5000  current step :  1612
total : 5000  current step :  1613
total : 5000  current step :  1614
total : 5000  current step :  1615
total : 5000  current step :  1616
total : 5000  current step :  1617
total : 5000  current step :  1618
total : 5000  current step :  1619
total : 5000  current step :  1620
total : 5000  current step :  1621
total : 5000  current step :  1622
total : 5000  current step :  1623
total : 5000  current step :  1624
total : 5000  current step :  1625
total : 5000  current step :  1626
total : 5000  current step :  1627
total : 5000  current step :  1628
total : 5000  current step :  1629
total : 5000  current step :  1630
total : 5000  current step :  1631
total : 5000  current step :  1632
total : 5000  current step :  1633
total : 5000  current step :  1634
total : 5000  current step :  1635
total : 5000  current step :  1636
total : 5000  current step :  1637
total : 5000  current step :  1638
total : 5000  current step :  1639
total : 5000  current step :  1640
total : 5000  current step :  1641
total : 5000  current step :  1642
total : 5000  current step :  1643
total : 5000  current step :  1644
total : 5000  current step :  1645
total : 5000  current step :  1646
total : 5000  current step :  1647
total : 5000  current step :  1648
total : 5000  current step :  1649
total : 5000  current step :  1650
total : 5000  current step :  1651
total : 5000  current step :  1652
total : 5000  current step :  1653
total : 5000  current step :  1654
total : 5000  current step :  1655
total : 5000  current step :  1656
total : 5000  current step :  1657
total : 5000  current step :  1658
total : 5000  current step :  1659
total : 5000  current step :  1660
total : 5000  current step :  1661
total : 5000  current step :  1662
total : 5000  current step :  1663
total : 5000  current step :  1664
total : 5000  current step :  1665
total : 5000  current step :  1666
total : 5000  current step :  1667
total : 5000  current step :  1668
total : 5000  current step :  1669
total : 5000  current step :  1670
total : 5000  current step :  1671
total : 5000  current step :  1672
total : 5000  current step :  1673
total : 5000  current step :  1674
total : 5000  current step :  1675
total : 5000  current step :  1676
total : 5000  current step :  1677
total : 5000  current step :  1678
total : 5000  current step :  1679
total : 5000  current step :  1680
total : 5000  current step :  1681
total : 5000  current step :  1682
total : 5000  current step :  1683
total : 5000  current step :  1684
total : 5000  current step :  1685
total : 5000  current step :  1686
total : 5000  current step :  1687
total : 5000  current step :  1688
total : 5000  current step :  1689
total : 5000  current step :  1690
total : 5000  current step :  1691
total : 5000  current step :  1692
total : 5000  current step :  1693
total : 5000  current step :  1694
total : 5000  current step :  1695
total : 5000  current step :  1696
total : 5000  current step :  1697
total : 5000  current step :  1698
total : 5000  current step :  1699
total : 5000  current step :  1700
total : 5000  current step :  1701
total : 5000  current step :  1702
total : 5000  current step :  1703
total : 5000  current step :  1704
total : 5000  current step :  1705
total : 5000  current step :  1706
total : 5000  current step :  1707
total : 5000  current step :  1708
total : 5000  current step :  1709
total : 5000  current step :  1710
total : 5000  current step :  1711
total : 5000  current step :  1712
total : 5000  current step :  1713
total : 5000  current step :  1714
total : 5000  current step :  1715
total : 5000  current step :  1716
total : 5000  current step :  1717
total : 5000  current step :  1718
total : 5000  current step :  1719
total : 5000  current step :  1720
total : 5000  current step :  1721
total : 5000  current step :  1722
total : 5000  current step :  1723
total : 5000  current step :  1724
total : 5000  current step :  1725
total : 5000  current step :  1726
total : 5000  current step :  1727
total : 5000  current step :  1728
total : 5000  current step :  1729
total : 5000  current step :  1730
total : 5000  current step :  1731
total : 5000  current step :  1732
total : 5000  current step :  1733
total : 5000  current step :  1734
total : 5000  current step :  1735
total : 5000  current step :  1736
total : 5000  current step :  1737
total : 5000  current step :  1738
total : 5000  current step :  1739
total : 5000  current step :  1740
total : 5000  current step :  1741
total : 5000  current step :  1742
total : 5000  current step :  1743
total : 5000  current step :  1744
total : 5000  current step :  1745
total : 5000  current step :  1746
total : 5000  current step :  1747
total : 5000  current step :  1748
total : 5000  current step :  1749
total : 5000  current step :  1750
total : 5000  current step :  1751
total : 5000  current step :  1752
total : 5000  current step :  1753
total : 5000  current step :  1754
total : 5000  current step :  1755
total : 5000  current step :  1756
total : 5000  current step :  1757
total : 5000  current step :  1758
total : 5000  current step :  1759
total : 5000  current step :  1760
total : 5000  current step :  1761
total : 5000  current step :  1762
total : 5000  current step :  1763
total : 5000  current step :  1764
total : 5000  current step :  1765
total : 5000  current step :  1766
total : 5000  current step :  1767
total : 5000  current step :  1768
total : 5000  current step :  1769
total : 5000  current step :  1770
total : 5000  current step :  1771
total : 5000  current step :  1772
total : 5000  current step :  1773
total : 5000  current step :  1774
total : 5000  current step :  1775
total : 5000  current step :  1776
total : 5000  current step :  1777
total : 5000  current step :  1778
total : 5000  current step :  1779
total : 5000  current step :  1780
total : 5000  current step :  1781
total : 5000  current step :  1782
total : 5000  current step :  1783
total : 5000  current step :  1784
total : 5000  current step :  1785
total : 5000  current step :  1786
total : 5000  current step :  1787
total : 5000  current step :  1788
total : 5000  current step :  1789
total : 5000  current step :  1790
total : 5000  current step :  1791
total : 5000  current step :  1792
total : 5000  current step :  1793
total : 5000  current step :  1794
total : 5000  current step :  1795
total : 5000  current step :  1796
total : 5000  current step :  1797
total : 5000  current step :  1798
total : 5000  current step :  1799
total : 5000  current step :  1800
total : 5000  current step :  1801
total : 5000  current step :  1802
total : 5000  current step :  1803
total : 5000  current step :  1804
total : 5000  current step :  1805
total : 5000  current step :  1806
total : 5000  current step :  1807
total : 5000  current step :  1808
total : 5000  current step :  1809
total : 5000  current step :  1810
total : 5000  current step :  1811
total : 5000  current step :  1812
total : 5000  current step :  1813
total : 5000  current step :  1814
total : 5000  current step :  1815
total : 5000  current step :  1816
total : 5000  current step :  1817
total : 5000  current step :  1818
total : 5000  current step :  1819
total : 5000  current step :  1820
total : 5000  current step :  1821
total : 5000  current step :  1822
total : 5000  current step :  1823
total : 5000  current step :  1824
total : 5000  current step :  1825
total : 5000  current step :  1826
total : 5000  current step :  1827
total : 5000  current step :  1828
total : 5000  current step :  1829
total : 5000  current step :  1830
total : 5000  current step :  1831
total : 5000  current step :  1832
total : 5000  current step :  1833
total : 5000  current step :  1834
total : 5000  current step :  1835
total : 5000  current step :  1836
total : 5000  current step :  1837
total : 5000  current step :  1838
total : 5000  current step :  1839
total : 5000  current step :  1840
total : 5000  current step :  1841
total : 5000  current step :  1842
total : 5000  current step :  1843
total : 5000  current step :  1844
total : 5000  current step :  1845
total : 5000  current step :  1846
total : 5000  current step :  1847
total : 5000  current step :  1848
total : 5000  current step :  1849
total : 5000  current step :  1850
total : 5000  current step :  1851
total : 5000  current step :  1852
total : 5000  current step :  1853
total : 5000  current step :  1854
total : 5000  current step :  1855
total : 5000  current step :  1856
total : 5000  current step :  1857
total : 5000  current step :  1858
total : 5000  current step :  1859
total : 5000  current step :  1860
total : 5000  current step :  1861
total : 5000  current step :  1862
total : 5000  current step :  1863
total : 5000  current step :  1864
total : 5000  current step :  1865
total : 5000  current step :  1866
total : 5000  current step :  1867
total : 5000  current step :  1868
total : 5000  current step :  1869
total : 5000  current step :  1870
total : 5000  current step :  1871
total : 5000  current step :  1872
total : 5000  current step :  1873
total : 5000  current step :  1874
total : 5000  current step :  1875
total : 5000  current step :  1876
total : 5000  current step :  1877
total : 5000  current step :  1878
total : 5000  current step :  1879
total : 5000  current step :  1880
total : 5000  current step :  1881
total : 5000  current step :  1882
total : 5000  current step :  1883
total : 5000  current step :  1884
total : 5000  current step :  1885
total : 5000  current step :  1886
total : 5000  current step :  1887
total : 5000  current step :  1888
total : 5000  current step :  1889
total : 5000  current step :  1890
total : 5000  current step :  1891
total : 5000  current step :  1892
total : 5000  current step :  1893
total : 5000  current step :  1894
total : 5000  current step :  1895
total : 5000  current step :  1896
total : 5000  current step :  1897
total : 5000  current step :  1898
total : 5000  current step :  1899
total : 5000  current step :  1900
total : 5000  current step :  1901
total : 5000  current step :  1902
total : 5000  current step :  1903
total : 5000  current step :  1904
total : 5000  current step :  1905
total : 5000  current step :  1906
total : 5000  current step :  1907
total : 5000  current step :  1908
total : 5000  current step :  1909
total : 5000  current step :  1910
total : 5000  current step :  1911
total : 5000  current step :  1912
total : 5000  current step :  1913
total : 5000  current step :  1914
total : 5000  current step :  1915
total : 5000  current step :  1916
total : 5000  current step :  1917
total : 5000  current step :  1918
total : 5000  current step :  1919
total : 5000  current step :  1920
total : 5000  current step :  1921
total : 5000  current step :  1922
total : 5000  current step :  1923
total : 5000  current step :  1924
total : 5000  current step :  1925
total : 5000  current step :  1926
total : 5000  current step :  1927
total : 5000  current step :  1928
total : 5000  current step :  1929
total : 5000  current step :  1930
total : 5000  current step :  1931
total : 5000  current step :  1932
total : 5000  current step :  1933
total : 5000  current step :  1934
total : 5000  current step :  1935
total : 5000  current step :  1936
total : 5000  current step :  1937
total : 5000  current step :  1938
total : 5000  current step :  1939
total : 5000  current step :  1940
total : 5000  current step :  1941
total : 5000  current step :  1942
total : 5000  current step :  1943
total : 5000  current step :  1944
total : 5000  current step :  1945
total : 5000  current step :  1946
total : 5000  current step :  1947
total : 5000  current step :  1948
total : 5000  current step :  1949
total : 5000  current step :  1950
total : 5000  current step :  1951
total : 5000  current step :  1952
total : 5000  current step :  1953
total : 5000  current step :  1954
total : 5000  current step :  1955
total : 5000  current step :  1956
total : 5000  current step :  1957
total : 5000  current step :  1958
total : 5000  current step :  1959
total : 5000  current step :  1960
total : 5000  current step :  1961
total : 5000  current step :  1962
total : 5000  current step :  1963
total : 5000  current step :  1964
total : 5000  current step :  1965
total : 5000  current step :  1966
total : 5000  current step :  1967
total : 5000  current step :  1968
total : 5000  current step :  1969
total : 5000  current step :  1970
total : 5000  current step :  1971
total : 5000  current step :  1972
total : 5000  current step :  1973
total : 5000  current step :  1974
total : 5000  current step :  1975
total : 5000  current step :  1976
total : 5000  current step :  1977
total : 5000  current step :  1978
total : 5000  current step :  1979
total : 5000  current step :  1980
total : 5000  current step :  1981
total : 5000  current step :  1982
total : 5000  current step :  1983
total : 5000  current step :  1984
total : 5000  current step :  1985
total : 5000  current step :  1986
total : 5000  current step :  1987
total : 5000  current step :  1988
total : 5000  current step :  1989
total : 5000  current step :  1990
total : 5000  current step :  1991
total : 5000  current step :  1992
total : 5000  current step :  1993
total : 5000  current step :  1994
total : 5000  current step :  1995
total : 5000  current step :  1996
total : 5000  current step :  1997
total : 5000  current step :  1998
total : 5000  current step :  1999
total : 5000  current step :  2000
total : 5000  current step :  2001
total : 5000  current step :  2002
total : 5000  current step :  2003
total : 5000  current step :  2004
total : 5000  current step :  2005
total : 5000  current step :  2006
total : 5000  current step :  2007
total : 5000  current step :  2008
total : 5000  current step :  2009
total : 5000  current step :  2010
total : 5000  current step :  2011
total : 5000  current step :  2012
total : 5000  current step :  2013
total : 5000  current step :  2014
total : 5000  current step :  2015
total : 5000  current step :  2016
total : 5000  current step :  2017
total : 5000  current step :  2018
total : 5000  current step :  2019
total : 5000  current step :  2020
total : 5000  current step :  2021
total : 5000  current step :  2022
total : 5000  current step :  2023
total : 5000  current step :  2024
total : 5000  current step :  2025
total : 5000  current step :  2026
total : 5000  current step :  2027
total : 5000  current step :  2028
total : 5000  current step :  2029
total : 5000  current step :  2030
total : 5000  current step :  2031
total : 5000  current step :  2032
total : 5000  current step :  2033
total : 5000  current step :  2034
total : 5000  current step :  2035
total : 5000  current step :  2036
total : 5000  current step :  2037
total : 5000  current step :  2038
total : 5000  current step :  2039
total : 5000  current step :  2040
total : 5000  current step :  2041
total : 5000  current step :  2042
total : 5000  current step :  2043
total : 5000  current step :  2044
total : 5000  current step :  2045
total : 5000  current step :  2046
total : 5000  current step :  2047
total : 5000  current step :  2048
total : 5000  current step :  2049
total : 5000  current step :  2050
total : 5000  current step :  2051
total : 5000  current step :  2052
total : 5000  current step :  2053
total : 5000  current step :  2054
total : 5000  current step :  2055
total : 5000  current step :  2056
total : 5000  current step :  2057
total : 5000  current step :  2058
total : 5000  current step :  2059
total : 5000  current step :  2060
total : 5000  current step :  2061
total : 5000  current step :  2062
total : 5000  current step :  2063
total : 5000  current step :  2064
total : 5000  current step :  2065
total : 5000  current step :  2066
total : 5000  current step :  2067
total : 5000  current step :  2068
total : 5000  current step :  2069
total : 5000  current step :  2070
total : 5000  current step :  2071
total : 5000  current step :  2072
total : 5000  current step :  2073
total : 5000  current step :  2074
total : 5000  current step :  2075
total : 5000  current step :  2076
total : 5000  current step :  2077
total : 5000  current step :  2078
total : 5000  current step :  2079
total : 5000  current step :  2080
total : 5000  current step :  2081
total : 5000  current step :  2082
total : 5000  current step :  2083
total : 5000  current step :  2084
total : 5000  current step :  2085
total : 5000  current step :  2086
total : 5000  current step :  2087
total : 5000  current step :  2088
total : 5000  current step :  2089
total : 5000  current step :  2090
total : 5000  current step :  2091
total : 5000  current step :  2092
total : 5000  current step :  2093
total : 5000  current step :  2094
total : 5000  current step :  2095
total : 5000  current step :  2096
total : 5000  current step :  2097
total : 5000  current step :  2098
total : 5000  current step :  2099
total : 5000  current step :  2100
total : 5000  current step :  2101
total : 5000  current step :  2102
total : 5000  current step :  2103
total : 5000  current step :  2104
total : 5000  current step :  2105
total : 5000  current step :  2106
total : 5000  current step :  2107
total : 5000  current step :  2108
total : 5000  current step :  2109
total : 5000  current step :  2110
total : 5000  current step :  2111
total : 5000  current step :  2112
total : 5000  current step :  2113
total : 5000  current step :  2114
total : 5000  current step :  2115
total : 5000  current step :  2116
total : 5000  current step :  2117
total : 5000  current step :  2118
total : 5000  current step :  2119
total : 5000  current step :  2120
total : 5000  current step :  2121
total : 5000  current step :  2122
total : 5000  current step :  2123
total : 5000  current step :  2124
total : 5000  current step :  2125
total : 5000  current step :  2126
total : 5000  current step :  2127
total : 5000  current step :  2128
total : 5000  current step :  2129
total : 5000  current step :  2130
total : 5000  current step :  2131
total : 5000  current step :  2132
total : 5000  current step :  2133
total : 5000  current step :  2134
total : 5000  current step :  2135
total : 5000  current step :  2136
total : 5000  current step :  2137
total : 5000  current step :  2138
total : 5000  current step :  2139
total : 5000  current step :  2140
total : 5000  current step :  2141
total : 5000  current step :  2142
total : 5000  current step :  2143
total : 5000  current step :  2144
total : 5000  current step :  2145
total : 5000  current step :  2146
total : 5000  current step :  2147
total : 5000  current step :  2148
total : 5000  current step :  2149
total : 5000  current step :  2150
total : 5000  current step :  2151
total : 5000  current step :  2152
total : 5000  current step :  2153
total : 5000  current step :  2154
total : 5000  current step :  2155
total : 5000  current step :  2156
total : 5000  current step :  2157
total : 5000  current step :  2158
total : 5000  current step :  2159
total : 5000  current step :  2160
total : 5000  current step :  2161
total : 5000  current step :  2162
total : 5000  current step :  2163
total : 5000  current step :  2164
total : 5000  current step :  2165
total : 5000  current step :  2166
total : 5000  current step :  2167
total : 5000  current step :  2168
total : 5000  current step :  2169
total : 5000  current step :  2170
total : 5000  current step :  2171
total : 5000  current step :  2172
total : 5000  current step :  2173
total : 5000  current step :  2174
total : 5000  current step :  2175
total : 5000  current step :  2176
total : 5000  current step :  2177
total : 5000  current step :  2178
total : 5000  current step :  2179
total : 5000  current step :  2180
total : 5000  current step :  2181
total : 5000  current step :  2182
total : 5000  current step :  2183
total : 5000  current step :  2184
total : 5000  current step :  2185
total : 5000  current step :  2186
total : 5000  current step :  2187
total : 5000  current step :  2188
total : 5000  current step :  2189
total : 5000  current step :  2190
total : 5000  current step :  2191
total : 5000  current step :  2192
total : 5000  current step :  2193
total : 5000  current step :  2194
total : 5000  current step :  2195
total : 5000  current step :  2196
total : 5000  current step :  2197
total : 5000  current step :  2198
total : 5000  current step :  2199
total : 5000  current step :  2200
total : 5000  current step :  2201
total : 5000  current step :  2202
total : 5000  current step :  2203
total : 5000  current step :  2204
total : 5000  current step :  2205
total : 5000  current step :  2206
total : 5000  current step :  2207
total : 5000  current step :  2208
total : 5000  current step :  2209
total : 5000  current step :  2210
total : 5000  current step :  2211
total : 5000  current step :  2212
total : 5000  current step :  2213
total : 5000  current step :  2214
total : 5000  current step :  2215
total : 5000  current step :  2216
total : 5000  current step :  2217
total : 5000  current step :  2218
total : 5000  current step :  2219
total : 5000  current step :  2220
total : 5000  current step :  2221
total : 5000  current step :  2222
total : 5000  current step :  2223
total : 5000  current step :  2224
total : 5000  current step :  2225
total : 5000  current step :  2226
total : 5000  current step :  2227
total : 5000  current step :  2228
total : 5000  current step :  2229
total : 5000  current step :  2230
total : 5000  current step :  2231
total : 5000  current step :  2232
total : 5000  current step :  2233
total : 5000  current step :  2234
total : 5000  current step :  2235
total : 5000  current step :  2236
total : 5000  current step :  2237
total : 5000  current step :  2238
total : 5000  current step :  2239
total : 5000  current step :  2240
total : 5000  current step :  2241
total : 5000  current step :  2242
total : 5000  current step :  2243
total : 5000  current step :  2244
total : 5000  current step :  2245
total : 5000  current step :  2246
total : 5000  current step :  2247
total : 5000  current step :  2248
total : 5000  current step :  2249
total : 5000  current step :  2250
total : 5000  current step :  2251
total : 5000  current step :  2252
total : 5000  current step :  2253
total : 5000  current step :  2254
total : 5000  current step :  2255
total : 5000  current step :  2256
total : 5000  current step :  2257
total : 5000  current step :  2258
total : 5000  current step :  2259
total : 5000  current step :  2260
total : 5000  current step :  2261
total : 5000  current step :  2262
total : 5000  current step :  2263
total : 5000  current step :  2264
total : 5000  current step :  2265
total : 5000  current step :  2266
total : 5000  current step :  2267
total : 5000  current step :  2268
total : 5000  current step :  2269
total : 5000  current step :  2270
total : 5000  current step :  2271
total : 5000  current step :  2272
total : 5000  current step :  2273
total : 5000  current step :  2274
total : 5000  current step :  2275
total : 5000  current step :  2276
total : 5000  current step :  2277
total : 5000  current step :  2278
total : 5000  current step :  2279
total : 5000  current step :  2280
total : 5000  current step :  2281
total : 5000  current step :  2282
total : 5000  current step :  2283
total : 5000  current step :  2284
total : 5000  current step :  2285
total : 5000  current step :  2286
total : 5000  current step :  2287
total : 5000  current step :  2288
total : 5000  current step :  2289
total : 5000  current step :  2290
total : 5000  current step :  2291
total : 5000  current step :  2292
total : 5000  current step :  2293
total : 5000  current step :  2294
total : 5000  current step :  2295
total : 5000  current step :  2296
total : 5000  current step :  2297
total : 5000  current step :  2298
total : 5000  current step :  2299
total : 5000  current step :  2300
total : 5000  current step :  2301
total : 5000  current step :  2302
total : 5000  current step :  2303
total : 5000  current step :  2304
total : 5000  current step :  2305
total : 5000  current step :  2306
total : 5000  current step :  2307
total : 5000  current step :  2308
total : 5000  current step :  2309
total : 5000  current step :  2310
total : 5000  current step :  2311
total : 5000  current step :  2312
total : 5000  current step :  2313
total : 5000  current step :  2314
total : 5000  current step :  2315
total : 5000  current step :  2316
total : 5000  current step :  2317
total : 5000  current step :  2318
total : 5000  current step :  2319
total : 5000  current step :  2320
total : 5000  current step :  2321
total : 5000  current step :  2322
total : 5000  current step :  2323
total : 5000  current step :  2324
total : 5000  current step :  2325
total : 5000  current step :  2326
total : 5000  current step :  2327
total : 5000  current step :  2328
total : 5000  current step :  2329
total : 5000  current step :  2330
total : 5000  current step :  2331
total : 5000  current step :  2332
total : 5000  current step :  2333
total : 5000  current step :  2334
total : 5000  current step :  2335
total : 5000  current step :  2336
total : 5000  current step :  2337
total : 5000  current step :  2338
total : 5000  current step :  2339
total : 5000  current step :  2340
total : 5000  current step :  2341
total : 5000  current step :  2342
total : 5000  current step :  2343
total : 5000  current step :  2344
total : 5000  current step :  2345
total : 5000  current step :  2346
total : 5000  current step :  2347
total : 5000  current step :  2348
total : 5000  current step :  2349
total : 5000  current step :  2350
total : 5000  current step :  2351
total : 5000  current step :  2352
total : 5000  current step :  2353
total : 5000  current step :  2354
total : 5000  current step :  2355
total : 5000  current step :  2356
total : 5000  current step :  2357
total : 5000  current step :  2358
total : 5000  current step :  2359
total : 5000  current step :  2360
total : 5000  current step :  2361
total : 5000  current step :  2362
total : 5000  current step :  2363
total : 5000  current step :  2364
total : 5000  current step :  2365
total : 5000  current step :  2366
total : 5000  current step :  2367
total : 5000  current step :  2368
total : 5000  current step :  2369
total : 5000  current step :  2370
total : 5000  current step :  2371
total : 5000  current step :  2372
total : 5000  current step :  2373
total : 5000  current step :  2374
total : 5000  current step :  2375
total : 5000  current step :  2376
total : 5000  current step :  2377
total : 5000  current step :  2378
total : 5000  current step :  2379
total : 5000  current step :  2380
total : 5000  current step :  2381
total : 5000  current step :  2382
total : 5000  current step :  2383
total : 5000  current step :  2384
total : 5000  current step :  2385
total : 5000  current step :  2386
total : 5000  current step :  2387
total : 5000  current step :  2388
total : 5000  current step :  2389
total : 5000  current step :  2390
total : 5000  current step :  2391
total : 5000  current step :  2392
total : 5000  current step :  2393
total : 5000  current step :  2394
total : 5000  current step :  2395
total : 5000  current step :  2396
total : 5000  current step :  2397
total : 5000  current step :  2398
total : 5000  current step :  2399
total : 5000  current step :  2400
total : 5000  current step :  2401
total : 5000  current step :  2402
total : 5000  current step :  2403
total : 5000  current step :  2404
total : 5000  current step :  2405
total : 5000  current step :  2406
total : 5000  current step :  2407
total : 5000  current step :  2408
total : 5000  current step :  2409
total : 5000  current step :  2410
total : 5000  current step :  2411
total : 5000  current step :  2412
total : 5000  current step :  2413
total : 5000  current step :  2414
total : 5000  current step :  2415
total : 5000  current step :  2416
total : 5000  current step :  2417
total : 5000  current step :  2418
total : 5000  current step :  2419
total : 5000  current step :  2420
total : 5000  current step :  2421
total : 5000  current step :  2422
total : 5000  current step :  2423
total : 5000  current step :  2424
total : 5000  current step :  2425
total : 5000  current step :  2426
total : 5000  current step :  2427
total : 5000  current step :  2428
total : 5000  current step :  2429
total : 5000  current step :  2430
total : 5000  current step :  2431
total : 5000  current step :  2432
total : 5000  current step :  2433
total : 5000  current step :  2434
total : 5000  current step :  2435
total : 5000  current step :  2436
total : 5000  current step :  2437
total : 5000  current step :  2438
total : 5000  current step :  2439
total : 5000  current step :  2440
total : 5000  current step :  2441
total : 5000  current step :  2442
total : 5000  current step :  2443
total : 5000  current step :  2444
total : 5000  current step :  2445
total : 5000  current step :  2446
total : 5000  current step :  2447
total : 5000  current step :  2448
total : 5000  current step :  2449
total : 5000  current step :  2450
total : 5000  current step :  2451
total : 5000  current step :  2452
total : 5000  current step :  2453
total : 5000  current step :  2454
total : 5000  current step :  2455
total : 5000  current step :  2456
total : 5000  current step :  2457
total : 5000  current step :  2458
total : 5000  current step :  2459
total : 5000  current step :  2460
total : 5000  current step :  2461
total : 5000  current step :  2462
total : 5000  current step :  2463
total : 5000  current step :  2464
total : 5000  current step :  2465
total : 5000  current step :  2466
total : 5000  current step :  2467
total : 5000  current step :  2468
total : 5000  current step :  2469
total : 5000  current step :  2470
total : 5000  current step :  2471
total : 5000  current step :  2472
total : 5000  current step :  2473
total : 5000  current step :  2474
total : 5000  current step :  2475
total : 5000  current step :  2476
total : 5000  current step :  2477
total : 5000  current step :  2478
total : 5000  current step :  2479
total : 5000  current step :  2480
total : 5000  current step :  2481
total : 5000  current step :  2482
total : 5000  current step :  2483
total : 5000  current step :  2484
total : 5000  current step :  2485
total : 5000  current step :  2486
total : 5000  current step :  2487
total : 5000  current step :  2488
total : 5000  current step :  2489
total : 5000  current step :  2490
total : 5000  current step :  2491
total : 5000  current step :  2492
total : 5000  current step :  2493
total : 5000  current step :  2494
total : 5000  current step :  2495
total : 5000  current step :  2496
total : 5000  current step :  2497
total : 5000  current step :  2498
total : 5000  current step :  2499
total : 5000  current step :  2500
total : 5000  current step :  2501
total : 5000  current step :  2502
total : 5000  current step :  2503
total : 5000  current step :  2504
total : 5000  current step :  2505
total : 5000  current step :  2506
total : 5000  current step :  2507
total : 5000  current step :  2508
total : 5000  current step :  2509
total : 5000  current step :  2510
total : 5000  current step :  2511
total : 5000  current step :  2512
total : 5000  current step :  2513
total : 5000  current step :  2514
total : 5000  current step :  2515
total : 5000  current step :  2516
total : 5000  current step :  2517
total : 5000  current step :  2518
total : 5000  current step :  2519
total : 5000  current step :  2520
total : 5000  current step :  2521
total : 5000  current step :  2522
total : 5000  current step :  2523
total : 5000  current step :  2524
total : 5000  current step :  2525
total : 5000  current step :  2526
total : 5000  current step :  2527
total : 5000  current step :  2528
total : 5000  current step :  2529
total : 5000  current step :  2530
total : 5000  current step :  2531
total : 5000  current step :  2532
total : 5000  current step :  2533
total : 5000  current step :  2534
total : 5000  current step :  2535
total : 5000  current step :  2536
total : 5000  current step :  2537
total : 5000  current step :  2538
total : 5000  current step :  2539
total : 5000  current step :  2540
total : 5000  current step :  2541
total : 5000  current step :  2542
total : 5000  current step :  2543
total : 5000  current step :  2544
total : 5000  current step :  2545
total : 5000  current step :  2546
total : 5000  current step :  2547
total : 5000  current step :  2548
total : 5000  current step :  2549
total : 5000  current step :  2550
total : 5000  current step :  2551
total : 5000  current step :  2552
total : 5000  current step :  2553
total : 5000  current step :  2554
total : 5000  current step :  2555
total : 5000  current step :  2556
total : 5000  current step :  2557
total : 5000  current step :  2558
total : 5000  current step :  2559
total : 5000  current step :  2560
total : 5000  current step :  2561
total : 5000  current step :  2562
total : 5000  current step :  2563
total : 5000  current step :  2564
total : 5000  current step :  2565
total : 5000  current step :  2566
total : 5000  current step :  2567
total : 5000  current step :  2568
total : 5000  current step :  2569
total : 5000  current step :  2570
total : 5000  current step :  2571
total : 5000  current step :  2572
total : 5000  current step :  2573
total : 5000  current step :  2574
total : 5000  current step :  2575
total : 5000  current step :  2576
total : 5000  current step :  2577
total : 5000  current step :  2578
total : 5000  current step :  2579
total : 5000  current step :  2580
total : 5000  current step :  2581
total : 5000  current step :  2582
total : 5000  current step :  2583
total : 5000  current step :  2584
total : 5000  current step :  2585
total : 5000  current step :  2586
total : 5000  current step :  2587
total : 5000  current step :  2588
total : 5000  current step :  2589
total : 5000  current step :  2590
total : 5000  current step :  2591
total : 5000  current step :  2592
total : 5000  current step :  2593
total : 5000  current step :  2594
total : 5000  current step :  2595
total : 5000  current step :  2596
total : 5000  current step :  2597
total : 5000  current step :  2598
total : 5000  current step :  2599
total : 5000  current step :  2600
total : 5000  current step :  2601
total : 5000  current step :  2602
total : 5000  current step :  2603
total : 5000  current step :  2604
total : 5000  current step :  2605
total : 5000  current step :  2606
total : 5000  current step :  2607
total : 5000  current step :  2608
total : 5000  current step :  2609
total : 5000  current step :  2610
total : 5000  current step :  2611
total : 5000  current step :  2612
total : 5000  current step :  2613
total : 5000  current step :  2614
total : 5000  current step :  2615
total : 5000  current step :  2616
total : 5000  current step :  2617
total : 5000  current step :  2618
total : 5000  current step :  2619
total : 5000  current step :  2620
total : 5000  current step :  2621
total : 5000  current step :  2622
total : 5000  current step :  2623
total : 5000  current step :  2624
total : 5000  current step :  2625
total : 5000  current step :  2626
total : 5000  current step :  2627
total : 5000  current step :  2628
total : 5000  current step :  2629
total : 5000  current step :  2630
total : 5000  current step :  2631
total : 5000  current step :  2632
total : 5000  current step :  2633
total : 5000  current step :  2634
total : 5000  current step :  2635
total : 5000  current step :  2636
total : 5000  current step :  2637
total : 5000  current step :  2638
total : 5000  current step :  2639
total : 5000  current step :  2640
total : 5000  current step :  2641
total : 5000  current step :  2642
total : 5000  current step :  2643
total : 5000  current step :  2644
total : 5000  current step :  2645
total : 5000  current step :  2646
total : 5000  current step :  2647
total : 5000  current step :  2648
total : 5000  current step :  2649
total : 5000  current step :  2650
total : 5000  current step :  2651
total : 5000  current step :  2652
total : 5000  current step :  2653
total : 5000  current step :  2654
total : 5000  current step :  2655
total : 5000  current step :  2656
total : 5000  current step :  2657
total : 5000  current step :  2658
total : 5000  current step :  2659
total : 5000  current step :  2660
total : 5000  current step :  2661
total : 5000  current step :  2662
total : 5000  current step :  2663
total : 5000  current step :  2664
total : 5000  current step :  2665
total : 5000  current step :  2666
total : 5000  current step :  2667
total : 5000  current step :  2668
total : 5000  current step :  2669
total : 5000  current step :  2670
total : 5000  current step :  2671
total : 5000  current step :  2672
total : 5000  current step :  2673
total : 5000  current step :  2674
total : 5000  current step :  2675
total : 5000  current step :  2676
total : 5000  current step :  2677
total : 5000  current step :  2678
total : 5000  current step :  2679
total : 5000  current step :  2680
total : 5000  current step :  2681
total : 5000  current step :  2682
total : 5000  current step :  2683
total : 5000  current step :  2684
total : 5000  current step :  2685
total : 5000  current step :  2686
total : 5000  current step :  2687
total : 5000  current step :  2688
total : 5000  current step :  2689
total : 5000  current step :  2690
total : 5000  current step :  2691
total : 5000  current step :  2692
total : 5000  current step :  2693
total : 5000  current step :  2694
total : 5000  current step :  2695
total : 5000  current step :  2696
total : 5000  current step :  2697
total : 5000  current step :  2698
total : 5000  current step :  2699
total : 5000  current step :  2700
total : 5000  current step :  2701
total : 5000  current step :  2702
total : 5000  current step :  2703
total : 5000  current step :  2704
total : 5000  current step :  2705
total : 5000  current step :  2706
total : 5000  current step :  2707
total : 5000  current step :  2708
total : 5000  current step :  2709
total : 5000  current step :  2710
total : 5000  current step :  2711
total : 5000  current step :  2712
total : 5000  current step :  2713
total : 5000  current step :  2714
total : 5000  current step :  2715
total : 5000  current step :  2716
total : 5000  current step :  2717
total : 5000  current step :  2718
total : 5000  current step :  2719
total : 5000  current step :  2720
total : 5000  current step :  2721
total : 5000  current step :  2722
total : 5000  current step :  2723
total : 5000  current step :  2724
total : 5000  current step :  2725
total : 5000  current step :  2726
total : 5000  current step :  2727
total : 5000  current step :  2728
total : 5000  current step :  2729
total : 5000  current step :  2730
total : 5000  current step :  2731
total : 5000  current step :  2732
total : 5000  current step :  2733
total : 5000  current step :  2734
total : 5000  current step :  2735
total : 5000  current step :  2736
total : 5000  current step :  2737
total : 5000  current step :  2738
total : 5000  current step :  2739
total : 5000  current step :  2740
total : 5000  current step :  2741
total : 5000  current step :  2742
total : 5000  current step :  2743
total : 5000  current step :  2744
total : 5000  current step :  2745
total : 5000  current step :  2746
total : 5000  current step :  2747
total : 5000  current step :  2748
total : 5000  current step :  2749
total : 5000  current step :  2750
total : 5000  current step :  2751
total : 5000  current step :  2752
total : 5000  current step :  2753
total : 5000  current step :  2754
total : 5000  current step :  2755
total : 5000  current step :  2756
total : 5000  current step :  2757
total : 5000  current step :  2758
total : 5000  current step :  2759
total : 5000  current step :  2760
total : 5000  current step :  2761
total : 5000  current step :  2762
total : 5000  current step :  2763
total : 5000  current step :  2764
total : 5000  current step :  2765
total : 5000  current step :  2766
total : 5000  current step :  2767
total : 5000  current step :  2768
total : 5000  current step :  2769
total : 5000  current step :  2770
total : 5000  current step :  2771
total : 5000  current step :  2772
total : 5000  current step :  2773
total : 5000  current step :  2774
total : 5000  current step :  2775
total : 5000  current step :  2776
total : 5000  current step :  2777
total : 5000  current step :  2778
total : 5000  current step :  2779
total : 5000  current step :  2780
total : 5000  current step :  2781
total : 5000  current step :  2782
total : 5000  current step :  2783
total : 5000  current step :  2784
total : 5000  current step :  2785
total : 5000  current step :  2786
total : 5000  current step :  2787
total : 5000  current step :  2788
total : 5000  current step :  2789
total : 5000  current step :  2790
total : 5000  current step :  2791
total : 5000  current step :  2792
total : 5000  current step :  2793
total : 5000  current step :  2794
total : 5000  current step :  2795
total : 5000  current step :  2796
total : 5000  current step :  2797
total : 5000  current step :  2798
total : 5000  current step :  2799
total : 5000  current step :  2800
total : 5000  current step :  2801
total : 5000  current step :  2802
total : 5000  current step :  2803
total : 5000  current step :  2804
total : 5000  current step :  2805
total : 5000  current step :  2806
total : 5000  current step :  2807
total : 5000  current step :  2808
total : 5000  current step :  2809
total : 5000  current step :  2810
total : 5000  current step :  2811
total : 5000  current step :  2812
total : 5000  current step :  2813
total : 5000  current step :  2814
total : 5000  current step :  2815
total : 5000  current step :  2816
total : 5000  current step :  2817
total : 5000  current step :  2818
total : 5000  current step :  2819
total : 5000  current step :  2820
total : 5000  current step :  2821
total : 5000  current step :  2822
total : 5000  current step :  2823
total : 5000  current step :  2824
total : 5000  current step :  2825
total : 5000  current step :  2826
total : 5000  current step :  2827
total : 5000  current step :  2828
total : 5000  current step :  2829
total : 5000  current step :  2830
total : 5000  current step :  2831
total : 5000  current step :  2832
total : 5000  current step :  2833
total : 5000  current step :  2834
total : 5000  current step :  2835
total : 5000  current step :  2836
total : 5000  current step :  2837
total : 5000  current step :  2838
total : 5000  current step :  2839
total : 5000  current step :  2840
total : 5000  current step :  2841
total : 5000  current step :  2842
total : 5000  current step :  2843
total : 5000  current step :  2844
total : 5000  current step :  2845
total : 5000  current step :  2846
total : 5000  current step :  2847
total : 5000  current step :  2848
total : 5000  current step :  2849
total : 5000  current step :  2850
total : 5000  current step :  2851
total : 5000  current step :  2852
total : 5000  current step :  2853
total : 5000  current step :  2854
total : 5000  current step :  2855
total : 5000  current step :  2856
total : 5000  current step :  2857
total : 5000  current step :  2858
total : 5000  current step :  2859
total : 5000  current step :  2860
total : 5000  current step :  2861
total : 5000  current step :  2862
total : 5000  current step :  2863
total : 5000  current step :  2864
total : 5000  current step :  2865
total : 5000  current step :  2866
total : 5000  current step :  2867
total : 5000  current step :  2868
total : 5000  current step :  2869
total : 5000  current step :  2870
total : 5000  current step :  2871
total : 5000  current step :  2872
total : 5000  current step :  2873
total : 5000  current step :  2874
total : 5000  current step :  2875
total : 5000  current step :  2876
total : 5000  current step :  2877
total : 5000  current step :  2878
total : 5000  current step :  2879
total : 5000  current step :  2880
total : 5000  current step :  2881
total : 5000  current step :  2882
total : 5000  current step :  2883
total : 5000  current step :  2884
total : 5000  current step :  2885
total : 5000  current step :  2886
total : 5000  current step :  2887
total : 5000  current step :  2888
total : 5000  current step :  2889
total : 5000  current step :  2890
total : 5000  current step :  2891
total : 5000  current step :  2892
total : 5000  current step :  2893
total : 5000  current step :  2894
total : 5000  current step :  2895
total : 5000  current step :  2896
total : 5000  current step :  2897
total : 5000  current step :  2898
total : 5000  current step :  2899
total : 5000  current step :  2900
total : 5000  current step :  2901
total : 5000  current step :  2902
total : 5000  current step :  2903
total : 5000  current step :  2904
total : 5000  current step :  2905
total : 5000  current step :  2906
total : 5000  current step :  2907
total : 5000  current step :  2908
total : 5000  current step :  2909
total : 5000  current step :  2910
total : 5000  current step :  2911
total : 5000  current step :  2912
total : 5000  current step :  2913
total : 5000  current step :  2914
total : 5000  current step :  2915
total : 5000  current step :  2916
total : 5000  current step :  2917
total : 5000  current step :  2918
total : 5000  current step :  2919
total : 5000  current step :  2920
total : 5000  current step :  2921
total : 5000  current step :  2922
total : 5000  current step :  2923
total : 5000  current step :  2924
total : 5000  current step :  2925
total : 5000  current step :  2926
total : 5000  current step :  2927
total : 5000  current step :  2928
total : 5000  current step :  2929
total : 5000  current step :  2930
total : 5000  current step :  2931
total : 5000  current step :  2932
total : 5000  current step :  2933
total : 5000  current step :  2934
total : 5000  current step :  2935
total : 5000  current step :  2936
total : 5000  current step :  2937
total : 5000  current step :  2938
total : 5000  current step :  2939
total : 5000  current step :  2940
total : 5000  current step :  2941
total : 5000  current step :  2942
total : 5000  current step :  2943
total : 5000  current step :  2944
total : 5000  current step :  2945
total : 5000  current step :  2946
total : 5000  current step :  2947
total : 5000  current step :  2948
total : 5000  current step :  2949
total : 5000  current step :  2950
total : 5000  current step :  2951
total : 5000  current step :  2952
total : 5000  current step :  2953
total : 5000  current step :  2954
total : 5000  current step :  2955
total : 5000  current step :  2956
total : 5000  current step :  2957
total : 5000  current step :  2958
total : 5000  current step :  2959
total : 5000  current step :  2960
total : 5000  current step :  2961
total : 5000  current step :  2962
total : 5000  current step :  2963
total : 5000  current step :  2964
total : 5000  current step :  2965
total : 5000  current step :  2966
total : 5000  current step :  2967
total : 5000  current step :  2968
total : 5000  current step :  2969
total : 5000  current step :  2970
total : 5000  current step :  2971
total : 5000  current step :  2972
total : 5000  current step :  2973
total : 5000  current step :  2974
total : 5000  current step :  2975
total : 5000  current step :  2976
total : 5000  current step :  2977
total : 5000  current step :  2978
total : 5000  current step :  2979
total : 5000  current step :  2980
total : 5000  current step :  2981
total : 5000  current step :  2982
total : 5000  current step :  2983
total : 5000  current step :  2984
total : 5000  current step :  2985
total : 5000  current step :  2986
total : 5000  current step :  2987
total : 5000  current step :  2988
total : 5000  current step :  2989
total : 5000  current step :  2990
total : 5000  current step :  2991
total : 5000  current step :  2992
total : 5000  current step :  2993
total : 5000  current step :  2994
total : 5000  current step :  2995
total : 5000  current step :  2996
total : 5000  current step :  2997
total : 5000  current step :  2998
total : 5000  current step :  2999
total : 5000  current step :  3000
total : 5000  current step :  3001
total : 5000  current step :  3002
total : 5000  current step :  3003
total : 5000  current step :  3004
total : 5000  current step :  3005
total : 5000  current step :  3006
total : 5000  current step :  3007
total : 5000  current step :  3008
total : 5000  current step :  3009
total : 5000  current step :  3010
total : 5000  current step :  3011
total : 5000  current step :  3012
total : 5000  current step :  3013
total : 5000  current step :  3014
total : 5000  current step :  3015
total : 5000  current step :  3016
total : 5000  current step :  3017
total : 5000  current step :  3018
total : 5000  current step :  3019
total : 5000  current step :  3020
total : 5000  current step :  3021
total : 5000  current step :  3022
total : 5000  current step :  3023
total : 5000  current step :  3024
total : 5000  current step :  3025
total : 5000  current step :  3026
total : 5000  current step :  3027
total : 5000  current step :  3028
total : 5000  current step :  3029
total : 5000  current step :  3030
total : 5000  current step :  3031
total : 5000  current step :  3032
total : 5000  current step :  3033
total : 5000  current step :  3034
total : 5000  current step :  3035
total : 5000  current step :  3036
total : 5000  current step :  3037
total : 5000  current step :  3038
total : 5000  current step :  3039
total : 5000  current step :  3040
total : 5000  current step :  3041
total : 5000  current step :  3042
total : 5000  current step :  3043
total : 5000  current step :  3044
total : 5000  current step :  3045
total : 5000  current step :  3046
total : 5000  current step :  3047
total : 5000  current step :  3048
total : 5000  current step :  3049
total : 5000  current step :  3050
total : 5000  current step :  3051
total : 5000  current step :  3052
total : 5000  current step :  3053
total : 5000  current step :  3054
total : 5000  current step :  3055
total : 5000  current step :  3056
total : 5000  current step :  3057
total : 5000  current step :  3058
total : 5000  current step :  3059
total : 5000  current step :  3060
total : 5000  current step :  3061
total : 5000  current step :  3062
total : 5000  current step :  3063
total : 5000  current step :  3064
total : 5000  current step :  3065
total : 5000  current step :  3066
total : 5000  current step :  3067
total : 5000  current step :  3068
total : 5000  current step :  3069
total : 5000  current step :  3070
total : 5000  current step :  3071
total : 5000  current step :  3072
total : 5000  current step :  3073
total : 5000  current step :  3074
total : 5000  current step :  3075
total : 5000  current step :  3076
total : 5000  current step :  3077
total : 5000  current step :  3078
total : 5000  current step :  3079
total : 5000  current step :  3080
total : 5000  current step :  3081
total : 5000  current step :  3082
total : 5000  current step :  3083
total : 5000  current step :  3084
total : 5000  current step :  3085
total : 5000  current step :  3086
total : 5000  current step :  3087
total : 5000  current step :  3088
total : 5000  current step :  3089
total : 5000  current step :  3090
total : 5000  current step :  3091
total : 5000  current step :  3092
total : 5000  current step :  3093
total : 5000  current step :  3094
total : 5000  current step :  3095
total : 5000  current step :  3096
total : 5000  current step :  3097
total : 5000  current step :  3098
total : 5000  current step :  3099
total : 5000  current step :  3100
total : 5000  current step :  3101
total : 5000  current step :  3102
total : 5000  current step :  3103
total : 5000  current step :  3104
total : 5000  current step :  3105
total : 5000  current step :  3106
total : 5000  current step :  3107
total : 5000  current step :  3108
total : 5000  current step :  3109
total : 5000  current step :  3110
total : 5000  current step :  3111
total : 5000  current step :  3112
total : 5000  current step :  3113
total : 5000  current step :  3114
total : 5000  current step :  3115
total : 5000  current step :  3116
total : 5000  current step :  3117
total : 5000  current step :  3118
total : 5000  current step :  3119
total : 5000  current step :  3120
total : 5000  current step :  3121
total : 5000  current step :  3122
total : 5000  current step :  3123
total : 5000  current step :  3124
total : 5000  current step :  3125
total : 5000  current step :  3126
total : 5000  current step :  3127
total : 5000  current step :  3128
total : 5000  current step :  3129
total : 5000  current step :  3130
total : 5000  current step :  3131
total : 5000  current step :  3132
total : 5000  current step :  3133
total : 5000  current step :  3134
total : 5000  current step :  3135
total : 5000  current step :  3136
total : 5000  current step :  3137
total : 5000  current step :  3138
total : 5000  current step :  3139
total : 5000  current step :  3140
total : 5000  current step :  3141
total : 5000  current step :  3142
total : 5000  current step :  3143
total : 5000  current step :  3144
total : 5000  current step :  3145
total : 5000  current step :  3146
total : 5000  current step :  3147
total : 5000  current step :  3148
total : 5000  current step :  3149
total : 5000  current step :  3150
total : 5000  current step :  3151
total : 5000  current step :  3152
total : 5000  current step :  3153
total : 5000  current step :  3154
total : 5000  current step :  3155
total : 5000  current step :  3156
total : 5000  current step :  3157
total : 5000  current step :  3158
total : 5000  current step :  3159
total : 5000  current step :  3160
total : 5000  current step :  3161
total : 5000  current step :  3162
total : 5000  current step :  3163
total : 5000  current step :  3164
total : 5000  current step :  3165
total : 5000  current step :  3166
total : 5000  current step :  3167
total : 5000  current step :  3168
total : 5000  current step :  3169
total : 5000  current step :  3170
total : 5000  current step :  3171
total : 5000  current step :  3172
total : 5000  current step :  3173
total : 5000  current step :  3174
total : 5000  current step :  3175
total : 5000  current step :  3176
total : 5000  current step :  3177
total : 5000  current step :  3178
total : 5000  current step :  3179
total : 5000  current step :  3180
total : 5000  current step :  3181
total : 5000  current step :  3182
total : 5000  current step :  3183
total : 5000  current step :  3184
total : 5000  current step :  3185
total : 5000  current step :  3186
total : 5000  current step :  3187
total : 5000  current step :  3188
total : 5000  current step :  3189
total : 5000  current step :  3190
total : 5000  current step :  3191
total : 5000  current step :  3192
total : 5000  current step :  3193
total : 5000  current step :  3194
total : 5000  current step :  3195
total : 5000  current step :  3196
total : 5000  current step :  3197
total : 5000  current step :  3198
total : 5000  current step :  3199
total : 5000  current step :  3200
total : 5000  current step :  3201
total : 5000  current step :  3202
total : 5000  current step :  3203
total : 5000  current step :  3204
total : 5000  current step :  3205
total : 5000  current step :  3206
total : 5000  current step :  3207
total : 5000  current step :  3208
total : 5000  current step :  3209
total : 5000  current step :  3210
total : 5000  current step :  3211
total : 5000  current step :  3212
total : 5000  current step :  3213
total : 5000  current step :  3214
total : 5000  current step :  3215
total : 5000  current step :  3216
total : 5000  current step :  3217
total : 5000  current step :  3218
total : 5000  current step :  3219
total : 5000  current step :  3220
total : 5000  current step :  3221
total : 5000  current step :  3222
total : 5000  current step :  3223
total : 5000  current step :  3224
total : 5000  current step :  3225
total : 5000  current step :  3226
total : 5000  current step :  3227
total : 5000  current step :  3228
total : 5000  current step :  3229
total : 5000  current step :  3230
total : 5000  current step :  3231
total : 5000  current step :  3232
total : 5000  current step :  3233
total : 5000  current step :  3234
total : 5000  current step :  3235
total : 5000  current step :  3236
total : 5000  current step :  3237
total : 5000  current step :  3238
total : 5000  current step :  3239
total : 5000  current step :  3240
total : 5000  current step :  3241
total : 5000  current step :  3242
total : 5000  current step :  3243
total : 5000  current step :  3244
total : 5000  current step :  3245
total : 5000  current step :  3246
total : 5000  current step :  3247
total : 5000  current step :  3248
total : 5000  current step :  3249
total : 5000  current step :  3250
total : 5000  current step :  3251
total : 5000  current step :  3252
total : 5000  current step :  3253
total : 5000  current step :  3254
total : 5000  current step :  3255
total : 5000  current step :  3256
total : 5000  current step :  3257
total : 5000  current step :  3258
total : 5000  current step :  3259
total : 5000  current step :  3260
total : 5000  current step :  3261
total : 5000  current step :  3262
total : 5000  current step :  3263
total : 5000  current step :  3264
total : 5000  current step :  3265
total : 5000  current step :  3266
total : 5000  current step :  3267
total : 5000  current step :  3268
total : 5000  current step :  3269
total : 5000  current step :  3270
total : 5000  current step :  3271
total : 5000  current step :  3272
total : 5000  current step :  3273
total : 5000  current step :  3274
total : 5000  current step :  3275
total : 5000  current step :  3276
total : 5000  current step :  3277
total : 5000  current step :  3278
total : 5000  current step :  3279
total : 5000  current step :  3280
total : 5000  current step :  3281
total : 5000  current step :  3282
total : 5000  current step :  3283
total : 5000  current step :  3284
total : 5000  current step :  3285
total : 5000  current step :  3286
total : 5000  current step :  3287
total : 5000  current step :  3288
total : 5000  current step :  3289
total : 5000  current step :  3290
total : 5000  current step :  3291
total : 5000  current step :  3292
total : 5000  current step :  3293
total : 5000  current step :  3294
total : 5000  current step :  3295
total : 5000  current step :  3296
total : 5000  current step :  3297
total : 5000  current step :  3298
total : 5000  current step :  3299
total : 5000  current step :  3300
total : 5000  current step :  3301
total : 5000  current step :  3302
total : 5000  current step :  3303
total : 5000  current step :  3304
total : 5000  current step :  3305
total : 5000  current step :  3306
total : 5000  current step :  3307
total : 5000  current step :  3308
total : 5000  current step :  3309
total : 5000  current step :  3310
total : 5000  current step :  3311
total : 5000  current step :  3312
total : 5000  current step :  3313
total : 5000  current step :  3314
total : 5000  current step :  3315
total : 5000  current step :  3316
total : 5000  current step :  3317
total : 5000  current step :  3318
total : 5000  current step :  3319
total : 5000  current step :  3320
total : 5000  current step :  3321
total : 5000  current step :  3322
total : 5000  current step :  3323
total : 5000  current step :  3324
total : 5000  current step :  3325
total : 5000  current step :  3326
total : 5000  current step :  3327
total : 5000  current step :  3328
total : 5000  current step :  3329
total : 5000  current step :  3330
total : 5000  current step :  3331
total : 5000  current step :  3332
total : 5000  current step :  3333
total : 5000  current step :  3334
total : 5000  current step :  3335
total : 5000  current step :  3336
total : 5000  current step :  3337
total : 5000  current step :  3338
total : 5000  current step :  3339
total : 5000  current step :  3340
total : 5000  current step :  3341
total : 5000  current step :  3342
total : 5000  current step :  3343
total : 5000  current step :  3344
total : 5000  current step :  3345
total : 5000  current step :  3346
total : 5000  current step :  3347
total : 5000  current step :  3348
total : 5000  current step :  3349
total : 5000  current step :  3350
total : 5000  current step :  3351
total : 5000  current step :  3352
total : 5000  current step :  3353
total : 5000  current step :  3354
total : 5000  current step :  3355
total : 5000  current step :  3356
total : 5000  current step :  3357
total : 5000  current step :  3358
total : 5000  current step :  3359
total : 5000  current step :  3360
total : 5000  current step :  3361
total : 5000  current step :  3362
total : 5000  current step :  3363
total : 5000  current step :  3364
total : 5000  current step :  3365
total : 5000  current step :  3366
total : 5000  current step :  3367
total : 5000  current step :  3368
total : 5000  current step :  3369
total : 5000  current step :  3370
total : 5000  current step :  3371
total : 5000  current step :  3372
total : 5000  current step :  3373
total : 5000  current step :  3374
total : 5000  current step :  3375
total : 5000  current step :  3376
total : 5000  current step :  3377
total : 5000  current step :  3378
total : 5000  current step :  3379
total : 5000  current step :  3380
total : 5000  current step :  3381
total : 5000  current step :  3382
total : 5000  current step :  3383
total : 5000  current step :  3384
total : 5000  current step :  3385
total : 5000  current step :  3386
total : 5000  current step :  3387
total : 5000  current step :  3388
total : 5000  current step :  3389
total : 5000  current step :  3390
total : 5000  current step :  3391
total : 5000  current step :  3392
total : 5000  current step :  3393
total : 5000  current step :  3394
total : 5000  current step :  3395
total : 5000  current step :  3396
total : 5000  current step :  3397
total : 5000  current step :  3398
total : 5000  current step :  3399
total : 5000  current step :  3400
total : 5000  current step :  3401
total : 5000  current step :  3402
total : 5000  current step :  3403
total : 5000  current step :  3404
total : 5000  current step :  3405
total : 5000  current step :  3406
total : 5000  current step :  3407
total : 5000  current step :  3408
total : 5000  current step :  3409
total : 5000  current step :  3410
total : 5000  current step :  3411
total : 5000  current step :  3412
total : 5000  current step :  3413
total : 5000  current step :  3414
total : 5000  current step :  3415
total : 5000  current step :  3416
total : 5000  current step :  3417
total : 5000  current step :  3418
total : 5000  current step :  3419
total : 5000  current step :  3420
total : 5000  current step :  3421
total : 5000  current step :  3422
total : 5000  current step :  3423
total : 5000  current step :  3424
total : 5000  current step :  3425
total : 5000  current step :  3426
total : 5000  current step :  3427
total : 5000  current step :  3428
total : 5000  current step :  3429
total : 5000  current step :  3430
total : 5000  current step :  3431
total : 5000  current step :  3432
total : 5000  current step :  3433
total : 5000  current step :  3434
total : 5000  current step :  3435
total : 5000  current step :  3436
total : 5000  current step :  3437
total : 5000  current step :  3438
total : 5000  current step :  3439
total : 5000  current step :  3440
total : 5000  current step :  3441
total : 5000  current step :  3442
total : 5000  current step :  3443
total : 5000  current step :  3444
total : 5000  current step :  3445
total : 5000  current step :  3446
total : 5000  current step :  3447
total : 5000  current step :  3448
total : 5000  current step :  3449
total : 5000  current step :  3450
total : 5000  current step :  3451
total : 5000  current step :  3452
total : 5000  current step :  3453
total : 5000  current step :  3454
total : 5000  current step :  3455
total : 5000  current step :  3456
total : 5000  current step :  3457
total : 5000  current step :  3458
total : 5000  current step :  3459
total : 5000  current step :  3460
total : 5000  current step :  3461
total : 5000  current step :  3462
total : 5000  current step :  3463
total : 5000  current step :  3464
total : 5000  current step :  3465
total : 5000  current step :  3466
total : 5000  current step :  3467
total : 5000  current step :  3468
total : 5000  current step :  3469
total : 5000  current step :  3470
total : 5000  current step :  3471
total : 5000  current step :  3472
total : 5000  current step :  3473
total : 5000  current step :  3474
total : 5000  current step :  3475
total : 5000  current step :  3476
total : 5000  current step :  3477
total : 5000  current step :  3478
total : 5000  current step :  3479
total : 5000  current step :  3480
total : 5000  current step :  3481
total : 5000  current step :  3482
total : 5000  current step :  3483
total : 5000  current step :  3484
total : 5000  current step :  3485
total : 5000  current step :  3486
total : 5000  current step :  3487
total : 5000  current step :  3488
total : 5000  current step :  3489
total : 5000  current step :  3490
total : 5000  current step :  3491
total : 5000  current step :  3492
total : 5000  current step :  3493
total : 5000  current step :  3494
total : 5000  current step :  3495
total : 5000  current step :  3496
total : 5000  current step :  3497
total : 5000  current step :  3498
total : 5000  current step :  3499
total : 5000  current step :  3500
total : 5000  current step :  3501
total : 5000  current step :  3502
total : 5000  current step :  3503
total : 5000  current step :  3504
total : 5000  current step :  3505
total : 5000  current step :  3506
total : 5000  current step :  3507
total : 5000  current step :  3508
total : 5000  current step :  3509
total : 5000  current step :  3510
total : 5000  current step :  3511
total : 5000  current step :  3512
total : 5000  current step :  3513
total : 5000  current step :  3514
total : 5000  current step :  3515
total : 5000  current step :  3516
total : 5000  current step :  3517
total : 5000  current step :  3518
total : 5000  current step :  3519
total : 5000  current step :  3520
total : 5000  current step :  3521
total : 5000  current step :  3522
total : 5000  current step :  3523
total : 5000  current step :  3524
total : 5000  current step :  3525
total : 5000  current step :  3526
total : 5000  current step :  3527
total : 5000  current step :  3528
total : 5000  current step :  3529
total : 5000  current step :  3530
total : 5000  current step :  3531
total : 5000  current step :  3532
total : 5000  current step :  3533
total : 5000  current step :  3534
total : 5000  current step :  3535
total : 5000  current step :  3536
total : 5000  current step :  3537
total : 5000  current step :  3538
total : 5000  current step :  3539
total : 5000  current step :  3540
total : 5000  current step :  3541
total : 5000  current step :  3542
total : 5000  current step :  3543
total : 5000  current step :  3544
total : 5000  current step :  3545
total : 5000  current step :  3546
total : 5000  current step :  3547
total : 5000  current step :  3548
total : 5000  current step :  3549
total : 5000  current step :  3550
total : 5000  current step :  3551
total : 5000  current step :  3552
total : 5000  current step :  3553
total : 5000  current step :  3554
total : 5000  current step :  3555
total : 5000  current step :  3556
total : 5000  current step :  3557
total : 5000  current step :  3558
total : 5000  current step :  3559
total : 5000  current step :  3560
total : 5000  current step :  3561
total : 5000  current step :  3562
total : 5000  current step :  3563
total : 5000  current step :  3564
total : 5000  current step :  3565
total : 5000  current step :  3566
total : 5000  current step :  3567
total : 5000  current step :  3568
total : 5000  current step :  3569
total : 5000  current step :  3570
total : 5000  current step :  3571
total : 5000  current step :  3572
total : 5000  current step :  3573
total : 5000  current step :  3574
total : 5000  current step :  3575
total : 5000  current step :  3576
total : 5000  current step :  3577
total : 5000  current step :  3578
total : 5000  current step :  3579
total : 5000  current step :  3580
total : 5000  current step :  3581
total : 5000  current step :  3582
total : 5000  current step :  3583
total : 5000  current step :  3584
total : 5000  current step :  3585
total : 5000  current step :  3586
total : 5000  current step :  3587
total : 5000  current step :  3588
total : 5000  current step :  3589
total : 5000  current step :  3590
total : 5000  current step :  3591
total : 5000  current step :  3592
total : 5000  current step :  3593
total : 5000  current step :  3594
total : 5000  current step :  3595
total : 5000  current step :  3596
total : 5000  current step :  3597
total : 5000  current step :  3598
total : 5000  current step :  3599
total : 5000  current step :  3600
total : 5000  current step :  3601
total : 5000  current step :  3602
total : 5000  current step :  3603
total : 5000  current step :  3604
total : 5000  current step :  3605
total : 5000  current step :  3606
total : 5000  current step :  3607
total : 5000  current step :  3608
total : 5000  current step :  3609
total : 5000  current step :  3610
total : 5000  current step :  3611
total : 5000  current step :  3612
total : 5000  current step :  3613
total : 5000  current step :  3614
total : 5000  current step :  3615
total : 5000  current step :  3616
total : 5000  current step :  3617
total : 5000  current step :  3618
total : 5000  current step :  3619
total : 5000  current step :  3620
total : 5000  current step :  3621
total : 5000  current step :  3622
total : 5000  current step :  3623
total : 5000  current step :  3624
total : 5000  current step :  3625
total : 5000  current step :  3626
total : 5000  current step :  3627
total : 5000  current step :  3628
total : 5000  current step :  3629
total : 5000  current step :  3630
total : 5000  current step :  3631
total : 5000  current step :  3632
total : 5000  current step :  3633
total : 5000  current step :  3634
total : 5000  current step :  3635
total : 5000  current step :  3636
total : 5000  current step :  3637
total : 5000  current step :  3638
total : 5000  current step :  3639
total : 5000  current step :  3640
total : 5000  current step :  3641
total : 5000  current step :  3642
total : 5000  current step :  3643
total : 5000  current step :  3644
total : 5000  current step :  3645
total : 5000  current step :  3646
total : 5000  current step :  3647
total : 5000  current step :  3648
total : 5000  current step :  3649
total : 5000  current step :  3650
total : 5000  current step :  3651
total : 5000  current step :  3652
total : 5000  current step :  3653
total : 5000  current step :  3654
total : 5000  current step :  3655
total : 5000  current step :  3656
total : 5000  current step :  3657
total : 5000  current step :  3658
total : 5000  current step :  3659
total : 5000  current step :  3660
total : 5000  current step :  3661
total : 5000  current step :  3662
total : 5000  current step :  3663
total : 5000  current step :  3664
total : 5000  current step :  3665
total : 5000  current step :  3666
total : 5000  current step :  3667
total : 5000  current step :  3668
total : 5000  current step :  3669
total : 5000  current step :  3670
total : 5000  current step :  3671
total : 5000  current step :  3672
total : 5000  current step :  3673
total : 5000  current step :  3674
total : 5000  current step :  3675
total : 5000  current step :  3676
total : 5000  current step :  3677
total : 5000  current step :  3678
total : 5000  current step :  3679
total : 5000  current step :  3680
total : 5000  current step :  3681
total : 5000  current step :  3682
total : 5000  current step :  3683
total : 5000  current step :  3684
total : 5000  current step :  3685
total : 5000  current step :  3686
total : 5000  current step :  3687
total : 5000  current step :  3688
total : 5000  current step :  3689
total : 5000  current step :  3690
total : 5000  current step :  3691
total : 5000  current step :  3692
total : 5000  current step :  3693
total : 5000  current step :  3694
total : 5000  current step :  3695
total : 5000  current step :  3696
total : 5000  current step :  3697
total : 5000  current step :  3698
total : 5000  current step :  3699
total : 5000  current step :  3700
total : 5000  current step :  3701
total : 5000  current step :  3702
total : 5000  current step :  3703
total : 5000  current step :  3704
total : 5000  current step :  3705
total : 5000  current step :  3706
total : 5000  current step :  3707
total : 5000  current step :  3708
total : 5000  current step :  3709
total : 5000  current step :  3710
total : 5000  current step :  3711
total : 5000  current step :  3712
total : 5000  current step :  3713
total : 5000  current step :  3714
total : 5000  current step :  3715
total : 5000  current step :  3716
total : 5000  current step :  3717
total : 5000  current step :  3718
total : 5000  current step :  3719
total : 5000  current step :  3720
total : 5000  current step :  3721
total : 5000  current step :  3722
total : 5000  current step :  3723
total : 5000  current step :  3724
total : 5000  current step :  3725
total : 5000  current step :  3726
total : 5000  current step :  3727
total : 5000  current step :  3728
total : 5000  current step :  3729
total : 5000  current step :  3730
total : 5000  current step :  3731
total : 5000  current step :  3732
total : 5000  current step :  3733
total : 5000  current step :  3734
total : 5000  current step :  3735
total : 5000  current step :  3736
total : 5000  current step :  3737
total : 5000  current step :  3738
total : 5000  current step :  3739
total : 5000  current step :  3740
total : 5000  current step :  3741
total : 5000  current step :  3742
total : 5000  current step :  3743
total : 5000  current step :  3744
total : 5000  current step :  3745
total : 5000  current step :  3746
total : 5000  current step :  3747
total : 5000  current step :  3748
total : 5000  current step :  3749
total : 5000  current step :  3750
total : 5000  current step :  3751
total : 5000  current step :  3752
total : 5000  current step :  3753
total : 5000  current step :  3754
total : 5000  current step :  3755
total : 5000  current step :  3756
total : 5000  current step :  3757
total : 5000  current step :  3758
total : 5000  current step :  3759
total : 5000  current step :  3760
total : 5000  current step :  3761
total : 5000  current step :  3762
total : 5000  current step :  3763
total : 5000  current step :  3764
total : 5000  current step :  3765
total : 5000  current step :  3766
total : 5000  current step :  3767
total : 5000  current step :  3768
total : 5000  current step :  3769
total : 5000  current step :  3770
total : 5000  current step :  3771
total : 5000  current step :  3772
total : 5000  current step :  3773
total : 5000  current step :  3774
total : 5000  current step :  3775
total : 5000  current step :  3776
total : 5000  current step :  3777
total : 5000  current step :  3778
total : 5000  current step :  3779
total : 5000  current step :  3780
total : 5000  current step :  3781
total : 5000  current step :  3782
total : 5000  current step :  3783
total : 5000  current step :  3784
total : 5000  current step :  3785
total : 5000  current step :  3786
total : 5000  current step :  3787
total : 5000  current step :  3788
total : 5000  current step :  3789
total : 5000  current step :  3790
total : 5000  current step :  3791
total : 5000  current step :  3792
total : 5000  current step :  3793
total : 5000  current step :  3794
total : 5000  current step :  3795
total : 5000  current step :  3796
total : 5000  current step :  3797
total : 5000  current step :  3798
total : 5000  current step :  3799
total : 5000  current step :  3800
total : 5000  current step :  3801
total : 5000  current step :  3802
total : 5000  current step :  3803
total : 5000  current step :  3804
total : 5000  current step :  3805
total : 5000  current step :  3806
total : 5000  current step :  3807
total : 5000  current step :  3808
total : 5000  current step :  3809
total : 5000  current step :  3810
total : 5000  current step :  3811
total : 5000  current step :  3812
total : 5000  current step :  3813
total : 5000  current step :  3814
total : 5000  current step :  3815
total : 5000  current step :  3816
total : 5000  current step :  3817
total : 5000  current step :  3818
total : 5000  current step :  3819
total : 5000  current step :  3820
total : 5000  current step :  3821
total : 5000  current step :  3822
total : 5000  current step :  3823
total : 5000  current step :  3824
total : 5000  current step :  3825
total : 5000  current step :  3826
total : 5000  current step :  3827
total : 5000  current step :  3828
total : 5000  current step :  3829
total : 5000  current step :  3830
total : 5000  current step :  3831
total : 5000  current step :  3832
total : 5000  current step :  3833
total : 5000  current step :  3834
total : 5000  current step :  3835
total : 5000  current step :  3836
total : 5000  current step :  3837
total : 5000  current step :  3838
total : 5000  current step :  3839
total : 5000  current step :  3840
total : 5000  current step :  3841
total : 5000  current step :  3842
total : 5000  current step :  3843
total : 5000  current step :  3844
total : 5000  current step :  3845
total : 5000  current step :  3846
total : 5000  current step :  3847
total : 5000  current step :  3848
total : 5000  current step :  3849
total : 5000  current step :  3850
total : 5000  current step :  3851
total : 5000  current step :  3852
total : 5000  current step :  3853
total : 5000  current step :  3854
total : 5000  current step :  3855
total : 5000  current step :  3856
total : 5000  current step :  3857
total : 5000  current step :  3858
total : 5000  current step :  3859
total : 5000  current step :  3860
total : 5000  current step :  3861
total : 5000  current step :  3862
total : 5000  current step :  3863
total : 5000  current step :  3864
total : 5000  current step :  3865
total : 5000  current step :  3866
total : 5000  current step :  3867
total : 5000  current step :  3868
total : 5000  current step :  3869
total : 5000  current step :  3870
total : 5000  current step :  3871
total : 5000  current step :  3872
total : 5000  current step :  3873
total : 5000  current step :  3874
total : 5000  current step :  3875
total : 5000  current step :  3876
total : 5000  current step :  3877
total : 5000  current step :  3878
total : 5000  current step :  3879
total : 5000  current step :  3880
total : 5000  current step :  3881
total : 5000  current step :  3882
total : 5000  current step :  3883
total : 5000  current step :  3884
total : 5000  current step :  3885
total : 5000  current step :  3886
total : 5000  current step :  3887
total : 5000  current step :  3888
total : 5000  current step :  3889
total : 5000  current step :  3890
total : 5000  current step :  3891
total : 5000  current step :  3892
total : 5000  current step :  3893
total : 5000  current step :  3894
total : 5000  current step :  3895
total : 5000  current step :  3896
total : 5000  current step :  3897
total : 5000  current step :  3898
total : 5000  current step :  3899
total : 5000  current step :  3900
total : 5000  current step :  3901
total : 5000  current step :  3902
total : 5000  current step :  3903
total : 5000  current step :  3904
total : 5000  current step :  3905
total : 5000  current step :  3906
total : 5000  current step :  3907
total : 5000  current step :  3908
total : 5000  current step :  3909
total : 5000  current step :  3910
total : 5000  current step :  3911
total : 5000  current step :  3912
total : 5000  current step :  3913
total : 5000  current step :  3914
total : 5000  current step :  3915
total : 5000  current step :  3916
total : 5000  current step :  3917
total : 5000  current step :  3918
total : 5000  current step :  3919
total : 5000  current step :  3920
total : 5000  current step :  3921
total : 5000  current step :  3922
total : 5000  current step :  3923
total : 5000  current step :  3924
total : 5000  current step :  3925
total : 5000  current step :  3926
total : 5000  current step :  3927
total : 5000  current step :  3928
total : 5000  current step :  3929
total : 5000  current step :  3930
total : 5000  current step :  3931
total : 5000  current step :  3932
total : 5000  current step :  3933
total : 5000  current step :  3934
total : 5000  current step :  3935
total : 5000  current step :  3936
total : 5000  current step :  3937
total : 5000  current step :  3938
total : 5000  current step :  3939
total : 5000  current step :  3940
total : 5000  current step :  3941
total : 5000  current step :  3942
total : 5000  current step :  3943
total : 5000  current step :  3944
total : 5000  current step :  3945
total : 5000  current step :  3946
total : 5000  current step :  3947
total : 5000  current step :  3948
total : 5000  current step :  3949
total : 5000  current step :  3950
total : 5000  current step :  3951
total : 5000  current step :  3952
total : 5000  current step :  3953
total : 5000  current step :  3954
total : 5000  current step :  3955
total : 5000  current step :  3956
total : 5000  current step :  3957
total : 5000  current step :  3958
total : 5000  current step :  3959
total : 5000  current step :  3960
total : 5000  current step :  3961
total : 5000  current step :  3962
total : 5000  current step :  3963
total : 5000  current step :  3964
total : 5000  current step :  3965
total : 5000  current step :  3966
total : 5000  current step :  3967
total : 5000  current step :  3968
total : 5000  current step :  3969
total : 5000  current step :  3970
total : 5000  current step :  3971
total : 5000  current step :  3972
total : 5000  current step :  3973
total : 5000  current step :  3974
total : 5000  current step :  3975
total : 5000  current step :  3976
total : 5000  current step :  3977
total : 5000  current step :  3978
total : 5000  current step :  3979
total : 5000  current step :  3980
total : 5000  current step :  3981
total : 5000  current step :  3982
total : 5000  current step :  3983
total : 5000  current step :  3984
total : 5000  current step :  3985
total : 5000  current step :  3986
total : 5000  current step :  3987
total : 5000  current step :  3988
total : 5000  current step :  3989
total : 5000  current step :  3990
total : 5000  current step :  3991
total : 5000  current step :  3992
total : 5000  current step :  3993
total : 5000  current step :  3994
total : 5000  current step :  3995
total : 5000  current step :  3996
total : 5000  current step :  3997
total : 5000  current step :  3998
total : 5000  current step :  3999
total : 5000  current step :  4000
total : 5000  current step :  4001
total : 5000  current step :  4002
total : 5000  current step :  4003
total : 5000  current step :  4004
total : 5000  current step :  4005
total : 5000  current step :  4006
total : 5000  current step :  4007
total : 5000  current step :  4008
total : 5000  current step :  4009
total : 5000  current step :  4010
total : 5000  current step :  4011
total : 5000  current step :  4012
total : 5000  current step :  4013
total : 5000  current step :  4014
total : 5000  current step :  4015
total : 5000  current step :  4016
total : 5000  current step :  4017
total : 5000  current step :  4018
total : 5000  current step :  4019
total : 5000  current step :  4020
total : 5000  current step :  4021
total : 5000  current step :  4022
total : 5000  current step :  4023
total : 5000  current step :  4024
total : 5000  current step :  4025
total : 5000  current step :  4026
total : 5000  current step :  4027
total : 5000  current step :  4028
total : 5000  current step :  4029
total : 5000  current step :  4030
total : 5000  current step :  4031
total : 5000  current step :  4032
total : 5000  current step :  4033
total : 5000  current step :  4034
total : 5000  current step :  4035
total : 5000  current step :  4036
total : 5000  current step :  4037
total : 5000  current step :  4038
total : 5000  current step :  4039
total : 5000  current step :  4040
total : 5000  current step :  4041
total : 5000  current step :  4042
total : 5000  current step :  4043
total : 5000  current step :  4044
total : 5000  current step :  4045
total : 5000  current step :  4046
total : 5000  current step :  4047
total : 5000  current step :  4048
total : 5000  current step :  4049
total : 5000  current step :  4050
total : 5000  current step :  4051
total : 5000  current step :  4052
total : 5000  current step :  4053
total : 5000  current step :  4054
total : 5000  current step :  4055
total : 5000  current step :  4056
total : 5000  current step :  4057
total : 5000  current step :  4058
total : 5000  current step :  4059
total : 5000  current step :  4060
total : 5000  current step :  4061
total : 5000  current step :  4062
total : 5000  current step :  4063
total : 5000  current step :  4064
total : 5000  current step :  4065
total : 5000  current step :  4066
total : 5000  current step :  4067
total : 5000  current step :  4068
total : 5000  current step :  4069
total : 5000  current step :  4070
total : 5000  current step :  4071
total : 5000  current step :  4072
total : 5000  current step :  4073
total : 5000  current step :  4074
total : 5000  current step :  4075
total : 5000  current step :  4076
total : 5000  current step :  4077
total : 5000  current step :  4078
total : 5000  current step :  4079
total : 5000  current step :  4080
total : 5000  current step :  4081
total : 5000  current step :  4082
total : 5000  current step :  4083
total : 5000  current step :  4084
total : 5000  current step :  4085
total : 5000  current step :  4086
total : 5000  current step :  4087
total : 5000  current step :  4088
total : 5000  current step :  4089
total : 5000  current step :  4090
total : 5000  current step :  4091
total : 5000  current step :  4092
total : 5000  current step :  4093
total : 5000  current step :  4094
total : 5000  current step :  4095
total : 5000  current step :  4096
total : 5000  current step :  4097
total : 5000  current step :  4098
total : 5000  current step :  4099
total : 5000  current step :  4100
total : 5000  current step :  4101
total : 5000  current step :  4102
total : 5000  current step :  4103
total : 5000  current step :  4104
total : 5000  current step :  4105
total : 5000  current step :  4106
total : 5000  current step :  4107
total : 5000  current step :  4108
total : 5000  current step :  4109
total : 5000  current step :  4110
total : 5000  current step :  4111
total : 5000  current step :  4112
total : 5000  current step :  4113
total : 5000  current step :  4114
total : 5000  current step :  4115
total : 5000  current step :  4116
total : 5000  current step :  4117
total : 5000  current step :  4118
total : 5000  current step :  4119
total : 5000  current step :  4120
total : 5000  current step :  4121
total : 5000  current step :  4122
total : 5000  current step :  4123
total : 5000  current step :  4124
total : 5000  current step :  4125
total : 5000  current step :  4126
total : 5000  current step :  4127
total : 5000  current step :  4128
total : 5000  current step :  4129
total : 5000  current step :  4130
total : 5000  current step :  4131
total : 5000  current step :  4132
total : 5000  current step :  4133
total : 5000  current step :  4134
total : 5000  current step :  4135
total : 5000  current step :  4136
total : 5000  current step :  4137
total : 5000  current step :  4138
total : 5000  current step :  4139
total : 5000  current step :  4140
total : 5000  current step :  4141
total : 5000  current step :  4142
total : 5000  current step :  4143
total : 5000  current step :  4144
total : 5000  current step :  4145
total : 5000  current step :  4146
total : 5000  current step :  4147
total : 5000  current step :  4148
total : 5000  current step :  4149
total : 5000  current step :  4150
total : 5000  current step :  4151
total : 5000  current step :  4152
total : 5000  current step :  4153
total : 5000  current step :  4154
total : 5000  current step :  4155
total : 5000  current step :  4156
total : 5000  current step :  4157
total : 5000  current step :  4158
total : 5000  current step :  4159
total : 5000  current step :  4160
total : 5000  current step :  4161
total : 5000  current step :  4162
total : 5000  current step :  4163
total : 5000  current step :  4164
total : 5000  current step :  4165
total : 5000  current step :  4166
total : 5000  current step :  4167
total : 5000  current step :  4168
total : 5000  current step :  4169
total : 5000  current step :  4170
total : 5000  current step :  4171
total : 5000  current step :  4172
total : 5000  current step :  4173
total : 5000  current step :  4174
total : 5000  current step :  4175
total : 5000  current step :  4176
total : 5000  current step :  4177
total : 5000  current step :  4178
total : 5000  current step :  4179
total : 5000  current step :  4180
total : 5000  current step :  4181
total : 5000  current step :  4182
total : 5000  current step :  4183
total : 5000  current step :  4184
total : 5000  current step :  4185
total : 5000  current step :  4186
total : 5000  current step :  4187
total : 5000  current step :  4188
total : 5000  current step :  4189
total : 5000  current step :  4190
total : 5000  current step :  4191
total : 5000  current step :  4192
total : 5000  current step :  4193
total : 5000  current step :  4194
total : 5000  current step :  4195
total : 5000  current step :  4196
total : 5000  current step :  4197
total : 5000  current step :  4198
total : 5000  current step :  4199
total : 5000  current step :  4200
total : 5000  current step :  4201
total : 5000  current step :  4202
total : 5000  current step :  4203
total : 5000  current step :  4204
total : 5000  current step :  4205
total : 5000  current step :  4206
total : 5000  current step :  4207
total : 5000  current step :  4208
total : 5000  current step :  4209
total : 5000  current step :  4210
total : 5000  current step :  4211
total : 5000  current step :  4212
total : 5000  current step :  4213
total : 5000  current step :  4214
total : 5000  current step :  4215
total : 5000  current step :  4216
total : 5000  current step :  4217
total : 5000  current step :  4218
total : 5000  current step :  4219
total : 5000  current step :  4220
total : 5000  current step :  4221
total : 5000  current step :  4222
total : 5000  current step :  4223
total : 5000  current step :  4224
total : 5000  current step :  4225
total : 5000  current step :  4226
total : 5000  current step :  4227
total : 5000  current step :  4228
total : 5000  current step :  4229
total : 5000  current step :  4230
total : 5000  current step :  4231
total : 5000  current step :  4232
total : 5000  current step :  4233
total : 5000  current step :  4234
total : 5000  current step :  4235
total : 5000  current step :  4236
total : 5000  current step :  4237
total : 5000  current step :  4238
total : 5000  current step :  4239
total : 5000  current step :  4240
total : 5000  current step :  4241
total : 5000  current step :  4242
total : 5000  current step :  4243
total : 5000  current step :  4244
total : 5000  current step :  4245
total : 5000  current step :  4246
total : 5000  current step :  4247
total : 5000  current step :  4248
total : 5000  current step :  4249
total : 5000  current step :  4250
total : 5000  current step :  4251
total : 5000  current step :  4252
total : 5000  current step :  4253
total : 5000  current step :  4254
total : 5000  current step :  4255
total : 5000  current step :  4256
total : 5000  current step :  4257
total : 5000  current step :  4258
total : 5000  current step :  4259
total : 5000  current step :  4260
total : 5000  current step :  4261
total : 5000  current step :  4262
total : 5000  current step :  4263
total : 5000  current step :  4264
total : 5000  current step :  4265
total : 5000  current step :  4266
total : 5000  current step :  4267
total : 5000  current step :  4268
total : 5000  current step :  4269
total : 5000  current step :  4270
total : 5000  current step :  4271
total : 5000  current step :  4272
total : 5000  current step :  4273
total : 5000  current step :  4274
total : 5000  current step :  4275
total : 5000  current step :  4276
total : 5000  current step :  4277
total : 5000  current step :  4278
total : 5000  current step :  4279
total : 5000  current step :  4280
total : 5000  current step :  4281
total : 5000  current step :  4282
total : 5000  current step :  4283
total : 5000  current step :  4284
total : 5000  current step :  4285
total : 5000  current step :  4286
total : 5000  current step :  4287
total : 5000  current step :  4288
total : 5000  current step :  4289
total : 5000  current step :  4290
total : 5000  current step :  4291
total : 5000  current step :  4292
total : 5000  current step :  4293
total : 5000  current step :  4294
total : 5000  current step :  4295
total : 5000  current step :  4296
total : 5000  current step :  4297
total : 5000  current step :  4298
total : 5000  current step :  4299
total : 5000  current step :  4300
total : 5000  current step :  4301
total : 5000  current step :  4302
total : 5000  current step :  4303
total : 5000  current step :  4304
total : 5000  current step :  4305
total : 5000  current step :  4306
total : 5000  current step :  4307
total : 5000  current step :  4308
total : 5000  current step :  4309
total : 5000  current step :  4310
total : 5000  current step :  4311
total : 5000  current step :  4312
total : 5000  current step :  4313
total : 5000  current step :  4314
total : 5000  current step :  4315
total : 5000  current step :  4316
total : 5000  current step :  4317
total : 5000  current step :  4318
total : 5000  current step :  4319
total : 5000  current step :  4320
total : 5000  current step :  4321
total : 5000  current step :  4322
total : 5000  current step :  4323
total : 5000  current step :  4324
total : 5000  current step :  4325
total : 5000  current step :  4326
total : 5000  current step :  4327
total : 5000  current step :  4328
total : 5000  current step :  4329
total : 5000  current step :  4330
total : 5000  current step :  4331
total : 5000  current step :  4332
total : 5000  current step :  4333
total : 5000  current step :  4334
total : 5000  current step :  4335
total : 5000  current step :  4336
total : 5000  current step :  4337
total : 5000  current step :  4338
total : 5000  current step :  4339
total : 5000  current step :  4340
total : 5000  current step :  4341
total : 5000  current step :  4342
total : 5000  current step :  4343
total : 5000  current step :  4344
total : 5000  current step :  4345
total : 5000  current step :  4346
total : 5000  current step :  4347
total : 5000  current step :  4348
total : 5000  current step :  4349
total : 5000  current step :  4350
total : 5000  current step :  4351
total : 5000  current step :  4352
total : 5000  current step :  4353
total : 5000  current step :  4354
total : 5000  current step :  4355
total : 5000  current step :  4356
total : 5000  current step :  4357
total : 5000  current step :  4358
total : 5000  current step :  4359
total : 5000  current step :  4360
total : 5000  current step :  4361
total : 5000  current step :  4362
total : 5000  current step :  4363
total : 5000  current step :  4364
total : 5000  current step :  4365
total : 5000  current step :  4366
total : 5000  current step :  4367
total : 5000  current step :  4368
total : 5000  current step :  4369
total : 5000  current step :  4370
total : 5000  current step :  4371
total : 5000  current step :  4372
total : 5000  current step :  4373
total : 5000  current step :  4374
total : 5000  current step :  4375
total : 5000  current step :  4376
total : 5000  current step :  4377
total : 5000  current step :  4378
total : 5000  current step :  4379
total : 5000  current step :  4380
total : 5000  current step :  4381
total : 5000  current step :  4382
total : 5000  current step :  4383
total : 5000  current step :  4384
total : 5000  current step :  4385
total : 5000  current step :  4386
total : 5000  current step :  4387
total : 5000  current step :  4388
total : 5000  current step :  4389
total : 5000  current step :  4390
total : 5000  current step :  4391
total : 5000  current step :  4392
total : 5000  current step :  4393
total : 5000  current step :  4394
total : 5000  current step :  4395
total : 5000  current step :  4396
total : 5000  current step :  4397
total : 5000  current step :  4398
total : 5000  current step :  4399
total : 5000  current step :  4400
total : 5000  current step :  4401
total : 5000  current step :  4402
total : 5000  current step :  4403
total : 5000  current step :  4404
total : 5000  current step :  4405
total : 5000  current step :  4406
total : 5000  current step :  4407
total : 5000  current step :  4408
total : 5000  current step :  4409
total : 5000  current step :  4410
total : 5000  current step :  4411
total : 5000  current step :  4412
total : 5000  current step :  4413
total : 5000  current step :  4414
total : 5000  current step :  4415
total : 5000  current step :  4416
total : 5000  current step :  4417
total : 5000  current step :  4418
total : 5000  current step :  4419
total : 5000  current step :  4420
total : 5000  current step :  4421
total : 5000  current step :  4422
total : 5000  current step :  4423
total : 5000  current step :  4424
total : 5000  current step :  4425
total : 5000  current step :  4426
total : 5000  current step :  4427
total : 5000  current step :  4428
total : 5000  current step :  4429
total : 5000  current step :  4430
total : 5000  current step :  4431
total : 5000  current step :  4432
total : 5000  current step :  4433
total : 5000  current step :  4434
total : 5000  current step :  4435
total : 5000  current step :  4436
total : 5000  current step :  4437
total : 5000  current step :  4438
total : 5000  current step :  4439
total : 5000  current step :  4440
total : 5000  current step :  4441
total : 5000  current step :  4442
total : 5000  current step :  4443
total : 5000  current step :  4444
total : 5000  current step :  4445
total : 5000  current step :  4446
total : 5000  current step :  4447
total : 5000  current step :  4448
total : 5000  current step :  4449
total : 5000  current step :  4450
total : 5000  current step :  4451
total : 5000  current step :  4452
total : 5000  current step :  4453
total : 5000  current step :  4454
total : 5000  current step :  4455
total : 5000  current step :  4456
total : 5000  current step :  4457
total : 5000  current step :  4458
total : 5000  current step :  4459
total : 5000  current step :  4460
total : 5000  current step :  4461
total : 5000  current step :  4462
total : 5000  current step :  4463
total : 5000  current step :  4464
total : 5000  current step :  4465
total : 5000  current step :  4466
total : 5000  current step :  4467
total : 5000  current step :  4468
total : 5000  current step :  4469
total : 5000  current step :  4470
total : 5000  current step :  4471
total : 5000  current step :  4472
total : 5000  current step :  4473
total : 5000  current step :  4474
total : 5000  current step :  4475
total : 5000  current step :  4476
total : 5000  current step :  4477
total : 5000  current step :  4478
total : 5000  current step :  4479
total : 5000  current step :  4480
total : 5000  current step :  4481
total : 5000  current step :  4482
total : 5000  current step :  4483
total : 5000  current step :  4484
total : 5000  current step :  4485
total : 5000  current step :  4486
total : 5000  current step :  4487
total : 5000  current step :  4488
total : 5000  current step :  4489
total : 5000  current step :  4490
total : 5000  current step :  4491
total : 5000  current step :  4492
total : 5000  current step :  4493
total : 5000  current step :  4494
total : 5000  current step :  4495
total : 5000  current step :  4496
total : 5000  current step :  4497
total : 5000  current step :  4498
total : 5000  current step :  4499
total : 5000  current step :  4500
total : 5000  current step :  4501
total : 5000  current step :  4502
total : 5000  current step :  4503
total : 5000  current step :  4504
total : 5000  current step :  4505
total : 5000  current step :  4506
total : 5000  current step :  4507
total : 5000  current step :  4508
total : 5000  current step :  4509
total : 5000  current step :  4510
total : 5000  current step :  4511
total : 5000  current step :  4512
total : 5000  current step :  4513
total : 5000  current step :  4514
total : 5000  current step :  4515
total : 5000  current step :  4516
total : 5000  current step :  4517
total : 5000  current step :  4518
total : 5000  current step :  4519
total : 5000  current step :  4520
total : 5000  current step :  4521
total : 5000  current step :  4522
total : 5000  current step :  4523
total : 5000  current step :  4524
total : 5000  current step :  4525
total : 5000  current step :  4526
total : 5000  current step :  4527
total : 5000  current step :  4528
total : 5000  current step :  4529
total : 5000  current step :  4530
total : 5000  current step :  4531
total : 5000  current step :  4532
total : 5000  current step :  4533
total : 5000  current step :  4534
total : 5000  current step :  4535
total : 5000  current step :  4536
total : 5000  current step :  4537
total : 5000  current step :  4538
total : 5000  current step :  4539
total : 5000  current step :  4540
total : 5000  current step :  4541
total : 5000  current step :  4542
total : 5000  current step :  4543
total : 5000  current step :  4544
total : 5000  current step :  4545
total : 5000  current step :  4546
total : 5000  current step :  4547
total : 5000  current step :  4548
total : 5000  current step :  4549
total : 5000  current step :  4550
total : 5000  current step :  4551
total : 5000  current step :  4552
total : 5000  current step :  4553
total : 5000  current step :  4554
total : 5000  current step :  4555
total : 5000  current step :  4556
total : 5000  current step :  4557
total : 5000  current step :  4558
total : 5000  current step :  4559
total : 5000  current step :  4560
total : 5000  current step :  4561
total : 5000  current step :  4562
total : 5000  current step :  4563
total : 5000  current step :  4564
total : 5000  current step :  4565
total : 5000  current step :  4566
total : 5000  current step :  4567
total : 5000  current step :  4568
total : 5000  current step :  4569
total : 5000  current step :  4570
total : 5000  current step :  4571
total : 5000  current step :  4572
total : 5000  current step :  4573
total : 5000  current step :  4574
total : 5000  current step :  4575
total : 5000  current step :  4576
total : 5000  current step :  4577
total : 5000  current step :  4578
total : 5000  current step :  4579
total : 5000  current step :  4580
total : 5000  current step :  4581
total : 5000  current step :  4582
total : 5000  current step :  4583
total : 5000  current step :  4584
total : 5000  current step :  4585
total : 5000  current step :  4586
total : 5000  current step :  4587
total : 5000  current step :  4588
total : 5000  current step :  4589
total : 5000  current step :  4590
total : 5000  current step :  4591
total : 5000  current step :  4592
total : 5000  current step :  4593
total : 5000  current step :  4594
total : 5000  current step :  4595
total : 5000  current step :  4596
total : 5000  current step :  4597
total : 5000  current step :  4598
total : 5000  current step :  4599
total : 5000  current step :  4600
total : 5000  current step :  4601
total : 5000  current step :  4602
total : 5000  current step :  4603
total : 5000  current step :  4604
total : 5000  current step :  4605
total : 5000  current step :  4606
total : 5000  current step :  4607
total : 5000  current step :  4608
total : 5000  current step :  4609
total : 5000  current step :  4610
total : 5000  current step :  4611
total : 5000  current step :  4612
total : 5000  current step :  4613
total : 5000  current step :  4614
total : 5000  current step :  4615
total : 5000  current step :  4616
total : 5000  current step :  4617
total : 5000  current step :  4618
total : 5000  current step :  4619
total : 5000  current step :  4620
total : 5000  current step :  4621
total : 5000  current step :  4622
total : 5000  current step :  4623
total : 5000  current step :  4624
total : 5000  current step :  4625
total : 5000  current step :  4626
total : 5000  current step :  4627
total : 5000  current step :  4628
total : 5000  current step :  4629
total : 5000  current step :  4630
total : 5000  current step :  4631
total : 5000  current step :  4632
total : 5000  current step :  4633
total : 5000  current step :  4634
total : 5000  current step :  4635
total : 5000  current step :  4636
total : 5000  current step :  4637
total : 5000  current step :  4638
total : 5000  current step :  4639
total : 5000  current step :  4640
total : 5000  current step :  4641
total : 5000  current step :  4642
total : 5000  current step :  4643
total : 5000  current step :  4644
total : 5000  current step :  4645
total : 5000  current step :  4646
total : 5000  current step :  4647
total : 5000  current step :  4648
total : 5000  current step :  4649
total : 5000  current step :  4650
total : 5000  current step :  4651
total : 5000  current step :  4652
total : 5000  current step :  4653
total : 5000  current step :  4654
total : 5000  current step :  4655
total : 5000  current step :  4656
total : 5000  current step :  4657
total : 5000  current step :  4658
total : 5000  current step :  4659
total : 5000  current step :  4660
total : 5000  current step :  4661
total : 5000  current step :  4662
total : 5000  current step :  4663
total : 5000  current step :  4664
total : 5000  current step :  4665
total : 5000  current step :  4666
total : 5000  current step :  4667
total : 5000  current step :  4668
total : 5000  current step :  4669
total : 5000  current step :  4670
total : 5000  current step :  4671
total : 5000  current step :  4672
total : 5000  current step :  4673
total : 5000  current step :  4674
total : 5000  current step :  4675
total : 5000  current step :  4676
total : 5000  current step :  4677
total : 5000  current step :  4678
total : 5000  current step :  4679
total : 5000  current step :  4680
total : 5000  current step :  4681
total : 5000  current step :  4682
total : 5000  current step :  4683
total : 5000  current step :  4684
total : 5000  current step :  4685
total : 5000  current step :  4686
total : 5000  current step :  4687
total : 5000  current step :  4688
total : 5000  current step :  4689
total : 5000  current step :  4690
total : 5000  current step :  4691
total : 5000  current step :  4692
total : 5000  current step :  4693
total : 5000  current step :  4694
total : 5000  current step :  4695
total : 5000  current step :  4696
total : 5000  current step :  4697
total : 5000  current step :  4698
total : 5000  current step :  4699
total : 5000  current step :  4700
total : 5000  current step :  4701
total : 5000  current step :  4702
total : 5000  current step :  4703
total : 5000  current step :  4704
total : 5000  current step :  4705
total : 5000  current step :  4706
total : 5000  current step :  4707
total : 5000  current step :  4708
total : 5000  current step :  4709
total : 5000  current step :  4710
total : 5000  current step :  4711
total : 5000  current step :  4712
total : 5000  current step :  4713
total : 5000  current step :  4714
total : 5000  current step :  4715
total : 5000  current step :  4716
total : 5000  current step :  4717
total : 5000  current step :  4718
total : 5000  current step :  4719
total : 5000  current step :  4720
total : 5000  current step :  4721
total : 5000  current step :  4722
total : 5000  current step :  4723
total : 5000  current step :  4724
total : 5000  current step :  4725
total : 5000  current step :  4726
total : 5000  current step :  4727
total : 5000  current step :  4728
total : 5000  current step :  4729
total : 5000  current step :  4730
total : 5000  current step :  4731
total : 5000  current step :  4732
total : 5000  current step :  4733
total : 5000  current step :  4734
total : 5000  current step :  4735
total : 5000  current step :  4736
total : 5000  current step :  4737
total : 5000  current step :  4738
total : 5000  current step :  4739
total : 5000  current step :  4740
total : 5000  current step :  4741
total : 5000  current step :  4742
total : 5000  current step :  4743
total : 5000  current step :  4744
total : 5000  current step :  4745
total : 5000  current step :  4746
total : 5000  current step :  4747
total : 5000  current step :  4748
total : 5000  current step :  4749
total : 5000  current step :  4750
total : 5000  current step :  4751
total : 5000  current step :  4752
total : 5000  current step :  4753
total : 5000  current step :  4754
total : 5000  current step :  4755
total : 5000  current step :  4756
total : 5000  current step :  4757
total : 5000  current step :  4758
total : 5000  current step :  4759
total : 5000  current step :  4760
total : 5000  current step :  4761
total : 5000  current step :  4762
total : 5000  current step :  4763
total : 5000  current step :  4764
total : 5000  current step :  4765
total : 5000  current step :  4766
total : 5000  current step :  4767
total : 5000  current step :  4768
total : 5000  current step :  4769
total : 5000  current step :  4770
total : 5000  current step :  4771
total : 5000  current step :  4772
total : 5000  current step :  4773
total : 5000  current step :  4774
total : 5000  current step :  4775
total : 5000  current step :  4776
total : 5000  current step :  4777
total : 5000  current step :  4778
total : 5000  current step :  4779
total : 5000  current step :  4780
total : 5000  current step :  4781
total : 5000  current step :  4782
total : 5000  current step :  4783
total : 5000  current step :  4784
total : 5000  current step :  4785
total : 5000  current step :  4786
total : 5000  current step :  4787
total : 5000  current step :  4788
total : 5000  current step :  4789
total : 5000  current step :  4790
total : 5000  current step :  4791
total : 5000  current step :  4792
total : 5000  current step :  4793
total : 5000  current step :  4794
total : 5000  current step :  4795
total : 5000  current step :  4796
total : 5000  current step :  4797
total : 5000  current step :  4798
total : 5000  current step :  4799
total : 5000  current step :  4800
total : 5000  current step :  4801
total : 5000  current step :  4802
total : 5000  current step :  4803
total : 5000  current step :  4804
total : 5000  current step :  4805
total : 5000  current step :  4806
total : 5000  current step :  4807
total : 5000  current step :  4808
total : 5000  current step :  4809
total : 5000  current step :  4810
total : 5000  current step :  4811
total : 5000  current step :  4812
total : 5000  current step :  4813
total : 5000  current step :  4814
total : 5000  current step :  4815
total : 5000  current step :  4816
total : 5000  current step :  4817
total : 5000  current step :  4818
total : 5000  current step :  4819
total : 5000  current step :  4820
total : 5000  current step :  4821
total : 5000  current step :  4822
total : 5000  current step :  4823
total : 5000  current step :  4824
total : 5000  current step :  4825
total : 5000  current step :  4826
total : 5000  current step :  4827
total : 5000  current step :  4828
total : 5000  current step :  4829
total : 5000  current step :  4830
total : 5000  current step :  4831
total : 5000  current step :  4832
total : 5000  current step :  4833
total : 5000  current step :  4834
total : 5000  current step :  4835
total : 5000  current step :  4836
total : 5000  current step :  4837
total : 5000  current step :  4838
total : 5000  current step :  4839
total : 5000  current step :  4840
total : 5000  current step :  4841
total : 5000  current step :  4842
total : 5000  current step :  4843
total : 5000  current step :  4844
total : 5000  current step :  4845
total : 5000  current step :  4846
total : 5000  current step :  4847
total : 5000  current step :  4848
total : 5000  current step :  4849
total : 5000  current step :  4850
total : 5000  current step :  4851
total : 5000  current step :  4852
total : 5000  current step :  4853
total : 5000  current step :  4854
total : 5000  current step :  4855
total : 5000  current step :  4856
total : 5000  current step :  4857
total : 5000  current step :  4858
total : 5000  current step :  4859
total : 5000  current step :  4860
total : 5000  current step :  4861
total : 5000  current step :  4862
total : 5000  current step :  4863
total : 5000  current step :  4864
total : 5000  current step :  4865
total : 5000  current step :  4866
total : 5000  current step :  4867
total : 5000  current step :  4868
total : 5000  current step :  4869
total : 5000  current step :  4870
total : 5000  current step :  4871
total : 5000  current step :  4872
total : 5000  current step :  4873
total : 5000  current step :  4874
total : 5000  current step :  4875
total : 5000  current step :  4876
total : 5000  current step :  4877
total : 5000  current step :  4878
total : 5000  current step :  4879
total : 5000  current step :  4880
total : 5000  current step :  4881
total : 5000  current step :  4882
total : 5000  current step :  4883
total : 5000  current step :  4884
total : 5000  current step :  4885
total : 5000  current step :  4886
total : 5000  current step :  4887
total : 5000  current step :  4888
total : 5000  current step :  4889
total : 5000  current step :  4890
total : 5000  current step :  4891
total : 5000  current step :  4892
total : 5000  current step :  4893
total : 5000  current step :  4894
total : 5000  current step :  4895
total : 5000  current step :  4896
total : 5000  current step :  4897
total : 5000  current step :  4898
total : 5000  current step :  4899
total : 5000  current step :  4900
total : 5000  current step :  4901
total : 5000  current step :  4902
total : 5000  current step :  4903
total : 5000  current step :  4904
total : 5000  current step :  4905
total : 5000  current step :  4906
total : 5000  current step :  4907
total : 5000  current step :  4908
total : 5000  current step :  4909
total : 5000  current step :  4910
total : 5000  current step :  4911
total : 5000  current step :  4912
total : 5000  current step :  4913
total : 5000  current step :  4914
total : 5000  current step :  4915
total : 5000  current step :  4916
total : 5000  current step :  4917
total : 5000  current step :  4918
total : 5000  current step :  4919
total : 5000  current step :  4920
total : 5000  current step :  4921
total : 5000  current step :  4922
total : 5000  current step :  4923
total : 5000  current step :  4924
total : 5000  current step :  4925
total : 5000  current step :  4926
total : 5000  current step :  4927
total : 5000  current step :  4928
total : 5000  current step :  4929
total : 5000  current step :  4930
total : 5000  current step :  4931
total : 5000  current step :  4932
total : 5000  current step :  4933
total : 5000  current step :  4934
total : 5000  current step :  4935
total : 5000  current step :  4936
total : 5000  current step :  4937
total : 5000  current step :  4938
total : 5000  current step :  4939
total : 5000  current step :  4940
total : 5000  current step :  4941
total : 5000  current step :  4942
total : 5000  current step :  4943
total : 5000  current step :  4944
total : 5000  current step :  4945
total : 5000  current step :  4946
total : 5000  current step :  4947
total : 5000  current step :  4948
total : 5000  current step :  4949
total : 5000  current step :  4950
total : 5000  current step :  4951
total : 5000  current step :  4952
total : 5000  current step :  4953
total : 5000  current step :  4954
total : 5000  current step :  4955
total : 5000  current step :  4956
total : 5000  current step :  4957
total : 5000  current step :  4958
total : 5000  current step :  4959
total : 5000  current step :  4960
total : 5000  current step :  4961
total : 5000  current step :  4962
total : 5000  current step :  4963
total : 5000  current step :  4964
total : 5000  current step :  4965
total : 5000  current step :  4966
total : 5000  current step :  4967
total : 5000  current step :  4968
total : 5000  current step :  4969
total : 5000  current step :  4970
total : 5000  current step :  4971
total : 5000  current step :  4972
total : 5000  current step :  4973
total : 5000  current step :  4974
total : 5000  current step :  4975
total : 5000  current step :  4976
total : 5000  current step :  4977
total : 5000  current step :  4978
total : 5000  current step :  4979
total : 5000  current step :  4980
total : 5000  current step :  4981
total : 5000  current step :  4982
total : 5000  current step :  4983
total : 5000  current step :  4984
total : 5000  current step :  4985
total : 5000  current step :  4986
total : 5000  current step :  4987
total : 5000  current step :  4988
total : 5000  current step :  4989
total : 5000  current step :  4990
total : 5000  current step :  4991
total : 5000  current step :  4992
total : 5000  current step :  4993
total : 5000  current step :  4994
total : 5000  current step :  4995
total : 5000  current step :  4996
total : 5000  current step :  4997
total : 5000  current step :  4998
total : 5000  current step :  4999
soft_pseudo_label
tensor([[1.5589e-01, 7.8858e-01, 7.0978e-03,  ..., 6.9131e-03, 7.1395e-03,
         7.1920e-03],
        [2.9044e-03, 9.8295e-01, 1.7176e-03,  ..., 1.8027e-03, 1.7869e-03,
         1.9031e-03],
        [6.3781e-01, 3.3742e-01, 3.0608e-03,  ..., 3.0415e-03, 3.1258e-03,
         3.1030e-03],
        ...,
        [9.2357e-01, 7.6108e-02, 4.0561e-05,  ..., 3.8931e-05, 4.3558e-05,
         3.8817e-05],
        [1.1366e-01, 8.6987e-01, 1.9404e-03,  ..., 1.9385e-03, 2.1699e-03,
         2.0118e-03],
        [5.0884e-02, 9.0282e-01, 6.0123e-03,  ..., 5.9538e-03, 5.8558e-03,
         5.8931e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 7, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        6, 1, 0, 1, 1, 0, 1, 0, 8, 0, 0, 1, 1, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 1,
        1, 8, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 5, 1, 1, 1,
        1, 1, 0, 1, 4, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        8, 1, 4, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[2.7141e-03, 9.9697e-01, 4.3870e-05,  ..., 3.8227e-05, 3.8078e-05,
         4.1616e-05],
        [8.1764e-01, 1.2637e-01, 6.6616e-03,  ..., 6.8563e-03, 7.1051e-03,
         7.1504e-03],
        [7.6265e-04, 9.8642e-01, 1.6879e-03,  ..., 1.8542e-03, 1.5145e-03,
         1.5856e-03],
        ...,
        [9.8555e-02, 8.7842e-01, 2.8761e-03,  ..., 2.8274e-03, 2.6913e-03,
         2.7404e-03],
        [9.8759e-01, 1.3473e-03, 1.4646e-03,  ..., 1.4092e-03, 1.4010e-03,
         1.4447e-03],
        [7.4703e-01, 2.3187e-01, 2.5910e-03,  ..., 2.7500e-03, 2.6062e-03,
         2.5533e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0]
original label
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 6, 1,
        7, 0, 1, 1, 0, 1, 0, 1, 3, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 6, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 5,
        6, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 5, 1, 0, 0, 5, 0, 0,
        4, 0, 1, 1, 1, 1, 4, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[9.8145e-01, 1.1624e-05, 2.6475e-03,  ..., 2.3504e-03, 2.1572e-03,
         2.1116e-03],
        [3.5657e-02, 9.1423e-01, 6.1151e-03,  ..., 6.3494e-03, 6.0321e-03,
         6.4525e-03],
        [2.0918e-05, 9.8690e-01, 1.9647e-03,  ..., 1.5733e-03, 1.8400e-03,
         1.4487e-03],
        ...,
        [1.5328e-03, 9.7951e-01, 2.6420e-03,  ..., 2.4734e-03, 2.3281e-03,
         2.1924e-03],
        [4.2475e-02, 9.5173e-01, 7.1389e-04,  ..., 7.1949e-04, 6.8654e-04,
         7.8559e-04],
        [9.8675e-01, 7.6514e-04, 1.6285e-03,  ..., 1.5885e-03, 1.6001e-03,
         1.5426e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]
original label
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 3, 9, 0, 1, 0, 1,
        3, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 3, 3, 1, 1,
        1, 0, 1, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 3, 2, 0, 0, 0, 0, 0, 8, 1, 0, 0, 1,
        1, 0, 0, 6, 0, 1, 1, 0])
soft_pseudo_label
tensor([[8.4491e-01, 1.3847e-01, 2.1066e-03,  ..., 2.2110e-03, 1.9732e-03,
         2.0679e-03],
        [8.0404e-02, 9.1480e-01, 6.0615e-04,  ..., 6.1992e-04, 5.9792e-04,
         5.5299e-04],
        [4.1710e-02, 9.3187e-01, 3.2321e-03,  ..., 3.2305e-03, 3.2511e-03,
         3.2211e-03],
        ...,
        [9.8025e-01, 7.0014e-05, 2.3936e-03,  ..., 2.6031e-03, 2.2767e-03,
         2.5881e-03],
        [7.1734e-03, 9.7744e-01, 1.9185e-03,  ..., 2.0784e-03, 1.8971e-03,
         2.0432e-03],
        [3.1617e-03, 9.8353e-01, 1.6854e-03,  ..., 1.6904e-03, 1.6280e-03,
         1.8188e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1]
original label
tensor([1, 6, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0,
        0, 0, 1, 1, 8, 1, 6, 9, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 7, 0, 1, 1, 1,
        0, 1, 1, 9, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 8, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 6, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 3, 1, 1, 2, 1, 8, 0, 0, 1, 9, 0,
        0, 0, 0, 0, 1, 0, 0, 1])
soft_pseudo_label
tensor([[7.0274e-01, 2.8172e-01, 1.8398e-03,  ..., 1.9585e-03, 1.9281e-03,
         2.0286e-03],
        [4.4554e-03, 9.8925e-01, 7.9143e-04,  ..., 7.9259e-04, 7.4457e-04,
         8.0941e-04],
        [2.1900e-02, 9.6358e-01, 1.8922e-03,  ..., 1.8349e-03, 1.8206e-03,
         1.6945e-03],
        ...,
        [3.3691e-01, 6.2089e-01, 5.2550e-03,  ..., 5.2550e-03, 4.9632e-03,
         5.1133e-03],
        [1.5144e-02, 9.7046e-01, 1.9320e-03,  ..., 1.8762e-03, 1.6332e-03,
         1.6182e-03],
        [3.2890e-02, 9.3253e-01, 4.3164e-03,  ..., 4.3972e-03, 4.4469e-03,
         4.4905e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1]
original label
tensor([0, 1, 1, 1, 0, 3, 1, 0, 4, 1, 0, 0, 1, 1, 3, 1, 1, 1, 1, 4, 1, 9, 1, 7,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 3, 0, 1, 1,
        5, 1, 1, 1, 3, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 4, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 2, 0, 0, 7, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 5, 6, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 8, 1,
        7, 0, 1, 0, 0, 0, 1, 1])
soft_pseudo_label
tensor([[9.7849e-01, 3.6655e-03, 2.2246e-03,  ..., 2.1279e-03, 2.2008e-03,
         2.3669e-03],
        [9.8161e-01, 6.1502e-06, 2.2878e-03,  ..., 1.9972e-03, 2.2204e-03,
         2.3097e-03],
        [5.3617e-02, 9.3748e-01, 1.1074e-03,  ..., 1.1128e-03, 1.1095e-03,
         1.0923e-03],
        ...,
        [1.5275e-03, 9.8570e-01, 1.6276e-03,  ..., 1.5275e-03, 1.6000e-03,
         1.5969e-03],
        [9.9083e-01, 9.8047e-06, 1.2154e-03,  ..., 1.2015e-03, 9.9353e-04,
         1.2463e-03],
        [3.8745e-04, 9.8693e-01, 1.5973e-03,  ..., 1.5965e-03, 1.6577e-03,
         1.5520e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]
original label
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 5, 1, 1, 1, 8, 1, 1, 0, 1, 2, 1, 1, 3, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 8, 0, 8, 1, 0, 5, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 8,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 7, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[7.9671e-01, 1.7196e-01, 3.7568e-03,  ..., 3.8290e-03, 3.9370e-03,
         3.8684e-03],
        [9.8435e-01, 7.4633e-04, 1.9288e-03,  ..., 2.0007e-03, 1.7344e-03,
         1.8391e-03],
        [1.0823e-02, 9.8094e-01, 1.0463e-03,  ..., 9.2832e-04, 1.0240e-03,
         9.8819e-04],
        ...,
        [9.7968e-01, 6.6593e-03, 1.6560e-03,  ..., 1.6920e-03, 1.7611e-03,
         1.7654e-03],
        [3.5506e-03, 9.7254e-01, 3.2662e-03,  ..., 2.9724e-03, 3.1258e-03,
         2.9594e-03],
        [8.0903e-01, 1.6277e-01, 3.5041e-03,  ..., 3.4532e-03, 3.4046e-03,
         3.4297e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]
original label
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 8, 1, 0, 1, 1, 0, 7, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 3, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 8, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 6, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 6, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 6, 0])
soft_pseudo_label
tensor([[9.8480e-01, 5.9821e-04, 1.7480e-03,  ..., 1.7877e-03, 1.7608e-03,
         1.8453e-03],
        [2.7184e-04, 9.9091e-01, 1.2023e-03,  ..., 9.8324e-04, 1.1267e-03,
         1.0969e-03],
        [9.8001e-01, 7.4025e-03, 1.4357e-03,  ..., 1.5753e-03, 1.6056e-03,
         1.6001e-03],
        ...,
        [2.9476e-02, 9.5911e-01, 1.3493e-03,  ..., 1.4483e-03, 1.4654e-03,
         1.3726e-03],
        [8.7261e-02, 9.0397e-01, 1.0793e-03,  ..., 1.1158e-03, 9.7794e-04,
         1.1125e-03],
        [8.4614e-01, 1.3362e-01, 2.3329e-03,  ..., 2.5937e-03, 2.5899e-03,
         2.5065e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
original label
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 4, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 0, 5, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 9, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 8, 0, 7, 0, 1, 0, 0,
        1, 1, 8, 0, 0, 1, 0, 1, 0, 1, 3, 0, 3, 6, 0, 1, 7, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[2.9629e-03, 9.7679e-01, 2.4188e-03,  ..., 2.5925e-03, 2.7449e-03,
         2.6553e-03],
        [2.4902e-02, 9.6411e-01, 1.4069e-03,  ..., 1.4347e-03, 1.3637e-03,
         1.2892e-03],
        [1.6638e-03, 9.8936e-01, 1.0263e-03,  ..., 1.0882e-03, 1.1917e-03,
         1.1483e-03],
        ...,
        [6.1975e-03, 9.8775e-01, 7.8946e-04,  ..., 8.0700e-04, 6.8322e-04,
         6.7724e-04],
        [1.1878e-01, 8.4243e-01, 4.7752e-03,  ..., 4.8056e-03, 4.9004e-03,
         4.8765e-03],
        [4.6866e-02, 9.5243e-01, 9.3072e-05,  ..., 8.6078e-05, 8.4495e-05,
         9.3072e-05]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
original label
tensor([1, 1, 1, 8, 0, 1, 0, 0, 0, 6, 7, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 5, 2, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 7, 4, 3, 0, 0, 0, 0, 5, 0, 1, 0, 4, 1,
        1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        6, 1, 1, 1, 0, 7, 8, 1, 0, 3, 5, 0, 0, 8, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0,
        9, 9, 1, 1, 1, 1, 0, 7, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1])
soft_pseudo_label
tensor([[1.0057e-01, 8.7727e-01, 2.8584e-03,  ..., 2.7922e-03, 2.5250e-03,
         2.8237e-03],
        [2.6699e-01, 6.7647e-01, 7.0150e-03,  ..., 6.9944e-03, 7.1254e-03,
         6.8860e-03],
        [9.9877e-01, 1.0544e-03, 2.4129e-05,  ..., 2.2142e-05, 2.1756e-05,
         2.3455e-05],
        ...,
        [9.8747e-01, 4.2916e-03, 9.6416e-04,  ..., 1.0883e-03, 1.1146e-03,
         1.0144e-03],
        [9.8755e-01, 1.5018e-05, 1.6177e-03,  ..., 1.6753e-03, 1.5022e-03,
         1.5998e-03],
        [9.9111e-01, 2.3814e-03, 7.1844e-04,  ..., 7.9098e-04, 8.0935e-04,
         8.0935e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]
original label
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 7, 1, 9, 0, 8, 0, 1, 8, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 4, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 5, 3, 1, 0, 1, 0, 1, 0,
        0, 1, 2, 0, 1, 0, 0, 1, 0, 4, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 6, 0, 6,
        1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 7, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 3, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0])
soft_pseudo_label
tensor([[3.0352e-03, 9.6914e-01, 3.4833e-03,  ..., 3.2947e-03, 3.5904e-03,
         3.7793e-03],
        [4.1789e-02, 9.4926e-01, 1.0741e-03,  ..., 1.0889e-03, 1.2047e-03,
         1.1535e-03],
        [9.4801e-01, 3.6687e-02, 1.8589e-03,  ..., 1.8817e-03, 1.8526e-03,
         1.8863e-03],
        ...,
        [9.9533e-01, 4.1237e-03, 6.6979e-05,  ..., 7.9695e-05, 7.0262e-05,
         6.2921e-05],
        [3.8805e-02, 9.4754e-01, 1.6174e-03,  ..., 1.7066e-03, 1.8265e-03,
         1.7117e-03],
        [8.9176e-01, 8.7952e-02, 2.4241e-03,  ..., 2.5554e-03, 2.5467e-03,
         2.5256e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0]
original label
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 7, 1, 1, 0, 0, 9, 0, 1, 0, 1, 0, 1,
        1, 0, 3, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 3, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 9, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        8, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 6, 1, 0, 7, 1, 1, 0, 0, 1, 5,
        0, 1, 1, 1, 1, 0, 1, 0])
soft_pseudo_label
tensor([[5.3059e-01, 4.4333e-01, 3.2047e-03,  ..., 3.3113e-03, 3.1091e-03,
         3.3815e-03],
        [9.7885e-01, 1.3597e-03, 2.3085e-03,  ..., 2.4621e-03, 2.6075e-03,
         2.5415e-03],
        [9.8356e-01, 4.8471e-05, 1.9756e-03,  ..., 2.0160e-03, 2.1055e-03,
         1.9860e-03],
        ...,
        [9.8946e-01, 3.0570e-06, 1.3597e-03,  ..., 1.3475e-03, 1.2861e-03,
         1.3286e-03],
        [8.5922e-06, 9.9163e-01, 1.1696e-03,  ..., 1.0107e-03, 1.0292e-03,
         1.0865e-03],
        [9.6096e-01, 4.5690e-03, 4.0519e-03,  ..., 4.2901e-03, 4.1480e-03,
         4.4306e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0]
original label
tensor([2, 8, 0, 0, 0, 0, 1, 4, 1, 0, 2, 0, 0, 9, 0, 0, 1, 1, 3, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 5, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 9, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 8, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 4, 7, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0])
soft_pseudo_label
tensor([[9.8340e-01, 3.5734e-05, 2.0192e-03,  ..., 1.7635e-03, 1.9182e-03,
         2.3438e-03],
        [2.2180e-04, 9.8674e-01, 1.5381e-03,  ..., 1.7226e-03, 1.5646e-03,
         1.8332e-03],
        [2.1242e-01, 7.6944e-01, 2.2473e-03,  ..., 2.2760e-03, 2.1256e-03,
         2.3368e-03],
        ...,
        [8.8915e-01, 9.2083e-02, 2.3313e-03,  ..., 2.4123e-03, 2.2409e-03,
         2.3064e-03],
        [9.7793e-01, 9.1662e-03, 1.6717e-03,  ..., 1.5118e-03, 1.4383e-03,
         1.6322e-03],
        [8.7769e-01, 1.2112e-01, 1.5698e-04,  ..., 1.5440e-04, 1.5821e-04,
         1.4377e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]
original label
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 6, 1, 0, 1, 0, 1, 1, 0, 0, 1, 7, 0,
        5, 0, 0, 0, 1, 1, 0, 6, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 9, 1, 0,
        8, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 6, 1, 1, 1, 3, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 3, 1, 0, 1, 1, 1, 1, 0, 0, 5, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        6, 1, 1, 0, 1, 0, 1, 1, 8, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 8, 0, 4])
soft_pseudo_label
tensor([[0.4260, 0.5534, 0.0025,  ..., 0.0026, 0.0026, 0.0027],
        [0.0018, 0.9840, 0.0020,  ..., 0.0017, 0.0018, 0.0017],
        [0.0020, 0.9889, 0.0011,  ..., 0.0012, 0.0011, 0.0012],
        ...,
        [0.7732, 0.1568, 0.0091,  ..., 0.0087, 0.0087, 0.0084],
        [0.0021, 0.9868, 0.0013,  ..., 0.0014, 0.0015, 0.0013],
        [0.0035, 0.9882, 0.0012,  ..., 0.0011, 0.0011, 0.0010]],
       device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 6, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 3, 8, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1])
soft_pseudo_label
tensor([[5.5599e-01, 4.4327e-01, 8.5142e-05,  ..., 9.0280e-05, 9.4613e-05,
         9.8671e-05],
        [9.6473e-01, 6.5044e-03, 3.4641e-03,  ..., 3.4964e-03, 3.7474e-03,
         3.7547e-03],
        [4.0300e-01, 5.7278e-01, 2.9475e-03,  ..., 3.0679e-03, 3.0027e-03,
         2.9852e-03],
        ...,
        [2.8564e-01, 6.8654e-01, 3.3728e-03,  ..., 3.4326e-03, 3.4545e-03,
         3.4545e-03],
        [1.8279e-04, 9.7897e-01, 3.0769e-03,  ..., 2.7437e-03, 2.4284e-03,
         2.5996e-03],
        [6.6507e-01, 2.9860e-01, 4.4117e-03,  ..., 4.5340e-03, 4.6825e-03,
         4.7609e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]
original label
tensor([1, 0, 0, 0, 0, 0, 6, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 3, 9, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 8, 1, 1, 1, 1,
        6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 6, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 1, 0, 1, 0, 9, 1, 0, 1, 1, 4, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 7, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 6, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0])
soft_pseudo_label
tensor([[9.7411e-01, 4.7207e-03, 2.6223e-03,  ..., 2.7121e-03, 2.7227e-03,
         2.6120e-03],
        [9.9037e-01, 5.4562e-04, 1.1289e-03,  ..., 1.0339e-03, 1.1361e-03,
         1.0369e-03],
        [4.1913e-02, 9.1559e-01, 5.2588e-03,  ..., 5.4710e-03, 5.2383e-03,
         5.4257e-03],
        ...,
        [8.2363e-03, 9.7552e-01, 2.1644e-03,  ..., 1.9668e-03, 2.0452e-03,
         2.0233e-03],
        [4.1837e-01, 5.3825e-01, 5.2949e-03,  ..., 5.2230e-03, 5.6447e-03,
         5.4284e-03],
        [9.8593e-01, 8.8265e-03, 6.1071e-04,  ..., 7.0189e-04, 6.8933e-04,
         6.3566e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 1, 3, 1, 1, 1, 1, 0, 1, 0, 0, 7, 1, 0, 1, 0, 1, 9, 0,
        2, 0, 7, 1, 0, 0, 0, 0, 1, 1, 5, 0, 0, 0, 1, 6, 1, 1, 1, 0, 0, 1, 0, 1,
        3, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 3, 1,
        0, 5, 1, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 8, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 4, 0, 1, 1, 0, 0, 0, 1, 7, 6, 1, 0, 0, 9, 0, 1, 0, 0, 7, 0, 0,
        0, 0, 0, 2, 1, 1, 6, 0])
soft_pseudo_label
tensor([[7.7679e-01, 2.2256e-01, 8.2882e-05,  ..., 8.7541e-05, 7.7633e-05,
         7.6280e-05],
        [5.3130e-02, 9.2715e-01, 2.5364e-03,  ..., 2.4704e-03, 2.4120e-03,
         2.3094e-03],
        [1.4860e-02, 9.7012e-01, 1.9144e-03,  ..., 1.8627e-03, 1.8893e-03,
         1.6722e-03],
        ...,
        [1.6649e-02, 9.8287e-01, 6.4169e-05,  ..., 7.0201e-05, 5.7972e-05,
         5.7746e-05],
        [1.1137e-02, 9.7171e-01, 2.2528e-03,  ..., 2.1381e-03, 2.1931e-03,
         1.8306e-03],
        [9.7885e-01, 7.4518e-03, 1.6290e-03,  ..., 1.6474e-03, 1.7847e-03,
         1.7055e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]
original label
tensor([0, 1, 1, 0, 1, 5, 0, 3, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        2, 0, 1, 0, 1, 1, 1, 7, 0, 1, 0, 1, 1, 0, 1, 0, 1, 5, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 5, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 5, 1, 1, 0, 1,
        0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 5, 0, 0, 1, 1, 1, 0, 5, 1, 1, 1, 6, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 8])
soft_pseudo_label
tensor([[9.0808e-01, 5.8851e-02, 3.8439e-03,  ..., 4.1684e-03, 4.1889e-03,
         4.0088e-03],
        [2.9541e-03, 9.8032e-01, 2.1560e-03,  ..., 2.1825e-03, 2.0423e-03,
         1.8869e-03],
        [3.4188e-02, 9.4595e-01, 2.3482e-03,  ..., 2.4621e-03, 2.6222e-03,
         2.4086e-03],
        ...,
        [7.7294e-01, 1.9735e-01, 3.4155e-03,  ..., 3.8684e-03, 3.7494e-03,
         3.5188e-03],
        [9.9208e-01, 1.5659e-04, 9.3978e-04,  ..., 1.0154e-03, 9.5972e-04,
         9.3657e-04],
        [9.8050e-01, 4.5399e-04, 2.3106e-03,  ..., 2.1643e-03, 2.3459e-03,
         2.3840e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0]
original label
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 0, 2, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3,
        0, 0, 0, 8, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 7, 8, 8, 4,
        1, 1, 0, 0, 0, 2, 0, 0])
soft_pseudo_label
tensor([[9.9611e-01, 1.3081e-03, 3.0425e-04,  ..., 3.4225e-04, 3.2530e-04,
         3.5242e-04],
        [5.4899e-01, 4.3259e-01, 2.2436e-03,  ..., 2.2590e-03, 2.4330e-03,
         2.2723e-03],
        [9.9177e-01, 5.3426e-04, 8.3234e-04,  ..., 1.0460e-03, 1.1067e-03,
         8.8949e-04],
        ...,
        [3.3353e-02, 9.4658e-01, 2.5001e-03,  ..., 2.4710e-03, 2.6588e-03,
         2.5657e-03],
        [3.1504e-03, 9.8478e-01, 1.4183e-03,  ..., 1.5760e-03, 1.4519e-03,
         1.5807e-03],
        [6.3803e-01, 3.3622e-01, 3.0784e-03,  ..., 3.2010e-03, 3.2372e-03,
         3.2594e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]
original label
tensor([0, 0, 0, 0, 2, 1, 9, 1, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 5, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 7, 0, 0, 1, 1, 3,
        1, 1, 7, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 8, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 4, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 8, 1, 5, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0])
soft_pseudo_label
tensor([[3.3476e-01, 6.6316e-01, 2.4745e-04,  ..., 2.9674e-04, 2.4125e-04,
         2.4219e-04],
        [8.5428e-02, 9.0067e-01, 1.7001e-03,  ..., 1.7421e-03, 1.7235e-03,
         1.6721e-03],
        [9.6637e-01, 1.6069e-02, 2.1006e-03,  ..., 2.0781e-03, 2.2602e-03,
         2.3399e-03],
        ...,
        [9.8303e-01, 1.4290e-06, 2.1513e-03,  ..., 2.2955e-03, 2.1105e-03,
         2.1988e-03],
        [8.6931e-01, 9.1446e-02, 4.8726e-03,  ..., 5.0052e-03, 5.1139e-03,
         4.8299e-03],
        [6.0601e-05, 9.7278e-01, 3.7074e-03,  ..., 3.3526e-03, 3.2132e-03,
         3.4038e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]
original label
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 8, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 9, 1, 1, 7, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 9, 0, 8, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 5, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 8, 9, 1, 4, 0, 6, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[5.3295e-03, 9.7982e-01, 1.9620e-03,  ..., 1.8558e-03, 1.6906e-03,
         1.9736e-03],
        [9.7942e-01, 9.9814e-05, 2.2790e-03,  ..., 2.0404e-03, 2.8890e-03,
         2.2447e-03],
        [9.8841e-01, 1.0916e-02, 8.6835e-05,  ..., 8.6412e-05, 8.4824e-05,
         8.4989e-05],
        ...,
        [3.5272e-01, 6.4621e-01, 1.4120e-04,  ..., 1.4597e-04, 1.2970e-04,
         1.2882e-04],
        [3.2769e-01, 6.3287e-01, 4.8890e-03,  ..., 5.0025e-03, 5.1111e-03,
         4.6446e-03],
        [2.8189e-02, 9.4175e-01, 3.7139e-03,  ..., 3.7909e-03, 3.7049e-03,
         3.6403e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 7, 0, 0, 0, 1, 4, 4, 1, 1, 1, 0, 1, 1,
        1, 1, 4, 1, 8, 1, 0, 1, 0, 0, 0, 1, 1, 7, 1, 7, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 7, 1, 0, 1, 1, 5, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 2, 0, 3, 1, 1, 0, 1, 3, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        4, 7, 0, 1, 1, 8, 0, 1])
soft_pseudo_label
tensor([[9.2577e-01, 6.1420e-02, 1.6020e-03,  ..., 1.6177e-03, 1.4991e-03,
         1.6504e-03],
        [1.4314e-02, 9.5520e-01, 4.0178e-03,  ..., 3.8114e-03, 4.0473e-03,
         3.6618e-03],
        [9.7977e-01, 1.7945e-02, 2.7301e-04,  ..., 2.7867e-04, 2.8306e-04,
         2.8278e-04],
        ...,
        [1.1297e-02, 9.7506e-01, 1.7248e-03,  ..., 1.6482e-03, 1.7787e-03,
         1.7030e-03],
        [1.4132e-01, 8.4731e-01, 1.4253e-03,  ..., 1.4170e-03, 1.4449e-03,
         1.3363e-03],
        [2.5789e-03, 9.9373e-01, 4.7915e-04,  ..., 5.1405e-04, 4.7519e-04,
         4.7962e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1]
original label
tensor([0, 1, 0, 6, 1, 0, 6, 0, 1, 0, 1, 1, 5, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 5,
        1, 1, 1, 0, 0, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 3, 0, 0, 0, 9, 0, 1, 3, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 5, 5, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 4, 1, 1, 1])
soft_pseudo_label
tensor([[0.9886, 0.0020, 0.0011,  ..., 0.0012, 0.0011, 0.0012],
        [0.0913, 0.8815, 0.0034,  ..., 0.0035, 0.0033, 0.0036],
        [0.9797, 0.0030, 0.0021,  ..., 0.0022, 0.0022, 0.0024],
        ...,
        [0.5682, 0.3669, 0.0082,  ..., 0.0080, 0.0082, 0.0082],
        [0.0961, 0.8799, 0.0030,  ..., 0.0031, 0.0030, 0.0031],
        [0.9697, 0.0010, 0.0036,  ..., 0.0041, 0.0036, 0.0036]],
       device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0]
original label
tensor([0, 1, 0, 5, 1, 1, 1, 1, 0, 1, 0, 9, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 3, 0, 8, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 5, 0, 1, 6, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 8, 0, 0, 1, 9, 0,
        4, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 9, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 5, 7, 7, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 8, 3, 0])
soft_pseudo_label
tensor([[9.6911e-01, 1.3101e-02, 2.0891e-03,  ..., 2.2866e-03, 2.3147e-03,
         2.2743e-03],
        [9.6898e-01, 1.4642e-02, 1.8416e-03,  ..., 2.1572e-03, 2.0615e-03,
         2.1741e-03],
        [2.8468e-03, 9.8530e-01, 1.5165e-03,  ..., 1.5083e-03, 1.4274e-03,
         1.5600e-03],
        ...,
        [9.8670e-01, 4.8538e-04, 1.5569e-03,  ..., 1.7045e-03, 1.7595e-03,
         1.4498e-03],
        [9.9578e-01, 4.1175e-03, 1.4435e-05,  ..., 1.2191e-05, 1.0603e-05,
         1.2085e-05],
        [9.8192e-01, 3.1248e-04, 2.2401e-03,  ..., 2.0546e-03, 2.0596e-03,
         2.2351e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]
original label
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 8, 1, 4, 2, 0, 0, 1, 0, 0, 1, 5, 0, 9, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 4, 1, 0, 1, 0, 1, 1, 1, 0,
        9, 0, 0, 1, 1, 1, 4, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 8, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 5, 0, 5, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 8, 9, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.9341e-01, 3.3591e-03, 4.0991e-04,  ..., 3.8696e-04, 4.0931e-04,
         3.8432e-04],
        [1.0705e-03, 9.7797e-01, 2.7929e-03,  ..., 2.8858e-03, 2.6455e-03,
         2.6391e-03],
        [1.4044e-01, 7.8640e-01, 8.4880e-03,  ..., 9.7506e-03, 9.0930e-03,
         9.2002e-03],
        ...,
        [6.8604e-01, 3.1165e-01, 2.9464e-04,  ..., 3.1611e-04, 2.7518e-04,
         2.8558e-04],
        [1.3709e-01, 7.9506e-01, 8.2770e-03,  ..., 8.5773e-03, 8.3134e-03,
         8.6869e-03],
        [1.6493e-02, 9.6513e-01, 2.4961e-03,  ..., 2.3369e-03, 2.1857e-03,
         2.3506e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1]
original label
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 8, 0, 4, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 7, 1, 0, 1, 7, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 3, 1, 1, 0, 7, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 4, 8, 1,
        0, 0, 0, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.4042e-01, 4.6638e-02, 1.5734e-03,  ..., 1.5982e-03, 1.6538e-03,
         1.6798e-03],
        [9.9299e-01, 1.8360e-04, 8.4113e-04,  ..., 8.1684e-04, 7.9326e-04,
         9.9448e-04],
        [9.1253e-01, 5.3689e-02, 4.0501e-03,  ..., 4.4007e-03, 4.1462e-03,
         4.1685e-03],
        ...,
        [7.6570e-02, 8.9183e-01, 4.0363e-03,  ..., 4.0010e-03, 4.0799e-03,
         3.8684e-03],
        [1.1546e-03, 9.8610e-01, 1.6426e-03,  ..., 1.6418e-03, 1.4775e-03,
         1.6652e-03],
        [7.8679e-01, 1.8871e-01, 2.9106e-03,  ..., 3.0998e-03, 3.0517e-03,
         3.0488e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]
original label
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 8, 0, 0, 0, 7, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 9, 1, 0, 1, 1,
        1, 5, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 9, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 8, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 4,
        1, 5, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2,
        1, 0, 1, 1, 0, 1, 1, 1])
soft_pseudo_label
tensor([[9.9800e-01, 2.0462e-05, 2.6349e-04,  ..., 2.1043e-04, 2.3510e-04,
         2.7586e-04],
        [5.3561e-04, 9.8366e-01, 1.8414e-03,  ..., 2.1050e-03, 1.9129e-03,
         2.1014e-03],
        [9.9832e-01, 7.1245e-04, 1.2417e-04,  ..., 1.0788e-04, 1.1196e-04,
         1.2949e-04],
        ...,
        [3.9222e-01, 5.9806e-01, 1.2005e-03,  ..., 1.1958e-03, 1.2855e-03,
         1.2484e-03],
        [8.8382e-02, 9.1023e-01, 1.8538e-04,  ..., 1.8812e-04, 1.5950e-04,
         1.6328e-04],
        [6.8832e-01, 3.1025e-01, 1.7739e-04,  ..., 1.8177e-04, 1.7826e-04,
         1.7687e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]
original label
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 9, 7, 0, 4, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 6,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 4, 1, 6, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 4, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 7, 0, 0, 0, 1, 6, 1, 1, 1, 0, 4, 7, 1, 1, 0, 0, 0, 1, 0, 0,
        9, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 5, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 2, 0, 1])
soft_pseudo_label
tensor([[1.5058e-04, 9.8237e-01, 2.7593e-03,  ..., 2.3190e-03, 2.1156e-03,
         2.4386e-03],
        [8.9458e-04, 9.8679e-01, 1.2883e-03,  ..., 1.5003e-03, 1.6709e-03,
         1.6906e-03],
        [9.9029e-01, 9.3703e-05, 1.2583e-03,  ..., 1.2409e-03, 1.1931e-03,
         1.2051e-03],
        ...,
        [9.1897e-01, 6.2050e-02, 2.3365e-03,  ..., 2.3468e-03, 2.3093e-03,
         2.4462e-03],
        [3.2458e-02, 9.6163e-01, 7.3986e-04,  ..., 7.5889e-04, 7.7689e-04,
         6.9504e-04],
        [9.0908e-01, 5.3070e-02, 4.6034e-03,  ..., 4.7541e-03, 5.0263e-03,
         4.4465e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]
original label
tensor([1, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 6, 0, 1, 1, 1, 0,
        7, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 3, 0, 0, 1, 0, 2, 1, 1, 0, 0, 1, 0, 6, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 2, 0, 0, 0, 0, 1, 5, 1, 0, 1, 1, 0, 1, 7, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 6, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 9, 1, 1])
soft_pseudo_label
tensor([[2.5760e-01, 7.2670e-01, 1.9100e-03,  ..., 1.8694e-03, 2.0056e-03,
         2.0592e-03],
        [9.7043e-01, 6.3395e-03, 2.9115e-03,  ..., 2.8260e-03, 3.0542e-03,
         2.9386e-03],
        [1.6876e-03, 9.9283e-01, 6.2558e-04,  ..., 7.2534e-04, 6.3234e-04,
         7.5607e-04],
        ...,
        [4.4305e-01, 5.4284e-01, 1.7227e-03,  ..., 1.8554e-03, 1.8195e-03,
         1.7362e-03],
        [9.8770e-01, 7.3152e-04, 1.4995e-03,  ..., 1.4445e-03, 1.3647e-03,
         1.5642e-03],
        [9.7584e-01, 5.2406e-05, 3.0911e-03,  ..., 2.8186e-03, 2.8553e-03,
         3.1780e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0]
original label
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 7, 1, 6, 2, 1, 2, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 6,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 8, 1, 0, 7, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 6, 3, 1, 1, 1, 0, 1, 5, 0, 1, 0, 1, 0, 1, 9, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 8, 3, 0, 1, 1, 1, 6,
        0, 1, 0, 1, 1, 7, 0, 0])
soft_pseudo_label
tensor([[9.7057e-01, 1.3392e-02, 1.9828e-03,  ..., 1.9799e-03, 2.1066e-03,
         2.1262e-03],
        [7.2735e-01, 2.5733e-01, 1.9043e-03,  ..., 1.9136e-03, 1.8674e-03,
         1.8729e-03],
        [9.7876e-01, 6.0029e-05, 2.5826e-03,  ..., 2.6545e-03, 2.8343e-03,
         2.4049e-03],
        ...,
        [4.3507e-03, 9.8554e-01, 1.3471e-03,  ..., 1.3670e-03, 1.1917e-03,
         1.2575e-03],
        [9.7405e-01, 4.0666e-04, 3.8531e-03,  ..., 3.1722e-03, 2.5708e-03,
         3.0573e-03],
        [1.5442e-01, 8.3642e-01, 1.1349e-03,  ..., 1.1539e-03, 1.0766e-03,
         1.1394e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]
original label
tensor([0, 6, 3, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 4, 0, 1, 1, 3, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 3, 1, 0, 0,
        1, 0, 0, 1, 0, 6, 0, 1, 1, 0, 1, 8, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 6, 0, 2, 0, 1, 1, 3, 0, 1, 1, 1, 1, 7, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0])
soft_pseudo_label
tensor([[0.0133, 0.9773, 0.0012,  ..., 0.0012, 0.0012, 0.0011],
        [0.6610, 0.3104, 0.0036,  ..., 0.0035, 0.0034, 0.0036],
        [0.0874, 0.8880, 0.0030,  ..., 0.0031, 0.0030, 0.0030],
        ...,
        [0.0122, 0.9763, 0.0015,  ..., 0.0015, 0.0013, 0.0013],
        [0.2045, 0.7644, 0.0038,  ..., 0.0039, 0.0038, 0.0040],
        [0.0098, 0.9777, 0.0015,  ..., 0.0016, 0.0016, 0.0016]],
       device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 1, 0, 6, 1, 1, 5, 1, 0, 0, 1, 5, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 8, 7, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 3, 0, 0, 1, 0, 0, 1,
        1, 6, 7, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 7, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 5, 0, 0, 1, 0, 1, 0, 0, 1, 4, 3, 1, 0, 1, 0, 0,
        8, 3, 0, 1, 0, 1, 0, 3, 0, 0, 0, 7, 0, 1, 0, 0, 3, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1])
soft_pseudo_label
tensor([[1.4330e-01, 8.5414e-01, 3.1965e-04,  ..., 3.2028e-04, 3.3141e-04,
         2.9766e-04],
        [2.1319e-03, 9.7792e-01, 2.5157e-03,  ..., 2.3911e-03, 2.6210e-03,
         2.6069e-03],
        [9.7595e-01, 1.9825e-02, 5.4508e-04,  ..., 5.1507e-04, 5.4615e-04,
         5.6459e-04],
        ...,
        [2.4714e-03, 9.9170e-01, 7.6487e-04,  ..., 7.2558e-04, 7.6040e-04,
         6.5936e-04],
        [3.2834e-01, 6.5809e-01, 1.7079e-03,  ..., 1.6814e-03, 1.7484e-03,
         1.7095e-03],
        [9.7957e-01, 1.0198e-05, 2.5550e-03,  ..., 2.2332e-03, 2.3347e-03,
         2.4585e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0]
original label
tensor([1, 1, 0, 0, 7, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 8, 1,
        0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 3, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 8, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 3, 1, 1, 1, 0])
soft_pseudo_label
tensor([[1.8679e-04, 9.9070e-01, 1.1047e-03,  ..., 1.0429e-03, 1.1437e-03,
         1.1414e-03],
        [1.5400e-02, 9.5418e-01, 3.8353e-03,  ..., 3.8673e-03, 3.7520e-03,
         3.6436e-03],
        [2.2286e-01, 7.5687e-01, 2.5419e-03,  ..., 2.5000e-03, 2.4113e-03,
         2.7111e-03],
        ...,
        [9.9564e-01, 6.9425e-04, 4.7773e-04,  ..., 4.2698e-04, 4.4356e-04,
         4.3392e-04],
        [2.4500e-04, 9.8662e-01, 1.5082e-03,  ..., 1.7654e-03, 1.4833e-03,
         2.0240e-03],
        [6.8150e-01, 3.1817e-01, 4.2002e-05,  ..., 4.3975e-05, 4.0869e-05,
         4.3166e-05]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]
original label
tensor([1, 1, 1, 1, 1, 0, 0, 1, 6, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 4, 1, 9, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 6, 1, 0, 0, 1, 1, 1, 0, 5, 1, 0,
        0, 1, 1, 0, 0, 6, 1, 0, 2, 1, 1, 4, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 5, 0, 0, 0,
        3, 0, 1, 1, 1, 1, 1, 1, 0, 7, 1, 0, 1, 5, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 3])
soft_pseudo_label
tensor([[2.1511e-03, 9.8649e-01, 1.4495e-03,  ..., 1.4008e-03, 1.4530e-03,
         1.2854e-03],
        [9.1693e-04, 9.7937e-01, 2.8237e-03,  ..., 2.2375e-03, 2.7812e-03,
         2.3495e-03],
        [9.5954e-01, 2.0367e-02, 2.4396e-03,  ..., 2.5097e-03, 2.5467e-03,
         2.4975e-03],
        ...,
        [5.4972e-01, 4.2729e-01, 2.7959e-03,  ..., 2.8539e-03, 2.6993e-03,
         2.8317e-03],
        [1.6979e-02, 9.6395e-01, 2.3535e-03,  ..., 2.3917e-03, 2.1734e-03,
         2.5911e-03],
        [3.3608e-02, 9.5569e-01, 1.3953e-03,  ..., 1.3353e-03, 1.3577e-03,
         1.3108e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1]
original label
tensor([1, 1, 2, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 6, 0, 0, 1, 0, 1, 0, 4, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 7, 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 0, 0, 7,
        3, 1, 0, 0, 1, 0, 7, 1, 0, 1, 7, 1, 0, 7, 0, 0, 1, 1, 0, 1, 1, 0, 0, 4,
        3, 0, 1, 0, 0, 0, 2, 0, 3, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[1.5344e-03, 9.8343e-01, 2.2539e-03,  ..., 1.8365e-03, 1.7992e-03,
         1.8500e-03],
        [6.0026e-04, 9.9010e-01, 1.1799e-03,  ..., 1.0281e-03, 1.1463e-03,
         1.1313e-03],
        [9.6820e-01, 4.2752e-03, 3.2548e-03,  ..., 3.3093e-03, 3.2358e-03,
         3.4143e-03],
        ...,
        [9.6646e-01, 7.3189e-03, 3.0926e-03,  ..., 3.0062e-03, 3.4215e-03,
         3.2142e-03],
        [4.6245e-02, 9.3980e-01, 1.7118e-03,  ..., 1.7567e-03, 1.6600e-03,
         1.7714e-03],
        [9.1083e-01, 6.3205e-02, 3.0995e-03,  ..., 3.1808e-03, 3.3059e-03,
         3.2834e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0]
original label
tensor([1, 1, 0, 0, 8, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 4, 1, 0, 0, 0, 0, 1,
        6, 1, 0, 0, 7, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 8, 1, 1, 0, 1, 1, 1, 6, 1, 1, 1,
        1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 7, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 9, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0])
soft_pseudo_label
tensor([[2.4640e-04, 9.8838e-01, 1.9770e-03,  ..., 1.2729e-03, 1.4041e-03,
         1.3088e-03],
        [6.1416e-01, 3.8433e-01, 1.7691e-04,  ..., 1.8869e-04, 1.9073e-04,
         1.9813e-04],
        [9.4974e-01, 3.8518e-02, 1.5244e-03,  ..., 1.4363e-03, 1.4044e-03,
         1.4725e-03],
        ...,
        [3.2694e-02, 9.5266e-01, 1.9189e-03,  ..., 1.8141e-03, 1.7531e-03,
         1.9548e-03],
        [7.4470e-02, 9.1972e-01, 7.2157e-04,  ..., 7.4666e-04, 7.0417e-04,
         6.8853e-04],
        [3.9521e-03, 9.9579e-01, 3.4292e-05,  ..., 3.6362e-05, 3.2754e-05,
         3.1071e-05]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 1, 0, 8, 1, 0, 8, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 1, 0, 7, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 7, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 8, 0, 0, 1, 1, 1, 0, 0, 0, 3, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1])
soft_pseudo_label
tensor([[3.9690e-03, 9.8889e-01, 8.6593e-04,  ..., 9.5477e-04, 9.1372e-04,
         8.8560e-04],
        [9.9255e-01, 1.2344e-03, 7.9329e-04,  ..., 8.1648e-04, 8.0815e-04,
         7.9174e-04],
        [2.6782e-01, 6.9603e-01, 4.5190e-03,  ..., 4.5345e-03, 4.4642e-03,
         4.4229e-03],
        ...,
        [7.0243e-03, 9.7886e-01, 1.7882e-03,  ..., 1.7726e-03, 1.8342e-03,
         1.6530e-03],
        [6.5552e-01, 3.4409e-01, 5.3522e-05,  ..., 4.9115e-05, 5.0084e-05,
         4.9840e-05],
        [1.5348e-02, 9.6731e-01, 2.1088e-03,  ..., 2.2197e-03, 2.1294e-03,
         2.1420e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1]
original label
tensor([1, 0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 5, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 4, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 8, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 5, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 9, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 6, 0, 1, 1, 3, 1, 1, 6, 1, 1, 0,
        1, 0, 0, 0, 5, 1, 0, 1])
soft_pseudo_label
tensor([[9.8988e-01, 1.3933e-03, 1.0846e-03,  ..., 9.6321e-04, 1.0425e-03,
         1.1097e-03],
        [5.3657e-01, 4.2695e-01, 4.4253e-03,  ..., 4.6558e-03, 4.6241e-03,
         4.3973e-03],
        [9.9831e-01, 1.4748e-03, 2.7436e-05,  ..., 2.4666e-05, 2.3977e-05,
         2.8197e-05],
        ...,
        [4.2804e-02, 9.3872e-01, 2.3099e-03,  ..., 2.5555e-03, 2.3984e-03,
         2.2432e-03],
        [9.9050e-01, 2.0869e-06, 1.0453e-03,  ..., 1.3372e-03, 1.0596e-03,
         1.1433e-03],
        [7.1065e-01, 2.5940e-01, 3.6713e-03,  ..., 3.7128e-03, 3.7934e-03,
         3.8194e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]
original label
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 6, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 5, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 3, 9, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 4, 0, 9, 0, 1, 1, 0, 8, 1, 0, 1, 1, 0, 0, 7, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 7, 0, 1, 1, 0, 1, 0, 1, 8, 1, 0, 0, 0, 0, 0, 4,
        1, 1, 8, 0, 0, 1, 0, 0])
soft_pseudo_label
tensor([[0.8259, 0.1525, 0.0025,  ..., 0.0028, 0.0027, 0.0027],
        [0.0110, 0.9609, 0.0037,  ..., 0.0036, 0.0036, 0.0034],
        [0.0642, 0.9266, 0.0011,  ..., 0.0012, 0.0012, 0.0012],
        ...,
        [0.9706, 0.0120, 0.0021,  ..., 0.0024, 0.0023, 0.0022],
        [0.0614, 0.9127, 0.0033,  ..., 0.0033, 0.0032, 0.0032],
        [0.0055, 0.9714, 0.0030,  ..., 0.0030, 0.0029, 0.0029]],
       device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1]
original label
tensor([6, 1, 0, 1, 1, 0, 1, 1, 0, 9, 4, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 4, 0, 5,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 4,
        0, 6, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1])
soft_pseudo_label
tensor([[9.5024e-03, 9.7974e-01, 1.2879e-03,  ..., 1.3696e-03, 1.3178e-03,
         1.3063e-03],
        [6.9135e-02, 9.1068e-01, 2.5442e-03,  ..., 2.5392e-03, 2.6071e-03,
         2.4194e-03],
        [7.7906e-01, 1.9659e-01, 2.8207e-03,  ..., 3.0218e-03, 3.0544e-03,
         2.9909e-03],
        ...,
        [9.9586e-01, 1.2653e-05, 5.7120e-04,  ..., 6.0441e-04, 4.9602e-04,
         5.4624e-04],
        [9.9548e-01, 4.1405e-03, 5.1313e-05,  ..., 4.6538e-05, 4.3041e-05,
         4.3379e-05],
        [6.6272e-02, 9.3108e-01, 3.2320e-04,  ..., 3.4607e-04, 3.2637e-04,
         3.3971e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]
original label
tensor([1, 1, 0, 0, 9, 5, 0, 8, 1, 1, 0, 1, 4, 1, 1, 1, 1, 0, 0, 0, 0, 7, 0, 0,
        0, 0, 1, 1, 0, 3, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 5, 0, 1, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 5, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 9, 1, 9, 0, 0, 1])
soft_pseudo_label
tensor([[0.0024, 0.9815, 0.0022,  ..., 0.0020, 0.0023, 0.0018],
        [0.3491, 0.6346, 0.0020,  ..., 0.0021, 0.0021, 0.0021],
        [0.0850, 0.8996, 0.0019,  ..., 0.0020, 0.0018, 0.0019],
        ...,
        [0.9485, 0.0245, 0.0033,  ..., 0.0033, 0.0036, 0.0034],
        [0.9228, 0.0523, 0.0029,  ..., 0.0030, 0.0031, 0.0030],
        [0.9789, 0.0021, 0.0022,  ..., 0.0026, 0.0024, 0.0024]],
       device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]
original label
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 7, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 5, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 4, 1, 0, 1, 0, 0, 1, 1, 0, 1, 9, 0, 0, 1, 7, 1,
        1, 6, 0, 8, 0, 0, 0, 4, 9, 0, 0, 1, 0, 0, 0, 1, 1, 6, 1, 0, 0, 0, 0, 1,
        3, 6, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 3, 1, 6, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 1, 1, 1, 0, 5, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0])
soft_pseudo_label
tensor([[2.9509e-02, 9.4808e-01, 2.7221e-03,  ..., 2.7758e-03, 2.7812e-03,
         2.8113e-03],
        [1.0070e-01, 8.9402e-01, 6.6343e-04,  ..., 6.7059e-04, 6.7651e-04,
         6.4806e-04],
        [9.9023e-01, 2.0471e-03, 9.5094e-04,  ..., 9.4862e-04, 9.7396e-04,
         1.0490e-03],
        ...,
        [5.5646e-03, 9.5801e-01, 4.4975e-03,  ..., 4.5840e-03, 4.7226e-03,
         4.8773e-03],
        [8.6607e-02, 9.1310e-01, 4.0812e-05,  ..., 3.8078e-05, 3.3967e-05,
         3.9134e-05],
        [6.0173e-01, 3.6711e-01, 3.6754e-03,  ..., 3.8896e-03, 4.0485e-03,
         3.7977e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0]
original label
tensor([0, 4, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 9, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 3, 1, 0, 6, 1, 0, 2, 2, 9, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 9, 1,
        0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 5, 0, 0, 1, 0, 1, 6, 1, 9, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 5, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 9,
        0, 1, 0, 0, 1, 1, 1, 0])
soft_pseudo_label
tensor([[4.0436e-03, 9.7744e-01, 2.4622e-03,  ..., 2.2683e-03, 2.2190e-03,
         2.4767e-03],
        [9.9591e-01, 7.9784e-08, 5.3284e-04,  ..., 4.4973e-04, 4.4899e-04,
         4.7652e-04],
        [9.9370e-01, 1.0428e-04, 8.1983e-04,  ..., 7.2279e-04, 7.3309e-04,
         8.0220e-04],
        ...,
        [3.1532e-01, 6.7014e-01, 1.6873e-03,  ..., 1.8280e-03, 1.7769e-03,
         1.8713e-03],
        [2.3067e-04, 9.7922e-01, 2.5481e-03,  ..., 2.7747e-03, 2.5190e-03,
         2.9207e-03],
        [7.9969e-03, 9.8681e-01, 6.6904e-04,  ..., 6.3809e-04, 6.0828e-04,
         6.5482e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 0, 1, 0, 4, 1, 0, 0, 1, 1, 0, 1, 1, 7, 1, 5, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 8, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 9, 0, 1, 0, 6, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 9, 1, 2, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[9.6359e-01, 1.7666e-02, 2.4131e-03,  ..., 2.3469e-03, 2.2791e-03,
         2.3757e-03],
        [6.6427e-01, 3.0651e-01, 3.5545e-03,  ..., 3.5719e-03, 3.6140e-03,
         3.4756e-03],
        [8.7383e-01, 8.3777e-02, 5.1179e-03,  ..., 5.2444e-03, 5.3218e-03,
         5.2675e-03],
        ...,
        [1.7493e-01, 8.2482e-01, 3.7192e-05,  ..., 3.1472e-05, 2.9507e-05,
         2.9594e-05],
        [6.0659e-01, 3.6576e-01, 3.3752e-03,  ..., 3.4367e-03, 3.4993e-03,
         3.5684e-03],
        [3.0805e-01, 6.7812e-01, 1.6024e-03,  ..., 1.6941e-03, 1.6776e-03,
         1.8954e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1]
original label
tensor([0, 0, 0, 0, 0, 1, 5, 1, 1, 1, 0, 1, 9, 1, 0, 0, 1, 0, 7, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 4, 0, 1, 0, 1, 0, 0, 1, 6, 0, 1, 0, 1, 0, 2, 2, 1,
        1, 1, 1, 0, 6, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 6, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 4, 0, 1, 1, 0, 0, 0, 4, 0, 1, 6, 1, 1, 1,
        1, 1, 1, 5, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 5, 1, 0,
        1, 0, 0, 0, 0, 4, 1, 0])
soft_pseudo_label
tensor([[9.8877e-04, 9.8152e-01, 2.2540e-03,  ..., 1.9190e-03, 2.3278e-03,
         1.9655e-03],
        [1.4937e-01, 8.3476e-01, 1.9362e-03,  ..., 2.0252e-03, 2.0681e-03,
         1.9899e-03],
        [5.8121e-02, 9.4170e-01, 2.3064e-05,  ..., 2.3587e-05, 2.3749e-05,
         2.0918e-05],
        ...,
        [9.9532e-01, 3.6953e-05, 5.6603e-04,  ..., 5.4942e-04, 6.1502e-04,
         5.9291e-04],
        [9.7083e-01, 1.0616e-03, 3.2812e-03,  ..., 3.4294e-03, 3.5713e-03,
         3.5400e-03],
        [6.7523e-01, 3.0854e-01, 2.0628e-03,  ..., 1.9549e-03, 1.9818e-03,
         2.0973e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]
original label
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 1, 0, 0, 1, 1, 6,
        0, 1, 1, 8, 0, 1, 1, 1, 0, 5, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 5, 0, 0,
        1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 4,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 6, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 6, 2, 1, 1, 8, 0, 1, 0, 1, 1, 0, 0, 0, 1, 9, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[0.8640, 0.1133, 0.0027,  ..., 0.0027, 0.0028, 0.0029],
        [0.9498, 0.0380, 0.0015,  ..., 0.0016, 0.0015, 0.0017],
        [0.0730, 0.9173, 0.0012,  ..., 0.0013, 0.0012, 0.0013],
        ...,
        [0.0054, 0.9773, 0.0022,  ..., 0.0022, 0.0023, 0.0021],
        [0.2810, 0.6996, 0.0024,  ..., 0.0024, 0.0023, 0.0026],
        [0.0037, 0.9766, 0.0021,  ..., 0.0025, 0.0027, 0.0025]],
       device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
original label
tensor([5, 0, 1, 0, 0, 9, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 5, 1, 3,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 6, 0, 1, 0, 0,
        0, 0, 0, 9, 0, 0, 0, 0, 1, 1, 0, 1, 6, 5, 1, 0, 0, 1, 9, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 5, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 7, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 8, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        4, 1, 3, 0, 1, 7, 1, 1])
soft_pseudo_label
tensor([[9.8920e-01, 8.4395e-05, 1.3818e-03,  ..., 1.5282e-03, 1.1794e-03,
         1.3893e-03],
        [1.4381e-01, 8.2919e-01, 3.4237e-03,  ..., 3.3607e-03, 3.4505e-03,
         3.2131e-03],
        [2.1983e-03, 9.8285e-01, 1.8167e-03,  ..., 2.0565e-03, 1.8762e-03,
         1.8256e-03],
        ...,
        [5.7261e-04, 9.8789e-01, 1.5489e-03,  ..., 1.4117e-03, 1.5680e-03,
         1.4083e-03],
        [4.5589e-01, 4.9198e-01, 6.4808e-03,  ..., 6.4084e-03, 6.6865e-03,
         6.2538e-03],
        [2.7563e-01, 7.1213e-01, 1.5113e-03,  ..., 1.5963e-03, 1.4835e-03,
         1.5593e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1]
original label
tensor([0, 1, 1, 3, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 7, 0, 4, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 4, 0, 1, 0, 1, 4, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 6, 1, 1,
        0, 5, 0, 1, 0, 1, 1, 6, 7, 0, 1, 0, 1, 7, 0, 0, 1, 0, 1, 0, 0, 7, 5, 1,
        1, 5, 0, 1, 0, 0, 1, 1, 1, 1, 0, 3, 1, 1, 0, 0, 1, 6, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 5, 1, 1, 0])
soft_pseudo_label
tensor([[9.8465e-01, 3.2324e-05, 1.8331e-03,  ..., 2.2593e-03, 1.9367e-03,
         1.9038e-03],
        [9.4805e-01, 3.8075e-02, 1.6128e-03,  ..., 1.6697e-03, 1.7447e-03,
         1.6270e-03],
        [4.7470e-04, 9.8307e-01, 2.0797e-03,  ..., 2.2830e-03, 1.8520e-03,
         2.1327e-03],
        ...,
        [9.8623e-01, 6.1319e-05, 1.6067e-03,  ..., 1.6975e-03, 1.7748e-03,
         1.7574e-03],
        [1.1223e-04, 9.9103e-01, 1.2247e-03,  ..., 1.0981e-03, 1.2495e-03,
         1.0576e-03],
        [9.8969e-01, 1.2924e-05, 1.3929e-03,  ..., 1.1837e-03, 1.1804e-03,
         1.3562e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 0, 8, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 9, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 7, 1, 9, 1, 0, 0, 1, 0, 2,
        0, 0, 1, 1, 1, 0, 4, 8, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 5, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        3, 0, 1, 0, 0, 5, 2, 9, 0, 1, 6, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 4, 0, 0, 1, 0])
soft_pseudo_label
tensor([[9.8081e-01, 1.0032e-06, 2.3281e-03,  ..., 2.2302e-03, 2.1983e-03,
         2.5435e-03],
        [9.9224e-01, 6.5269e-07, 9.3410e-04,  ..., 8.5609e-04, 8.7889e-04,
         8.8949e-04],
        [9.2862e-01, 4.5918e-02, 3.1709e-03,  ..., 3.1570e-03, 3.1187e-03,
         3.5201e-03],
        ...,
        [9.4934e-01, 2.6852e-02, 2.9588e-03,  ..., 2.8594e-03, 3.0542e-03,
         3.1068e-03],
        [7.5186e-02, 9.1060e-01, 1.7239e-03,  ..., 1.8181e-03, 1.8676e-03,
         1.6988e-03],
        [3.4371e-01, 6.5479e-01, 1.8954e-04,  ..., 1.8606e-04, 1.9159e-04,
         1.9825e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]
original label
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 9, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        5, 1, 0, 1, 1, 1, 1, 0, 1, 0, 3, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 3,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 4, 0, 1, 0, 1, 1, 1, 1, 1, 1, 5, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[8.9693e-01, 1.0142e-01, 1.9810e-04,  ..., 2.2035e-04, 2.0903e-04,
         1.9868e-04],
        [8.4353e-01, 1.1461e-01, 4.9647e-03,  ..., 5.1248e-03, 5.3576e-03,
         5.2029e-03],
        [9.9373e-01, 1.5655e-04, 6.9887e-04,  ..., 8.0517e-04, 7.6158e-04,
         7.9230e-04],
        ...,
        [9.7485e-01, 8.3265e-04, 2.7529e-03,  ..., 2.9780e-03, 3.0051e-03,
         3.3589e-03],
        [1.3447e-02, 9.6792e-01, 2.2849e-03,  ..., 2.5107e-03, 2.1454e-03,
         2.3528e-03],
        [1.2424e-03, 9.9780e-01, 1.2680e-04,  ..., 1.2338e-04, 1.1912e-04,
         1.0772e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]
original label
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 6, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 8,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 5, 1, 0, 7, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 9, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 8, 0, 1, 9, 0,
        0, 1, 0, 8, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 4, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 3])
soft_pseudo_label
tensor([[2.3401e-04, 9.9914e-01, 7.8453e-05,  ..., 7.1642e-05, 9.1319e-05,
         9.0476e-05],
        [9.8353e-01, 3.2546e-04, 1.9038e-03,  ..., 2.0736e-03, 1.9864e-03,
         2.1011e-03],
        [5.9941e-01, 3.7218e-01, 3.3944e-03,  ..., 3.6577e-03, 3.5193e-03,
         3.5313e-03],
        ...,
        [4.7370e-01, 4.6910e-01, 6.7537e-03,  ..., 7.0847e-03, 7.3454e-03,
         7.0159e-03],
        [9.7749e-01, 2.1045e-05, 3.1628e-03,  ..., 3.0266e-03, 2.4710e-03,
         2.9177e-03],
        [7.8142e-04, 9.8440e-01, 2.0135e-03,  ..., 1.7931e-03, 2.0588e-03,
         1.7835e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]
original label
tensor([1, 0, 1, 1, 1, 1, 5, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 7, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 8, 1, 0, 0, 1, 4,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 1, 0, 1, 1, 1, 1, 5, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 8, 1, 0, 1, 1, 1, 7, 1, 1, 6, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 4, 1, 0, 0, 1, 4, 1, 0, 1, 1,
        1, 0, 7, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[1.0223e-01, 8.9707e-01, 8.9217e-05,  ..., 9.3772e-05, 8.6387e-05,
         8.8869e-05],
        [9.8878e-01, 4.2412e-06, 1.4552e-03,  ..., 1.1903e-03, 1.4325e-03,
         1.5589e-03],
        [2.3446e-01, 7.2359e-01, 5.1395e-03,  ..., 5.1220e-03, 5.1798e-03,
         5.2743e-03],
        ...,
        [3.6453e-02, 9.4659e-01, 1.9585e-03,  ..., 2.1637e-03, 2.1817e-03,
         1.9749e-03],
        [1.6302e-01, 8.1983e-01, 2.0783e-03,  ..., 2.1951e-03, 1.9967e-03,
         2.2637e-03],
        [4.3021e-03, 9.8350e-01, 1.5602e-03,  ..., 1.6471e-03, 1.6011e-03,
         1.4837e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1]
original label
tensor([0, 0, 1, 0, 8, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 8, 1, 1, 0, 9, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 4, 0, 0, 8, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 5, 1, 0, 0, 4, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 8, 0,
        0, 1, 0, 0, 1, 9, 1, 1])
soft_pseudo_label
tensor([[9.7726e-01, 7.5631e-04, 2.5523e-03,  ..., 2.7309e-03, 2.8654e-03,
         3.1241e-03],
        [1.3753e-02, 9.6748e-01, 2.2473e-03,  ..., 2.2277e-03, 2.3714e-03,
         2.5428e-03],
        [7.7863e-01, 1.9997e-01, 2.5295e-03,  ..., 2.6432e-03, 2.7258e-03,
         2.7512e-03],
        ...,
        [2.5932e-01, 7.0628e-01, 4.0467e-03,  ..., 4.2018e-03, 4.2409e-03,
         4.3755e-03],
        [9.8002e-01, 4.6659e-03, 1.9586e-03,  ..., 2.0687e-03, 1.8114e-03,
         1.9730e-03],
        [2.4081e-01, 7.3453e-01, 3.1306e-03,  ..., 2.9814e-03, 3.0641e-03,
         3.1567e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]
original label
tensor([0, 3, 2, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 6, 1, 1, 1, 0, 1, 7, 0,
        0, 0, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 6, 0, 0,
        0, 1, 9, 5, 1, 0, 0, 1, 8, 1, 7, 3, 4, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 6, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 3, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 8, 0, 0, 0, 1, 0, 2])
soft_pseudo_label
tensor([[4.6983e-02, 9.3634e-01, 2.0333e-03,  ..., 2.1497e-03, 1.9698e-03,
         2.2125e-03],
        [9.4792e-01, 5.1429e-02, 8.4424e-05,  ..., 8.0009e-05, 8.5502e-05,
         8.4013e-05],
        [2.2478e-02, 9.6612e-01, 1.4747e-03,  ..., 1.4696e-03, 1.4826e-03,
         1.3493e-03],
        ...,
        [4.4139e-04, 9.8642e-01, 1.8268e-03,  ..., 1.5710e-03, 1.6871e-03,
         1.5256e-03],
        [6.8532e-02, 9.3139e-01, 9.0736e-06,  ..., 9.9266e-06, 9.6400e-06,
         1.1348e-05],
        [9.7871e-01, 3.0727e-03, 2.2003e-03,  ..., 2.2613e-03, 2.2338e-03,
         2.3025e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]
original label
tensor([1, 0, 1, 0, 0, 1, 9, 7, 0, 1, 1, 6, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 4, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 5, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        2, 1, 0, 4, 0, 1, 0, 0])
soft_pseudo_label
tensor([[9.7415e-01, 6.3340e-03, 2.4194e-03,  ..., 2.3335e-03, 2.3959e-03,
         2.4076e-03],
        [3.4972e-02, 9.5264e-01, 1.5456e-03,  ..., 1.5209e-03, 1.5261e-03,
         1.5017e-03],
        [2.4363e-04, 9.9269e-01, 9.4958e-04,  ..., 8.2662e-04, 8.5286e-04,
         8.9861e-04],
        ...,
        [9.9067e-01, 7.7386e-07, 1.0227e-03,  ..., 1.1191e-03, 1.2203e-03,
         1.1730e-03],
        [5.9599e-01, 4.0327e-01, 9.6116e-05,  ..., 1.0102e-04, 8.9241e-05,
         9.3250e-05],
        [6.0529e-03, 9.8374e-01, 1.2479e-03,  ..., 1.2787e-03, 1.2768e-03,
         1.2333e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]
original label
tensor([3, 1, 5, 1, 1, 0, 0, 0, 0, 1, 9, 0, 1, 0, 0, 1, 4, 0, 9, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 5, 1, 0, 0, 1, 0, 0, 0, 0, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 9, 4, 4, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 5, 1, 1, 1, 0, 1, 0, 1, 5,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 7, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 7])
soft_pseudo_label
tensor([[8.9532e-01, 8.2308e-02, 2.6966e-03,  ..., 2.8524e-03, 2.7863e-03,
         2.8427e-03],
        [9.5974e-01, 8.2604e-03, 3.9222e-03,  ..., 4.2763e-03, 3.8407e-03,
         3.8314e-03],
        [6.9343e-02, 9.2058e-01, 1.2701e-03,  ..., 1.2577e-03, 1.2651e-03,
         1.2431e-03],
        ...,
        [9.8850e-01, 4.8154e-04, 1.5236e-03,  ..., 1.3452e-03, 1.3122e-03,
         1.4191e-03],
        [6.7538e-02, 8.8877e-01, 5.3237e-03,  ..., 5.3055e-03, 5.2360e-03,
         6.2088e-03],
        [1.8193e-03, 9.8861e-01, 1.1655e-03,  ..., 1.2364e-03, 1.1943e-03,
         1.1984e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]
original label
tensor([0, 0, 1, 0, 8, 0, 0, 1, 1, 8, 1, 2, 1, 0, 3, 0, 0, 0, 1, 1, 7, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 6, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 6, 0, 1, 8])
soft_pseudo_label
tensor([[1.9406e-02, 9.7039e-01, 1.3116e-03,  ..., 1.2461e-03, 1.2303e-03,
         1.1942e-03],
        [9.6455e-01, 3.3459e-02, 2.5472e-04,  ..., 2.4712e-04, 2.5348e-04,
         2.5747e-04],
        [3.7329e-01, 5.4420e-01, 1.0109e-02,  ..., 1.0011e-02, 1.1033e-02,
         1.0105e-02],
        ...,
        [3.3371e-03, 9.7374e-01, 2.8013e-03,  ..., 2.7646e-03, 2.6200e-03,
         2.9965e-03],
        [3.5598e-02, 9.5093e-01, 1.6367e-03,  ..., 1.6496e-03, 1.6343e-03,
         1.7052e-03],
        [9.8801e-01, 1.7568e-04, 1.5192e-03,  ..., 1.4535e-03, 1.3930e-03,
         1.4909e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]
original label
tensor([1, 3, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 9, 1, 4, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 7, 1, 0, 7, 0, 1, 1, 1, 1, 0, 0, 1, 0, 7, 0, 0, 0, 0, 0,
        0, 9, 1, 0, 1, 1, 0, 5, 0, 0, 1, 1, 0, 4, 0, 3, 0, 1, 0, 1, 2, 1, 1, 1,
        1, 1, 1, 1, 1, 7, 9, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 8, 1, 1, 0, 0,
        0, 1, 0, 4, 1, 1, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[8.1763e-01, 1.6546e-01, 1.9972e-03,  ..., 2.1680e-03, 2.0707e-03,
         2.0606e-03],
        [6.5037e-01, 3.2259e-01, 3.3322e-03,  ..., 3.3583e-03, 3.3305e-03,
         3.2982e-03],
        [9.7067e-01, 5.1405e-03, 2.9523e-03,  ..., 2.9108e-03, 3.0193e-03,
         3.1046e-03],
        ...,
        [4.8942e-03, 9.8847e-01, 8.2957e-04,  ..., 9.0711e-04, 8.2755e-04,
         8.5340e-04],
        [9.7830e-01, 2.6516e-03, 2.3862e-03,  ..., 2.3700e-03, 2.2781e-03,
         2.4608e-03],
        [9.8493e-01, 3.7943e-03, 1.5458e-03,  ..., 1.4635e-03, 1.3516e-03,
         1.4997e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]
original label
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 3, 0, 1, 8, 0, 1, 1, 0, 0, 1, 0, 6, 1,
        0, 0, 0, 1, 0, 1, 1, 9, 1, 1, 1, 1, 0, 0, 1, 8, 8, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 4, 1, 1, 1, 0, 1, 2, 1, 1, 4, 1, 1, 1, 0, 7, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 4, 1, 0, 0, 0, 0, 1, 0,
        1, 8, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 2, 0, 1, 0, 0])
soft_pseudo_label
tensor([[1.5802e-02, 9.6627e-01, 2.1397e-03,  ..., 2.3226e-03, 2.0891e-03,
         2.1691e-03],
        [2.3294e-03, 9.7764e-01, 2.5558e-03,  ..., 2.5960e-03, 2.6126e-03,
         2.6190e-03],
        [2.2604e-04, 9.9000e-01, 1.2077e-03,  ..., 1.1230e-03, 1.3339e-03,
         1.2906e-03],
        ...,
        [4.6579e-01, 5.2370e-01, 1.2656e-03,  ..., 1.3472e-03, 1.3121e-03,
         1.2868e-03],
        [5.9589e-01, 3.8776e-01, 1.9837e-03,  ..., 2.0932e-03, 2.1660e-03,
         2.0170e-03],
        [4.6307e-02, 9.3101e-01, 2.8014e-03,  ..., 2.8861e-03, 2.9158e-03,
         2.8069e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
original label
tensor([1, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 7, 1, 1, 1, 0, 1, 8, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 6, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 1, 7, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 9, 1, 0,
        0, 0, 0, 0, 9, 0, 0, 1])
soft_pseudo_label
tensor([[9.9048e-01, 9.3019e-03, 2.7089e-05,  ..., 2.3859e-05, 2.9577e-05,
         2.8333e-05],
        [7.0405e-01, 2.7303e-01, 2.6846e-03,  ..., 2.8914e-03, 2.9141e-03,
         2.9803e-03],
        [1.5276e-02, 9.6748e-01, 2.0264e-03,  ..., 2.1236e-03, 2.2738e-03,
         2.1030e-03],
        ...,
        [3.4372e-03, 9.8258e-01, 1.6756e-03,  ..., 1.8556e-03, 1.6871e-03,
         1.8793e-03],
        [9.4341e-04, 9.9883e-01, 2.8937e-05,  ..., 2.9164e-05, 2.5562e-05,
         2.8965e-05],
        [2.6669e-03, 9.7343e-01, 3.5678e-03,  ..., 3.0937e-03, 3.0058e-03,
         3.0428e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 8, 0, 0, 0, 2,
        0, 1, 0, 0, 0, 4, 1, 2, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 7, 1, 1, 0, 1, 1,
        1, 0, 4, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 7, 0, 1, 9,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 5, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0,
        0, 2, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[3.4912e-03, 9.6992e-01, 3.7786e-03,  ..., 3.2006e-03, 3.4087e-03,
         3.2084e-03],
        [9.9878e-01, 9.7902e-04, 3.1043e-05,  ..., 3.0772e-05, 3.0772e-05,
         2.8210e-05],
        [9.6405e-01, 1.5700e-04, 4.8312e-03,  ..., 4.1684e-03, 3.9447e-03,
         4.5589e-03],
        ...,
        [9.4354e-01, 2.7887e-02, 3.2424e-03,  ..., 3.6867e-03, 3.7575e-03,
         3.8223e-03],
        [9.8181e-01, 6.7393e-03, 1.4251e-03,  ..., 1.5326e-03, 1.3786e-03,
         1.3989e-03],
        [2.6309e-01, 7.1935e-01, 2.2040e-03,  ..., 2.2104e-03, 2.1847e-03,
         2.0867e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]
original label
tensor([1, 0, 0, 9, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 8, 1, 1, 8, 1, 1, 8, 1, 0, 1, 0, 1, 5, 1, 4, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 4, 1, 1, 1, 5, 9, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 4, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 6, 0, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 3, 9, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[4.7208e-01, 4.6385e-01, 7.5085e-03,  ..., 7.9383e-03, 8.2545e-03,
         7.9383e-03],
        [8.8733e-01, 1.1237e-01, 4.1604e-05,  ..., 3.8066e-05, 3.8817e-05,
         3.4424e-05],
        [9.9338e-01, 1.3703e-04, 7.0410e-04,  ..., 8.3655e-04, 8.7264e-04,
         7.4513e-04],
        ...,
        [2.2157e-01, 7.7639e-01, 2.5170e-04,  ..., 2.5767e-04, 2.5417e-04,
         2.5767e-04],
        [8.4410e-01, 1.2670e-01, 3.5661e-03,  ..., 3.6134e-03, 3.7647e-03,
         3.5958e-03],
        [8.2671e-01, 1.4793e-01, 3.1155e-03,  ..., 3.1354e-03, 3.1262e-03,
         3.2571e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]
original label
tensor([1, 1, 0, 0, 1, 8, 1, 0, 0, 0, 6, 1, 0, 1, 8, 5, 0, 0, 1, 0, 1, 0, 5, 1,
        0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 9, 1, 0, 1,
        6, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 5, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 9, 0, 1,
        8, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 7, 0, 1, 0,
        5, 0, 0, 4, 0, 1, 0, 1])
soft_pseudo_label
tensor([[3.5009e-02, 9.5550e-01, 1.1101e-03,  ..., 1.1599e-03, 1.1047e-03,
         1.3442e-03],
        [2.0559e-02, 9.7143e-01, 1.0092e-03,  ..., 9.7005e-04, 1.0018e-03,
         9.7195e-04],
        [9.6281e-01, 3.6325e-02, 1.0527e-04,  ..., 1.1460e-04, 1.1686e-04,
         1.0766e-04],
        ...,
        [3.2666e-02, 9.5092e-01, 1.8765e-03,  ..., 2.0191e-03, 1.9446e-03,
         2.3422e-03],
        [2.8561e-03, 9.7860e-01, 2.2500e-03,  ..., 2.5508e-03, 2.3557e-03,
         2.2910e-03],
        [1.7194e-02, 9.6196e-01, 2.4855e-03,  ..., 2.6953e-03, 2.6214e-03,
         2.7178e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]
original label
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 6, 1, 1, 1, 0, 1, 0, 1, 1, 6, 1, 0, 0, 9, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 8, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 7, 1])
soft_pseudo_label
tensor([[9.8398e-04, 9.8346e-01, 1.9877e-03,  ..., 1.9819e-03, 1.8773e-03,
         1.9974e-03],
        [9.7872e-01, 2.7971e-03, 2.2946e-03,  ..., 2.3954e-03, 2.1619e-03,
         2.4666e-03],
        [1.5809e-02, 9.7427e-01, 1.2529e-03,  ..., 1.2996e-03, 1.2334e-03,
         1.1897e-03],
        ...,
        [5.3588e-03, 9.9365e-01, 1.1253e-04,  ..., 1.3028e-04, 1.2553e-04,
         1.2652e-04],
        [7.6698e-01, 2.1009e-01, 2.7153e-03,  ..., 2.8792e-03, 2.6837e-03,
         3.0814e-03],
        [8.7934e-03, 9.6865e-01, 2.8457e-03,  ..., 2.8485e-03, 2.8625e-03,
         2.6089e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]
original label
tensor([7, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 8, 6, 0, 1, 1, 1,
        0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 7, 0, 6, 2, 1, 0, 1, 1, 0, 1, 1, 6, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 7, 0, 1, 0, 4, 0, 0, 1, 1, 1, 0, 0, 1, 4,
        1, 1, 0, 6, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1])
soft_pseudo_label
tensor([[7.9780e-01, 7.7163e-02, 1.5039e-02,  ..., 1.5142e-02, 1.5231e-02,
         1.6088e-02],
        [9.8736e-01, 1.2458e-03, 1.4844e-03,  ..., 1.3642e-03, 1.3523e-03,
         1.3649e-03],
        [9.7966e-01, 1.7630e-02, 3.1977e-04,  ..., 3.4140e-04, 3.5155e-04,
         3.7276e-04],
        ...,
        [7.6803e-01, 1.9381e-01, 4.5959e-03,  ..., 4.6049e-03, 4.7395e-03,
         4.7418e-03],
        [1.1113e-02, 9.6112e-01, 3.6291e-03,  ..., 3.4731e-03, 3.7170e-03,
         3.2803e-03],
        [6.2088e-02, 9.3768e-01, 3.2354e-05,  ..., 3.1728e-05, 2.5845e-05,
         2.9835e-05]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1]
original label
tensor([8, 9, 0, 0, 1, 1, 6, 1, 1, 9, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 5, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 3, 9, 0,
        0, 0, 1, 1, 0, 0, 8, 0, 1, 1, 1, 0, 6, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 9, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 8, 0, 2, 0, 1, 0])
soft_pseudo_label
tensor([[1.3852e-01, 8.3867e-01, 2.6771e-03,  ..., 2.8207e-03, 2.8736e-03,
         2.9331e-03],
        [7.2807e-01, 2.4771e-01, 3.0120e-03,  ..., 2.9523e-03, 2.9842e-03,
         3.2298e-03],
        [1.8311e-02, 9.6898e-01, 1.6605e-03,  ..., 1.6188e-03, 1.6078e-03,
         1.5208e-03],
        ...,
        [9.7208e-01, 1.2213e-02, 1.9154e-03,  ..., 1.9399e-03, 1.9295e-03,
         2.0761e-03],
        [9.9147e-01, 1.3946e-04, 1.1042e-03,  ..., 1.1396e-03, 8.6650e-04,
         9.3417e-04],
        [8.8776e-01, 7.2446e-02, 5.0346e-03,  ..., 4.9180e-03, 4.7250e-03,
         4.5953e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]
original label
tensor([1, 6, 1, 1, 1, 1, 0, 1, 1, 1, 0, 4, 0, 3, 1, 0, 3, 9, 1, 9, 0, 0, 1, 1,
        1, 0, 9, 1, 0, 1, 8, 0, 1, 1, 5, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        7, 4, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 4, 1, 1, 5, 0, 4, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 3, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 2, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 7, 4, 3,
        0, 7, 1, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[2.7361e-02, 9.5888e-01, 1.6747e-03,  ..., 1.7245e-03, 1.6879e-03,
         1.6928e-03],
        [5.6074e-01, 4.1185e-01, 3.1785e-03,  ..., 3.2890e-03, 3.5424e-03,
         3.5719e-03],
        [9.8535e-01, 1.9901e-03, 1.6664e-03,  ..., 1.7507e-03, 1.6852e-03,
         1.5025e-03],
        ...,
        [9.7766e-01, 2.1939e-02, 4.9710e-05,  ..., 5.5945e-05, 4.8748e-05,
         4.7156e-05],
        [9.8218e-01, 7.1000e-03, 1.2046e-03,  ..., 1.3791e-03, 1.3393e-03,
         1.2817e-03],
        [7.3887e-03, 9.8153e-01, 1.4185e-03,  ..., 1.4164e-03, 1.3608e-03,
         1.3151e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 1, 0, 1, 0, 9, 1, 0, 1, 6, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 8, 1, 1, 1, 0,
        1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 6, 0, 1, 0, 0, 0, 3,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 7, 5, 1,
        1, 1, 0, 1, 0, 0, 0, 0])
soft_pseudo_label
tensor([[2.1106e-01, 7.8725e-01, 2.1241e-04,  ..., 2.1366e-04, 1.9722e-04,
         2.3374e-04],
        [9.6675e-01, 1.1037e-02, 2.6473e-03,  ..., 2.5101e-03, 2.8625e-03,
         2.8990e-03],
        [9.8044e-01, 2.7727e-03, 1.9952e-03,  ..., 1.9971e-03, 2.2411e-03,
         2.1374e-03],
        ...,
        [2.2124e-03, 9.8218e-01, 1.9822e-03,  ..., 1.8877e-03, 1.8377e-03,
         2.1802e-03],
        [1.0810e-02, 9.7311e-01, 2.0352e-03,  ..., 1.9803e-03, 2.0723e-03,
         2.1590e-03],
        [9.8965e-01, 1.1741e-03, 1.0901e-03,  ..., 1.1352e-03, 1.1833e-03,
         1.1413e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 9, 1, 0, 2, 1, 1, 9, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 5, 0, 1, 1, 1, 0, 0, 1, 0, 1, 3, 1, 0, 8, 1, 7, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 4, 1, 6, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[1.6019e-03, 9.8930e-01, 1.1709e-03,  ..., 1.2283e-03, 1.1010e-03,
         1.1618e-03],
        [9.8112e-01, 1.5608e-04, 2.1342e-03,  ..., 2.2690e-03, 2.2170e-03,
         2.3223e-03],
        [9.8011e-01, 1.3155e-04, 2.6810e-03,  ..., 2.5047e-03, 2.3564e-03,
         2.9150e-03],
        ...,
        [9.6655e-01, 8.5857e-03, 3.2972e-03,  ..., 3.1263e-03, 2.9977e-03,
         3.2240e-03],
        [3.7816e-03, 9.8008e-01, 2.1252e-03,  ..., 1.8080e-03, 1.9847e-03,
         2.0861e-03],
        [2.4691e-02, 9.7097e-01, 5.2510e-04,  ..., 5.4230e-04, 5.1344e-04,
         5.6722e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
original label
tensor([1, 0, 0, 1, 1, 1, 6, 1, 1, 0, 1, 1, 1, 0, 0, 1, 3, 1, 3, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3, 1, 1, 1, 0, 1, 1, 1, 0, 8, 0, 1, 0, 0,
        1, 5, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 4, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 5, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        9, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 5, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 2])
soft_pseudo_label
tensor([[8.8642e-01, 1.1226e-01, 1.6960e-04,  ..., 1.6976e-04, 1.6664e-04,
         1.6486e-04],
        [8.1719e-01, 1.5749e-01, 3.0902e-03,  ..., 3.2055e-03, 3.1650e-03,
         3.1821e-03],
        [1.0135e-01, 8.8416e-01, 1.7644e-03,  ..., 1.8134e-03, 1.8976e-03,
         1.6886e-03],
        ...,
        [9.7823e-01, 4.2234e-03, 2.0862e-03,  ..., 2.2349e-03, 2.2404e-03,
         2.1630e-03],
        [6.1212e-04, 9.8148e-01, 2.1659e-03,  ..., 2.1554e-03, 2.3282e-03,
         2.4459e-03],
        [2.8982e-01, 6.9118e-01, 2.2965e-03,  ..., 2.3010e-03, 2.3879e-03,
         2.4422e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 1, 0, 1, 0, 0, 7, 1, 0, 0, 1, 0, 1, 0, 6, 1, 1, 0, 2, 1, 0, 4,
        1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        9, 0, 0, 1, 1, 1, 8, 1, 0, 0, 5, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 1, 0,
        1, 1, 0, 0, 1, 5, 8, 1, 0, 0, 6, 7, 1, 1, 1, 0, 0, 1, 0, 7, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 6, 0, 0, 1, 1])
soft_pseudo_label
tensor([[1.4421e-03, 9.6954e-01, 3.7077e-03,  ..., 3.6753e-03, 3.5087e-03,
         3.4560e-03],
        [9.9129e-01, 7.8677e-05, 1.0468e-03,  ..., 1.0126e-03, 1.0511e-03,
         1.1296e-03],
        [9.7882e-01, 1.8970e-03, 2.4996e-03,  ..., 2.0905e-03, 2.1685e-03,
         2.4080e-03],
        ...,
        [3.8007e-02, 9.4266e-01, 2.3207e-03,  ..., 2.3549e-03, 2.5940e-03,
         2.3094e-03],
        [9.8182e-01, 3.3344e-03, 1.8156e-03,  ..., 1.7962e-03, 1.8497e-03,
         1.8551e-03],
        [9.4153e-01, 4.4122e-02, 1.8043e-03,  ..., 1.7833e-03, 1.8043e-03,
         1.7599e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0]
original label
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 2, 6, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 9,
        0, 0, 0, 6, 0, 5, 8, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 5, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 6, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0])
soft_pseudo_label
tensor([[4.9054e-04, 9.8460e-01, 1.9856e-03,  ..., 1.8576e-03, 1.8667e-03,
         1.8101e-03],
        [1.7759e-01, 8.0213e-01, 2.4361e-03,  ..., 2.4290e-03, 2.5605e-03,
         2.6225e-03],
        [6.6624e-02, 9.1613e-01, 2.0596e-03,  ..., 2.1146e-03, 2.1690e-03,
         2.3579e-03],
        ...,
        [2.6398e-01, 6.4955e-01, 1.0928e-02,  ..., 1.1758e-02, 1.0525e-02,
         1.0407e-02],
        [9.2566e-01, 5.4091e-02, 2.4105e-03,  ..., 2.4688e-03, 2.4785e-03,
         2.5472e-03],
        [9.5865e-01, 2.1916e-02, 2.3821e-03,  ..., 2.4338e-03, 2.3337e-03,
         2.6073e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]
original label
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 9, 2, 1, 0, 0, 4, 0, 1, 2, 1, 1, 5, 1,
        1, 8, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 9, 0, 0, 1,
        0, 0, 0, 2, 3, 1, 8, 1, 5, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 5, 1, 7, 1, 1, 1, 9, 1, 4, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1])
soft_pseudo_label
tensor([[9.8580e-01, 6.1413e-05, 1.8223e-03,  ..., 1.7368e-03, 1.6656e-03,
         1.9773e-03],
        [9.8506e-01, 3.6222e-04, 1.7973e-03,  ..., 1.8758e-03, 1.8873e-03,
         1.8603e-03],
        [2.3359e-01, 7.4236e-01, 2.8893e-03,  ..., 3.1332e-03, 3.1195e-03,
         2.8921e-03],
        ...,
        [5.9018e-04, 9.8303e-01, 1.9219e-03,  ..., 2.2580e-03, 2.1912e-03,
         1.8375e-03],
        [8.0738e-01, 1.6858e-01, 3.0546e-03,  ..., 2.9635e-03, 2.9062e-03,
         3.0028e-03],
        [9.8390e-01, 1.6371e-04, 1.8509e-03,  ..., 1.9199e-03, 1.8924e-03,
         1.8823e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 6, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        7, 1, 1, 5, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 3, 1, 5, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        2, 1, 1, 1, 0, 0, 0, 1, 1, 9, 1, 6, 0, 0, 1, 0, 0, 1, 9, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0])
soft_pseudo_label
tensor([[9.9147e-01, 1.1008e-06, 9.4830e-04,  ..., 1.0451e-03, 1.2059e-03,
         1.1978e-03],
        [9.8931e-01, 4.0498e-05, 1.4356e-03,  ..., 1.2903e-03, 1.3253e-03,
         1.2973e-03],
        [4.4356e-02, 9.4468e-01, 1.3699e-03,  ..., 1.3732e-03, 1.3316e-03,
         1.3513e-03],
        ...,
        [9.5198e-03, 9.9019e-01, 3.7123e-05,  ..., 3.9556e-05, 3.5251e-05,
         3.7488e-05],
        [9.7086e-01, 1.1498e-02, 2.0058e-03,  ..., 2.3087e-03, 2.2585e-03,
         2.4672e-03],
        [1.9394e-01, 8.0384e-01, 2.6835e-04,  ..., 2.8068e-04, 2.8789e-04,
         2.7525e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]
original label
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0,
        0, 0, 5, 1, 0, 0, 0, 1, 1, 3, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 4, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 9, 1, 0, 2, 9, 1, 9, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 1,
        7, 0, 1, 0, 0, 1, 8, 0])
soft_pseudo_label
tensor([[1.9218e-02, 9.5819e-01, 2.9200e-03,  ..., 2.8178e-03, 2.7538e-03,
         2.8357e-03],
        [9.8904e-01, 2.0966e-04, 1.3473e-03,  ..., 1.2412e-03, 1.3309e-03,
         1.4126e-03],
        [1.1547e-01, 8.8373e-01, 1.1274e-04,  ..., 9.4754e-05, 9.3833e-05,
         1.0478e-04],
        ...,
        [7.4805e-01, 2.0773e-01, 5.4766e-03,  ..., 5.6010e-03, 5.5846e-03,
         5.3107e-03],
        [9.6404e-01, 1.7813e-02, 2.3803e-03,  ..., 2.2635e-03, 2.3652e-03,
         2.3978e-03],
        [9.6553e-01, 1.1896e-02, 2.6068e-03,  ..., 2.8214e-03, 2.8700e-03,
         2.7641e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0]
original label
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 4, 7, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1,
        0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 3, 0, 1, 0, 0, 1, 1, 8,
        1, 1, 6, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 7, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[8.3092e-05, 9.8927e-01, 1.3225e-03,  ..., 1.3061e-03, 1.4826e-03,
         1.3277e-03],
        [8.4958e-01, 9.5506e-02, 6.6534e-03,  ..., 6.5247e-03, 6.8847e-03,
         7.2257e-03],
        [7.6814e-01, 2.1123e-01, 2.6512e-03,  ..., 2.6956e-03, 2.5397e-03,
         2.5987e-03],
        ...,
        [9.8218e-01, 1.1157e-03, 2.1664e-03,  ..., 2.2097e-03, 2.1485e-03,
         2.0302e-03],
        [9.8585e-01, 1.1272e-05, 1.8185e-03,  ..., 1.5359e-03, 1.5743e-03,
         1.9247e-03],
        [8.4636e-01, 1.1343e-01, 4.9983e-03,  ..., 5.1520e-03, 4.8826e-03,
         5.2357e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0]
original label
tensor([1, 0, 0, 0, 0, 7, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 9, 0, 0, 1, 7, 2, 0, 0,
        0, 3, 4, 0, 2, 4, 1, 0, 0, 0, 1, 1, 4, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 9, 1, 0, 9, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 5, 1, 0, 1, 1, 8, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.8016e-01, 1.1187e-04, 2.5935e-03,  ..., 2.6040e-03, 2.2355e-03,
         2.7074e-03],
        [3.6757e-04, 9.9183e-01, 9.0355e-04,  ..., 1.0597e-03, 9.9332e-04,
         8.8607e-04],
        [9.8888e-01, 2.7181e-04, 1.2741e-03,  ..., 1.2611e-03, 1.3540e-03,
         1.3402e-03],
        ...,
        [1.3311e-02, 9.5623e-01, 3.9194e-03,  ..., 3.7988e-03, 3.7692e-03,
         3.7035e-03],
        [1.7286e-01, 8.1508e-01, 1.5000e-03,  ..., 1.5340e-03, 1.6029e-03,
         1.4567e-03],
        [9.8835e-01, 2.0552e-03, 1.1107e-03,  ..., 1.1893e-03, 1.2494e-03,
         1.3287e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]
original label
tensor([0, 1, 0, 0, 5, 1, 1, 8, 0, 1, 0, 0, 1, 6, 0, 0, 1, 1, 2, 0, 0, 0, 0, 8,
        1, 0, 1, 1, 4, 1, 5, 5, 0, 0, 1, 0, 1, 0, 9, 0, 1, 3, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 9, 0, 1, 1, 0, 1, 0, 0, 1, 1, 5, 0, 0, 1, 0, 7, 6,
        0, 0, 4, 0, 0, 0, 1, 0, 1, 0, 1, 1, 7, 8, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 8, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 8, 1, 0, 0])
soft_pseudo_label
tensor([[2.8109e-03, 9.8896e-01, 1.0319e-03,  ..., 1.0375e-03, 1.0878e-03,
         9.5111e-04],
        [1.6930e-01, 7.9204e-01, 4.5916e-03,  ..., 4.7768e-03, 4.8758e-03,
         4.9188e-03],
        [9.9199e-01, 2.6535e-03, 6.5027e-04,  ..., 7.5249e-04, 6.7157e-04,
         6.5218e-04],
        ...,
        [2.2113e-02, 9.4303e-01, 4.4056e-03,  ..., 4.3778e-03, 4.4186e-03,
         4.5789e-03],
        [9.7559e-01, 3.4691e-03, 2.5195e-03,  ..., 2.5542e-03, 2.7416e-03,
         2.6469e-03],
        [1.5904e-01, 8.3986e-01, 1.4503e-04,  ..., 1.4404e-04, 1.3089e-04,
         1.2849e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1]
original label
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 8, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 8, 2, 1, 0, 0, 1, 1, 0, 1, 0, 8, 1, 0, 1, 0, 0, 1, 0, 7, 7,
        0, 0, 1, 0, 1, 9, 1, 0, 0, 0, 0, 1, 1, 9, 0, 1, 1, 0, 1, 0, 5, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 4, 1, 0, 1, 0, 0, 4, 1, 7, 1, 1, 5, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1])
[INFO] main.py:340 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:348 > [2-3] Start to train under online
[INFO] main.py:363 > Train over streamed data once
batch_size : 128 stream_batch_size : 64 memory_batch_size : 42
[INFO] rainbow_memory.py:119 > Streamed samples: 800
[INFO] rainbow_memory.py:120 > In-memory samples: 0
[INFO] rainbow_memory.py:121 > Pseudo samples: 9984
[INFO] rainbow_memory.py:127 > Train samples: 10784
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 1/1 | train_loss 1.0713 | train_acc 0.4727 | test_loss 0.7760 | test_acc 0.5010 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[WARNING] finetune.py:736 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
dog     251
deer    249
Name: klass, dtype: int64
[INFO] main.py:379 > Train over memory
batch_size : 64 stream_batch_size : 22 memory_batch_size : 21
[INFO] rainbow_memory.py:119 > Streamed samples: 0
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 0
[INFO] rainbow_memory.py:127 > Train samples: 500
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 1/256 | train_loss 1.2670 | train_acc 0.4880 | test_loss 1.3320 | test_acc 0.5000 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 2/256 | train_loss 3.7506 | train_acc 0.5280 | test_loss 51.2730 | test_acc 0.5000 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 3/256 | train_loss 2.7458 | train_acc 0.5140 | test_loss 1.6752 | test_acc 0.5000 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 4/256 | train_loss 1.0911 | train_acc 0.5160 | test_loss 0.7266 | test_acc 0.5000 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 5/256 | train_loss 1.0131 | train_acc 0.5020 | test_loss 1.3548 | test_acc 0.5000 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 6/256 | train_loss 1.0093 | train_acc 0.4980 | test_loss 0.6781 | test_acc 0.6640 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 7/256 | train_loss 0.7255 | train_acc 0.5380 | test_loss 0.8080 | test_acc 0.5005 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 8/256 | train_loss 0.7419 | train_acc 0.5680 | test_loss 0.6845 | test_acc 0.5365 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 9/256 | train_loss 0.8455 | train_acc 0.4960 | test_loss 0.8838 | test_acc 0.5000 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 10/256 | train_loss 0.7927 | train_acc 0.5000 | test_loss 0.8989 | test_acc 0.5000 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 11/256 | train_loss 0.8413 | train_acc 0.5020 | test_loss 0.6825 | test_acc 0.5245 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 12/256 | train_loss 0.7520 | train_acc 0.5180 | test_loss 0.6575 | test_acc 0.6750 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 13/256 | train_loss 0.7131 | train_acc 0.5540 | test_loss 0.6467 | test_acc 0.6390 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 14/256 | train_loss 0.7269 | train_acc 0.5220 | test_loss 0.6536 | test_acc 0.6745 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 15/256 | train_loss 0.7083 | train_acc 0.5420 | test_loss 0.6522 | test_acc 0.6065 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 16/256 | train_loss 0.6920 | train_acc 0.5500 | test_loss 0.6500 | test_acc 0.6255 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 17/256 | train_loss 0.7829 | train_acc 0.5460 | test_loss 0.6726 | test_acc 0.5560 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 18/256 | train_loss 0.7916 | train_acc 0.5340 | test_loss 0.6796 | test_acc 0.6835 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 19/256 | train_loss 0.7190 | train_acc 0.5480 | test_loss 0.7140 | test_acc 0.5000 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 20/256 | train_loss 0.7395 | train_acc 0.5120 | test_loss 0.6439 | test_acc 0.6285 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 21/256 | train_loss 0.7205 | train_acc 0.5680 | test_loss 0.6618 | test_acc 0.5915 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 22/256 | train_loss 0.7368 | train_acc 0.5340 | test_loss 0.6422 | test_acc 0.6930 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 23/256 | train_loss 0.7050 | train_acc 0.5680 | test_loss 0.6803 | test_acc 0.5115 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 24/256 | train_loss 0.6988 | train_acc 0.5900 | test_loss 0.5908 | test_acc 0.6685 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 25/256 | train_loss 0.6854 | train_acc 0.5960 | test_loss 0.6354 | test_acc 0.6085 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 26/256 | train_loss 0.6663 | train_acc 0.6180 | test_loss 0.6925 | test_acc 0.5995 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 27/256 | train_loss 0.6665 | train_acc 0.6060 | test_loss 0.5844 | test_acc 0.6985 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 28/256 | train_loss 0.6394 | train_acc 0.6500 | test_loss 0.5449 | test_acc 0.7315 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 29/256 | train_loss 0.6634 | train_acc 0.6180 | test_loss 0.6052 | test_acc 0.6510 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 30/256 | train_loss 0.6503 | train_acc 0.6140 | test_loss 0.6284 | test_acc 0.6175 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 31/256 | train_loss 0.6361 | train_acc 0.6620 | test_loss 0.5784 | test_acc 0.6880 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 32/256 | train_loss 0.6385 | train_acc 0.6100 | test_loss 0.6006 | test_acc 0.6555 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 33/256 | train_loss 0.6850 | train_acc 0.5960 | test_loss 0.6905 | test_acc 0.6200 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 34/256 | train_loss 0.7075 | train_acc 0.5720 | test_loss 0.5641 | test_acc 0.7475 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 35/256 | train_loss 0.6858 | train_acc 0.5960 | test_loss 0.6065 | test_acc 0.6990 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 36/256 | train_loss 0.6714 | train_acc 0.6060 | test_loss 0.7549 | test_acc 0.5295 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 37/256 | train_loss 0.7125 | train_acc 0.5820 | test_loss 0.6199 | test_acc 0.6355 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 38/256 | train_loss 0.6776 | train_acc 0.6360 | test_loss 0.6202 | test_acc 0.6690 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 39/256 | train_loss 0.7132 | train_acc 0.5380 | test_loss 0.8754 | test_acc 0.5005 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 40/256 | train_loss 0.6906 | train_acc 0.6260 | test_loss 0.5695 | test_acc 0.7005 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 41/256 | train_loss 0.6648 | train_acc 0.6260 | test_loss 0.5843 | test_acc 0.6795 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 42/256 | train_loss 0.6627 | train_acc 0.6280 | test_loss 0.6951 | test_acc 0.5930 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 43/256 | train_loss 0.6894 | train_acc 0.5800 | test_loss 0.6190 | test_acc 0.6240 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 44/256 | train_loss 0.6646 | train_acc 0.6060 | test_loss 0.6628 | test_acc 0.6250 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 45/256 | train_loss 0.6427 | train_acc 0.6700 | test_loss 0.8126 | test_acc 0.5805 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 46/256 | train_loss 0.6448 | train_acc 0.6580 | test_loss 0.5613 | test_acc 0.7170 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 47/256 | train_loss 0.6604 | train_acc 0.6120 | test_loss 0.5320 | test_acc 0.7360 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 48/256 | train_loss 0.6419 | train_acc 0.6500 | test_loss 0.5532 | test_acc 0.7000 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 49/256 | train_loss 0.6620 | train_acc 0.6120 | test_loss 0.5317 | test_acc 0.7235 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 50/256 | train_loss 0.6421 | train_acc 0.6280 | test_loss 0.5677 | test_acc 0.6805 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 51/256 | train_loss 0.6107 | train_acc 0.6680 | test_loss 0.5683 | test_acc 0.6975 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 52/256 | train_loss 0.6317 | train_acc 0.6260 | test_loss 0.5712 | test_acc 0.6960 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 53/256 | train_loss 0.6188 | train_acc 0.6560 | test_loss 0.5477 | test_acc 0.7105 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 54/256 | train_loss 0.6105 | train_acc 0.6940 | test_loss 0.5385 | test_acc 0.7230 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 55/256 | train_loss 0.5962 | train_acc 0.6740 | test_loss 0.5057 | test_acc 0.7580 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 56/256 | train_loss 0.6058 | train_acc 0.6580 | test_loss 0.5783 | test_acc 0.6950 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 57/256 | train_loss 0.5853 | train_acc 0.7060 | test_loss 0.5191 | test_acc 0.7425 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 58/256 | train_loss 0.6013 | train_acc 0.6780 | test_loss 0.5104 | test_acc 0.7415 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 59/256 | train_loss 0.6068 | train_acc 0.6780 | test_loss 0.5253 | test_acc 0.7305 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 60/256 | train_loss 0.5819 | train_acc 0.6820 | test_loss 0.5678 | test_acc 0.6975 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 61/256 | train_loss 0.5701 | train_acc 0.7120 | test_loss 0.5458 | test_acc 0.7170 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 62/256 | train_loss 0.6202 | train_acc 0.6700 | test_loss 0.5280 | test_acc 0.7310 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 63/256 | train_loss 0.5829 | train_acc 0.6680 | test_loss 0.5110 | test_acc 0.7435 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 64/256 | train_loss 0.5835 | train_acc 0.6900 | test_loss 0.5457 | test_acc 0.7190 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 65/256 | train_loss 0.6532 | train_acc 0.6460 | test_loss 0.5495 | test_acc 0.7285 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 66/256 | train_loss 0.7051 | train_acc 0.5940 | test_loss 0.5950 | test_acc 0.7350 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 67/256 | train_loss 0.6399 | train_acc 0.6340 | test_loss 0.5306 | test_acc 0.7280 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 68/256 | train_loss 0.6395 | train_acc 0.6240 | test_loss 0.4940 | test_acc 0.7655 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 69/256 | train_loss 0.6384 | train_acc 0.6520 | test_loss 0.4902 | test_acc 0.7725 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 70/256 | train_loss 0.6523 | train_acc 0.6560 | test_loss 0.5129 | test_acc 0.7490 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 71/256 | train_loss 0.6552 | train_acc 0.6620 | test_loss 0.5515 | test_acc 0.7295 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 72/256 | train_loss 0.6363 | train_acc 0.6360 | test_loss 0.5301 | test_acc 0.7465 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 73/256 | train_loss 0.6204 | train_acc 0.6600 | test_loss 0.5420 | test_acc 0.7235 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 74/256 | train_loss 0.6336 | train_acc 0.6520 | test_loss 0.5413 | test_acc 0.7170 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 75/256 | train_loss 0.6491 | train_acc 0.6140 | test_loss 0.5436 | test_acc 0.7260 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 76/256 | train_loss 0.6170 | train_acc 0.6580 | test_loss 0.5063 | test_acc 0.7500 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 77/256 | train_loss 0.6580 | train_acc 0.6540 | test_loss 0.6216 | test_acc 0.6680 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 78/256 | train_loss 0.6419 | train_acc 0.6460 | test_loss 0.5079 | test_acc 0.7490 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 79/256 | train_loss 0.6321 | train_acc 0.6400 | test_loss 0.5088 | test_acc 0.7665 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 80/256 | train_loss 0.6219 | train_acc 0.6460 | test_loss 0.5051 | test_acc 0.7550 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 81/256 | train_loss 0.6279 | train_acc 0.6520 | test_loss 0.5178 | test_acc 0.7690 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 82/256 | train_loss 0.6145 | train_acc 0.6620 | test_loss 0.5430 | test_acc 0.7160 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 83/256 | train_loss 0.6068 | train_acc 0.6880 | test_loss 0.5610 | test_acc 0.6955 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 84/256 | train_loss 0.5948 | train_acc 0.6740 | test_loss 0.5503 | test_acc 0.7160 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 85/256 | train_loss 0.5997 | train_acc 0.6840 | test_loss 0.4949 | test_acc 0.7565 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 86/256 | train_loss 0.6166 | train_acc 0.6780 | test_loss 0.5223 | test_acc 0.7320 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 87/256 | train_loss 0.5745 | train_acc 0.6780 | test_loss 0.4722 | test_acc 0.7755 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 88/256 | train_loss 0.5823 | train_acc 0.7100 | test_loss 0.5106 | test_acc 0.7465 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 89/256 | train_loss 0.6068 | train_acc 0.6720 | test_loss 0.4780 | test_acc 0.7810 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 90/256 | train_loss 0.6083 | train_acc 0.6940 | test_loss 0.5509 | test_acc 0.7095 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 91/256 | train_loss 0.5737 | train_acc 0.6940 | test_loss 0.4894 | test_acc 0.7730 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 92/256 | train_loss 0.6037 | train_acc 0.6780 | test_loss 0.4897 | test_acc 0.7670 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 93/256 | train_loss 0.5822 | train_acc 0.7060 | test_loss 0.5042 | test_acc 0.7525 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 94/256 | train_loss 0.6119 | train_acc 0.6760 | test_loss 0.4826 | test_acc 0.7675 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 95/256 | train_loss 0.5993 | train_acc 0.6760 | test_loss 0.4651 | test_acc 0.7775 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 96/256 | train_loss 0.6002 | train_acc 0.6780 | test_loss 0.5325 | test_acc 0.7175 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 97/256 | train_loss 0.5940 | train_acc 0.6580 | test_loss 0.4953 | test_acc 0.7670 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 98/256 | train_loss 0.5780 | train_acc 0.6980 | test_loss 0.4974 | test_acc 0.7520 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 99/256 | train_loss 0.5540 | train_acc 0.7400 | test_loss 0.4950 | test_acc 0.7565 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 100/256 | train_loss 0.5801 | train_acc 0.6780 | test_loss 0.4985 | test_acc 0.7500 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 101/256 | train_loss 0.5678 | train_acc 0.7180 | test_loss 0.5609 | test_acc 0.7135 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 102/256 | train_loss 0.5631 | train_acc 0.7220 | test_loss 0.5243 | test_acc 0.7305 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 103/256 | train_loss 0.5426 | train_acc 0.7180 | test_loss 0.4985 | test_acc 0.7560 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 104/256 | train_loss 0.5767 | train_acc 0.7120 | test_loss 0.5283 | test_acc 0.7490 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 105/256 | train_loss 0.5675 | train_acc 0.7100 | test_loss 0.4695 | test_acc 0.7705 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 106/256 | train_loss 0.6084 | train_acc 0.6800 | test_loss 0.5068 | test_acc 0.7545 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 107/256 | train_loss 0.5331 | train_acc 0.7180 | test_loss 0.4778 | test_acc 0.7685 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 108/256 | train_loss 0.5725 | train_acc 0.7020 | test_loss 0.4747 | test_acc 0.7695 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 109/256 | train_loss 0.5529 | train_acc 0.6940 | test_loss 0.4714 | test_acc 0.7710 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 110/256 | train_loss 0.5538 | train_acc 0.7140 | test_loss 0.4689 | test_acc 0.7750 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 111/256 | train_loss 0.5916 | train_acc 0.6720 | test_loss 0.5117 | test_acc 0.7495 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 112/256 | train_loss 0.5337 | train_acc 0.7280 | test_loss 0.4896 | test_acc 0.7645 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 113/256 | train_loss 0.5434 | train_acc 0.7220 | test_loss 0.4557 | test_acc 0.7900 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 114/256 | train_loss 0.5508 | train_acc 0.7460 | test_loss 0.4849 | test_acc 0.7700 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 115/256 | train_loss 0.5480 | train_acc 0.7360 | test_loss 0.4541 | test_acc 0.7810 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 116/256 | train_loss 0.5282 | train_acc 0.7120 | test_loss 0.4587 | test_acc 0.7785 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 117/256 | train_loss 0.5724 | train_acc 0.6900 | test_loss 0.4992 | test_acc 0.7535 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 118/256 | train_loss 0.5455 | train_acc 0.7480 | test_loss 0.4706 | test_acc 0.7715 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 119/256 | train_loss 0.5305 | train_acc 0.7380 | test_loss 0.4638 | test_acc 0.7745 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 120/256 | train_loss 0.5000 | train_acc 0.7420 | test_loss 0.4587 | test_acc 0.7810 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 121/256 | train_loss 0.5081 | train_acc 0.7540 | test_loss 0.4658 | test_acc 0.7730 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 122/256 | train_loss 0.5087 | train_acc 0.7600 | test_loss 0.4682 | test_acc 0.7740 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 123/256 | train_loss 0.5082 | train_acc 0.7560 | test_loss 0.4575 | test_acc 0.7765 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 124/256 | train_loss 0.5474 | train_acc 0.7200 | test_loss 0.4623 | test_acc 0.7730 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 125/256 | train_loss 0.5363 | train_acc 0.7300 | test_loss 0.4613 | test_acc 0.7775 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 126/256 | train_loss 0.5132 | train_acc 0.7520 | test_loss 0.4690 | test_acc 0.7750 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 127/256 | train_loss 0.5560 | train_acc 0.7280 | test_loss 0.4635 | test_acc 0.7760 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 128/256 | train_loss 0.5259 | train_acc 0.7200 | test_loss 0.4591 | test_acc 0.7775 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 129/256 | train_loss 0.6295 | train_acc 0.6620 | test_loss 0.5461 | test_acc 0.7125 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 130/256 | train_loss 0.6229 | train_acc 0.6680 | test_loss 0.5120 | test_acc 0.7525 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 131/256 | train_loss 0.6013 | train_acc 0.6940 | test_loss 0.5249 | test_acc 0.7355 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 132/256 | train_loss 0.6156 | train_acc 0.6940 | test_loss 0.5083 | test_acc 0.7470 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 133/256 | train_loss 0.5965 | train_acc 0.7160 | test_loss 0.5007 | test_acc 0.7490 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 134/256 | train_loss 0.5759 | train_acc 0.6580 | test_loss 0.4766 | test_acc 0.7745 | lr 0.0498
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 135/256 | train_loss 0.5851 | train_acc 0.6820 | test_loss 0.5767 | test_acc 0.6950 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 136/256 | train_loss 0.5991 | train_acc 0.6700 | test_loss 0.5464 | test_acc 0.7430 | lr 0.0496
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 137/256 | train_loss 0.5964 | train_acc 0.6660 | test_loss 0.4924 | test_acc 0.7710 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 138/256 | train_loss 0.5734 | train_acc 0.7040 | test_loss 0.4658 | test_acc 0.7800 | lr 0.0494
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 139/256 | train_loss 0.6085 | train_acc 0.6760 | test_loss 0.5012 | test_acc 0.7605 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 140/256 | train_loss 0.5963 | train_acc 0.6920 | test_loss 0.4640 | test_acc 0.7800 | lr 0.0491
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 141/256 | train_loss 0.5919 | train_acc 0.6720 | test_loss 0.4672 | test_acc 0.7740 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 142/256 | train_loss 0.6172 | train_acc 0.6680 | test_loss 0.4684 | test_acc 0.7760 | lr 0.0488
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 143/256 | train_loss 0.5950 | train_acc 0.6980 | test_loss 0.4591 | test_acc 0.7870 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 144/256 | train_loss 0.5340 | train_acc 0.7300 | test_loss 0.6066 | test_acc 0.7150 | lr 0.0483
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 145/256 | train_loss 0.5546 | train_acc 0.7360 | test_loss 0.4747 | test_acc 0.7750 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 146/256 | train_loss 0.5978 | train_acc 0.7020 | test_loss 0.5342 | test_acc 0.7310 | lr 0.0479
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 147/256 | train_loss 0.5740 | train_acc 0.7140 | test_loss 0.5248 | test_acc 0.7345 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 148/256 | train_loss 0.5431 | train_acc 0.7400 | test_loss 0.5686 | test_acc 0.7180 | lr 0.0474
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 149/256 | train_loss 0.5422 | train_acc 0.7220 | test_loss 0.5491 | test_acc 0.7245 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 150/256 | train_loss 0.5785 | train_acc 0.6940 | test_loss 0.4447 | test_acc 0.7965 | lr 0.0468
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 151/256 | train_loss 0.5044 | train_acc 0.7700 | test_loss 0.4612 | test_acc 0.7885 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 152/256 | train_loss 0.5691 | train_acc 0.7020 | test_loss 0.4663 | test_acc 0.7780 | lr 0.0462
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 153/256 | train_loss 0.5406 | train_acc 0.7380 | test_loss 0.4681 | test_acc 0.7645 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 154/256 | train_loss 0.5771 | train_acc 0.6880 | test_loss 0.4766 | test_acc 0.7675 | lr 0.0455
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 155/256 | train_loss 0.6022 | train_acc 0.6760 | test_loss 0.6196 | test_acc 0.6960 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 156/256 | train_loss 0.5142 | train_acc 0.7700 | test_loss 0.4609 | test_acc 0.7875 | lr 0.0448
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 157/256 | train_loss 0.5333 | train_acc 0.7440 | test_loss 0.4383 | test_acc 0.7965 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 158/256 | train_loss 0.5642 | train_acc 0.7280 | test_loss 0.4875 | test_acc 0.7720 | lr 0.0440
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 159/256 | train_loss 0.5470 | train_acc 0.7240 | test_loss 0.4664 | test_acc 0.7770 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 160/256 | train_loss 0.5427 | train_acc 0.7200 | test_loss 0.4299 | test_acc 0.8025 | lr 0.0432
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 161/256 | train_loss 0.5394 | train_acc 0.7480 | test_loss 0.5047 | test_acc 0.7490 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 162/256 | train_loss 0.5435 | train_acc 0.7500 | test_loss 0.4576 | test_acc 0.7835 | lr 0.0423
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 163/256 | train_loss 0.5551 | train_acc 0.7020 | test_loss 0.4576 | test_acc 0.7825 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 164/256 | train_loss 0.5840 | train_acc 0.6840 | test_loss 0.4736 | test_acc 0.7780 | lr 0.0414
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 165/256 | train_loss 0.5410 | train_acc 0.7260 | test_loss 0.4415 | test_acc 0.7965 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 166/256 | train_loss 0.5454 | train_acc 0.7240 | test_loss 0.5203 | test_acc 0.7600 | lr 0.0405
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 167/256 | train_loss 0.5689 | train_acc 0.6960 | test_loss 0.5066 | test_acc 0.7480 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 168/256 | train_loss 0.5315 | train_acc 0.7120 | test_loss 0.4587 | test_acc 0.7810 | lr 0.0395
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 169/256 | train_loss 0.5529 | train_acc 0.7200 | test_loss 0.4218 | test_acc 0.8085 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 170/256 | train_loss 0.5577 | train_acc 0.7120 | test_loss 0.5025 | test_acc 0.7530 | lr 0.0385
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 171/256 | train_loss 0.5442 | train_acc 0.7120 | test_loss 0.4727 | test_acc 0.7690 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 172/256 | train_loss 0.5608 | train_acc 0.7180 | test_loss 0.4171 | test_acc 0.8070 | lr 0.0374
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 173/256 | train_loss 0.5361 | train_acc 0.7480 | test_loss 0.4564 | test_acc 0.7840 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 174/256 | train_loss 0.5398 | train_acc 0.7360 | test_loss 0.5003 | test_acc 0.7650 | lr 0.0364
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 175/256 | train_loss 0.4853 | train_acc 0.7940 | test_loss 0.4467 | test_acc 0.7935 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 176/256 | train_loss 0.5418 | train_acc 0.7300 | test_loss 0.4998 | test_acc 0.7570 | lr 0.0353
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 177/256 | train_loss 0.5164 | train_acc 0.7700 | test_loss 0.4501 | test_acc 0.7855 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 178/256 | train_loss 0.4579 | train_acc 0.7980 | test_loss 0.4493 | test_acc 0.8020 | lr 0.0342
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 179/256 | train_loss 0.5687 | train_acc 0.7240 | test_loss 0.4976 | test_acc 0.7605 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 180/256 | train_loss 0.4910 | train_acc 0.7640 | test_loss 0.4949 | test_acc 0.7760 | lr 0.0330
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 181/256 | train_loss 0.4748 | train_acc 0.7800 | test_loss 0.4517 | test_acc 0.8000 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 182/256 | train_loss 0.5070 | train_acc 0.7420 | test_loss 0.4270 | test_acc 0.8010 | lr 0.0319
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 183/256 | train_loss 0.5550 | train_acc 0.7260 | test_loss 0.4536 | test_acc 0.7845 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 184/256 | train_loss 0.5176 | train_acc 0.7740 | test_loss 0.4166 | test_acc 0.8095 | lr 0.0307
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 185/256 | train_loss 0.5198 | train_acc 0.7300 | test_loss 0.4217 | test_acc 0.8030 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 186/256 | train_loss 0.5522 | train_acc 0.7220 | test_loss 0.4109 | test_acc 0.8135 | lr 0.0295
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 187/256 | train_loss 0.5232 | train_acc 0.7500 | test_loss 0.4445 | test_acc 0.7940 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 188/256 | train_loss 0.4909 | train_acc 0.7660 | test_loss 0.4884 | test_acc 0.7725 | lr 0.0283
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 189/256 | train_loss 0.4652 | train_acc 0.7780 | test_loss 0.4836 | test_acc 0.7730 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 190/256 | train_loss 0.5143 | train_acc 0.7680 | test_loss 0.4840 | test_acc 0.7750 | lr 0.0271
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 191/256 | train_loss 0.5124 | train_acc 0.7480 | test_loss 0.4220 | test_acc 0.8090 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 192/256 | train_loss 0.5072 | train_acc 0.7580 | test_loss 0.4362 | test_acc 0.7945 | lr 0.0259
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 193/256 | train_loss 0.4743 | train_acc 0.7820 | test_loss 0.4326 | test_acc 0.8070 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 194/256 | train_loss 0.5393 | train_acc 0.7340 | test_loss 0.4341 | test_acc 0.8030 | lr 0.0246
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 195/256 | train_loss 0.4985 | train_acc 0.7400 | test_loss 0.3981 | test_acc 0.8210 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 196/256 | train_loss 0.5265 | train_acc 0.7540 | test_loss 0.5412 | test_acc 0.7540 | lr 0.0234
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 197/256 | train_loss 0.4707 | train_acc 0.7680 | test_loss 0.4414 | test_acc 0.7965 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 198/256 | train_loss 0.4685 | train_acc 0.7640 | test_loss 0.3865 | test_acc 0.8305 | lr 0.0222
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 199/256 | train_loss 0.5101 | train_acc 0.7300 | test_loss 0.4681 | test_acc 0.7985 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 200/256 | train_loss 0.4878 | train_acc 0.7700 | test_loss 0.4495 | test_acc 0.8120 | lr 0.0210
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 201/256 | train_loss 0.5213 | train_acc 0.7620 | test_loss 0.4267 | test_acc 0.8035 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 202/256 | train_loss 0.4774 | train_acc 0.7880 | test_loss 0.5420 | test_acc 0.7585 | lr 0.0198
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 203/256 | train_loss 0.5025 | train_acc 0.7500 | test_loss 0.3881 | test_acc 0.8245 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 204/256 | train_loss 0.4871 | train_acc 0.8000 | test_loss 0.4206 | test_acc 0.8090 | lr 0.0186
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 205/256 | train_loss 0.4056 | train_acc 0.8200 | test_loss 0.3776 | test_acc 0.8340 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 206/256 | train_loss 0.4895 | train_acc 0.7600 | test_loss 0.4112 | test_acc 0.8215 | lr 0.0175
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 207/256 | train_loss 0.4501 | train_acc 0.7720 | test_loss 0.4511 | test_acc 0.8120 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 208/256 | train_loss 0.4768 | train_acc 0.7560 | test_loss 0.4326 | test_acc 0.8090 | lr 0.0163
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 209/256 | train_loss 0.4601 | train_acc 0.7740 | test_loss 0.4335 | test_acc 0.8170 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 210/256 | train_loss 0.4363 | train_acc 0.8060 | test_loss 0.3917 | test_acc 0.8340 | lr 0.0152
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 211/256 | train_loss 0.4166 | train_acc 0.8020 | test_loss 0.4143 | test_acc 0.8295 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 212/256 | train_loss 0.5020 | train_acc 0.7500 | test_loss 0.4110 | test_acc 0.8220 | lr 0.0141
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 213/256 | train_loss 0.5004 | train_acc 0.7380 | test_loss 0.4270 | test_acc 0.8125 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 214/256 | train_loss 0.4230 | train_acc 0.8020 | test_loss 0.3932 | test_acc 0.8345 | lr 0.0131
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 215/256 | train_loss 0.4622 | train_acc 0.7760 | test_loss 0.4402 | test_acc 0.8100 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 216/256 | train_loss 0.4806 | train_acc 0.7760 | test_loss 0.3975 | test_acc 0.8200 | lr 0.0120
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 217/256 | train_loss 0.4260 | train_acc 0.8200 | test_loss 0.4167 | test_acc 0.8195 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 218/256 | train_loss 0.4474 | train_acc 0.7940 | test_loss 0.4013 | test_acc 0.8285 | lr 0.0110
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 219/256 | train_loss 0.4653 | train_acc 0.7980 | test_loss 0.4268 | test_acc 0.8210 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 220/256 | train_loss 0.5097 | train_acc 0.7540 | test_loss 0.3715 | test_acc 0.8365 | lr 0.0100
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 221/256 | train_loss 0.4146 | train_acc 0.8040 | test_loss 0.3716 | test_acc 0.8365 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 222/256 | train_loss 0.4307 | train_acc 0.8000 | test_loss 0.4040 | test_acc 0.8230 | lr 0.0091
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 223/256 | train_loss 0.4844 | train_acc 0.7980 | test_loss 0.4459 | test_acc 0.8075 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 224/256 | train_loss 0.4250 | train_acc 0.8020 | test_loss 0.3664 | test_acc 0.8375 | lr 0.0082
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 225/256 | train_loss 0.4247 | train_acc 0.8020 | test_loss 0.3785 | test_acc 0.8270 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 226/256 | train_loss 0.4289 | train_acc 0.8080 | test_loss 0.3934 | test_acc 0.8200 | lr 0.0073
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 227/256 | train_loss 0.4178 | train_acc 0.7980 | test_loss 0.3564 | test_acc 0.8415 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 228/256 | train_loss 0.4526 | train_acc 0.7860 | test_loss 0.3962 | test_acc 0.8235 | lr 0.0065
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 229/256 | train_loss 0.4578 | train_acc 0.7740 | test_loss 0.3712 | test_acc 0.8415 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 230/256 | train_loss 0.4936 | train_acc 0.7660 | test_loss 0.3754 | test_acc 0.8320 | lr 0.0057
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 231/256 | train_loss 0.4431 | train_acc 0.7920 | test_loss 0.3705 | test_acc 0.8390 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 232/256 | train_loss 0.4279 | train_acc 0.8180 | test_loss 0.3651 | test_acc 0.8445 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 233/256 | train_loss 0.4017 | train_acc 0.8160 | test_loss 0.3858 | test_acc 0.8395 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 234/256 | train_loss 0.3873 | train_acc 0.8100 | test_loss 0.4052 | test_acc 0.8295 | lr 0.0043
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 235/256 | train_loss 0.4259 | train_acc 0.7700 | test_loss 0.3908 | test_acc 0.8385 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 236/256 | train_loss 0.3439 | train_acc 0.8540 | test_loss 0.3755 | test_acc 0.8390 | lr 0.0037
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 237/256 | train_loss 0.3962 | train_acc 0.8060 | test_loss 0.3802 | test_acc 0.8345 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 238/256 | train_loss 0.4079 | train_acc 0.8180 | test_loss 0.3939 | test_acc 0.8330 | lr 0.0031
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 239/256 | train_loss 0.4504 | train_acc 0.8120 | test_loss 0.3597 | test_acc 0.8395 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 240/256 | train_loss 0.4145 | train_acc 0.7920 | test_loss 0.3689 | test_acc 0.8405 | lr 0.0026
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 241/256 | train_loss 0.4074 | train_acc 0.8140 | test_loss 0.3655 | test_acc 0.8425 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 242/256 | train_loss 0.4291 | train_acc 0.7740 | test_loss 0.3761 | test_acc 0.8335 | lr 0.0022
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 243/256 | train_loss 0.4594 | train_acc 0.8100 | test_loss 0.3667 | test_acc 0.8440 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 244/256 | train_loss 0.4149 | train_acc 0.8160 | test_loss 0.3653 | test_acc 0.8405 | lr 0.0017
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 245/256 | train_loss 0.4137 | train_acc 0.8240 | test_loss 0.3725 | test_acc 0.8375 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 246/256 | train_loss 0.4270 | train_acc 0.8220 | test_loss 0.3919 | test_acc 0.8290 | lr 0.0014
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 247/256 | train_loss 0.4174 | train_acc 0.8340 | test_loss 0.3808 | test_acc 0.8310 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 248/256 | train_loss 0.3972 | train_acc 0.8100 | test_loss 0.3743 | test_acc 0.8400 | lr 0.0011
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 249/256 | train_loss 0.3968 | train_acc 0.8280 | test_loss 0.3809 | test_acc 0.8320 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 250/256 | train_loss 0.4408 | train_acc 0.7920 | test_loss 0.3796 | test_acc 0.8360 | lr 0.0009
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 251/256 | train_loss 0.4130 | train_acc 0.8000 | test_loss 0.3734 | test_acc 0.8350 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 252/256 | train_loss 0.4388 | train_acc 0.7940 | test_loss 0.3819 | test_acc 0.8330 | lr 0.0007
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 253/256 | train_loss 0.4119 | train_acc 0.8080 | test_loss 0.3783 | test_acc 0.8345 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 254/256 | train_loss 0.3887 | train_acc 0.8480 | test_loss 0.3676 | test_acc 0.8405 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 255/256 | train_loss 0.3850 | train_acc 0.8240 | test_loss 0.3633 | test_acc 0.8405 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 0 | Epoch 256/256 | train_loss 0.4836 | train_acc 0.7560 | test_loss 0.3920 | test_acc 0.8285 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (0)
[INFO] main.py:389 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (0)
[INFO] main.py:396 > [2-5] Report task result

##################################################
# Task 1 iteration
##################################################

[INFO] main.py:308 > [2-1] Prepare a datalist for the current task
meta_pseudo_init
total : 5000  current step :  0
total : 5000  current step :  1
total : 5000  current step :  2
total : 5000  current step :  3
total : 5000  current step :  4
total : 5000  current step :  5
total : 5000  current step :  6
total : 5000  current step :  7
total : 5000  current step :  8
total : 5000  current step :  9
total : 5000  current step :  10
total : 5000  current step :  11
total : 5000  current step :  12
total : 5000  current step :  13
total : 5000  current step :  14
total : 5000  current step :  15
total : 5000  current step :  16
total : 5000  current step :  17
total : 5000  current step :  18
total : 5000  current step :  19
total : 5000  current step :  20
total : 5000  current step :  21
total : 5000  current step :  22
total : 5000  current step :  23
total : 5000  current step :  24
total : 5000  current step :  25
total : 5000  current step :  26
total : 5000  current step :  27
total : 5000  current step :  28
total : 5000  current step :  29
total : 5000  current step :  30
total : 5000  current step :  31
total : 5000  current step :  32
total : 5000  current step :  33
total : 5000  current step :  34
total : 5000  current step :  35
total : 5000  current step :  36
total : 5000  current step :  37
total : 5000  current step :  38
total : 5000  current step :  39
total : 5000  current step :  40
total : 5000  current step :  41
total : 5000  current step :  42
total : 5000  current step :  43
total : 5000  current step :  44
total : 5000  current step :  45
total : 5000  current step :  46
total : 5000  current step :  47
total : 5000  current step :  48
total : 5000  current step :  49
total : 5000  current step :  50
total : 5000  current step :  51
total : 5000  current step :  52
total : 5000  current step :  53
total : 5000  current step :  54
total : 5000  current step :  55
total : 5000  current step :  56
total : 5000  current step :  57
total : 5000  current step :  58
total : 5000  current step :  59
total : 5000  current step :  60
total : 5000  current step :  61
total : 5000  current step :  62
total : 5000  current step :  63
total : 5000  current step :  64
total : 5000  current step :  65
total : 5000  current step :  66
total : 5000  current step :  67
total : 5000  current step :  68
total : 5000  current step :  69
total : 5000  current step :  70
total : 5000  current step :  71
total : 5000  current step :  72
total : 5000  current step :  73
total : 5000  current step :  74
total : 5000  current step :  75
total : 5000  current step :  76
total : 5000  current step :  77
total : 5000  current step :  78
total : 5000  current step :  79
total : 5000  current step :  80
total : 5000  current step :  81
total : 5000  current step :  82
total : 5000  current step :  83
total : 5000  current step :  84
total : 5000  current step :  85
total : 5000  current step :  86
total : 5000  current step :  87
total : 5000  current step :  88
total : 5000  current step :  89
total : 5000  current step :  90
total : 5000  current step :  91
total : 5000  current step :  92
total : 5000  current step :  93
total : 5000  current step :  94
total : 5000  current step :  95
total : 5000  current step :  96
total : 5000  current step :  97
total : 5000  current step :  98
total : 5000  current step :  99
total : 5000  current step :  100
total : 5000  current step :  101
total : 5000  current step :  102
total : 5000  current step :  103
total : 5000  current step :  104
total : 5000  current step :  105
total : 5000  current step :  106
total : 5000  current step :  107
total : 5000  current step :  108
total : 5000  current step :  109
total : 5000  current step :  110
total : 5000  current step :  111
total : 5000  current step :  112
total : 5000  current step :  113
total : 5000  current step :  114
total : 5000  current step :  115
total : 5000  current step :  116
total : 5000  current step :  117
total : 5000  current step :  118
total : 5000  current step :  119
total : 5000  current step :  120
total : 5000  current step :  121
total : 5000  current step :  122
total : 5000  current step :  123
total : 5000  current step :  124
total : 5000  current step :  125
total : 5000  current step :  126
total : 5000  current step :  127
total : 5000  current step :  128
total : 5000  current step :  129
total : 5000  current step :  130
total : 5000  current step :  131
total : 5000  current step :  132
total : 5000  current step :  133
total : 5000  current step :  134
total : 5000  current step :  135
total : 5000  current step :  136
total : 5000  current step :  137
total : 5000  current step :  138
total : 5000  current step :  139
total : 5000  current step :  140
total : 5000  current step :  141
total : 5000  current step :  142
total : 5000  current step :  143
total : 5000  current step :  144
total : 5000  current step :  145
total : 5000  current step :  146
total : 5000  current step :  147
total : 5000  current step :  148
total : 5000  current step :  149
total : 5000  current step :  150
total : 5000  current step :  151
total : 5000  current step :  152
total : 5000  current step :  153
total : 5000  current step :  154
total : 5000  current step :  155
total : 5000  current step :  156
total : 5000  current step :  157
total : 5000  current step :  158
total : 5000  current step :  159
total : 5000  current step :  160
total : 5000  current step :  161
total : 5000  current step :  162
total : 5000  current step :  163
total : 5000  current step :  164
total : 5000  current step :  165
total : 5000  current step :  166
total : 5000  current step :  167
total : 5000  current step :  168
total : 5000  current step :  169
total : 5000  current step :  170
total : 5000  current step :  171
total : 5000  current step :  172
total : 5000  current step :  173
total : 5000  current step :  174
total : 5000  current step :  175
total : 5000  current step :  176
total : 5000  current step :  177
total : 5000  current step :  178
total : 5000  current step :  179
total : 5000  current step :  180
total : 5000  current step :  181
total : 5000  current step :  182
total : 5000  current step :  183
total : 5000  current step :  184
total : 5000  current step :  185
total : 5000  current step :  186
total : 5000  current step :  187
total : 5000  current step :  188
total : 5000  current step :  189
total : 5000  current step :  190
total : 5000  current step :  191
total : 5000  current step :  192
total : 5000  current step :  193
total : 5000  current step :  194
total : 5000  current step :  195
total : 5000  current step :  196
total : 5000  current step :  197
total : 5000  current step :  198
total : 5000  current step :  199
total : 5000  current step :  200
total : 5000  current step :  201
total : 5000  current step :  202
total : 5000  current step :  203
total : 5000  current step :  204
total : 5000  current step :  205
total : 5000  current step :  206
total : 5000  current step :  207
total : 5000  current step :  208
total : 5000  current step :  209
total : 5000  current step :  210
total : 5000  current step :  211
total : 5000  current step :  212
total : 5000  current step :  213
total : 5000  current step :  214
total : 5000  current step :  215
total : 5000  current step :  216
total : 5000  current step :  217
total : 5000  current step :  218
total : 5000  current step :  219
total : 5000  current step :  220
total : 5000  current step :  221
total : 5000  current step :  222
total : 5000  current step :  223
total : 5000  current step :  224
total : 5000  current step :  225
total : 5000  current step :  226
total : 5000  current step :  227
total : 5000  current step :  228
total : 5000  current step :  229
total : 5000  current step :  230
total : 5000  current step :  231
total : 5000  current step :  232
total : 5000  current step :  233
total : 5000  current step :  234
total : 5000  current step :  235
total : 5000  current step :  236
total : 5000  current step :  237
total : 5000  current step :  238
total : 5000  current step :  239
total : 5000  current step :  240
total : 5000  current step :  241
total : 5000  current step :  242
total : 5000  current step :  243
total : 5000  current step :  244
total : 5000  current step :  245
total : 5000  current step :  246
total : 5000  current step :  247
total : 5000  current step :  248
total : 5000  current step :  249
total : 5000  current step :  250
total : 5000  current step :  251
total : 5000  current step :  252
total : 5000  current step :  253
total : 5000  current step :  254
total : 5000  current step :  255
total : 5000  current step :  256
total : 5000  current step :  257
total : 5000  current step :  258
total : 5000  current step :  259
total : 5000  current step :  260
total : 5000  current step :  261
total : 5000  current step :  262
total : 5000  current step :  263
total : 5000  current step :  264
total : 5000  current step :  265
total : 5000  current step :  266
total : 5000  current step :  267
total : 5000  current step :  268
total : 5000  current step :  269
total : 5000  current step :  270
total : 5000  current step :  271
total : 5000  current step :  272
total : 5000  current step :  273
total : 5000  current step :  274
total : 5000  current step :  275
total : 5000  current step :  276
total : 5000  current step :  277
total : 5000  current step :  278
total : 5000  current step :  279
total : 5000  current step :  280
total : 5000  current step :  281
total : 5000  current step :  282
total : 5000  current step :  283
total : 5000  current step :  284
total : 5000  current step :  285
total : 5000  current step :  286
total : 5000  current step :  287
total : 5000  current step :  288
total : 5000  current step :  289
total : 5000  current step :  290
total : 5000  current step :  291
total : 5000  current step :  292
total : 5000  current step :  293
total : 5000  current step :  294
total : 5000  current step :  295
total : 5000  current step :  296
total : 5000  current step :  297
total : 5000  current step :  298
total : 5000  current step :  299
total : 5000  current step :  300
total : 5000  current step :  301
total : 5000  current step :  302
total : 5000  current step :  303
total : 5000  current step :  304
total : 5000  current step :  305
total : 5000  current step :  306
total : 5000  current step :  307
total : 5000  current step :  308
total : 5000  current step :  309
total : 5000  current step :  310
total : 5000  current step :  311
total : 5000  current step :  312
total : 5000  current step :  313
total : 5000  current step :  314
total : 5000  current step :  315
total : 5000  current step :  316
total : 5000  current step :  317
total : 5000  current step :  318
total : 5000  current step :  319
total : 5000  current step :  320
total : 5000  current step :  321
total : 5000  current step :  322
total : 5000  current step :  323
total : 5000  current step :  324
total : 5000  current step :  325
total : 5000  current step :  326
total : 5000  current step :  327
total : 5000  current step :  328
total : 5000  current step :  329
total : 5000  current step :  330
total : 5000  current step :  331
total : 5000  current step :  332
total : 5000  current step :  333
total : 5000  current step :  334
total : 5000  current step :  335
total : 5000  current step :  336
total : 5000  current step :  337
total : 5000  current step :  338
total : 5000  current step :  339
total : 5000  current step :  340
total : 5000  current step :  341
total : 5000  current step :  342
total : 5000  current step :  343
total : 5000  current step :  344
total : 5000  current step :  345
total : 5000  current step :  346
total : 5000  current step :  347
total : 5000  current step :  348
total : 5000  current step :  349
total : 5000  current step :  350
total : 5000  current step :  351
total : 5000  current step :  352
total : 5000  current step :  353
total : 5000  current step :  354
total : 5000  current step :  355
total : 5000  current step :  356
total : 5000  current step :  357
total : 5000  current step :  358
total : 5000  current step :  359
total : 5000  current step :  360
total : 5000  current step :  361
total : 5000  current step :  362
total : 5000  current step :  363
total : 5000  current step :  364
total : 5000  current step :  365
total : 5000  current step :  366
total : 5000  current step :  367
total : 5000  current step :  368
total : 5000  current step :  369
total : 5000  current step :  370
total : 5000  current step :  371
total : 5000  current step :  372
total : 5000  current step :  373
total : 5000  current step :  374
total : 5000  current step :  375
total : 5000  current step :  376
total : 5000  current step :  377
total : 5000  current step :  378
total : 5000  current step :  379
total : 5000  current step :  380
total : 5000  current step :  381
total : 5000  current step :  382
total : 5000  current step :  383
total : 5000  current step :  384
total : 5000  current step :  385
total : 5000  current step :  386
total : 5000  current step :  387
total : 5000  current step :  388
total : 5000  current step :  389
total : 5000  current step :  390
total : 5000  current step :  391
total : 5000  current step :  392
total : 5000  current step :  393
total : 5000  current step :  394
total : 5000  current step :  395
total : 5000  current step :  396
total : 5000  current step :  397
total : 5000  current step :  398
total : 5000  current step :  399
total : 5000  current step :  400
total : 5000  current step :  401
total : 5000  current step :  402
total : 5000  current step :  403
total : 5000  current step :  404
total : 5000  current step :  405
total : 5000  current step :  406
total : 5000  current step :  407
total : 5000  current step :  408
total : 5000  current step :  409
total : 5000  current step :  410
total : 5000  current step :  411
total : 5000  current step :  412
total : 5000  current step :  413
total : 5000  current step :  414
total : 5000  current step :  415
total : 5000  current step :  416
total : 5000  current step :  417
total : 5000  current step :  418
total : 5000  current step :  419
total : 5000  current step :  420
total : 5000  current step :  421
total : 5000  current step :  422
total : 5000  current step :  423
total : 5000  current step :  424
total : 5000  current step :  425
total : 5000  current step :  426
total : 5000  current step :  427
total : 5000  current step :  428
total : 5000  current step :  429
total : 5000  current step :  430
total : 5000  current step :  431
total : 5000  current step :  432
total : 5000  current step :  433
total : 5000  current step :  434
total : 5000  current step :  435
total : 5000  current step :  436
total : 5000  current step :  437
total : 5000  current step :  438
total : 5000  current step :  439
total : 5000  current step :  440
total : 5000  current step :  441
total : 5000  current step :  442
total : 5000  current step :  443
total : 5000  current step :  444
total : 5000  current step :  445
total : 5000  current step :  446
total : 5000  current step :  447
total : 5000  current step :  448
total : 5000  current step :  449
total : 5000  current step :  450
total : 5000  current step :  451
total : 5000  current step :  452
total : 5000  current step :  453
total : 5000  current step :  454
total : 5000  current step :  455
total : 5000  current step :  456
total : 5000  current step :  457
total : 5000  current step :  458
total : 5000  current step :  459
total : 5000  current step :  460
total : 5000  current step :  461
total : 5000  current step :  462
total : 5000  current step :  463
total : 5000  current step :  464
total : 5000  current step :  465
total : 5000  current step :  466
total : 5000  current step :  467
total : 5000  current step :  468
total : 5000  current step :  469
total : 5000  current step :  470
total : 5000  current step :  471
total : 5000  current step :  472
total : 5000  current step :  473
total : 5000  current step :  474
total : 5000  current step :  475
total : 5000  current step :  476
total : 5000  current step :  477
total : 5000  current step :  478
total : 5000  current step :  479
total : 5000  current step :  480
total : 5000  current step :  481
total : 5000  current step :  482
total : 5000  current step :  483
total : 5000  current step :  484
total : 5000  current step :  485
total : 5000  current step :  486
total : 5000  current step :  487
total : 5000  current step :  488
total : 5000  current step :  489
total : 5000  current step :  490
total : 5000  current step :  491
total : 5000  current step :  492
total : 5000  current step :  493
total : 5000  current step :  494
total : 5000  current step :  495
total : 5000  current step :  496
total : 5000  current step :  497
total : 5000  current step :  498
total : 5000  current step :  499
total : 5000  current step :  500
total : 5000  current step :  501
total : 5000  current step :  502
total : 5000  current step :  503
total : 5000  current step :  504
total : 5000  current step :  505
total : 5000  current step :  506
total : 5000  current step :  507
total : 5000  current step :  508
total : 5000  current step :  509
total : 5000  current step :  510
total : 5000  current step :  511
total : 5000  current step :  512
total : 5000  current step :  513
total : 5000  current step :  514
total : 5000  current step :  515
total : 5000  current step :  516
total : 5000  current step :  517
total : 5000  current step :  518
total : 5000  current step :  519
total : 5000  current step :  520
total : 5000  current step :  521
total : 5000  current step :  522
total : 5000  current step :  523
total : 5000  current step :  524
total : 5000  current step :  525
total : 5000  current step :  526
total : 5000  current step :  527
total : 5000  current step :  528
total : 5000  current step :  529
total : 5000  current step :  530
total : 5000  current step :  531
total : 5000  current step :  532
total : 5000  current step :  533
total : 5000  current step :  534
total : 5000  current step :  535
total : 5000  current step :  536
total : 5000  current step :  537
total : 5000  current step :  538
total : 5000  current step :  539
total : 5000  current step :  540
total : 5000  current step :  541
total : 5000  current step :  542
total : 5000  current step :  543
total : 5000  current step :  544
total : 5000  current step :  545
total : 5000  current step :  546
total : 5000  current step :  547
total : 5000  current step :  548
total : 5000  current step :  549
total : 5000  current step :  550
total : 5000  current step :  551
total : 5000  current step :  552
total : 5000  current step :  553
total : 5000  current step :  554
total : 5000  current step :  555
total : 5000  current step :  556
total : 5000  current step :  557
total : 5000  current step :  558
total : 5000  current step :  559
total : 5000  current step :  560
total : 5000  current step :  561
total : 5000  current step :  562
total : 5000  current step :  563
total : 5000  current step :  564
total : 5000  current step :  565
total : 5000  current step :  566
total : 5000  current step :  567
total : 5000  current step :  568
total : 5000  current step :  569
total : 5000  current step :  570
total : 5000  current step :  571
total : 5000  current step :  572
total : 5000  current step :  573
total : 5000  current step :  574
total : 5000  current step :  575
total : 5000  current step :  576
total : 5000  current step :  577
total : 5000  current step :  578
total : 5000  current step :  579
total : 5000  current step :  580
total : 5000  current step :  581
total : 5000  current step :  582
total : 5000  current step :  583
total : 5000  current step :  584
total : 5000  current step :  585
total : 5000  current step :  586
total : 5000  current step :  587
total : 5000  current step :  588
total : 5000  current step :  589
total : 5000  current step :  590
total : 5000  current step :  591
total : 5000  current step :  592
total : 5000  current step :  593
total : 5000  current step :  594
total : 5000  current step :  595
total : 5000  current step :  596
total : 5000  current step :  597
total : 5000  current step :  598
total : 5000  current step :  599
total : 5000  current step :  600
total : 5000  current step :  601
total : 5000  current step :  602
total : 5000  current step :  603
total : 5000  current step :  604
total : 5000  current step :  605
total : 5000  current step :  606
total : 5000  current step :  607
total : 5000  current step :  608
total : 5000  current step :  609
total : 5000  current step :  610
total : 5000  current step :  611
total : 5000  current step :  612
total : 5000  current step :  613
total : 5000  current step :  614
total : 5000  current step :  615
total : 5000  current step :  616
total : 5000  current step :  617
total : 5000  current step :  618
total : 5000  current step :  619
total : 5000  current step :  620
total : 5000  current step :  621
total : 5000  current step :  622
total : 5000  current step :  623
total : 5000  current step :  624
total : 5000  current step :  625
total : 5000  current step :  626
total : 5000  current step :  627
total : 5000  current step :  628
total : 5000  current step :  629
total : 5000  current step :  630
total : 5000  current step :  631
total : 5000  current step :  632
total : 5000  current step :  633
total : 5000  current step :  634
total : 5000  current step :  635
total : 5000  current step :  636
total : 5000  current step :  637
total : 5000  current step :  638
total : 5000  current step :  639
total : 5000  current step :  640
total : 5000  current step :  641
total : 5000  current step :  642
total : 5000  current step :  643
total : 5000  current step :  644
total : 5000  current step :  645
total : 5000  current step :  646
total : 5000  current step :  647
total : 5000  current step :  648
total : 5000  current step :  649
total : 5000  current step :  650
total : 5000  current step :  651
total : 5000  current step :  652
total : 5000  current step :  653
total : 5000  current step :  654
total : 5000  current step :  655
total : 5000  current step :  656
total : 5000  current step :  657
total : 5000  current step :  658
total : 5000  current step :  659
total : 5000  current step :  660
total : 5000  current step :  661
total : 5000  current step :  662
total : 5000  current step :  663
total : 5000  current step :  664
total : 5000  current step :  665
total : 5000  current step :  666
total : 5000  current step :  667
total : 5000  current step :  668
total : 5000  current step :  669
total : 5000  current step :  670
total : 5000  current step :  671
total : 5000  current step :  672
total : 5000  current step :  673
total : 5000  current step :  674
total : 5000  current step :  675
total : 5000  current step :  676
total : 5000  current step :  677
total : 5000  current step :  678
total : 5000  current step :  679
total : 5000  current step :  680
total : 5000  current step :  681
total : 5000  current step :  682
total : 5000  current step :  683
total : 5000  current step :  684
total : 5000  current step :  685
total : 5000  current step :  686
total : 5000  current step :  687
total : 5000  current step :  688
total : 5000  current step :  689
total : 5000  current step :  690
total : 5000  current step :  691
total : 5000  current step :  692
total : 5000  current step :  693
total : 5000  current step :  694
total : 5000  current step :  695
total : 5000  current step :  696
total : 5000  current step :  697
total : 5000  current step :  698
total : 5000  current step :  699
total : 5000  current step :  700
total : 5000  current step :  701
total : 5000  current step :  702
total : 5000  current step :  703
total : 5000  current step :  704
total : 5000  current step :  705
total : 5000  current step :  706
total : 5000  current step :  707
total : 5000  current step :  708
total : 5000  current step :  709
total : 5000  current step :  710
total : 5000  current step :  711
total : 5000  current step :  712
total : 5000  current step :  713
total : 5000  current step :  714
total : 5000  current step :  715
total : 5000  current step :  716
total : 5000  current step :  717
total : 5000  current step :  718
total : 5000  current step :  719
total : 5000  current step :  720
total : 5000  current step :  721
total : 5000  current step :  722
total : 5000  current step :  723
total : 5000  current step :  724
total : 5000  current step :  725
total : 5000  current step :  726
total : 5000  current step :  727
total : 5000  current step :  728
total : 5000  current step :  729
total : 5000  current step :  730
total : 5000  current step :  731
total : 5000  current step :  732
total : 5000  current step :  733
total : 5000  current step :  734
total : 5000  current step :  735
total : 5000  current step :  736
total : 5000  current step :  737
total : 5000  current step :  738
total : 5000  current step :  739
total : 5000  current step :  740
total : 5000  current step :  741
total : 5000  current step :  742
total : 5000  current step :  743
total : 5000  current step :  744
total : 5000  current step :  745
total : 5000  current step :  746
total : 5000  current step :  747
total : 5000  current step :  748
total : 5000  current step :  749
total : 5000  current step :  750
total : 5000  current step :  751
total : 5000  current step :  752
total : 5000  current step :  753
total : 5000  current step :  754
total : 5000  current step :  755
total : 5000  current step :  756
total : 5000  current step :  757
total : 5000  current step :  758
total : 5000  current step :  759
total : 5000  current step :  760
total : 5000  current step :  761
total : 5000  current step :  762
total : 5000  current step :  763
total : 5000  current step :  764
total : 5000  current step :  765
total : 5000  current step :  766
total : 5000  current step :  767
total : 5000  current step :  768
total : 5000  current step :  769
total : 5000  current step :  770
total : 5000  current step :  771
total : 5000  current step :  772
total : 5000  current step :  773
total : 5000  current step :  774
total : 5000  current step :  775
total : 5000  current step :  776
total : 5000  current step :  777
total : 5000  current step :  778
total : 5000  current step :  779
total : 5000  current step :  780
total : 5000  current step :  781
total : 5000  current step :  782
total : 5000  current step :  783
total : 5000  current step :  784
total : 5000  current step :  785
total : 5000  current step :  786
total : 5000  current step :  787
total : 5000  current step :  788
total : 5000  current step :  789
total : 5000  current step :  790
total : 5000  current step :  791
total : 5000  current step :  792
total : 5000  current step :  793
total : 5000  current step :  794
total : 5000  current step :  795
total : 5000  current step :  796
total : 5000  current step :  797
total : 5000  current step :  798
total : 5000  current step :  799
total : 5000  current step :  800
total : 5000  current step :  801
total : 5000  current step :  802
total : 5000  current step :  803
total : 5000  current step :  804
total : 5000  current step :  805
total : 5000  current step :  806
total : 5000  current step :  807
total : 5000  current step :  808
total : 5000  current step :  809
total : 5000  current step :  810
total : 5000  current step :  811
total : 5000  current step :  812
total : 5000  current step :  813
total : 5000  current step :  814
total : 5000  current step :  815
total : 5000  current step :  816
total : 5000  current step :  817
total : 5000  current step :  818
total : 5000  current step :  819
total : 5000  current step :  820
total : 5000  current step :  821
total : 5000  current step :  822
total : 5000  current step :  823
total : 5000  current step :  824
total : 5000  current step :  825
total : 5000  current step :  826
total : 5000  current step :  827
total : 5000  current step :  828
total : 5000  current step :  829
total : 5000  current step :  830
total : 5000  current step :  831
total : 5000  current step :  832
total : 5000  current step :  833
total : 5000  current step :  834
total : 5000  current step :  835
total : 5000  current step :  836
total : 5000  current step :  837
total : 5000  current step :  838
total : 5000  current step :  839
total : 5000  current step :  840
total : 5000  current step :  841
total : 5000  current step :  842
total : 5000  current step :  843
total : 5000  current step :  844
total : 5000  current step :  845
total : 5000  current step :  846
total : 5000  current step :  847
total : 5000  current step :  848
total : 5000  current step :  849
total : 5000  current step :  850
total : 5000  current step :  851
total : 5000  current step :  852
total : 5000  current step :  853
total : 5000  current step :  854
total : 5000  current step :  855
total : 5000  current step :  856
total : 5000  current step :  857
total : 5000  current step :  858
total : 5000  current step :  859
total : 5000  current step :  860
total : 5000  current step :  861
total : 5000  current step :  862
total : 5000  current step :  863
total : 5000  current step :  864
total : 5000  current step :  865
total : 5000  current step :  866
total : 5000  current step :  867
total : 5000  current step :  868
total : 5000  current step :  869
total : 5000  current step :  870
total : 5000  current step :  871
total : 5000  current step :  872
total : 5000  current step :  873
total : 5000  current step :  874
total : 5000  current step :  875
total : 5000  current step :  876
total : 5000  current step :  877
total : 5000  current step :  878
total : 5000  current step :  879
total : 5000  current step :  880
total : 5000  current step :  881
total : 5000  current step :  882
total : 5000  current step :  883
total : 5000  current step :  884
total : 5000  current step :  885
total : 5000  current step :  886
total : 5000  current step :  887
total : 5000  current step :  888
total : 5000  current step :  889
total : 5000  current step :  890
total : 5000  current step :  891
total : 5000  current step :  892
total : 5000  current step :  893
total : 5000  current step :  894
total : 5000  current step :  895
total : 5000  current step :  896
total : 5000  current step :  897
total : 5000  current step :  898
total : 5000  current step :  899
total : 5000  current step :  900
total : 5000  current step :  901
total : 5000  current step :  902
total : 5000  current step :  903
total : 5000  current step :  904
total : 5000  current step :  905
total : 5000  current step :  906
total : 5000  current step :  907
total : 5000  current step :  908
total : 5000  current step :  909
total : 5000  current step :  910
total : 5000  current step :  911
total : 5000  current step :  912
total : 5000  current step :  913
total : 5000  current step :  914
total : 5000  current step :  915
total : 5000  current step :  916
total : 5000  current step :  917
total : 5000  current step :  918
total : 5000  current step :  919
total : 5000  current step :  920
total : 5000  current step :  921
total : 5000  current step :  922
total : 5000  current step :  923
total : 5000  current step :  924
total : 5000  current step :  925
total : 5000  current step :  926
total : 5000  current step :  927
total : 5000  current step :  928
total : 5000  current step :  929
total : 5000  current step :  930
total : 5000  current step :  931
total : 5000  current step :  932
total : 5000  current step :  933
total : 5000  current step :  934
total : 5000  current step :  935
total : 5000  current step :  936
total : 5000  current step :  937
total : 5000  current step :  938
total : 5000  current step :  939
total : 5000  current step :  940
total : 5000  current step :  941
total : 5000  current step :  942
total : 5000  current step :  943
total : 5000  current step :  944
total : 5000  current step :  945
total : 5000  current step :  946
total : 5000  current step :  947
total : 5000  current step :  948
total : 5000  current step :  949
total : 5000  current step :  950
total : 5000  current step :  951
total : 5000  current step :  952
total : 5000  current step :  953
total : 5000  current step :  954
total : 5000  current step :  955
total : 5000  current step :  956
total : 5000  current step :  957
total : 5000  current step :  958
total : 5000  current step :  959
total : 5000  current step :  960
total : 5000  current step :  961
total : 5000  current step :  962
total : 5000  current step :  963
total : 5000  current step :  964
total : 5000  current step :  965
total : 5000  current step :  966
total : 5000  current step :  967
total : 5000  current step :  968
total : 5000  current step :  969
total : 5000  current step :  970
total : 5000  current step :  971
total : 5000  current step :  972
total : 5000  current step :  973
total : 5000  current step :  974
total : 5000  current step :  975
total : 5000  current step :  976
total : 5000  current step :  977
total : 5000  current step :  978
total : 5000  current step :  979
total : 5000  current step :  980
total : 5000  current step :  981
total : 5000  current step :  982
total : 5000  current step :  983
total : 5000  current step :  984
total : 5000  current step :  985
total : 5000  current step :  986
total : 5000  current step :  987
total : 5000  current step :  988
total : 5000  current step :  989
total : 5000  current step :  990
total : 5000  current step :  991
total : 5000  current step :  992
total : 5000  current step :  993
total : 5000  current step :  994
total : 5000  current step :  995
total : 5000  current step :  996
total : 5000  current step :  997
total : 5000  current step :  998
total : 5000  current step :  999
total : 5000  current step :  1000
total : 5000  current step :  1001
total : 5000  current step :  1002
total : 5000  current step :  1003
total : 5000  current step :  1004
total : 5000  current step :  1005
total : 5000  current step :  1006
total : 5000  current step :  1007
total : 5000  current step :  1008
total : 5000  current step :  1009
total : 5000  current step :  1010
total : 5000  current step :  1011
total : 5000  current step :  1012
total : 5000  current step :  1013
total : 5000  current step :  1014
total : 5000  current step :  1015
total : 5000  current step :  1016
total : 5000  current step :  1017
total : 5000  current step :  1018
total : 5000  current step :  1019
total : 5000  current step :  1020
total : 5000  current step :  1021
total : 5000  current step :  1022
total : 5000  current step :  1023
total : 5000  current step :  1024
total : 5000  current step :  1025
total : 5000  current step :  1026
total : 5000  current step :  1027
total : 5000  current step :  1028
total : 5000  current step :  1029
total : 5000  current step :  1030
total : 5000  current step :  1031
total : 5000  current step :  1032
total : 5000  current step :  1033
total : 5000  current step :  1034
total : 5000  current step :  1035
total : 5000  current step :  1036
total : 5000  current step :  1037
total : 5000  current step :  1038
total : 5000  current step :  1039
total : 5000  current step :  1040
total : 5000  current step :  1041
total : 5000  current step :  1042
total : 5000  current step :  1043
total : 5000  current step :  1044
total : 5000  current step :  1045
total : 5000  current step :  1046
total : 5000  current step :  1047
total : 5000  current step :  1048
total : 5000  current step :  1049
total : 5000  current step :  1050
total : 5000  current step :  1051
total : 5000  current step :  1052
total : 5000  current step :  1053
total : 5000  current step :  1054
total : 5000  current step :  1055
total : 5000  current step :  1056
total : 5000  current step :  1057
total : 5000  current step :  1058
total : 5000  current step :  1059
total : 5000  current step :  1060
total : 5000  current step :  1061
total : 5000  current step :  1062
total : 5000  current step :  1063
total : 5000  current step :  1064
total : 5000  current step :  1065
total : 5000  current step :  1066
total : 5000  current step :  1067
total : 5000  current step :  1068
total : 5000  current step :  1069
total : 5000  current step :  1070
total : 5000  current step :  1071
total : 5000  current step :  1072
total : 5000  current step :  1073
total : 5000  current step :  1074
total : 5000  current step :  1075
total : 5000  current step :  1076
total : 5000  current step :  1077
total : 5000  current step :  1078
total : 5000  current step :  1079
total : 5000  current step :  1080
total : 5000  current step :  1081
total : 5000  current step :  1082
total : 5000  current step :  1083
total : 5000  current step :  1084
total : 5000  current step :  1085
total : 5000  current step :  1086
total : 5000  current step :  1087
total : 5000  current step :  1088
total : 5000  current step :  1089
total : 5000  current step :  1090
total : 5000  current step :  1091
total : 5000  current step :  1092
total : 5000  current step :  1093
total : 5000  current step :  1094
total : 5000  current step :  1095
total : 5000  current step :  1096
total : 5000  current step :  1097
total : 5000  current step :  1098
total : 5000  current step :  1099
total : 5000  current step :  1100
total : 5000  current step :  1101
total : 5000  current step :  1102
total : 5000  current step :  1103
total : 5000  current step :  1104
total : 5000  current step :  1105
total : 5000  current step :  1106
total : 5000  current step :  1107
total : 5000  current step :  1108
total : 5000  current step :  1109
total : 5000  current step :  1110
total : 5000  current step :  1111
total : 5000  current step :  1112
total : 5000  current step :  1113
total : 5000  current step :  1114
total : 5000  current step :  1115
total : 5000  current step :  1116
total : 5000  current step :  1117
total : 5000  current step :  1118
total : 5000  current step :  1119
total : 5000  current step :  1120
total : 5000  current step :  1121
total : 5000  current step :  1122
total : 5000  current step :  1123
total : 5000  current step :  1124
total : 5000  current step :  1125
total : 5000  current step :  1126
total : 5000  current step :  1127
total : 5000  current step :  1128
total : 5000  current step :  1129
total : 5000  current step :  1130
total : 5000  current step :  1131
total : 5000  current step :  1132
total : 5000  current step :  1133
total : 5000  current step :  1134
total : 5000  current step :  1135
total : 5000  current step :  1136
total : 5000  current step :  1137
total : 5000  current step :  1138
total : 5000  current step :  1139
total : 5000  current step :  1140
total : 5000  current step :  1141
total : 5000  current step :  1142
total : 5000  current step :  1143
total : 5000  current step :  1144
total : 5000  current step :  1145
total : 5000  current step :  1146
total : 5000  current step :  1147
total : 5000  current step :  1148
total : 5000  current step :  1149
total : 5000  current step :  1150
total : 5000  current step :  1151
total : 5000  current step :  1152
total : 5000  current step :  1153
total : 5000  current step :  1154
total : 5000  current step :  1155
total : 5000  current step :  1156
total : 5000  current step :  1157
total : 5000  current step :  1158
total : 5000  current step :  1159
total : 5000  current step :  1160
total : 5000  current step :  1161
total : 5000  current step :  1162
total : 5000  current step :  1163
total : 5000  current step :  1164
total : 5000  current step :  1165
total : 5000  current step :  1166
total : 5000  current step :  1167
total : 5000  current step :  1168
total : 5000  current step :  1169
total : 5000  current step :  1170
total : 5000  current step :  1171
total : 5000  current step :  1172
total : 5000  current step :  1173
total : 5000  current step :  1174
total : 5000  current step :  1175
total : 5000  current step :  1176
total : 5000  current step :  1177
total : 5000  current step :  1178
total : 5000  current step :  1179
total : 5000  current step :  1180
total : 5000  current step :  1181
total : 5000  current step :  1182
total : 5000  current step :  1183
total : 5000  current step :  1184
total : 5000  current step :  1185
total : 5000  current step :  1186
total : 5000  current step :  1187
total : 5000  current step :  1188
total : 5000  current step :  1189
total : 5000  current step :  1190
total : 5000  current step :  1191
total : 5000  current step :  1192
total : 5000  current step :  1193
total : 5000  current step :  1194
total : 5000  current step :  1195
total : 5000  current step :  1196
total : 5000  current step :  1197
total : 5000  current step :  1198
total : 5000  current step :  1199
total : 5000  current step :  1200
total : 5000  current step :  1201
total : 5000  current step :  1202
total : 5000  current step :  1203
total : 5000  current step :  1204
total : 5000  current step :  1205
total : 5000  current step :  1206
total : 5000  current step :  1207
total : 5000  current step :  1208
total : 5000  current step :  1209
total : 5000  current step :  1210
total : 5000  current step :  1211
total : 5000  current step :  1212
total : 5000  current step :  1213
total : 5000  current step :  1214
total : 5000  current step :  1215
total : 5000  current step :  1216
total : 5000  current step :  1217
total : 5000  current step :  1218
total : 5000  current step :  1219
total : 5000  current step :  1220
total : 5000  current step :  1221
total : 5000  current step :  1222
total : 5000  current step :  1223
total : 5000  current step :  1224
total : 5000  current step :  1225
total : 5000  current step :  1226
total : 5000  current step :  1227
total : 5000  current step :  1228
total : 5000  current step :  1229
total : 5000  current step :  1230
total : 5000  current step :  1231
total : 5000  current step :  1232
total : 5000  current step :  1233
total : 5000  current step :  1234
total : 5000  current step :  1235
total : 5000  current step :  1236
total : 5000  current step :  1237
total : 5000  current step :  1238
total : 5000  current step :  1239
total : 5000  current step :  1240
total : 5000  current step :  1241
total : 5000  current step :  1242
total : 5000  current step :  1243
total : 5000  current step :  1244
total : 5000  current step :  1245
total : 5000  current step :  1246
total : 5000  current step :  1247
total : 5000  current step :  1248
total : 5000  current step :  1249
total : 5000  current step :  1250
total : 5000  current step :  1251
total : 5000  current step :  1252
total : 5000  current step :  1253
total : 5000  current step :  1254
total : 5000  current step :  1255
total : 5000  current step :  1256
total : 5000  current step :  1257
total : 5000  current step :  1258
total : 5000  current step :  1259
total : 5000  current step :  1260
total : 5000  current step :  1261
total : 5000  current step :  1262
total : 5000  current step :  1263
total : 5000  current step :  1264
total : 5000  current step :  1265
total : 5000  current step :  1266
total : 5000  current step :  1267
total : 5000  current step :  1268
total : 5000  current step :  1269
total : 5000  current step :  1270
total : 5000  current step :  1271
total : 5000  current step :  1272
total : 5000  current step :  1273
total : 5000  current step :  1274
total : 5000  current step :  1275
total : 5000  current step :  1276
total : 5000  current step :  1277
total : 5000  current step :  1278
total : 5000  current step :  1279
total : 5000  current step :  1280
total : 5000  current step :  1281
total : 5000  current step :  1282
total : 5000  current step :  1283
total : 5000  current step :  1284
total : 5000  current step :  1285
total : 5000  current step :  1286
total : 5000  current step :  1287
total : 5000  current step :  1288
total : 5000  current step :  1289
total : 5000  current step :  1290
total : 5000  current step :  1291
total : 5000  current step :  1292
total : 5000  current step :  1293
total : 5000  current step :  1294
total : 5000  current step :  1295
total : 5000  current step :  1296
total : 5000  current step :  1297
total : 5000  current step :  1298
total : 5000  current step :  1299
total : 5000  current step :  1300
total : 5000  current step :  1301
total : 5000  current step :  1302
total : 5000  current step :  1303
total : 5000  current step :  1304
total : 5000  current step :  1305
total : 5000  current step :  1306
total : 5000  current step :  1307
total : 5000  current step :  1308
total : 5000  current step :  1309
total : 5000  current step :  1310
total : 5000  current step :  1311
total : 5000  current step :  1312
total : 5000  current step :  1313
total : 5000  current step :  1314
total : 5000  current step :  1315
total : 5000  current step :  1316
total : 5000  current step :  1317
total : 5000  current step :  1318
total : 5000  current step :  1319
total : 5000  current step :  1320
total : 5000  current step :  1321
total : 5000  current step :  1322
total : 5000  current step :  1323
total : 5000  current step :  1324
total : 5000  current step :  1325
total : 5000  current step :  1326
total : 5000  current step :  1327
total : 5000  current step :  1328
total : 5000  current step :  1329
total : 5000  current step :  1330
total : 5000  current step :  1331
total : 5000  current step :  1332
total : 5000  current step :  1333
total : 5000  current step :  1334
total : 5000  current step :  1335
total : 5000  current step :  1336
total : 5000  current step :  1337
total : 5000  current step :  1338
total : 5000  current step :  1339
total : 5000  current step :  1340
total : 5000  current step :  1341
total : 5000  current step :  1342
total : 5000  current step :  1343
total : 5000  current step :  1344
total : 5000  current step :  1345
total : 5000  current step :  1346
total : 5000  current step :  1347
total : 5000  current step :  1348
total : 5000  current step :  1349
total : 5000  current step :  1350
total : 5000  current step :  1351
total : 5000  current step :  1352
total : 5000  current step :  1353
total : 5000  current step :  1354
total : 5000  current step :  1355
total : 5000  current step :  1356
total : 5000  current step :  1357
total : 5000  current step :  1358
total : 5000  current step :  1359
total : 5000  current step :  1360
total : 5000  current step :  1361
total : 5000  current step :  1362
total : 5000  current step :  1363
total : 5000  current step :  1364
total : 5000  current step :  1365
total : 5000  current step :  1366
total : 5000  current step :  1367
total : 5000  current step :  1368
total : 5000  current step :  1369
total : 5000  current step :  1370
total : 5000  current step :  1371
total : 5000  current step :  1372
total : 5000  current step :  1373
total : 5000  current step :  1374
total : 5000  current step :  1375
total : 5000  current step :  1376
total : 5000  current step :  1377
total : 5000  current step :  1378
total : 5000  current step :  1379
total : 5000  current step :  1380
total : 5000  current step :  1381
total : 5000  current step :  1382
total : 5000  current step :  1383
total : 5000  current step :  1384
total : 5000  current step :  1385
total : 5000  current step :  1386
total : 5000  current step :  1387
total : 5000  current step :  1388
total : 5000  current step :  1389
total : 5000  current step :  1390
total : 5000  current step :  1391
total : 5000  current step :  1392
total : 5000  current step :  1393
total : 5000  current step :  1394
total : 5000  current step :  1395
total : 5000  current step :  1396
total : 5000  current step :  1397
total : 5000  current step :  1398
total : 5000  current step :  1399
total : 5000  current step :  1400
total : 5000  current step :  1401
total : 5000  current step :  1402
total : 5000  current step :  1403
total : 5000  current step :  1404
total : 5000  current step :  1405
total : 5000  current step :  1406
total : 5000  current step :  1407
total : 5000  current step :  1408
total : 5000  current step :  1409
total : 5000  current step :  1410
total : 5000  current step :  1411
total : 5000  current step :  1412
total : 5000  current step :  1413
total : 5000  current step :  1414
total : 5000  current step :  1415
total : 5000  current step :  1416
total : 5000  current step :  1417
total : 5000  current step :  1418
total : 5000  current step :  1419
total : 5000  current step :  1420
total : 5000  current step :  1421
total : 5000  current step :  1422
total : 5000  current step :  1423
total : 5000  current step :  1424
total : 5000  current step :  1425
total : 5000  current step :  1426
total : 5000  current step :  1427
total : 5000  current step :  1428
total : 5000  current step :  1429
total : 5000  current step :  1430
total : 5000  current step :  1431
total : 5000  current step :  1432
total : 5000  current step :  1433
total : 5000  current step :  1434
total : 5000  current step :  1435
total : 5000  current step :  1436
total : 5000  current step :  1437
total : 5000  current step :  1438
total : 5000  current step :  1439
total : 5000  current step :  1440
total : 5000  current step :  1441
total : 5000  current step :  1442
total : 5000  current step :  1443
total : 5000  current step :  1444
total : 5000  current step :  1445
total : 5000  current step :  1446
total : 5000  current step :  1447
total : 5000  current step :  1448
total : 5000  current step :  1449
total : 5000  current step :  1450
total : 5000  current step :  1451
total : 5000  current step :  1452
total : 5000  current step :  1453
total : 5000  current step :  1454
total : 5000  current step :  1455
total : 5000  current step :  1456
total : 5000  current step :  1457
total : 5000  current step :  1458
total : 5000  current step :  1459
total : 5000  current step :  1460
total : 5000  current step :  1461
total : 5000  current step :  1462
total : 5000  current step :  1463
total : 5000  current step :  1464
total : 5000  current step :  1465
total : 5000  current step :  1466
total : 5000  current step :  1467
total : 5000  current step :  1468
total : 5000  current step :  1469
total : 5000  current step :  1470
total : 5000  current step :  1471
total : 5000  current step :  1472
total : 5000  current step :  1473
total : 5000  current step :  1474
total : 5000  current step :  1475
total : 5000  current step :  1476
total : 5000  current step :  1477
total : 5000  current step :  1478
total : 5000  current step :  1479
total : 5000  current step :  1480
total : 5000  current step :  1481
total : 5000  current step :  1482
total : 5000  current step :  1483
total : 5000  current step :  1484
total : 5000  current step :  1485
total : 5000  current step :  1486
total : 5000  current step :  1487
total : 5000  current step :  1488
total : 5000  current step :  1489
total : 5000  current step :  1490
total : 5000  current step :  1491
total : 5000  current step :  1492
total : 5000  current step :  1493
total : 5000  current step :  1494
total : 5000  current step :  1495
total : 5000  current step :  1496
total : 5000  current step :  1497
total : 5000  current step :  1498
total : 5000  current step :  1499
total : 5000  current step :  1500
total : 5000  current step :  1501
total : 5000  current step :  1502
total : 5000  current step :  1503
total : 5000  current step :  1504
total : 5000  current step :  1505
total : 5000  current step :  1506
total : 5000  current step :  1507
total : 5000  current step :  1508
total : 5000  current step :  1509
total : 5000  current step :  1510
total : 5000  current step :  1511
total : 5000  current step :  1512
total : 5000  current step :  1513
total : 5000  current step :  1514
total : 5000  current step :  1515
total : 5000  current step :  1516
total : 5000  current step :  1517
total : 5000  current step :  1518
total : 5000  current step :  1519
total : 5000  current step :  1520
total : 5000  current step :  1521
total : 5000  current step :  1522
total : 5000  current step :  1523
total : 5000  current step :  1524
total : 5000  current step :  1525
total : 5000  current step :  1526
total : 5000  current step :  1527
total : 5000  current step :  1528
total : 5000  current step :  1529
total : 5000  current step :  1530
total : 5000  current step :  1531
total : 5000  current step :  1532
total : 5000  current step :  1533
total : 5000  current step :  1534
total : 5000  current step :  1535
total : 5000  current step :  1536
total : 5000  current step :  1537
total : 5000  current step :  1538
total : 5000  current step :  1539
total : 5000  current step :  1540
total : 5000  current step :  1541
total : 5000  current step :  1542
total : 5000  current step :  1543
total : 5000  current step :  1544
total : 5000  current step :  1545
total : 5000  current step :  1546
total : 5000  current step :  1547
total : 5000  current step :  1548
total : 5000  current step :  1549
total : 5000  current step :  1550
total : 5000  current step :  1551
total : 5000  current step :  1552
total : 5000  current step :  1553
total : 5000  current step :  1554
total : 5000  current step :  1555
total : 5000  current step :  1556
total : 5000  current step :  1557
total : 5000  current step :  1558
total : 5000  current step :  1559
total : 5000  current step :  1560
total : 5000  current step :  1561
total : 5000  current step :  1562
total : 5000  current step :  1563
total : 5000  current step :  1564
total : 5000  current step :  1565
total : 5000  current step :  1566
total : 5000  current step :  1567
total : 5000  current step :  1568
total : 5000  current step :  1569
total : 5000  current step :  1570
total : 5000  current step :  1571
total : 5000  current step :  1572
total : 5000  current step :  1573
total : 5000  current step :  1574
total : 5000  current step :  1575
total : 5000  current step :  1576
total : 5000  current step :  1577
total : 5000  current step :  1578
total : 5000  current step :  1579
total : 5000  current step :  1580
total : 5000  current step :  1581
total : 5000  current step :  1582
total : 5000  current step :  1583
total : 5000  current step :  1584
total : 5000  current step :  1585
total : 5000  current step :  1586
total : 5000  current step :  1587
total : 5000  current step :  1588
total : 5000  current step :  1589
total : 5000  current step :  1590
total : 5000  current step :  1591
total : 5000  current step :  1592
total : 5000  current step :  1593
total : 5000  current step :  1594
total : 5000  current step :  1595
total : 5000  current step :  1596
total : 5000  current step :  1597
total : 5000  current step :  1598
total : 5000  current step :  1599
total : 5000  current step :  1600
total : 5000  current step :  1601
total : 5000  current step :  1602
total : 5000  current step :  1603
total : 5000  current step :  1604
total : 5000  current step :  1605
total : 5000  current step :  1606
total : 5000  current step :  1607
total : 5000  current step :  1608
total : 5000  current step :  1609
total : 5000  current step :  1610
total : 5000  current step :  1611
total : 5000  current step :  1612
total : 5000  current step :  1613
total : 5000  current step :  1614
total : 5000  current step :  1615
total : 5000  current step :  1616
total : 5000  current step :  1617
total : 5000  current step :  1618
total : 5000  current step :  1619
total : 5000  current step :  1620
total : 5000  current step :  1621
total : 5000  current step :  1622
total : 5000  current step :  1623
total : 5000  current step :  1624
total : 5000  current step :  1625
total : 5000  current step :  1626
total : 5000  current step :  1627
total : 5000  current step :  1628
total : 5000  current step :  1629
total : 5000  current step :  1630
total : 5000  current step :  1631
total : 5000  current step :  1632
total : 5000  current step :  1633
total : 5000  current step :  1634
total : 5000  current step :  1635
total : 5000  current step :  1636
total : 5000  current step :  1637
total : 5000  current step :  1638
total : 5000  current step :  1639
total : 5000  current step :  1640
total : 5000  current step :  1641
total : 5000  current step :  1642
total : 5000  current step :  1643
total : 5000  current step :  1644
total : 5000  current step :  1645
total : 5000  current step :  1646
total : 5000  current step :  1647
total : 5000  current step :  1648
total : 5000  current step :  1649
total : 5000  current step :  1650
total : 5000  current step :  1651
total : 5000  current step :  1652
total : 5000  current step :  1653
total : 5000  current step :  1654
total : 5000  current step :  1655
total : 5000  current step :  1656
total : 5000  current step :  1657
total : 5000  current step :  1658
total : 5000  current step :  1659
total : 5000  current step :  1660
total : 5000  current step :  1661
total : 5000  current step :  1662
total : 5000  current step :  1663
total : 5000  current step :  1664
total : 5000  current step :  1665
total : 5000  current step :  1666
total : 5000  current step :  1667
total : 5000  current step :  1668
total : 5000  current step :  1669
total : 5000  current step :  1670
total : 5000  current step :  1671
total : 5000  current step :  1672
total : 5000  current step :  1673
total : 5000  current step :  1674
total : 5000  current step :  1675
total : 5000  current step :  1676
total : 5000  current step :  1677
total : 5000  current step :  1678
total : 5000  current step :  1679
total : 5000  current step :  1680
total : 5000  current step :  1681
total : 5000  current step :  1682
total : 5000  current step :  1683
total : 5000  current step :  1684
total : 5000  current step :  1685
total : 5000  current step :  1686
total : 5000  current step :  1687
total : 5000  current step :  1688
total : 5000  current step :  1689
total : 5000  current step :  1690
total : 5000  current step :  1691
total : 5000  current step :  1692
total : 5000  current step :  1693
total : 5000  current step :  1694
total : 5000  current step :  1695
total : 5000  current step :  1696
total : 5000  current step :  1697
total : 5000  current step :  1698
total : 5000  current step :  1699
total : 5000  current step :  1700
total : 5000  current step :  1701
total : 5000  current step :  1702
total : 5000  current step :  1703
total : 5000  current step :  1704
total : 5000  current step :  1705
total : 5000  current step :  1706
total : 5000  current step :  1707
total : 5000  current step :  1708
total : 5000  current step :  1709
total : 5000  current step :  1710
total : 5000  current step :  1711
total : 5000  current step :  1712
total : 5000  current step :  1713
total : 5000  current step :  1714
total : 5000  current step :  1715
total : 5000  current step :  1716
total : 5000  current step :  1717
total : 5000  current step :  1718
total : 5000  current step :  1719
total : 5000  current step :  1720
total : 5000  current step :  1721
total : 5000  current step :  1722
total : 5000  current step :  1723
total : 5000  current step :  1724
total : 5000  current step :  1725
total : 5000  current step :  1726
total : 5000  current step :  1727
total : 5000  current step :  1728
total : 5000  current step :  1729
total : 5000  current step :  1730
total : 5000  current step :  1731
total : 5000  current step :  1732
total : 5000  current step :  1733
total : 5000  current step :  1734
total : 5000  current step :  1735
total : 5000  current step :  1736
total : 5000  current step :  1737
total : 5000  current step :  1738
total : 5000  current step :  1739
total : 5000  current step :  1740
total : 5000  current step :  1741
total : 5000  current step :  1742
total : 5000  current step :  1743
total : 5000  current step :  1744
total : 5000  current step :  1745
total : 5000  current step :  1746
total : 5000  current step :  1747
total : 5000  current step :  1748
total : 5000  current step :  1749
total : 5000  current step :  1750
total : 5000  current step :  1751
total : 5000  current step :  1752
total : 5000  current step :  1753
total : 5000  current step :  1754
total : 5000  current step :  1755
total : 5000  current step :  1756
total : 5000  current step :  1757
total : 5000  current step :  1758
total : 5000  current step :  1759
total : 5000  current step :  1760
total : 5000  current step :  1761
total : 5000  current step :  1762
total : 5000  current step :  1763
total : 5000  current step :  1764
total : 5000  current step :  1765
total : 5000  current step :  1766
total : 5000  current step :  1767
total : 5000  current step :  1768
total : 5000  current step :  1769
total : 5000  current step :  1770
total : 5000  current step :  1771
total : 5000  current step :  1772
total : 5000  current step :  1773
total : 5000  current step :  1774
total : 5000  current step :  1775
total : 5000  current step :  1776
total : 5000  current step :  1777
total : 5000  current step :  1778
total : 5000  current step :  1779
total : 5000  current step :  1780
total : 5000  current step :  1781
total : 5000  current step :  1782
total : 5000  current step :  1783
total : 5000  current step :  1784
total : 5000  current step :  1785
total : 5000  current step :  1786
total : 5000  current step :  1787
total : 5000  current step :  1788
total : 5000  current step :  1789
total : 5000  current step :  1790
total : 5000  current step :  1791
total : 5000  current step :  1792
total : 5000  current step :  1793
total : 5000  current step :  1794
total : 5000  current step :  1795
total : 5000  current step :  1796
total : 5000  current step :  1797
total : 5000  current step :  1798
total : 5000  current step :  1799
total : 5000  current step :  1800
total : 5000  current step :  1801
total : 5000  current step :  1802
total : 5000  current step :  1803
total : 5000  current step :  1804
total : 5000  current step :  1805
total : 5000  current step :  1806
total : 5000  current step :  1807
total : 5000  current step :  1808
total : 5000  current step :  1809
total : 5000  current step :  1810
total : 5000  current step :  1811
total : 5000  current step :  1812
total : 5000  current step :  1813
total : 5000  current step :  1814
total : 5000  current step :  1815
total : 5000  current step :  1816
total : 5000  current step :  1817
total : 5000  current step :  1818
total : 5000  current step :  1819
total : 5000  current step :  1820
total : 5000  current step :  1821
total : 5000  current step :  1822
total : 5000  current step :  1823
total : 5000  current step :  1824
total : 5000  current step :  1825
total : 5000  current step :  1826
total : 5000  current step :  1827
total : 5000  current step :  1828
total : 5000  current step :  1829
total : 5000  current step :  1830
total : 5000  current step :  1831
total : 5000  current step :  1832
total : 5000  current step :  1833
total : 5000  current step :  1834
total : 5000  current step :  1835
total : 5000  current step :  1836
total : 5000  current step :  1837
total : 5000  current step :  1838
total : 5000  current step :  1839
total : 5000  current step :  1840
total : 5000  current step :  1841
total : 5000  current step :  1842
total : 5000  current step :  1843
total : 5000  current step :  1844
total : 5000  current step :  1845
total : 5000  current step :  1846
total : 5000  current step :  1847
total : 5000  current step :  1848
total : 5000  current step :  1849
total : 5000  current step :  1850
total : 5000  current step :  1851
total : 5000  current step :  1852
total : 5000  current step :  1853
total : 5000  current step :  1854
total : 5000  current step :  1855
total : 5000  current step :  1856
total : 5000  current step :  1857
total : 5000  current step :  1858
total : 5000  current step :  1859
total : 5000  current step :  1860
total : 5000  current step :  1861
total : 5000  current step :  1862
total : 5000  current step :  1863
total : 5000  current step :  1864
total : 5000  current step :  1865
total : 5000  current step :  1866
total : 5000  current step :  1867
total : 5000  current step :  1868
total : 5000  current step :  1869
total : 5000  current step :  1870
total : 5000  current step :  1871
total : 5000  current step :  1872
total : 5000  current step :  1873
total : 5000  current step :  1874
total : 5000  current step :  1875
total : 5000  current step :  1876
total : 5000  current step :  1877
total : 5000  current step :  1878
total : 5000  current step :  1879
total : 5000  current step :  1880
total : 5000  current step :  1881
total : 5000  current step :  1882
total : 5000  current step :  1883
total : 5000  current step :  1884
total : 5000  current step :  1885
total : 5000  current step :  1886
total : 5000  current step :  1887
total : 5000  current step :  1888
total : 5000  current step :  1889
total : 5000  current step :  1890
total : 5000  current step :  1891
total : 5000  current step :  1892
total : 5000  current step :  1893
total : 5000  current step :  1894
total : 5000  current step :  1895
total : 5000  current step :  1896
total : 5000  current step :  1897
total : 5000  current step :  1898
total : 5000  current step :  1899
total : 5000  current step :  1900
total : 5000  current step :  1901
total : 5000  current step :  1902
total : 5000  current step :  1903
total : 5000  current step :  1904
total : 5000  current step :  1905
total : 5000  current step :  1906
total : 5000  current step :  1907
total : 5000  current step :  1908
total : 5000  current step :  1909
total : 5000  current step :  1910
total : 5000  current step :  1911
total : 5000  current step :  1912
total : 5000  current step :  1913
total : 5000  current step :  1914
total : 5000  current step :  1915
total : 5000  current step :  1916
total : 5000  current step :  1917
total : 5000  current step :  1918
total : 5000  current step :  1919
total : 5000  current step :  1920
total : 5000  current step :  1921
total : 5000  current step :  1922
total : 5000  current step :  1923
total : 5000  current step :  1924
total : 5000  current step :  1925
total : 5000  current step :  1926
total : 5000  current step :  1927
total : 5000  current step :  1928
total : 5000  current step :  1929
total : 5000  current step :  1930
total : 5000  current step :  1931
total : 5000  current step :  1932
total : 5000  current step :  1933
total : 5000  current step :  1934
total : 5000  current step :  1935
total : 5000  current step :  1936
total : 5000  current step :  1937
total : 5000  current step :  1938
total : 5000  current step :  1939
total : 5000  current step :  1940
total : 5000  current step :  1941
total : 5000  current step :  1942
total : 5000  current step :  1943
total : 5000  current step :  1944
total : 5000  current step :  1945
total : 5000  current step :  1946
total : 5000  current step :  1947
total : 5000  current step :  1948
total : 5000  current step :  1949
total : 5000  current step :  1950
total : 5000  current step :  1951
total : 5000  current step :  1952
total : 5000  current step :  1953
total : 5000  current step :  1954
total : 5000  current step :  1955
total : 5000  current step :  1956
total : 5000  current step :  1957
total : 5000  current step :  1958
total : 5000  current step :  1959
total : 5000  current step :  1960
total : 5000  current step :  1961
total : 5000  current step :  1962
total : 5000  current step :  1963
total : 5000  current step :  1964
total : 5000  current step :  1965
total : 5000  current step :  1966
total : 5000  current step :  1967
total : 5000  current step :  1968
total : 5000  current step :  1969
total : 5000  current step :  1970
total : 5000  current step :  1971
total : 5000  current step :  1972
total : 5000  current step :  1973
total : 5000  current step :  1974
total : 5000  current step :  1975
total : 5000  current step :  1976
total : 5000  current step :  1977
total : 5000  current step :  1978
total : 5000  current step :  1979
total : 5000  current step :  1980
total : 5000  current step :  1981
total : 5000  current step :  1982
total : 5000  current step :  1983
total : 5000  current step :  1984
total : 5000  current step :  1985
total : 5000  current step :  1986
total : 5000  current step :  1987
total : 5000  current step :  1988
total : 5000  current step :  1989
total : 5000  current step :  1990
total : 5000  current step :  1991
total : 5000  current step :  1992
total : 5000  current step :  1993
total : 5000  current step :  1994
total : 5000  current step :  1995
total : 5000  current step :  1996
total : 5000  current step :  1997
total : 5000  current step :  1998
total : 5000  current step :  1999
total : 5000  current step :  2000
total : 5000  current step :  2001
total : 5000  current step :  2002
total : 5000  current step :  2003
total : 5000  current step :  2004
total : 5000  current step :  2005
total : 5000  current step :  2006
total : 5000  current step :  2007
total : 5000  current step :  2008
total : 5000  current step :  2009
total : 5000  current step :  2010
total : 5000  current step :  2011
total : 5000  current step :  2012
total : 5000  current step :  2013
total : 5000  current step :  2014
total : 5000  current step :  2015
total : 5000  current step :  2016
total : 5000  current step :  2017
total : 5000  current step :  2018
total : 5000  current step :  2019
total : 5000  current step :  2020
total : 5000  current step :  2021
total : 5000  current step :  2022
total : 5000  current step :  2023
total : 5000  current step :  2024
total : 5000  current step :  2025
total : 5000  current step :  2026
total : 5000  current step :  2027
total : 5000  current step :  2028
total : 5000  current step :  2029
total : 5000  current step :  2030
total : 5000  current step :  2031
total : 5000  current step :  2032
total : 5000  current step :  2033
total : 5000  current step :  2034
total : 5000  current step :  2035
total : 5000  current step :  2036
total : 5000  current step :  2037
total : 5000  current step :  2038
total : 5000  current step :  2039
total : 5000  current step :  2040
total : 5000  current step :  2041
total : 5000  current step :  2042
total : 5000  current step :  2043
total : 5000  current step :  2044
total : 5000  current step :  2045
total : 5000  current step :  2046
total : 5000  current step :  2047
total : 5000  current step :  2048
total : 5000  current step :  2049
total : 5000  current step :  2050
total : 5000  current step :  2051
total : 5000  current step :  2052
total : 5000  current step :  2053
total : 5000  current step :  2054
total : 5000  current step :  2055
total : 5000  current step :  2056
total : 5000  current step :  2057
total : 5000  current step :  2058
total : 5000  current step :  2059
total : 5000  current step :  2060
total : 5000  current step :  2061
total : 5000  current step :  2062
total : 5000  current step :  2063
total : 5000  current step :  2064
total : 5000  current step :  2065
total : 5000  current step :  2066
total : 5000  current step :  2067
total : 5000  current step :  2068
total : 5000  current step :  2069
total : 5000  current step :  2070
total : 5000  current step :  2071
total : 5000  current step :  2072
total : 5000  current step :  2073
total : 5000  current step :  2074
total : 5000  current step :  2075
total : 5000  current step :  2076
total : 5000  current step :  2077
total : 5000  current step :  2078
total : 5000  current step :  2079
total : 5000  current step :  2080
total : 5000  current step :  2081
total : 5000  current step :  2082
total : 5000  current step :  2083
total : 5000  current step :  2084
total : 5000  current step :  2085
total : 5000  current step :  2086
total : 5000  current step :  2087
total : 5000  current step :  2088
total : 5000  current step :  2089
total : 5000  current step :  2090
total : 5000  current step :  2091
total : 5000  current step :  2092
total : 5000  current step :  2093
total : 5000  current step :  2094
total : 5000  current step :  2095
total : 5000  current step :  2096
total : 5000  current step :  2097
total : 5000  current step :  2098
total : 5000  current step :  2099
total : 5000  current step :  2100
total : 5000  current step :  2101
total : 5000  current step :  2102
total : 5000  current step :  2103
total : 5000  current step :  2104
total : 5000  current step :  2105
total : 5000  current step :  2106
total : 5000  current step :  2107
total : 5000  current step :  2108
total : 5000  current step :  2109
total : 5000  current step :  2110
total : 5000  current step :  2111
total : 5000  current step :  2112
total : 5000  current step :  2113
total : 5000  current step :  2114
total : 5000  current step :  2115
total : 5000  current step :  2116
total : 5000  current step :  2117
total : 5000  current step :  2118
total : 5000  current step :  2119
total : 5000  current step :  2120
total : 5000  current step :  2121
total : 5000  current step :  2122
total : 5000  current step :  2123
total : 5000  current step :  2124
total : 5000  current step :  2125
total : 5000  current step :  2126
total : 5000  current step :  2127
total : 5000  current step :  2128
total : 5000  current step :  2129
total : 5000  current step :  2130
total : 5000  current step :  2131
total : 5000  current step :  2132
total : 5000  current step :  2133
total : 5000  current step :  2134
total : 5000  current step :  2135
total : 5000  current step :  2136
total : 5000  current step :  2137
total : 5000  current step :  2138
total : 5000  current step :  2139
total : 5000  current step :  2140
total : 5000  current step :  2141
total : 5000  current step :  2142
total : 5000  current step :  2143
total : 5000  current step :  2144
total : 5000  current step :  2145
total : 5000  current step :  2146
total : 5000  current step :  2147
total : 5000  current step :  2148
total : 5000  current step :  2149
total : 5000  current step :  2150
total : 5000  current step :  2151
total : 5000  current step :  2152
total : 5000  current step :  2153
total : 5000  current step :  2154
total : 5000  current step :  2155
total : 5000  current step :  2156
total : 5000  current step :  2157
total : 5000  current step :  2158
total : 5000  current step :  2159
total : 5000  current step :  2160
total : 5000  current step :  2161
total : 5000  current step :  2162
total : 5000  current step :  2163
total : 5000  current step :  2164
total : 5000  current step :  2165
total : 5000  current step :  2166
total : 5000  current step :  2167
total : 5000  current step :  2168
total : 5000  current step :  2169
total : 5000  current step :  2170
total : 5000  current step :  2171
total : 5000  current step :  2172
total : 5000  current step :  2173
total : 5000  current step :  2174
total : 5000  current step :  2175
total : 5000  current step :  2176
total : 5000  current step :  2177
total : 5000  current step :  2178
total : 5000  current step :  2179
total : 5000  current step :  2180
total : 5000  current step :  2181
total : 5000  current step :  2182
total : 5000  current step :  2183
total : 5000  current step :  2184
total : 5000  current step :  2185
total : 5000  current step :  2186
total : 5000  current step :  2187
total : 5000  current step :  2188
total : 5000  current step :  2189
total : 5000  current step :  2190
total : 5000  current step :  2191
total : 5000  current step :  2192
total : 5000  current step :  2193
total : 5000  current step :  2194
total : 5000  current step :  2195
total : 5000  current step :  2196
total : 5000  current step :  2197
total : 5000  current step :  2198
total : 5000  current step :  2199
total : 5000  current step :  2200
total : 5000  current step :  2201
total : 5000  current step :  2202
total : 5000  current step :  2203
total : 5000  current step :  2204
total : 5000  current step :  2205
total : 5000  current step :  2206
total : 5000  current step :  2207
total : 5000  current step :  2208
total : 5000  current step :  2209
total : 5000  current step :  2210
total : 5000  current step :  2211
total : 5000  current step :  2212
total : 5000  current step :  2213
total : 5000  current step :  2214
total : 5000  current step :  2215
total : 5000  current step :  2216
total : 5000  current step :  2217
total : 5000  current step :  2218
total : 5000  current step :  2219
total : 5000  current step :  2220
total : 5000  current step :  2221
total : 5000  current step :  2222
total : 5000  current step :  2223
total : 5000  current step :  2224
total : 5000  current step :  2225
total : 5000  current step :  2226
total : 5000  current step :  2227
total : 5000  current step :  2228
total : 5000  current step :  2229
total : 5000  current step :  2230
total : 5000  current step :  2231
total : 5000  current step :  2232
total : 5000  current step :  2233
total : 5000  current step :  2234
total : 5000  current step :  2235
total : 5000  current step :  2236
total : 5000  current step :  2237
total : 5000  current step :  2238
total : 5000  current step :  2239
total : 5000  current step :  2240
total : 5000  current step :  2241
total : 5000  current step :  2242
total : 5000  current step :  2243
total : 5000  current step :  2244
total : 5000  current step :  2245
total : 5000  current step :  2246
total : 5000  current step :  2247
total : 5000  current step :  2248
total : 5000  current step :  2249
total : 5000  current step :  2250
total : 5000  current step :  2251
total : 5000  current step :  2252
total : 5000  current step :  2253
total : 5000  current step :  2254
total : 5000  current step :  2255
total : 5000  current step :  2256
total : 5000  current step :  2257
total : 5000  current step :  2258
total : 5000  current step :  2259
total : 5000  current step :  2260
total : 5000  current step :  2261
total : 5000  current step :  2262
total : 5000  current step :  2263
total : 5000  current step :  2264
total : 5000  current step :  2265
total : 5000  current step :  2266
total : 5000  current step :  2267
total : 5000  current step :  2268
total : 5000  current step :  2269
total : 5000  current step :  2270
total : 5000  current step :  2271
total : 5000  current step :  2272
total : 5000  current step :  2273
total : 5000  current step :  2274
total : 5000  current step :  2275
total : 5000  current step :  2276
total : 5000  current step :  2277
total : 5000  current step :  2278
total : 5000  current step :  2279
total : 5000  current step :  2280
total : 5000  current step :  2281
total : 5000  current step :  2282
total : 5000  current step :  2283
total : 5000  current step :  2284
total : 5000  current step :  2285
total : 5000  current step :  2286
total : 5000  current step :  2287
total : 5000  current step :  2288
total : 5000  current step :  2289
total : 5000  current step :  2290
total : 5000  current step :  2291
total : 5000  current step :  2292
total : 5000  current step :  2293
total : 5000  current step :  2294
total : 5000  current step :  2295
total : 5000  current step :  2296
total : 5000  current step :  2297
total : 5000  current step :  2298
total : 5000  current step :  2299
total : 5000  current step :  2300
total : 5000  current step :  2301
total : 5000  current step :  2302
total : 5000  current step :  2303
total : 5000  current step :  2304
total : 5000  current step :  2305
total : 5000  current step :  2306
total : 5000  current step :  2307
total : 5000  current step :  2308
total : 5000  current step :  2309
total : 5000  current step :  2310
total : 5000  current step :  2311
total : 5000  current step :  2312
total : 5000  current step :  2313
total : 5000  current step :  2314
total : 5000  current step :  2315
total : 5000  current step :  2316
total : 5000  current step :  2317
total : 5000  current step :  2318
total : 5000  current step :  2319
total : 5000  current step :  2320
total : 5000  current step :  2321
total : 5000  current step :  2322
total : 5000  current step :  2323
total : 5000  current step :  2324
total : 5000  current step :  2325
total : 5000  current step :  2326
total : 5000  current step :  2327
total : 5000  current step :  2328
total : 5000  current step :  2329
total : 5000  current step :  2330
total : 5000  current step :  2331
total : 5000  current step :  2332
total : 5000  current step :  2333
total : 5000  current step :  2334
total : 5000  current step :  2335
total : 5000  current step :  2336
total : 5000  current step :  2337
total : 5000  current step :  2338
total : 5000  current step :  2339
total : 5000  current step :  2340
total : 5000  current step :  2341
total : 5000  current step :  2342
total : 5000  current step :  2343
total : 5000  current step :  2344
total : 5000  current step :  2345
total : 5000  current step :  2346
total : 5000  current step :  2347
total : 5000  current step :  2348
total : 5000  current step :  2349
total : 5000  current step :  2350
total : 5000  current step :  2351
total : 5000  current step :  2352
total : 5000  current step :  2353
total : 5000  current step :  2354
total : 5000  current step :  2355
total : 5000  current step :  2356
total : 5000  current step :  2357
total : 5000  current step :  2358
total : 5000  current step :  2359
total : 5000  current step :  2360
total : 5000  current step :  2361
total : 5000  current step :  2362
total : 5000  current step :  2363
total : 5000  current step :  2364
total : 5000  current step :  2365
total : 5000  current step :  2366
total : 5000  current step :  2367
total : 5000  current step :  2368
total : 5000  current step :  2369
total : 5000  current step :  2370
total : 5000  current step :  2371
total : 5000  current step :  2372
total : 5000  current step :  2373
total : 5000  current step :  2374
total : 5000  current step :  2375
total : 5000  current step :  2376
total : 5000  current step :  2377
total : 5000  current step :  2378
total : 5000  current step :  2379
total : 5000  current step :  2380
total : 5000  current step :  2381
total : 5000  current step :  2382
total : 5000  current step :  2383
total : 5000  current step :  2384
total : 5000  current step :  2385
total : 5000  current step :  2386
total : 5000  current step :  2387
total : 5000  current step :  2388
total : 5000  current step :  2389
total : 5000  current step :  2390
total : 5000  current step :  2391
total : 5000  current step :  2392
total : 5000  current step :  2393
total : 5000  current step :  2394
total : 5000  current step :  2395
total : 5000  current step :  2396
total : 5000  current step :  2397
total : 5000  current step :  2398
total : 5000  current step :  2399
total : 5000  current step :  2400
total : 5000  current step :  2401
total : 5000  current step :  2402
total : 5000  current step :  2403
total : 5000  current step :  2404
total : 5000  current step :  2405
total : 5000  current step :  2406
total : 5000  current step :  2407
total : 5000  current step :  2408
total : 5000  current step :  2409
total : 5000  current step :  2410
total : 5000  current step :  2411
total : 5000  current step :  2412
total : 5000  current step :  2413
total : 5000  current step :  2414
total : 5000  current step :  2415
total : 5000  current step :  2416
total : 5000  current step :  2417
total : 5000  current step :  2418
total : 5000  current step :  2419
total : 5000  current step :  2420
total : 5000  current step :  2421
total : 5000  current step :  2422
total : 5000  current step :  2423
total : 5000  current step :  2424
total : 5000  current step :  2425
total : 5000  current step :  2426
total : 5000  current step :  2427
total : 5000  current step :  2428
total : 5000  current step :  2429
total : 5000  current step :  2430
total : 5000  current step :  2431
total : 5000  current step :  2432
total : 5000  current step :  2433
total : 5000  current step :  2434
total : 5000  current step :  2435
total : 5000  current step :  2436
total : 5000  current step :  2437
total : 5000  current step :  2438
total : 5000  current step :  2439
total : 5000  current step :  2440
total : 5000  current step :  2441
total : 5000  current step :  2442
total : 5000  current step :  2443
total : 5000  current step :  2444
total : 5000  current step :  2445
total : 5000  current step :  2446
total : 5000  current step :  2447
total : 5000  current step :  2448
total : 5000  current step :  2449
total : 5000  current step :  2450
total : 5000  current step :  2451
total : 5000  current step :  2452
total : 5000  current step :  2453
total : 5000  current step :  2454
total : 5000  current step :  2455
total : 5000  current step :  2456
total : 5000  current step :  2457
total : 5000  current step :  2458
total : 5000  current step :  2459
total : 5000  current step :  2460
total : 5000  current step :  2461
total : 5000  current step :  2462
total : 5000  current step :  2463
total : 5000  current step :  2464
total : 5000  current step :  2465
total : 5000  current step :  2466
total : 5000  current step :  2467
total : 5000  current step :  2468
total : 5000  current step :  2469
total : 5000  current step :  2470
total : 5000  current step :  2471
total : 5000  current step :  2472
total : 5000  current step :  2473
total : 5000  current step :  2474
total : 5000  current step :  2475
total : 5000  current step :  2476
total : 5000  current step :  2477
total : 5000  current step :  2478
total : 5000  current step :  2479
total : 5000  current step :  2480
total : 5000  current step :  2481
total : 5000  current step :  2482
total : 5000  current step :  2483
total : 5000  current step :  2484
total : 5000  current step :  2485
total : 5000  current step :  2486
total : 5000  current step :  2487
total : 5000  current step :  2488
total : 5000  current step :  2489
total : 5000  current step :  2490
total : 5000  current step :  2491
total : 5000  current step :  2492
total : 5000  current step :  2493
total : 5000  current step :  2494
total : 5000  current step :  2495
total : 5000  current step :  2496
total : 5000  current step :  2497
total : 5000  current step :  2498
total : 5000  current step :  2499
total : 5000  current step :  2500
total : 5000  current step :  2501
total : 5000  current step :  2502
total : 5000  current step :  2503
total : 5000  current step :  2504
total : 5000  current step :  2505
total : 5000  current step :  2506
total : 5000  current step :  2507
total : 5000  current step :  2508
total : 5000  current step :  2509
total : 5000  current step :  2510
total : 5000  current step :  2511
total : 5000  current step :  2512
total : 5000  current step :  2513
total : 5000  current step :  2514
total : 5000  current step :  2515
total : 5000  current step :  2516
total : 5000  current step :  2517
total : 5000  current step :  2518
total : 5000  current step :  2519
total : 5000  current step :  2520
total : 5000  current step :  2521
total : 5000  current step :  2522
total : 5000  current step :  2523
total : 5000  current step :  2524
total : 5000  current step :  2525
total : 5000  current step :  2526
total : 5000  current step :  2527
total : 5000  current step :  2528
total : 5000  current step :  2529
total : 5000  current step :  2530
total : 5000  current step :  2531
total : 5000  current step :  2532
total : 5000  current step :  2533
total : 5000  current step :  2534
total : 5000  current step :  2535
total : 5000  current step :  2536
total : 5000  current step :  2537
total : 5000  current step :  2538
total : 5000  current step :  2539
total : 5000  current step :  2540
total : 5000  current step :  2541
total : 5000  current step :  2542
total : 5000  current step :  2543
total : 5000  current step :  2544
total : 5000  current step :  2545
total : 5000  current step :  2546
total : 5000  current step :  2547
total : 5000  current step :  2548
total : 5000  current step :  2549
total : 5000  current step :  2550
total : 5000  current step :  2551
total : 5000  current step :  2552
total : 5000  current step :  2553
total : 5000  current step :  2554
total : 5000  current step :  2555
total : 5000  current step :  2556
total : 5000  current step :  2557
total : 5000  current step :  2558
total : 5000  current step :  2559
total : 5000  current step :  2560
total : 5000  current step :  2561
total : 5000  current step :  2562
total : 5000  current step :  2563
total : 5000  current step :  2564
total : 5000  current step :  2565
total : 5000  current step :  2566
total : 5000  current step :  2567
total : 5000  current step :  2568
total : 5000  current step :  2569
total : 5000  current step :  2570
total : 5000  current step :  2571
total : 5000  current step :  2572
total : 5000  current step :  2573
total : 5000  current step :  2574
total : 5000  current step :  2575
total : 5000  current step :  2576
total : 5000  current step :  2577
total : 5000  current step :  2578
total : 5000  current step :  2579
total : 5000  current step :  2580
total : 5000  current step :  2581
total : 5000  current step :  2582
total : 5000  current step :  2583
total : 5000  current step :  2584
total : 5000  current step :  2585
total : 5000  current step :  2586
total : 5000  current step :  2587
total : 5000  current step :  2588
total : 5000  current step :  2589
total : 5000  current step :  2590
total : 5000  current step :  2591
total : 5000  current step :  2592
total : 5000  current step :  2593
total : 5000  current step :  2594
total : 5000  current step :  2595
total : 5000  current step :  2596
total : 5000  current step :  2597
total : 5000  current step :  2598
total : 5000  current step :  2599
total : 5000  current step :  2600
total : 5000  current step :  2601
total : 5000  current step :  2602
total : 5000  current step :  2603
total : 5000  current step :  2604
total : 5000  current step :  2605
total : 5000  current step :  2606
total : 5000  current step :  2607
total : 5000  current step :  2608
total : 5000  current step :  2609
total : 5000  current step :  2610
total : 5000  current step :  2611
total : 5000  current step :  2612
total : 5000  current step :  2613
total : 5000  current step :  2614
total : 5000  current step :  2615
total : 5000  current step :  2616
total : 5000  current step :  2617
total : 5000  current step :  2618
total : 5000  current step :  2619
total : 5000  current step :  2620
total : 5000  current step :  2621
total : 5000  current step :  2622
total : 5000  current step :  2623
total : 5000  current step :  2624
total : 5000  current step :  2625
total : 5000  current step :  2626
total : 5000  current step :  2627
total : 5000  current step :  2628
total : 5000  current step :  2629
total : 5000  current step :  2630
total : 5000  current step :  2631
total : 5000  current step :  2632
total : 5000  current step :  2633
total : 5000  current step :  2634
total : 5000  current step :  2635
total : 5000  current step :  2636
total : 5000  current step :  2637
total : 5000  current step :  2638
total : 5000  current step :  2639
total : 5000  current step :  2640
total : 5000  current step :  2641
total : 5000  current step :  2642
total : 5000  current step :  2643
total : 5000  current step :  2644
total : 5000  current step :  2645
total : 5000  current step :  2646
total : 5000  current step :  2647
total : 5000  current step :  2648
total : 5000  current step :  2649
total : 5000  current step :  2650
total : 5000  current step :  2651
total : 5000  current step :  2652
total : 5000  current step :  2653
total : 5000  current step :  2654
total : 5000  current step :  2655
total : 5000  current step :  2656
total : 5000  current step :  2657
total : 5000  current step :  2658
total : 5000  current step :  2659
total : 5000  current step :  2660
total : 5000  current step :  2661
total : 5000  current step :  2662
total : 5000  current step :  2663
total : 5000  current step :  2664
total : 5000  current step :  2665
total : 5000  current step :  2666
total : 5000  current step :  2667
total : 5000  current step :  2668
total : 5000  current step :  2669
total : 5000  current step :  2670
total : 5000  current step :  2671
total : 5000  current step :  2672
total : 5000  current step :  2673
total : 5000  current step :  2674
total : 5000  current step :  2675
total : 5000  current step :  2676
total : 5000  current step :  2677
total : 5000  current step :  2678
total : 5000  current step :  2679
total : 5000  current step :  2680
total : 5000  current step :  2681
total : 5000  current step :  2682
total : 5000  current step :  2683
total : 5000  current step :  2684
total : 5000  current step :  2685
total : 5000  current step :  2686
total : 5000  current step :  2687
total : 5000  current step :  2688
total : 5000  current step :  2689
total : 5000  current step :  2690
total : 5000  current step :  2691
total : 5000  current step :  2692
total : 5000  current step :  2693
total : 5000  current step :  2694
total : 5000  current step :  2695
total : 5000  current step :  2696
total : 5000  current step :  2697
total : 5000  current step :  2698
total : 5000  current step :  2699
total : 5000  current step :  2700
total : 5000  current step :  2701
total : 5000  current step :  2702
total : 5000  current step :  2703
total : 5000  current step :  2704
total : 5000  current step :  2705
total : 5000  current step :  2706
total : 5000  current step :  2707
total : 5000  current step :  2708
total : 5000  current step :  2709
total : 5000  current step :  2710
total : 5000  current step :  2711
total : 5000  current step :  2712
total : 5000  current step :  2713
total : 5000  current step :  2714
total : 5000  current step :  2715
total : 5000  current step :  2716
total : 5000  current step :  2717
total : 5000  current step :  2718
total : 5000  current step :  2719
total : 5000  current step :  2720
total : 5000  current step :  2721
total : 5000  current step :  2722
total : 5000  current step :  2723
total : 5000  current step :  2724
total : 5000  current step :  2725
total : 5000  current step :  2726
total : 5000  current step :  2727
total : 5000  current step :  2728
total : 5000  current step :  2729
total : 5000  current step :  2730
total : 5000  current step :  2731
total : 5000  current step :  2732
total : 5000  current step :  2733
total : 5000  current step :  2734
total : 5000  current step :  2735
total : 5000  current step :  2736
total : 5000  current step :  2737
total : 5000  current step :  2738
total : 5000  current step :  2739
total : 5000  current step :  2740
total : 5000  current step :  2741
total : 5000  current step :  2742
total : 5000  current step :  2743
total : 5000  current step :  2744
total : 5000  current step :  2745
total : 5000  current step :  2746
total : 5000  current step :  2747
total : 5000  current step :  2748
total : 5000  current step :  2749
total : 5000  current step :  2750
total : 5000  current step :  2751
total : 5000  current step :  2752
total : 5000  current step :  2753
total : 5000  current step :  2754
total : 5000  current step :  2755
total : 5000  current step :  2756
total : 5000  current step :  2757
total : 5000  current step :  2758
total : 5000  current step :  2759
total : 5000  current step :  2760
total : 5000  current step :  2761
total : 5000  current step :  2762
total : 5000  current step :  2763
total : 5000  current step :  2764
total : 5000  current step :  2765
total : 5000  current step :  2766
total : 5000  current step :  2767
total : 5000  current step :  2768
total : 5000  current step :  2769
total : 5000  current step :  2770
total : 5000  current step :  2771
total : 5000  current step :  2772
total : 5000  current step :  2773
total : 5000  current step :  2774
total : 5000  current step :  2775
total : 5000  current step :  2776
total : 5000  current step :  2777
total : 5000  current step :  2778
total : 5000  current step :  2779
total : 5000  current step :  2780
total : 5000  current step :  2781
total : 5000  current step :  2782
total : 5000  current step :  2783
total : 5000  current step :  2784
total : 5000  current step :  2785
total : 5000  current step :  2786
total : 5000  current step :  2787
total : 5000  current step :  2788
total : 5000  current step :  2789
total : 5000  current step :  2790
total : 5000  current step :  2791
total : 5000  current step :  2792
total : 5000  current step :  2793
total : 5000  current step :  2794
total : 5000  current step :  2795
total : 5000  current step :  2796
total : 5000  current step :  2797
total : 5000  current step :  2798
total : 5000  current step :  2799
total : 5000  current step :  2800
total : 5000  current step :  2801
total : 5000  current step :  2802
total : 5000  current step :  2803
total : 5000  current step :  2804
total : 5000  current step :  2805
total : 5000  current step :  2806
total : 5000  current step :  2807
total : 5000  current step :  2808
total : 5000  current step :  2809
total : 5000  current step :  2810
total : 5000  current step :  2811
total : 5000  current step :  2812
total : 5000  current step :  2813
total : 5000  current step :  2814
total : 5000  current step :  2815
total : 5000  current step :  2816
total : 5000  current step :  2817
total : 5000  current step :  2818
total : 5000  current step :  2819
total : 5000  current step :  2820
total : 5000  current step :  2821
total : 5000  current step :  2822
total : 5000  current step :  2823
total : 5000  current step :  2824
total : 5000  current step :  2825
total : 5000  current step :  2826
total : 5000  current step :  2827
total : 5000  current step :  2828
total : 5000  current step :  2829
total : 5000  current step :  2830
total : 5000  current step :  2831
total : 5000  current step :  2832
total : 5000  current step :  2833
total : 5000  current step :  2834
total : 5000  current step :  2835
total : 5000  current step :  2836
total : 5000  current step :  2837
total : 5000  current step :  2838
total : 5000  current step :  2839
total : 5000  current step :  2840
total : 5000  current step :  2841
total : 5000  current step :  2842
total : 5000  current step :  2843
total : 5000  current step :  2844
total : 5000  current step :  2845
total : 5000  current step :  2846
total : 5000  current step :  2847
total : 5000  current step :  2848
total : 5000  current step :  2849
total : 5000  current step :  2850
total : 5000  current step :  2851
total : 5000  current step :  2852
total : 5000  current step :  2853
total : 5000  current step :  2854
total : 5000  current step :  2855
total : 5000  current step :  2856
total : 5000  current step :  2857
total : 5000  current step :  2858
total : 5000  current step :  2859
total : 5000  current step :  2860
total : 5000  current step :  2861
total : 5000  current step :  2862
total : 5000  current step :  2863
total : 5000  current step :  2864
total : 5000  current step :  2865
total : 5000  current step :  2866
total : 5000  current step :  2867
total : 5000  current step :  2868
total : 5000  current step :  2869
total : 5000  current step :  2870
total : 5000  current step :  2871
total : 5000  current step :  2872
total : 5000  current step :  2873
total : 5000  current step :  2874
total : 5000  current step :  2875
total : 5000  current step :  2876
total : 5000  current step :  2877
total : 5000  current step :  2878
total : 5000  current step :  2879
total : 5000  current step :  2880
total : 5000  current step :  2881
total : 5000  current step :  2882
total : 5000  current step :  2883
total : 5000  current step :  2884
total : 5000  current step :  2885
total : 5000  current step :  2886
total : 5000  current step :  2887
total : 5000  current step :  2888
total : 5000  current step :  2889
total : 5000  current step :  2890
total : 5000  current step :  2891
total : 5000  current step :  2892
total : 5000  current step :  2893
total : 5000  current step :  2894
total : 5000  current step :  2895
total : 5000  current step :  2896
total : 5000  current step :  2897
total : 5000  current step :  2898
total : 5000  current step :  2899
total : 5000  current step :  2900
total : 5000  current step :  2901
total : 5000  current step :  2902
total : 5000  current step :  2903
total : 5000  current step :  2904
total : 5000  current step :  2905
total : 5000  current step :  2906
total : 5000  current step :  2907
total : 5000  current step :  2908
total : 5000  current step :  2909
total : 5000  current step :  2910
total : 5000  current step :  2911
total : 5000  current step :  2912
total : 5000  current step :  2913
total : 5000  current step :  2914
total : 5000  current step :  2915
total : 5000  current step :  2916
total : 5000  current step :  2917
total : 5000  current step :  2918
total : 5000  current step :  2919
total : 5000  current step :  2920
total : 5000  current step :  2921
total : 5000  current step :  2922
total : 5000  current step :  2923
total : 5000  current step :  2924
total : 5000  current step :  2925
total : 5000  current step :  2926
total : 5000  current step :  2927
total : 5000  current step :  2928
total : 5000  current step :  2929
total : 5000  current step :  2930
total : 5000  current step :  2931
total : 5000  current step :  2932
total : 5000  current step :  2933
total : 5000  current step :  2934
total : 5000  current step :  2935
total : 5000  current step :  2936
total : 5000  current step :  2937
total : 5000  current step :  2938
total : 5000  current step :  2939
total : 5000  current step :  2940
total : 5000  current step :  2941
total : 5000  current step :  2942
total : 5000  current step :  2943
total : 5000  current step :  2944
total : 5000  current step :  2945
total : 5000  current step :  2946
total : 5000  current step :  2947
total : 5000  current step :  2948
total : 5000  current step :  2949
total : 5000  current step :  2950
total : 5000  current step :  2951
total : 5000  current step :  2952
total : 5000  current step :  2953
total : 5000  current step :  2954
total : 5000  current step :  2955
total : 5000  current step :  2956
total : 5000  current step :  2957
total : 5000  current step :  2958
total : 5000  current step :  2959
total : 5000  current step :  2960
total : 5000  current step :  2961
total : 5000  current step :  2962
total : 5000  current step :  2963
total : 5000  current step :  2964
total : 5000  current step :  2965
total : 5000  current step :  2966
total : 5000  current step :  2967
total : 5000  current step :  2968
total : 5000  current step :  2969
total : 5000  current step :  2970
total : 5000  current step :  2971
total : 5000  current step :  2972
total : 5000  current step :  2973
total : 5000  current step :  2974
total : 5000  current step :  2975
total : 5000  current step :  2976
total : 5000  current step :  2977
total : 5000  current step :  2978
total : 5000  current step :  2979
total : 5000  current step :  2980
total : 5000  current step :  2981
total : 5000  current step :  2982
total : 5000  current step :  2983
total : 5000  current step :  2984
total : 5000  current step :  2985
total : 5000  current step :  2986
total : 5000  current step :  2987
total : 5000  current step :  2988
total : 5000  current step :  2989
total : 5000  current step :  2990
total : 5000  current step :  2991
total : 5000  current step :  2992
total : 5000  current step :  2993
total : 5000  current step :  2994
total : 5000  current step :  2995
total : 5000  current step :  2996
total : 5000  current step :  2997
total : 5000  current step :  2998
total : 5000  current step :  2999
total : 5000  current step :  3000
total : 5000  current step :  3001
total : 5000  current step :  3002
total : 5000  current step :  3003
total : 5000  current step :  3004
total : 5000  current step :  3005
total : 5000  current step :  3006
total : 5000  current step :  3007
total : 5000  current step :  3008
total : 5000  current step :  3009
total : 5000  current step :  3010
total : 5000  current step :  3011
total : 5000  current step :  3012
total : 5000  current step :  3013
total : 5000  current step :  3014
total : 5000  current step :  3015
total : 5000  current step :  3016
total : 5000  current step :  3017
total : 5000  current step :  3018
total : 5000  current step :  3019
total : 5000  current step :  3020
total : 5000  current step :  3021
total : 5000  current step :  3022
total : 5000  current step :  3023
total : 5000  current step :  3024
total : 5000  current step :  3025
total : 5000  current step :  3026
total : 5000  current step :  3027
total : 5000  current step :  3028
total : 5000  current step :  3029
total : 5000  current step :  3030
total : 5000  current step :  3031
total : 5000  current step :  3032
total : 5000  current step :  3033
total : 5000  current step :  3034
total : 5000  current step :  3035
total : 5000  current step :  3036
total : 5000  current step :  3037
total : 5000  current step :  3038
total : 5000  current step :  3039
total : 5000  current step :  3040
total : 5000  current step :  3041
total : 5000  current step :  3042
total : 5000  current step :  3043
total : 5000  current step :  3044
total : 5000  current step :  3045
total : 5000  current step :  3046
total : 5000  current step :  3047
total : 5000  current step :  3048
total : 5000  current step :  3049
total : 5000  current step :  3050
total : 5000  current step :  3051
total : 5000  current step :  3052
total : 5000  current step :  3053
total : 5000  current step :  3054
total : 5000  current step :  3055
total : 5000  current step :  3056
total : 5000  current step :  3057
total : 5000  current step :  3058
total : 5000  current step :  3059
total : 5000  current step :  3060
total : 5000  current step :  3061
total : 5000  current step :  3062
total : 5000  current step :  3063
total : 5000  current step :  3064
total : 5000  current step :  3065
total : 5000  current step :  3066
total : 5000  current step :  3067
total : 5000  current step :  3068
total : 5000  current step :  3069
total : 5000  current step :  3070
total : 5000  current step :  3071
total : 5000  current step :  3072
total : 5000  current step :  3073
total : 5000  current step :  3074
total : 5000  current step :  3075
total : 5000  current step :  3076
total : 5000  current step :  3077
total : 5000  current step :  3078
total : 5000  current step :  3079
total : 5000  current step :  3080
total : 5000  current step :  3081
total : 5000  current step :  3082
total : 5000  current step :  3083
total : 5000  current step :  3084
total : 5000  current step :  3085
total : 5000  current step :  3086
total : 5000  current step :  3087
total : 5000  current step :  3088
total : 5000  current step :  3089
total : 5000  current step :  3090
total : 5000  current step :  3091
total : 5000  current step :  3092
total : 5000  current step :  3093
total : 5000  current step :  3094
total : 5000  current step :  3095
total : 5000  current step :  3096
total : 5000  current step :  3097
total : 5000  current step :  3098
total : 5000  current step :  3099
total : 5000  current step :  3100
total : 5000  current step :  3101
total : 5000  current step :  3102
total : 5000  current step :  3103
total : 5000  current step :  3104
total : 5000  current step :  3105
total : 5000  current step :  3106
total : 5000  current step :  3107
total : 5000  current step :  3108
total : 5000  current step :  3109
total : 5000  current step :  3110
total : 5000  current step :  3111
total : 5000  current step :  3112
total : 5000  current step :  3113
total : 5000  current step :  3114
total : 5000  current step :  3115
total : 5000  current step :  3116
total : 5000  current step :  3117
total : 5000  current step :  3118
total : 5000  current step :  3119
total : 5000  current step :  3120
total : 5000  current step :  3121
total : 5000  current step :  3122
total : 5000  current step :  3123
total : 5000  current step :  3124
total : 5000  current step :  3125
total : 5000  current step :  3126
total : 5000  current step :  3127
total : 5000  current step :  3128
total : 5000  current step :  3129
total : 5000  current step :  3130
total : 5000  current step :  3131
total : 5000  current step :  3132
total : 5000  current step :  3133
total : 5000  current step :  3134
total : 5000  current step :  3135
total : 5000  current step :  3136
total : 5000  current step :  3137
total : 5000  current step :  3138
total : 5000  current step :  3139
total : 5000  current step :  3140
total : 5000  current step :  3141
total : 5000  current step :  3142
total : 5000  current step :  3143
total : 5000  current step :  3144
total : 5000  current step :  3145
total : 5000  current step :  3146
total : 5000  current step :  3147
total : 5000  current step :  3148
total : 5000  current step :  3149
total : 5000  current step :  3150
total : 5000  current step :  3151
total : 5000  current step :  3152
total : 5000  current step :  3153
total : 5000  current step :  3154
total : 5000  current step :  3155
total : 5000  current step :  3156
total : 5000  current step :  3157
total : 5000  current step :  3158
total : 5000  current step :  3159
total : 5000  current step :  3160
total : 5000  current step :  3161
total : 5000  current step :  3162
total : 5000  current step :  3163
total : 5000  current step :  3164
total : 5000  current step :  3165
total : 5000  current step :  3166
total : 5000  current step :  3167
total : 5000  current step :  3168
total : 5000  current step :  3169
total : 5000  current step :  3170
total : 5000  current step :  3171
total : 5000  current step :  3172
total : 5000  current step :  3173
total : 5000  current step :  3174
total : 5000  current step :  3175
total : 5000  current step :  3176
total : 5000  current step :  3177
total : 5000  current step :  3178
total : 5000  current step :  3179
total : 5000  current step :  3180
total : 5000  current step :  3181
total : 5000  current step :  3182
total : 5000  current step :  3183
total : 5000  current step :  3184
total : 5000  current step :  3185
total : 5000  current step :  3186
total : 5000  current step :  3187
total : 5000  current step :  3188
total : 5000  current step :  3189
total : 5000  current step :  3190
total : 5000  current step :  3191
total : 5000  current step :  3192
total : 5000  current step :  3193
total : 5000  current step :  3194
total : 5000  current step :  3195
total : 5000  current step :  3196
total : 5000  current step :  3197
total : 5000  current step :  3198
total : 5000  current step :  3199
total : 5000  current step :  3200
total : 5000  current step :  3201
total : 5000  current step :  3202
total : 5000  current step :  3203
total : 5000  current step :  3204
total : 5000  current step :  3205
total : 5000  current step :  3206
total : 5000  current step :  3207
total : 5000  current step :  3208
total : 5000  current step :  3209
total : 5000  current step :  3210
total : 5000  current step :  3211
total : 5000  current step :  3212
total : 5000  current step :  3213
total : 5000  current step :  3214
total : 5000  current step :  3215
total : 5000  current step :  3216
total : 5000  current step :  3217
total : 5000  current step :  3218
total : 5000  current step :  3219
total : 5000  current step :  3220
total : 5000  current step :  3221
total : 5000  current step :  3222
total : 5000  current step :  3223
total : 5000  current step :  3224
total : 5000  current step :  3225
total : 5000  current step :  3226
total : 5000  current step :  3227
total : 5000  current step :  3228
total : 5000  current step :  3229
total : 5000  current step :  3230
total : 5000  current step :  3231
total : 5000  current step :  3232
total : 5000  current step :  3233
total : 5000  current step :  3234
total : 5000  current step :  3235
total : 5000  current step :  3236
total : 5000  current step :  3237
total : 5000  current step :  3238
total : 5000  current step :  3239
total : 5000  current step :  3240
total : 5000  current step :  3241
total : 5000  current step :  3242
total : 5000  current step :  3243
total : 5000  current step :  3244
total : 5000  current step :  3245
total : 5000  current step :  3246
total : 5000  current step :  3247
total : 5000  current step :  3248
total : 5000  current step :  3249
total : 5000  current step :  3250
total : 5000  current step :  3251
total : 5000  current step :  3252
total : 5000  current step :  3253
total : 5000  current step :  3254
total : 5000  current step :  3255
total : 5000  current step :  3256
total : 5000  current step :  3257
total : 5000  current step :  3258
total : 5000  current step :  3259
total : 5000  current step :  3260
total : 5000  current step :  3261
total : 5000  current step :  3262
total : 5000  current step :  3263
total : 5000  current step :  3264
total : 5000  current step :  3265
total : 5000  current step :  3266
total : 5000  current step :  3267
total : 5000  current step :  3268
total : 5000  current step :  3269
total : 5000  current step :  3270
total : 5000  current step :  3271
total : 5000  current step :  3272
total : 5000  current step :  3273
total : 5000  current step :  3274
total : 5000  current step :  3275
total : 5000  current step :  3276
total : 5000  current step :  3277
total : 5000  current step :  3278
total : 5000  current step :  3279
total : 5000  current step :  3280
total : 5000  current step :  3281
total : 5000  current step :  3282
total : 5000  current step :  3283
total : 5000  current step :  3284
total : 5000  current step :  3285
total : 5000  current step :  3286
total : 5000  current step :  3287
total : 5000  current step :  3288
total : 5000  current step :  3289
total : 5000  current step :  3290
total : 5000  current step :  3291
total : 5000  current step :  3292
total : 5000  current step :  3293
total : 5000  current step :  3294
total : 5000  current step :  3295
total : 5000  current step :  3296
total : 5000  current step :  3297
total : 5000  current step :  3298
total : 5000  current step :  3299
total : 5000  current step :  3300
total : 5000  current step :  3301
total : 5000  current step :  3302
total : 5000  current step :  3303
total : 5000  current step :  3304
total : 5000  current step :  3305
total : 5000  current step :  3306
total : 5000  current step :  3307
total : 5000  current step :  3308
total : 5000  current step :  3309
total : 5000  current step :  3310
total : 5000  current step :  3311
total : 5000  current step :  3312
total : 5000  current step :  3313
total : 5000  current step :  3314
total : 5000  current step :  3315
total : 5000  current step :  3316
total : 5000  current step :  3317
total : 5000  current step :  3318
total : 5000  current step :  3319
total : 5000  current step :  3320
total : 5000  current step :  3321
total : 5000  current step :  3322
total : 5000  current step :  3323
total : 5000  current step :  3324
total : 5000  current step :  3325
total : 5000  current step :  3326
total : 5000  current step :  3327
total : 5000  current step :  3328
total : 5000  current step :  3329
total : 5000  current step :  3330
total : 5000  current step :  3331
total : 5000  current step :  3332
total : 5000  current step :  3333
total : 5000  current step :  3334
total : 5000  current step :  3335
total : 5000  current step :  3336
total : 5000  current step :  3337
total : 5000  current step :  3338
total : 5000  current step :  3339
total : 5000  current step :  3340
total : 5000  current step :  3341
total : 5000  current step :  3342
total : 5000  current step :  3343
total : 5000  current step :  3344
total : 5000  current step :  3345
total : 5000  current step :  3346
total : 5000  current step :  3347
total : 5000  current step :  3348
total : 5000  current step :  3349
total : 5000  current step :  3350
total : 5000  current step :  3351
total : 5000  current step :  3352
total : 5000  current step :  3353
total : 5000  current step :  3354
total : 5000  current step :  3355
total : 5000  current step :  3356
total : 5000  current step :  3357
total : 5000  current step :  3358
total : 5000  current step :  3359
total : 5000  current step :  3360
total : 5000  current step :  3361
total : 5000  current step :  3362
total : 5000  current step :  3363
total : 5000  current step :  3364
total : 5000  current step :  3365
total : 5000  current step :  3366
total : 5000  current step :  3367
total : 5000  current step :  3368
total : 5000  current step :  3369
total : 5000  current step :  3370
total : 5000  current step :  3371
total : 5000  current step :  3372
total : 5000  current step :  3373
total : 5000  current step :  3374
total : 5000  current step :  3375
total : 5000  current step :  3376
total : 5000  current step :  3377
total : 5000  current step :  3378
total : 5000  current step :  3379
total : 5000  current step :  3380
total : 5000  current step :  3381
total : 5000  current step :  3382
total : 5000  current step :  3383
total : 5000  current step :  3384
total : 5000  current step :  3385
total : 5000  current step :  3386
total : 5000  current step :  3387
total : 5000  current step :  3388
total : 5000  current step :  3389
total : 5000  current step :  3390
total : 5000  current step :  3391
total : 5000  current step :  3392
total : 5000  current step :  3393
total : 5000  current step :  3394
total : 5000  current step :  3395
total : 5000  current step :  3396
total : 5000  current step :  3397
total : 5000  current step :  3398
total : 5000  current step :  3399
total : 5000  current step :  3400
total : 5000  current step :  3401
total : 5000  current step :  3402
total : 5000  current step :  3403
total : 5000  current step :  3404
total : 5000  current step :  3405
total : 5000  current step :  3406
total : 5000  current step :  3407
total : 5000  current step :  3408
total : 5000  current step :  3409
total : 5000  current step :  3410
total : 5000  current step :  3411
total : 5000  current step :  3412
total : 5000  current step :  3413
total : 5000  current step :  3414
total : 5000  current step :  3415
total : 5000  current step :  3416
total : 5000  current step :  3417
total : 5000  current step :  3418
total : 5000  current step :  3419
total : 5000  current step :  3420
total : 5000  current step :  3421
total : 5000  current step :  3422
total : 5000  current step :  3423
total : 5000  current step :  3424
total : 5000  current step :  3425
total : 5000  current step :  3426
total : 5000  current step :  3427
total : 5000  current step :  3428
total : 5000  current step :  3429
total : 5000  current step :  3430
total : 5000  current step :  3431
total : 5000  current step :  3432
total : 5000  current step :  3433
total : 5000  current step :  3434
total : 5000  current step :  3435
total : 5000  current step :  3436
total : 5000  current step :  3437
total : 5000  current step :  3438
total : 5000  current step :  3439
total : 5000  current step :  3440
total : 5000  current step :  3441
total : 5000  current step :  3442
total : 5000  current step :  3443
total : 5000  current step :  3444
total : 5000  current step :  3445
total : 5000  current step :  3446
total : 5000  current step :  3447
total : 5000  current step :  3448
total : 5000  current step :  3449
total : 5000  current step :  3450
total : 5000  current step :  3451
total : 5000  current step :  3452
total : 5000  current step :  3453
total : 5000  current step :  3454
total : 5000  current step :  3455
total : 5000  current step :  3456
total : 5000  current step :  3457
total : 5000  current step :  3458
total : 5000  current step :  3459
total : 5000  current step :  3460
total : 5000  current step :  3461
total : 5000  current step :  3462
total : 5000  current step :  3463
total : 5000  current step :  3464
total : 5000  current step :  3465
total : 5000  current step :  3466
total : 5000  current step :  3467
total : 5000  current step :  3468
total : 5000  current step :  3469
total : 5000  current step :  3470
total : 5000  current step :  3471
total : 5000  current step :  3472
total : 5000  current step :  3473
total : 5000  current step :  3474
total : 5000  current step :  3475
total : 5000  current step :  3476
total : 5000  current step :  3477
total : 5000  current step :  3478
total : 5000  current step :  3479
total : 5000  current step :  3480
total : 5000  current step :  3481
total : 5000  current step :  3482
total : 5000  current step :  3483
total : 5000  current step :  3484
total : 5000  current step :  3485
total : 5000  current step :  3486
total : 5000  current step :  3487
total : 5000  current step :  3488
total : 5000  current step :  3489
total : 5000  current step :  3490
total : 5000  current step :  3491
total : 5000  current step :  3492
total : 5000  current step :  3493
total : 5000  current step :  3494
total : 5000  current step :  3495
total : 5000  current step :  3496
total : 5000  current step :  3497
total : 5000  current step :  3498
total : 5000  current step :  3499
total : 5000  current step :  3500
total : 5000  current step :  3501
total : 5000  current step :  3502
total : 5000  current step :  3503
total : 5000  current step :  3504
total : 5000  current step :  3505
total : 5000  current step :  3506
total : 5000  current step :  3507
total : 5000  current step :  3508
total : 5000  current step :  3509
total : 5000  current step :  3510
total : 5000  current step :  3511
total : 5000  current step :  3512
total : 5000  current step :  3513
total : 5000  current step :  3514
total : 5000  current step :  3515
total : 5000  current step :  3516
total : 5000  current step :  3517
total : 5000  current step :  3518
total : 5000  current step :  3519
total : 5000  current step :  3520
total : 5000  current step :  3521
total : 5000  current step :  3522
total : 5000  current step :  3523
total : 5000  current step :  3524
total : 5000  current step :  3525
total : 5000  current step :  3526
total : 5000  current step :  3527
total : 5000  current step :  3528
total : 5000  current step :  3529
total : 5000  current step :  3530
total : 5000  current step :  3531
total : 5000  current step :  3532
total : 5000  current step :  3533
total : 5000  current step :  3534
total : 5000  current step :  3535
total : 5000  current step :  3536
total : 5000  current step :  3537
total : 5000  current step :  3538
total : 5000  current step :  3539
total : 5000  current step :  3540
total : 5000  current step :  3541
total : 5000  current step :  3542
total : 5000  current step :  3543
total : 5000  current step :  3544
total : 5000  current step :  3545
total : 5000  current step :  3546
total : 5000  current step :  3547
total : 5000  current step :  3548
total : 5000  current step :  3549
total : 5000  current step :  3550
total : 5000  current step :  3551
total : 5000  current step :  3552
total : 5000  current step :  3553
total : 5000  current step :  3554
total : 5000  current step :  3555
total : 5000  current step :  3556
total : 5000  current step :  3557
total : 5000  current step :  3558
total : 5000  current step :  3559
total : 5000  current step :  3560
total : 5000  current step :  3561
total : 5000  current step :  3562
total : 5000  current step :  3563
total : 5000  current step :  3564
total : 5000  current step :  3565
total : 5000  current step :  3566
total : 5000  current step :  3567
total : 5000  current step :  3568
total : 5000  current step :  3569
total : 5000  current step :  3570
total : 5000  current step :  3571
total : 5000  current step :  3572
total : 5000  current step :  3573
total : 5000  current step :  3574
total : 5000  current step :  3575
total : 5000  current step :  3576
total : 5000  current step :  3577
total : 5000  current step :  3578
total : 5000  current step :  3579
total : 5000  current step :  3580
total : 5000  current step :  3581
total : 5000  current step :  3582
total : 5000  current step :  3583
total : 5000  current step :  3584
total : 5000  current step :  3585
total : 5000  current step :  3586
total : 5000  current step :  3587
total : 5000  current step :  3588
total : 5000  current step :  3589
total : 5000  current step :  3590
total : 5000  current step :  3591
total : 5000  current step :  3592
total : 5000  current step :  3593
total : 5000  current step :  3594
total : 5000  current step :  3595
total : 5000  current step :  3596
total : 5000  current step :  3597
total : 5000  current step :  3598
total : 5000  current step :  3599
total : 5000  current step :  3600
total : 5000  current step :  3601
total : 5000  current step :  3602
total : 5000  current step :  3603
total : 5000  current step :  3604
total : 5000  current step :  3605
total : 5000  current step :  3606
total : 5000  current step :  3607
total : 5000  current step :  3608
total : 5000  current step :  3609
total : 5000  current step :  3610
total : 5000  current step :  3611
total : 5000  current step :  3612
total : 5000  current step :  3613
total : 5000  current step :  3614
total : 5000  current step :  3615
total : 5000  current step :  3616
total : 5000  current step :  3617
total : 5000  current step :  3618
total : 5000  current step :  3619
total : 5000  current step :  3620
total : 5000  current step :  3621
total : 5000  current step :  3622
total : 5000  current step :  3623
total : 5000  current step :  3624
total : 5000  current step :  3625
total : 5000  current step :  3626
total : 5000  current step :  3627
total : 5000  current step :  3628
total : 5000  current step :  3629
total : 5000  current step :  3630
total : 5000  current step :  3631
total : 5000  current step :  3632
total : 5000  current step :  3633
total : 5000  current step :  3634
total : 5000  current step :  3635
total : 5000  current step :  3636
total : 5000  current step :  3637
total : 5000  current step :  3638
total : 5000  current step :  3639
total : 5000  current step :  3640
total : 5000  current step :  3641
total : 5000  current step :  3642
total : 5000  current step :  3643
total : 5000  current step :  3644
total : 5000  current step :  3645
total : 5000  current step :  3646
total : 5000  current step :  3647
total : 5000  current step :  3648
total : 5000  current step :  3649
total : 5000  current step :  3650
total : 5000  current step :  3651
total : 5000  current step :  3652
total : 5000  current step :  3653
total : 5000  current step :  3654
total : 5000  current step :  3655
total : 5000  current step :  3656
total : 5000  current step :  3657
total : 5000  current step :  3658
total : 5000  current step :  3659
total : 5000  current step :  3660
total : 5000  current step :  3661
total : 5000  current step :  3662
total : 5000  current step :  3663
total : 5000  current step :  3664
total : 5000  current step :  3665
total : 5000  current step :  3666
total : 5000  current step :  3667
total : 5000  current step :  3668
total : 5000  current step :  3669
total : 5000  current step :  3670
total : 5000  current step :  3671
total : 5000  current step :  3672
total : 5000  current step :  3673
total : 5000  current step :  3674
total : 5000  current step :  3675
total : 5000  current step :  3676
total : 5000  current step :  3677
total : 5000  current step :  3678
total : 5000  current step :  3679
total : 5000  current step :  3680
total : 5000  current step :  3681
total : 5000  current step :  3682
total : 5000  current step :  3683
total : 5000  current step :  3684
total : 5000  current step :  3685
total : 5000  current step :  3686
total : 5000  current step :  3687
total : 5000  current step :  3688
total : 5000  current step :  3689
total : 5000  current step :  3690
total : 5000  current step :  3691
total : 5000  current step :  3692
total : 5000  current step :  3693
total : 5000  current step :  3694
total : 5000  current step :  3695
total : 5000  current step :  3696
total : 5000  current step :  3697
total : 5000  current step :  3698
total : 5000  current step :  3699
total : 5000  current step :  3700
total : 5000  current step :  3701
total : 5000  current step :  3702
total : 5000  current step :  3703
total : 5000  current step :  3704
total : 5000  current step :  3705
total : 5000  current step :  3706
total : 5000  current step :  3707
total : 5000  current step :  3708
total : 5000  current step :  3709
total : 5000  current step :  3710
total : 5000  current step :  3711
total : 5000  current step :  3712
total : 5000  current step :  3713
total : 5000  current step :  3714
total : 5000  current step :  3715
total : 5000  current step :  3716
total : 5000  current step :  3717
total : 5000  current step :  3718
total : 5000  current step :  3719
total : 5000  current step :  3720
total : 5000  current step :  3721
total : 5000  current step :  3722
total : 5000  current step :  3723
total : 5000  current step :  3724
total : 5000  current step :  3725
total : 5000  current step :  3726
total : 5000  current step :  3727
total : 5000  current step :  3728
total : 5000  current step :  3729
total : 5000  current step :  3730
total : 5000  current step :  3731
total : 5000  current step :  3732
total : 5000  current step :  3733
total : 5000  current step :  3734
total : 5000  current step :  3735
total : 5000  current step :  3736
total : 5000  current step :  3737
total : 5000  current step :  3738
total : 5000  current step :  3739
total : 5000  current step :  3740
total : 5000  current step :  3741
total : 5000  current step :  3742
total : 5000  current step :  3743
total : 5000  current step :  3744
total : 5000  current step :  3745
total : 5000  current step :  3746
total : 5000  current step :  3747
total : 5000  current step :  3748
total : 5000  current step :  3749
total : 5000  current step :  3750
total : 5000  current step :  3751
total : 5000  current step :  3752
total : 5000  current step :  3753
total : 5000  current step :  3754
total : 5000  current step :  3755
total : 5000  current step :  3756
total : 5000  current step :  3757
total : 5000  current step :  3758
total : 5000  current step :  3759
total : 5000  current step :  3760
total : 5000  current step :  3761
total : 5000  current step :  3762
total : 5000  current step :  3763
total : 5000  current step :  3764
total : 5000  current step :  3765
total : 5000  current step :  3766
total : 5000  current step :  3767
total : 5000  current step :  3768
total : 5000  current step :  3769
total : 5000  current step :  3770
total : 5000  current step :  3771
total : 5000  current step :  3772
total : 5000  current step :  3773
total : 5000  current step :  3774
total : 5000  current step :  3775
total : 5000  current step :  3776
total : 5000  current step :  3777
total : 5000  current step :  3778
total : 5000  current step :  3779
total : 5000  current step :  3780
total : 5000  current step :  3781
total : 5000  current step :  3782
total : 5000  current step :  3783
total : 5000  current step :  3784
total : 5000  current step :  3785
total : 5000  current step :  3786
total : 5000  current step :  3787
total : 5000  current step :  3788
total : 5000  current step :  3789
total : 5000  current step :  3790
total : 5000  current step :  3791
total : 5000  current step :  3792
total : 5000  current step :  3793
total : 5000  current step :  3794
total : 5000  current step :  3795
total : 5000  current step :  3796
total : 5000  current step :  3797
total : 5000  current step :  3798
total : 5000  current step :  3799
total : 5000  current step :  3800
total : 5000  current step :  3801
total : 5000  current step :  3802
total : 5000  current step :  3803
total : 5000  current step :  3804
total : 5000  current step :  3805
total : 5000  current step :  3806
total : 5000  current step :  3807
total : 5000  current step :  3808
total : 5000  current step :  3809
total : 5000  current step :  3810
total : 5000  current step :  3811
total : 5000  current step :  3812
total : 5000  current step :  3813
total : 5000  current step :  3814
total : 5000  current step :  3815
total : 5000  current step :  3816
total : 5000  current step :  3817
total : 5000  current step :  3818
total : 5000  current step :  3819
total : 5000  current step :  3820
total : 5000  current step :  3821
total : 5000  current step :  3822
total : 5000  current step :  3823
total : 5000  current step :  3824
total : 5000  current step :  3825
total : 5000  current step :  3826
total : 5000  current step :  3827
total : 5000  current step :  3828
total : 5000  current step :  3829
total : 5000  current step :  3830
total : 5000  current step :  3831
total : 5000  current step :  3832
total : 5000  current step :  3833
total : 5000  current step :  3834
total : 5000  current step :  3835
total : 5000  current step :  3836
total : 5000  current step :  3837
total : 5000  current step :  3838
total : 5000  current step :  3839
total : 5000  current step :  3840
total : 5000  current step :  3841
total : 5000  current step :  3842
total : 5000  current step :  3843
total : 5000  current step :  3844
total : 5000  current step :  3845
total : 5000  current step :  3846
total : 5000  current step :  3847
total : 5000  current step :  3848
total : 5000  current step :  3849
total : 5000  current step :  3850
total : 5000  current step :  3851
total : 5000  current step :  3852
total : 5000  current step :  3853
total : 5000  current step :  3854
total : 5000  current step :  3855
total : 5000  current step :  3856
total : 5000  current step :  3857
total : 5000  current step :  3858
total : 5000  current step :  3859
total : 5000  current step :  3860
total : 5000  current step :  3861
total : 5000  current step :  3862
total : 5000  current step :  3863
total : 5000  current step :  3864
total : 5000  current step :  3865
total : 5000  current step :  3866
total : 5000  current step :  3867
total : 5000  current step :  3868
total : 5000  current step :  3869
total : 5000  current step :  3870
total : 5000  current step :  3871
total : 5000  current step :  3872
total : 5000  current step :  3873
total : 5000  current step :  3874
total : 5000  current step :  3875
total : 5000  current step :  3876
total : 5000  current step :  3877
total : 5000  current step :  3878
total : 5000  current step :  3879
total : 5000  current step :  3880
total : 5000  current step :  3881
total : 5000  current step :  3882
total : 5000  current step :  3883
total : 5000  current step :  3884
total : 5000  current step :  3885
total : 5000  current step :  3886
total : 5000  current step :  3887
total : 5000  current step :  3888
total : 5000  current step :  3889
total : 5000  current step :  3890
total : 5000  current step :  3891
total : 5000  current step :  3892
total : 5000  current step :  3893
total : 5000  current step :  3894
total : 5000  current step :  3895
total : 5000  current step :  3896
total : 5000  current step :  3897
total : 5000  current step :  3898
total : 5000  current step :  3899
total : 5000  current step :  3900
total : 5000  current step :  3901
total : 5000  current step :  3902
total : 5000  current step :  3903
total : 5000  current step :  3904
total : 5000  current step :  3905
total : 5000  current step :  3906
total : 5000  current step :  3907
total : 5000  current step :  3908
total : 5000  current step :  3909
total : 5000  current step :  3910
total : 5000  current step :  3911
total : 5000  current step :  3912
total : 5000  current step :  3913
total : 5000  current step :  3914
total : 5000  current step :  3915
total : 5000  current step :  3916
total : 5000  current step :  3917
total : 5000  current step :  3918
total : 5000  current step :  3919
total : 5000  current step :  3920
total : 5000  current step :  3921
total : 5000  current step :  3922
total : 5000  current step :  3923
total : 5000  current step :  3924
total : 5000  current step :  3925
total : 5000  current step :  3926
total : 5000  current step :  3927
total : 5000  current step :  3928
total : 5000  current step :  3929
total : 5000  current step :  3930
total : 5000  current step :  3931
total : 5000  current step :  3932
total : 5000  current step :  3933
total : 5000  current step :  3934
total : 5000  current step :  3935
total : 5000  current step :  3936
total : 5000  current step :  3937
total : 5000  current step :  3938
total : 5000  current step :  3939
total : 5000  current step :  3940
total : 5000  current step :  3941
total : 5000  current step :  3942
total : 5000  current step :  3943
total : 5000  current step :  3944
total : 5000  current step :  3945
total : 5000  current step :  3946
total : 5000  current step :  3947
total : 5000  current step :  3948
total : 5000  current step :  3949
total : 5000  current step :  3950
total : 5000  current step :  3951
total : 5000  current step :  3952
total : 5000  current step :  3953
total : 5000  current step :  3954
total : 5000  current step :  3955
total : 5000  current step :  3956
total : 5000  current step :  3957
total : 5000  current step :  3958
total : 5000  current step :  3959
total : 5000  current step :  3960
total : 5000  current step :  3961
total : 5000  current step :  3962
total : 5000  current step :  3963
total : 5000  current step :  3964
total : 5000  current step :  3965
total : 5000  current step :  3966
total : 5000  current step :  3967
total : 5000  current step :  3968
total : 5000  current step :  3969
total : 5000  current step :  3970
total : 5000  current step :  3971
total : 5000  current step :  3972
total : 5000  current step :  3973
total : 5000  current step :  3974
total : 5000  current step :  3975
total : 5000  current step :  3976
total : 5000  current step :  3977
total : 5000  current step :  3978
total : 5000  current step :  3979
total : 5000  current step :  3980
total : 5000  current step :  3981
total : 5000  current step :  3982
total : 5000  current step :  3983
total : 5000  current step :  3984
total : 5000  current step :  3985
total : 5000  current step :  3986
total : 5000  current step :  3987
total : 5000  current step :  3988
total : 5000  current step :  3989
total : 5000  current step :  3990
total : 5000  current step :  3991
total : 5000  current step :  3992
total : 5000  current step :  3993
total : 5000  current step :  3994
total : 5000  current step :  3995
total : 5000  current step :  3996
total : 5000  current step :  3997
total : 5000  current step :  3998
total : 5000  current step :  3999
total : 5000  current step :  4000
total : 5000  current step :  4001
total : 5000  current step :  4002
total : 5000  current step :  4003
total : 5000  current step :  4004
total : 5000  current step :  4005
total : 5000  current step :  4006
total : 5000  current step :  4007
total : 5000  current step :  4008
total : 5000  current step :  4009
total : 5000  current step :  4010
total : 5000  current step :  4011
total : 5000  current step :  4012
total : 5000  current step :  4013
total : 5000  current step :  4014
total : 5000  current step :  4015
total : 5000  current step :  4016
total : 5000  current step :  4017
total : 5000  current step :  4018
total : 5000  current step :  4019
total : 5000  current step :  4020
total : 5000  current step :  4021
total : 5000  current step :  4022
total : 5000  current step :  4023
total : 5000  current step :  4024
total : 5000  current step :  4025
total : 5000  current step :  4026
total : 5000  current step :  4027
total : 5000  current step :  4028
total : 5000  current step :  4029
total : 5000  current step :  4030
total : 5000  current step :  4031
total : 5000  current step :  4032
total : 5000  current step :  4033
total : 5000  current step :  4034
total : 5000  current step :  4035
total : 5000  current step :  4036
total : 5000  current step :  4037
total : 5000  current step :  4038
total : 5000  current step :  4039
total : 5000  current step :  4040
total : 5000  current step :  4041
total : 5000  current step :  4042
total : 5000  current step :  4043
total : 5000  current step :  4044
total : 5000  current step :  4045
total : 5000  current step :  4046
total : 5000  current step :  4047
total : 5000  current step :  4048
total : 5000  current step :  4049
total : 5000  current step :  4050
total : 5000  current step :  4051
total : 5000  current step :  4052
total : 5000  current step :  4053
total : 5000  current step :  4054
total : 5000  current step :  4055
total : 5000  current step :  4056
total : 5000  current step :  4057
total : 5000  current step :  4058
total : 5000  current step :  4059
total : 5000  current step :  4060
total : 5000  current step :  4061
total : 5000  current step :  4062
total : 5000  current step :  4063
total : 5000  current step :  4064
total : 5000  current step :  4065
total : 5000  current step :  4066
total : 5000  current step :  4067
total : 5000  current step :  4068
total : 5000  current step :  4069
total : 5000  current step :  4070
total : 5000  current step :  4071
total : 5000  current step :  4072
total : 5000  current step :  4073
total : 5000  current step :  4074
total : 5000  current step :  4075
total : 5000  current step :  4076
total : 5000  current step :  4077
total : 5000  current step :  4078
total : 5000  current step :  4079
total : 5000  current step :  4080
total : 5000  current step :  4081
total : 5000  current step :  4082
total : 5000  current step :  4083
total : 5000  current step :  4084
total : 5000  current step :  4085
total : 5000  current step :  4086
total : 5000  current step :  4087
total : 5000  current step :  4088
total : 5000  current step :  4089
total : 5000  current step :  4090
total : 5000  current step :  4091
total : 5000  current step :  4092
total : 5000  current step :  4093
total : 5000  current step :  4094
total : 5000  current step :  4095
total : 5000  current step :  4096
total : 5000  current step :  4097
total : 5000  current step :  4098
total : 5000  current step :  4099
total : 5000  current step :  4100
total : 5000  current step :  4101
total : 5000  current step :  4102
total : 5000  current step :  4103
total : 5000  current step :  4104
total : 5000  current step :  4105
total : 5000  current step :  4106
total : 5000  current step :  4107
total : 5000  current step :  4108
total : 5000  current step :  4109
total : 5000  current step :  4110
total : 5000  current step :  4111
total : 5000  current step :  4112
total : 5000  current step :  4113
total : 5000  current step :  4114
total : 5000  current step :  4115
total : 5000  current step :  4116
total : 5000  current step :  4117
total : 5000  current step :  4118
total : 5000  current step :  4119
total : 5000  current step :  4120
total : 5000  current step :  4121
total : 5000  current step :  4122
total : 5000  current step :  4123
total : 5000  current step :  4124
total : 5000  current step :  4125
total : 5000  current step :  4126
total : 5000  current step :  4127
total : 5000  current step :  4128
total : 5000  current step :  4129
total : 5000  current step :  4130
total : 5000  current step :  4131
total : 5000  current step :  4132
total : 5000  current step :  4133
total : 5000  current step :  4134
total : 5000  current step :  4135
total : 5000  current step :  4136
total : 5000  current step :  4137
total : 5000  current step :  4138
total : 5000  current step :  4139
total : 5000  current step :  4140
total : 5000  current step :  4141
total : 5000  current step :  4142
total : 5000  current step :  4143
total : 5000  current step :  4144
total : 5000  current step :  4145
total : 5000  current step :  4146
total : 5000  current step :  4147
total : 5000  current step :  4148
total : 5000  current step :  4149
total : 5000  current step :  4150
total : 5000  current step :  4151
total : 5000  current step :  4152
total : 5000  current step :  4153
total : 5000  current step :  4154
total : 5000  current step :  4155
total : 5000  current step :  4156
total : 5000  current step :  4157
total : 5000  current step :  4158
total : 5000  current step :  4159
total : 5000  current step :  4160
total : 5000  current step :  4161
total : 5000  current step :  4162
total : 5000  current step :  4163
total : 5000  current step :  4164
total : 5000  current step :  4165
total : 5000  current step :  4166
total : 5000  current step :  4167
total : 5000  current step :  4168
total : 5000  current step :  4169
total : 5000  current step :  4170
total : 5000  current step :  4171
total : 5000  current step :  4172
total : 5000  current step :  4173
total : 5000  current step :  4174
total : 5000  current step :  4175
total : 5000  current step :  4176
total : 5000  current step :  4177
total : 5000  current step :  4178
total : 5000  current step :  4179
total : 5000  current step :  4180
total : 5000  current step :  4181
total : 5000  current step :  4182
total : 5000  current step :  4183
total : 5000  current step :  4184
total : 5000  current step :  4185
total : 5000  current step :  4186
total : 5000  current step :  4187
total : 5000  current step :  4188
total : 5000  current step :  4189
total : 5000  current step :  4190
total : 5000  current step :  4191
total : 5000  current step :  4192
total : 5000  current step :  4193
total : 5000  current step :  4194
total : 5000  current step :  4195
total : 5000  current step :  4196
total : 5000  current step :  4197
total : 5000  current step :  4198
total : 5000  current step :  4199
total : 5000  current step :  4200
total : 5000  current step :  4201
total : 5000  current step :  4202
total : 5000  current step :  4203
total : 5000  current step :  4204
total : 5000  current step :  4205
total : 5000  current step :  4206
total : 5000  current step :  4207
total : 5000  current step :  4208
total : 5000  current step :  4209
total : 5000  current step :  4210
total : 5000  current step :  4211
total : 5000  current step :  4212
total : 5000  current step :  4213
total : 5000  current step :  4214
total : 5000  current step :  4215
total : 5000  current step :  4216
total : 5000  current step :  4217
total : 5000  current step :  4218
total : 5000  current step :  4219
total : 5000  current step :  4220
total : 5000  current step :  4221
total : 5000  current step :  4222
total : 5000  current step :  4223
total : 5000  current step :  4224
total : 5000  current step :  4225
total : 5000  current step :  4226
total : 5000  current step :  4227
total : 5000  current step :  4228
total : 5000  current step :  4229
total : 5000  current step :  4230
total : 5000  current step :  4231
total : 5000  current step :  4232
total : 5000  current step :  4233
total : 5000  current step :  4234
total : 5000  current step :  4235
total : 5000  current step :  4236
total : 5000  current step :  4237
total : 5000  current step :  4238
total : 5000  current step :  4239
total : 5000  current step :  4240
total : 5000  current step :  4241
total : 5000  current step :  4242
total : 5000  current step :  4243
total : 5000  current step :  4244
total : 5000  current step :  4245
total : 5000  current step :  4246
total : 5000  current step :  4247
total : 5000  current step :  4248
total : 5000  current step :  4249
total : 5000  current step :  4250
total : 5000  current step :  4251
total : 5000  current step :  4252
total : 5000  current step :  4253
total : 5000  current step :  4254
total : 5000  current step :  4255
total : 5000  current step :  4256
total : 5000  current step :  4257
total : 5000  current step :  4258
total : 5000  current step :  4259
total : 5000  current step :  4260
total : 5000  current step :  4261
total : 5000  current step :  4262
total : 5000  current step :  4263
total : 5000  current step :  4264
total : 5000  current step :  4265
total : 5000  current step :  4266
total : 5000  current step :  4267
total : 5000  current step :  4268
total : 5000  current step :  4269
total : 5000  current step :  4270
total : 5000  current step :  4271
total : 5000  current step :  4272
total : 5000  current step :  4273
total : 5000  current step :  4274
total : 5000  current step :  4275
total : 5000  current step :  4276
total : 5000  current step :  4277
total : 5000  current step :  4278
total : 5000  current step :  4279
total : 5000  current step :  4280
total : 5000  current step :  4281
total : 5000  current step :  4282
total : 5000  current step :  4283
total : 5000  current step :  4284
total : 5000  current step :  4285
total : 5000  current step :  4286
total : 5000  current step :  4287
total : 5000  current step :  4288
total : 5000  current step :  4289
total : 5000  current step :  4290
total : 5000  current step :  4291
total : 5000  current step :  4292
total : 5000  current step :  4293
total : 5000  current step :  4294
total : 5000  current step :  4295
total : 5000  current step :  4296
total : 5000  current step :  4297
total : 5000  current step :  4298
total : 5000  current step :  4299
total : 5000  current step :  4300
total : 5000  current step :  4301
total : 5000  current step :  4302
total : 5000  current step :  4303
total : 5000  current step :  4304
total : 5000  current step :  4305
total : 5000  current step :  4306
total : 5000  current step :  4307
total : 5000  current step :  4308
total : 5000  current step :  4309
total : 5000  current step :  4310
total : 5000  current step :  4311
total : 5000  current step :  4312
total : 5000  current step :  4313
total : 5000  current step :  4314
total : 5000  current step :  4315
total : 5000  current step :  4316
total : 5000  current step :  4317
total : 5000  current step :  4318
total : 5000  current step :  4319
total : 5000  current step :  4320
total : 5000  current step :  4321
total : 5000  current step :  4322
total : 5000  current step :  4323
total : 5000  current step :  4324
total : 5000  current step :  4325
total : 5000  current step :  4326
total : 5000  current step :  4327
total : 5000  current step :  4328
total : 5000  current step :  4329
total : 5000  current step :  4330
total : 5000  current step :  4331
total : 5000  current step :  4332
total : 5000  current step :  4333
total : 5000  current step :  4334
total : 5000  current step :  4335
total : 5000  current step :  4336
total : 5000  current step :  4337
total : 5000  current step :  4338
total : 5000  current step :  4339
total : 5000  current step :  4340
total : 5000  current step :  4341
total : 5000  current step :  4342
total : 5000  current step :  4343
total : 5000  current step :  4344
total : 5000  current step :  4345
total : 5000  current step :  4346
total : 5000  current step :  4347
total : 5000  current step :  4348
total : 5000  current step :  4349
total : 5000  current step :  4350
total : 5000  current step :  4351
total : 5000  current step :  4352
total : 5000  current step :  4353
total : 5000  current step :  4354
total : 5000  current step :  4355
total : 5000  current step :  4356
total : 5000  current step :  4357
total : 5000  current step :  4358
total : 5000  current step :  4359
total : 5000  current step :  4360
total : 5000  current step :  4361
total : 5000  current step :  4362
total : 5000  current step :  4363
total : 5000  current step :  4364
total : 5000  current step :  4365
total : 5000  current step :  4366
total : 5000  current step :  4367
total : 5000  current step :  4368
total : 5000  current step :  4369
total : 5000  current step :  4370
total : 5000  current step :  4371
total : 5000  current step :  4372
total : 5000  current step :  4373
total : 5000  current step :  4374
total : 5000  current step :  4375
total : 5000  current step :  4376
total : 5000  current step :  4377
total : 5000  current step :  4378
total : 5000  current step :  4379
total : 5000  current step :  4380
total : 5000  current step :  4381
total : 5000  current step :  4382
total : 5000  current step :  4383
total : 5000  current step :  4384
total : 5000  current step :  4385
total : 5000  current step :  4386
total : 5000  current step :  4387
total : 5000  current step :  4388
total : 5000  current step :  4389
total : 5000  current step :  4390
total : 5000  current step :  4391
total : 5000  current step :  4392
total : 5000  current step :  4393
total : 5000  current step :  4394
total : 5000  current step :  4395
total : 5000  current step :  4396
total : 5000  current step :  4397
total : 5000  current step :  4398
total : 5000  current step :  4399
total : 5000  current step :  4400
total : 5000  current step :  4401
total : 5000  current step :  4402
total : 5000  current step :  4403
total : 5000  current step :  4404
total : 5000  current step :  4405
total : 5000  current step :  4406
total : 5000  current step :  4407
total : 5000  current step :  4408
total : 5000  current step :  4409
total : 5000  current step :  4410
total : 5000  current step :  4411
total : 5000  current step :  4412
total : 5000  current step :  4413
total : 5000  current step :  4414
total : 5000  current step :  4415
total : 5000  current step :  4416
total : 5000  current step :  4417
total : 5000  current step :  4418
total : 5000  current step :  4419
total : 5000  current step :  4420
total : 5000  current step :  4421
total : 5000  current step :  4422
total : 5000  current step :  4423
total : 5000  current step :  4424
total : 5000  current step :  4425
total : 5000  current step :  4426
total : 5000  current step :  4427
total : 5000  current step :  4428
total : 5000  current step :  4429
total : 5000  current step :  4430
total : 5000  current step :  4431
total : 5000  current step :  4432
total : 5000  current step :  4433
total : 5000  current step :  4434
total : 5000  current step :  4435
total : 5000  current step :  4436
total : 5000  current step :  4437
total : 5000  current step :  4438
total : 5000  current step :  4439
total : 5000  current step :  4440
total : 5000  current step :  4441
total : 5000  current step :  4442
total : 5000  current step :  4443
total : 5000  current step :  4444
total : 5000  current step :  4445
total : 5000  current step :  4446
total : 5000  current step :  4447
total : 5000  current step :  4448
total : 5000  current step :  4449
total : 5000  current step :  4450
total : 5000  current step :  4451
total : 5000  current step :  4452
total : 5000  current step :  4453
total : 5000  current step :  4454
total : 5000  current step :  4455
total : 5000  current step :  4456
total : 5000  current step :  4457
total : 5000  current step :  4458
total : 5000  current step :  4459
total : 5000  current step :  4460
total : 5000  current step :  4461
total : 5000  current step :  4462
total : 5000  current step :  4463
total : 5000  current step :  4464
total : 5000  current step :  4465
total : 5000  current step :  4466
total : 5000  current step :  4467
total : 5000  current step :  4468
total : 5000  current step :  4469
total : 5000  current step :  4470
total : 5000  current step :  4471
total : 5000  current step :  4472
total : 5000  current step :  4473
total : 5000  current step :  4474
total : 5000  current step :  4475
total : 5000  current step :  4476
total : 5000  current step :  4477
total : 5000  current step :  4478
total : 5000  current step :  4479
total : 5000  current step :  4480
total : 5000  current step :  4481
total : 5000  current step :  4482
total : 5000  current step :  4483
total : 5000  current step :  4484
total : 5000  current step :  4485
total : 5000  current step :  4486
total : 5000  current step :  4487
total : 5000  current step :  4488
total : 5000  current step :  4489
total : 5000  current step :  4490
total : 5000  current step :  4491
total : 5000  current step :  4492
total : 5000  current step :  4493
total : 5000  current step :  4494
total : 5000  current step :  4495
total : 5000  current step :  4496
total : 5000  current step :  4497
total : 5000  current step :  4498
total : 5000  current step :  4499
total : 5000  current step :  4500
total : 5000  current step :  4501
total : 5000  current step :  4502
total : 5000  current step :  4503
total : 5000  current step :  4504
total : 5000  current step :  4505
total : 5000  current step :  4506
total : 5000  current step :  4507
total : 5000  current step :  4508
total : 5000  current step :  4509
total : 5000  current step :  4510
total : 5000  current step :  4511
total : 5000  current step :  4512
total : 5000  current step :  4513
total : 5000  current step :  4514
total : 5000  current step :  4515
total : 5000  current step :  4516
total : 5000  current step :  4517
total : 5000  current step :  4518
total : 5000  current step :  4519
total : 5000  current step :  4520
total : 5000  current step :  4521
total : 5000  current step :  4522
total : 5000  current step :  4523
total : 5000  current step :  4524
total : 5000  current step :  4525
total : 5000  current step :  4526
total : 5000  current step :  4527
total : 5000  current step :  4528
total : 5000  current step :  4529
total : 5000  current step :  4530
total : 5000  current step :  4531
total : 5000  current step :  4532
total : 5000  current step :  4533
total : 5000  current step :  4534
total : 5000  current step :  4535
total : 5000  current step :  4536
total : 5000  current step :  4537
total : 5000  current step :  4538
total : 5000  current step :  4539
total : 5000  current step :  4540
total : 5000  current step :  4541
total : 5000  current step :  4542
total : 5000  current step :  4543
total : 5000  current step :  4544
total : 5000  current step :  4545
total : 5000  current step :  4546
total : 5000  current step :  4547
total : 5000  current step :  4548
total : 5000  current step :  4549
total : 5000  current step :  4550
total : 5000  current step :  4551
total : 5000  current step :  4552
total : 5000  current step :  4553
total : 5000  current step :  4554
total : 5000  current step :  4555
total : 5000  current step :  4556
total : 5000  current step :  4557
total : 5000  current step :  4558
total : 5000  current step :  4559
total : 5000  current step :  4560
total : 5000  current step :  4561
total : 5000  current step :  4562
total : 5000  current step :  4563
total : 5000  current step :  4564
total : 5000  current step :  4565
total : 5000  current step :  4566
total : 5000  current step :  4567
total : 5000  current step :  4568
total : 5000  current step :  4569
total : 5000  current step :  4570
total : 5000  current step :  4571
total : 5000  current step :  4572
total : 5000  current step :  4573
total : 5000  current step :  4574
total : 5000  current step :  4575
total : 5000  current step :  4576
total : 5000  current step :  4577
total : 5000  current step :  4578
total : 5000  current step :  4579
total : 5000  current step :  4580
total : 5000  current step :  4581
total : 5000  current step :  4582
total : 5000  current step :  4583
total : 5000  current step :  4584
total : 5000  current step :  4585
total : 5000  current step :  4586
total : 5000  current step :  4587
total : 5000  current step :  4588
total : 5000  current step :  4589
total : 5000  current step :  4590
total : 5000  current step :  4591
total : 5000  current step :  4592
total : 5000  current step :  4593
total : 5000  current step :  4594
total : 5000  current step :  4595
total : 5000  current step :  4596
total : 5000  current step :  4597
total : 5000  current step :  4598
total : 5000  current step :  4599
total : 5000  current step :  4600
total : 5000  current step :  4601
total : 5000  current step :  4602
total : 5000  current step :  4603
total : 5000  current step :  4604
total : 5000  current step :  4605
total : 5000  current step :  4606
total : 5000  current step :  4607
total : 5000  current step :  4608
total : 5000  current step :  4609
total : 5000  current step :  4610
total : 5000  current step :  4611
total : 5000  current step :  4612
total : 5000  current step :  4613
total : 5000  current step :  4614
total : 5000  current step :  4615
total : 5000  current step :  4616
total : 5000  current step :  4617
total : 5000  current step :  4618
total : 5000  current step :  4619
total : 5000  current step :  4620
total : 5000  current step :  4621
total : 5000  current step :  4622
total : 5000  current step :  4623
total : 5000  current step :  4624
total : 5000  current step :  4625
total : 5000  current step :  4626
total : 5000  current step :  4627
total : 5000  current step :  4628
total : 5000  current step :  4629
total : 5000  current step :  4630
total : 5000  current step :  4631
total : 5000  current step :  4632
total : 5000  current step :  4633
total : 5000  current step :  4634
total : 5000  current step :  4635
total : 5000  current step :  4636
total : 5000  current step :  4637
total : 5000  current step :  4638
total : 5000  current step :  4639
total : 5000  current step :  4640
total : 5000  current step :  4641
total : 5000  current step :  4642
total : 5000  current step :  4643
total : 5000  current step :  4644
total : 5000  current step :  4645
total : 5000  current step :  4646
total : 5000  current step :  4647
total : 5000  current step :  4648
total : 5000  current step :  4649
total : 5000  current step :  4650
total : 5000  current step :  4651
total : 5000  current step :  4652
total : 5000  current step :  4653
total : 5000  current step :  4654
total : 5000  current step :  4655
total : 5000  current step :  4656
total : 5000  current step :  4657
total : 5000  current step :  4658
total : 5000  current step :  4659
total : 5000  current step :  4660
total : 5000  current step :  4661
total : 5000  current step :  4662
total : 5000  current step :  4663
total : 5000  current step :  4664
total : 5000  current step :  4665
total : 5000  current step :  4666
total : 5000  current step :  4667
total : 5000  current step :  4668
total : 5000  current step :  4669
total : 5000  current step :  4670
total : 5000  current step :  4671
total : 5000  current step :  4672
total : 5000  current step :  4673
total : 5000  current step :  4674
total : 5000  current step :  4675
total : 5000  current step :  4676
total : 5000  current step :  4677
total : 5000  current step :  4678
total : 5000  current step :  4679
total : 5000  current step :  4680
total : 5000  current step :  4681
total : 5000  current step :  4682
total : 5000  current step :  4683
total : 5000  current step :  4684
total : 5000  current step :  4685
total : 5000  current step :  4686
total : 5000  current step :  4687
total : 5000  current step :  4688
total : 5000  current step :  4689
total : 5000  current step :  4690
total : 5000  current step :  4691
total : 5000  current step :  4692
total : 5000  current step :  4693
total : 5000  current step :  4694
total : 5000  current step :  4695
total : 5000  current step :  4696
total : 5000  current step :  4697
total : 5000  current step :  4698
total : 5000  current step :  4699
total : 5000  current step :  4700
total : 5000  current step :  4701
total : 5000  current step :  4702
total : 5000  current step :  4703
total : 5000  current step :  4704
total : 5000  current step :  4705
total : 5000  current step :  4706
total : 5000  current step :  4707
total : 5000  current step :  4708
total : 5000  current step :  4709
total : 5000  current step :  4710
total : 5000  current step :  4711
total : 5000  current step :  4712
total : 5000  current step :  4713
total : 5000  current step :  4714
total : 5000  current step :  4715
total : 5000  current step :  4716
total : 5000  current step :  4717
total : 5000  current step :  4718
total : 5000  current step :  4719
total : 5000  current step :  4720
total : 5000  current step :  4721
total : 5000  current step :  4722
total : 5000  current step :  4723
total : 5000  current step :  4724
total : 5000  current step :  4725
total : 5000  current step :  4726
total : 5000  current step :  4727
total : 5000  current step :  4728
total : 5000  current step :  4729
total : 5000  current step :  4730
total : 5000  current step :  4731
total : 5000  current step :  4732
total : 5000  current step :  4733
total : 5000  current step :  4734
total : 5000  current step :  4735
total : 5000  current step :  4736
total : 5000  current step :  4737
total : 5000  current step :  4738
total : 5000  current step :  4739
total : 5000  current step :  4740
total : 5000  current step :  4741
total : 5000  current step :  4742
total : 5000  current step :  4743
total : 5000  current step :  4744
total : 5000  current step :  4745
total : 5000  current step :  4746
total : 5000  current step :  4747
total : 5000  current step :  4748
total : 5000  current step :  4749
total : 5000  current step :  4750
total : 5000  current step :  4751
total : 5000  current step :  4752
total : 5000  current step :  4753
total : 5000  current step :  4754
total : 5000  current step :  4755
total : 5000  current step :  4756
total : 5000  current step :  4757
total : 5000  current step :  4758
total : 5000  current step :  4759
total : 5000  current step :  4760
total : 5000  current step :  4761
total : 5000  current step :  4762
total : 5000  current step :  4763
total : 5000  current step :  4764
total : 5000  current step :  4765
total : 5000  current step :  4766
total : 5000  current step :  4767
total : 5000  current step :  4768
total : 5000  current step :  4769
total : 5000  current step :  4770
total : 5000  current step :  4771
total : 5000  current step :  4772
total : 5000  current step :  4773
total : 5000  current step :  4774
total : 5000  current step :  4775
total : 5000  current step :  4776
total : 5000  current step :  4777
total : 5000  current step :  4778
total : 5000  current step :  4779
total : 5000  current step :  4780
total : 5000  current step :  4781
total : 5000  current step :  4782
total : 5000  current step :  4783
total : 5000  current step :  4784
total : 5000  current step :  4785
total : 5000  current step :  4786
total : 5000  current step :  4787
total : 5000  current step :  4788
total : 5000  current step :  4789
total : 5000  current step :  4790
total : 5000  current step :  4791
total : 5000  current step :  4792
total : 5000  current step :  4793
total : 5000  current step :  4794
total : 5000  current step :  4795
total : 5000  current step :  4796
total : 5000  current step :  4797
total : 5000  current step :  4798
total : 5000  current step :  4799
total : 5000  current step :  4800
total : 5000  current step :  4801
total : 5000  current step :  4802
total : 5000  current step :  4803
total : 5000  current step :  4804
total : 5000  current step :  4805
total : 5000  current step :  4806
total : 5000  current step :  4807
total : 5000  current step :  4808
total : 5000  current step :  4809
total : 5000  current step :  4810
total : 5000  current step :  4811
total : 5000  current step :  4812
total : 5000  current step :  4813
total : 5000  current step :  4814
total : 5000  current step :  4815
total : 5000  current step :  4816
total : 5000  current step :  4817
total : 5000  current step :  4818
total : 5000  current step :  4819
total : 5000  current step :  4820
total : 5000  current step :  4821
total : 5000  current step :  4822
total : 5000  current step :  4823
total : 5000  current step :  4824
total : 5000  current step :  4825
total : 5000  current step :  4826
total : 5000  current step :  4827
total : 5000  current step :  4828
total : 5000  current step :  4829
total : 5000  current step :  4830
total : 5000  current step :  4831
total : 5000  current step :  4832
total : 5000  current step :  4833
total : 5000  current step :  4834
total : 5000  current step :  4835
total : 5000  current step :  4836
total : 5000  current step :  4837
total : 5000  current step :  4838
total : 5000  current step :  4839
total : 5000  current step :  4840
total : 5000  current step :  4841
total : 5000  current step :  4842
total : 5000  current step :  4843
total : 5000  current step :  4844
total : 5000  current step :  4845
total : 5000  current step :  4846
total : 5000  current step :  4847
total : 5000  current step :  4848
total : 5000  current step :  4849
total : 5000  current step :  4850
total : 5000  current step :  4851
total : 5000  current step :  4852
total : 5000  current step :  4853
total : 5000  current step :  4854
total : 5000  current step :  4855
total : 5000  current step :  4856
total : 5000  current step :  4857
total : 5000  current step :  4858
total : 5000  current step :  4859
total : 5000  current step :  4860
total : 5000  current step :  4861
total : 5000  current step :  4862
total : 5000  current step :  4863
total : 5000  current step :  4864
total : 5000  current step :  4865
total : 5000  current step :  4866
total : 5000  current step :  4867
total : 5000  current step :  4868
total : 5000  current step :  4869
total : 5000  current step :  4870
total : 5000  current step :  4871
total : 5000  current step :  4872
total : 5000  current step :  4873
total : 5000  current step :  4874
total : 5000  current step :  4875
total : 5000  current step :  4876
total : 5000  current step :  4877
total : 5000  current step :  4878
total : 5000  current step :  4879
total : 5000  current step :  4880
total : 5000  current step :  4881
total : 5000  current step :  4882
total : 5000  current step :  4883
total : 5000  current step :  4884
total : 5000  current step :  4885
total : 5000  current step :  4886
total : 5000  current step :  4887
total : 5000  current step :  4888
total : 5000  current step :  4889
total : 5000  current step :  4890
total : 5000  current step :  4891
total : 5000  current step :  4892
total : 5000  current step :  4893
total : 5000  current step :  4894
total : 5000  current step :  4895
total : 5000  current step :  4896
total : 5000  current step :  4897
total : 5000  current step :  4898
total : 5000  current step :  4899
total : 5000  current step :  4900
total : 5000  current step :  4901
total : 5000  current step :  4902
total : 5000  current step :  4903
total : 5000  current step :  4904
total : 5000  current step :  4905
total : 5000  current step :  4906
total : 5000  current step :  4907
total : 5000  current step :  4908
total : 5000  current step :  4909
total : 5000  current step :  4910
total : 5000  current step :  4911
total : 5000  current step :  4912
total : 5000  current step :  4913
total : 5000  current step :  4914
total : 5000  current step :  4915
total : 5000  current step :  4916
total : 5000  current step :  4917
total : 5000  current step :  4918
total : 5000  current step :  4919
total : 5000  current step :  4920
total : 5000  current step :  4921
total : 5000  current step :  4922
total : 5000  current step :  4923
total : 5000  current step :  4924
total : 5000  current step :  4925
total : 5000  current step :  4926
total : 5000  current step :  4927
total : 5000  current step :  4928
total : 5000  current step :  4929
total : 5000  current step :  4930
total : 5000  current step :  4931
total : 5000  current step :  4932
total : 5000  current step :  4933
total : 5000  current step :  4934
total : 5000  current step :  4935
total : 5000  current step :  4936
total : 5000  current step :  4937
total : 5000  current step :  4938
total : 5000  current step :  4939
total : 5000  current step :  4940
total : 5000  current step :  4941
total : 5000  current step :  4942
total : 5000  current step :  4943
total : 5000  current step :  4944
total : 5000  current step :  4945
total : 5000  current step :  4946
total : 5000  current step :  4947
total : 5000  current step :  4948
total : 5000  current step :  4949
total : 5000  current step :  4950
total : 5000  current step :  4951
total : 5000  current step :  4952
total : 5000  current step :  4953
total : 5000  current step :  4954
total : 5000  current step :  4955
total : 5000  current step :  4956
total : 5000  current step :  4957
total : 5000  current step :  4958
total : 5000  current step :  4959
total : 5000  current step :  4960
total : 5000  current step :  4961
total : 5000  current step :  4962
total : 5000  current step :  4963
total : 5000  current step :  4964
total : 5000  current step :  4965
total : 5000  current step :  4966
total : 5000  current step :  4967
total : 5000  current step :  4968
total : 5000  current step :  4969
total : 5000  current step :  4970
total : 5000  current step :  4971
total : 5000  current step :  4972
total : 5000  current step :  4973
total : 5000  current step :  4974
total : 5000  current step :  4975
total : 5000  current step :  4976
total : 5000  current step :  4977
total : 5000  current step :  4978
total : 5000  current step :  4979
total : 5000  current step :  4980
total : 5000  current step :  4981
total : 5000  current step :  4982
total : 5000  current step :  4983
total : 5000  current step :  4984
total : 5000  current step :  4985
total : 5000  current step :  4986
total : 5000  current step :  4987
total : 5000  current step :  4988
total : 5000  current step :  4989
total : 5000  current step :  4990
total : 5000  current step :  4991
total : 5000  current step :  4992
total : 5000  current step :  4993
total : 5000  current step :  4994
total : 5000  current step :  4995
total : 5000  current step :  4996
total : 5000  current step :  4997
total : 5000  current step :  4998
total : 5000  current step :  4999
soft_pseudo_label
tensor([[1.9635e-01, 5.1280e-01, 7.1452e-03,  ..., 3.8641e-02, 2.0545e-02,
         2.7849e-02],
        [1.0790e-03, 9.5820e-01, 4.7879e-04,  ..., 8.6697e-04, 1.1712e-03,
         1.0067e-03],
        [2.5628e-01, 6.7652e-01, 2.2720e-03,  ..., 2.9202e-03, 9.4310e-03,
         3.0964e-03],
        ...,
        [8.4015e-01, 1.5360e-01, 3.3339e-04,  ..., 1.1163e-03, 1.5735e-03,
         5.6214e-04],
        [5.7623e-02, 9.0845e-01, 9.8664e-04,  ..., 3.8902e-04, 4.4892e-03,
         1.0595e-03],
        [1.0521e-01, 7.3105e-01, 2.4624e-03,  ..., 1.3237e-02, 9.8777e-03,
         6.0588e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 5, 0, 1, 1, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 5, 4, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 5, 0, 1, 1, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 5, 4, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 7, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        6, 1, 0, 1, 1, 0, 1, 0, 8, 0, 0, 1, 1, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 1,
        1, 8, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 5, 1, 1, 1,
        1, 1, 0, 1, 4, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        8, 1, 4, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[4.5035e-04, 9.8650e-01, 2.5320e-05,  ..., 1.9414e-05, 3.7949e-04,
         1.8001e-04],
        [9.2359e-01, 2.4830e-02, 1.3886e-03,  ..., 2.6273e-03, 2.1647e-02,
         2.2298e-03],
        [1.7834e-04, 9.7019e-01, 1.5374e-04,  ..., 2.2427e-03, 5.8020e-04,
         2.0133e-03],
        ...,
        [8.4026e-01, 8.0637e-02, 1.7798e-03,  ..., 1.7857e-02, 1.2732e-02,
         7.9978e-03],
        [9.7665e-01, 7.5300e-03, 4.8044e-04,  ..., 4.3702e-04, 4.6641e-03,
         5.3649e-04],
        [7.9403e-01, 1.4919e-01, 1.0211e-03,  ..., 1.7626e-03, 1.5455e-02,
         1.8256e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 2, 7, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 2, 7, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
original label
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 6, 1,
        7, 0, 1, 1, 0, 1, 0, 1, 3, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 6, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 5,
        6, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 5, 1, 0, 0, 5, 0, 0,
        4, 0, 1, 1, 1, 1, 4, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[9.6089e-01, 1.5952e-06, 9.2924e-03,  ..., 1.6152e-03, 1.8123e-02,
         3.5442e-03],
        [7.9189e-01, 6.3187e-02, 3.5198e-03,  ..., 2.1474e-03, 4.7118e-02,
         3.4720e-03],
        [2.4094e-05, 9.6853e-01, 1.4754e-03,  ..., 2.3953e-05, 6.8504e-05,
         9.4000e-05],
        ...,
        [1.4257e-02, 8.9420e-01, 2.2248e-04,  ..., 2.7473e-04, 1.7003e-03,
         3.7698e-04],
        [3.4460e-04, 8.6335e-01, 7.7886e-04,  ..., 8.1887e-02, 6.4886e-04,
         2.8355e-02],
        [8.5668e-01, 3.9248e-03, 5.9855e-02,  ..., 5.6800e-03, 1.7366e-02,
         1.0863e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 7,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 7, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]
original label
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 3, 9, 0, 1, 0, 1,
        3, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 3, 3, 1, 1,
        1, 0, 1, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 3, 2, 0, 0, 0, 0, 0, 8, 1, 0, 0, 1,
        1, 0, 0, 6, 0, 1, 1, 0])
soft_pseudo_label
tensor([[7.1975e-01, 1.5309e-01, 1.5236e-02,  ..., 2.4975e-02, 1.8085e-02,
         1.2634e-02],
        [1.2166e-01, 7.8793e-01, 4.2162e-03,  ..., 4.4794e-03, 1.3448e-02,
         6.2008e-03],
        [9.1419e-03, 6.8892e-01, 8.1481e-02,  ..., 6.1256e-03, 8.9651e-03,
         2.5771e-02],
        ...,
        [9.6832e-01, 1.8222e-04, 5.2571e-04,  ..., 6.5044e-04, 1.8245e-02,
         9.4454e-04],
        [1.5495e-01, 3.0786e-01, 1.2583e-02,  ..., 6.3769e-03, 1.1777e-01,
         6.1350e-02],
        [2.0619e-03, 9.5323e-01, 3.1420e-04,  ..., 1.1350e-02, 1.9512e-03,
         1.4347e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 9, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 5, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 9, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 5, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1]
original label
tensor([1, 6, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0,
        0, 0, 1, 1, 8, 1, 6, 9, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 7, 0, 1, 1, 1,
        0, 1, 1, 9, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 8, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 6, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 3, 1, 1, 2, 1, 8, 0, 0, 1, 9, 0,
        0, 0, 0, 0, 1, 0, 0, 1])
soft_pseudo_label
tensor([[9.5673e-01, 1.6688e-02, 1.0943e-03,  ..., 8.2923e-04, 1.1724e-02,
         9.7707e-04],
        [2.2954e-02, 6.7921e-01, 1.0952e-02,  ..., 9.2280e-02, 1.3550e-02,
         1.1164e-01],
        [6.4686e-02, 9.1414e-01, 3.7426e-04,  ..., 4.0945e-04, 9.4458e-04,
         3.2580e-04],
        ...,
        [1.8612e-01, 5.9845e-01, 1.6260e-03,  ..., 9.7665e-04, 4.7566e-02,
         4.0205e-03],
        [5.1621e-03, 1.4650e-01, 9.2261e-03,  ..., 1.0086e-03, 3.1560e-02,
         4.8076e-02],
        [5.7163e-02, 8.4082e-01, 1.4578e-03,  ..., 3.6365e-03, 9.1914e-03,
         3.4128e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 7, 2, 7, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        2, 0, 1, 0, 0, 1, 5, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 7, 2, 7, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 5, 1]
original label
tensor([0, 1, 1, 1, 0, 3, 1, 0, 4, 1, 0, 0, 1, 1, 3, 1, 1, 1, 1, 4, 1, 9, 1, 7,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 3, 0, 1, 1,
        5, 1, 1, 1, 3, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 4, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 2, 0, 0, 7, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 5, 6, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 8, 1,
        7, 0, 1, 0, 0, 0, 1, 1])
soft_pseudo_label
tensor([[8.0678e-01, 1.9331e-02, 1.5363e-03,  ..., 1.0258e-02, 8.0903e-02,
         1.5817e-02],
        [9.7842e-01, 2.1792e-03, 2.0500e-04,  ..., 5.6271e-04, 8.8713e-03,
         6.4578e-04],
        [3.2609e-01, 6.2123e-01, 2.9537e-03,  ..., 2.2669e-03, 7.3697e-03,
         1.8885e-03],
        ...,
        [6.5289e-04, 9.3564e-01, 3.8796e-04,  ..., 1.8095e-04, 2.9514e-04,
         2.6174e-04],
        [9.9465e-01, 1.3774e-04, 1.5113e-04,  ..., 1.2801e-04, 1.9033e-03,
         1.2990e-04],
        [3.2295e-03, 8.2654e-01, 6.2363e-04,  ..., 8.7800e-02, 2.3535e-03,
         4.8916e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]
original label
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 5, 1, 1, 1, 8, 1, 1, 0, 1, 2, 1, 1, 3, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 8, 0, 8, 1, 0, 5, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 8,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 7, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[8.8048e-01, 6.6129e-02, 2.2496e-03,  ..., 7.7660e-03, 8.2045e-03,
         3.0509e-03],
        [9.8373e-01, 3.3319e-05, 9.7874e-04,  ..., 5.9961e-04, 9.7103e-03,
         7.5686e-04],
        [3.5276e-02, 8.7025e-01, 3.6160e-03,  ..., 1.1761e-02, 3.2193e-03,
         7.0042e-03],
        ...,
        [9.6365e-01, 2.8207e-03, 4.3927e-04,  ..., 2.9095e-03, 1.4058e-02,
         2.0531e-03],
        [1.1990e-03, 8.8624e-01, 6.2489e-03,  ..., 3.7186e-03, 6.0066e-03,
         2.4982e-02],
        [6.3679e-01, 1.9012e-02, 4.2573e-03,  ..., 1.9340e-03, 2.1123e-01,
         1.0366e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 5, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 5, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]
original label
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 8, 1, 0, 1, 1, 0, 7, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 3, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 8, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 6, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 6, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 6, 0])
soft_pseudo_label
tensor([[9.5828e-01, 6.2467e-03, 1.5336e-03,  ..., 5.1370e-04, 1.6075e-02,
         1.0137e-03],
        [9.6279e-04, 8.9170e-01, 9.3719e-03,  ..., 1.1300e-03, 7.0646e-04,
         2.2561e-03],
        [9.3820e-01, 3.5873e-04, 3.3394e-03,  ..., 1.9526e-03, 3.5709e-02,
         3.3053e-03],
        ...,
        [1.1738e-03, 9.8500e-01, 2.1922e-03,  ..., 5.5662e-04, 3.0620e-04,
         4.2476e-03],
        [9.7702e-01, 2.1925e-02, 1.1915e-05,  ..., 8.6634e-05, 1.7757e-04,
         2.5572e-05],
        [6.2291e-01, 1.9909e-01, 4.8735e-03,  ..., 2.1437e-03, 4.8814e-02,
         5.4901e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 4, 5,
        1, 0, 1, 5, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 9, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 6, 9, 0, 0, 1, 0, 0, 1, 5, 0, 0, 0, 1, 0, 0, 0, 7, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 6, 0,
        0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 4, 5, 1, 0, 1, 5, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 9, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 6, 9, 0, 0, 1, 0, 0, 1, 5, 0, 0, 0, 1, 0, 0, 0, 7, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 6, 0, 0, 0, 1, 1, 1, 1, 0, 0]
original label
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 4, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 0, 5, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 9, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 8, 0, 7, 0, 1, 0, 0,
        1, 1, 8, 0, 0, 1, 0, 1, 0, 1, 3, 0, 3, 6, 0, 1, 7, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[1.4350e-03, 9.8671e-01, 1.0354e-03,  ..., 9.0491e-05, 3.1065e-04,
         1.6939e-04],
        [2.8062e-02, 8.6196e-01, 5.8448e-03,  ..., 1.2883e-03, 1.5200e-02,
         7.6194e-03],
        [9.2136e-04, 7.4066e-01, 1.3299e-04,  ..., 3.3234e-05, 7.1696e-03,
         4.8505e-04],
        ...,
        [4.4781e-01, 4.8420e-01, 8.9541e-04,  ..., 3.1319e-02, 3.9373e-03,
         5.5039e-03],
        [2.7500e-01, 1.9788e-01, 1.2369e-02,  ..., 1.4548e-01, 5.5398e-02,
         1.3233e-01],
        [1.3083e-02, 9.8589e-01, 3.3068e-05,  ..., 7.6064e-05, 6.7653e-05,
         3.7913e-05]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 7, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        9, 9, 1, 1, 1, 1, 0, 7, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 7, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 9, 9, 1, 1, 1, 1, 0, 7, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1]
original label
tensor([1, 1, 1, 8, 0, 1, 0, 0, 0, 6, 7, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 5, 2, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 7, 4, 3, 0, 0, 0, 0, 5, 0, 1, 0, 4, 1,
        1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        6, 1, 1, 1, 0, 7, 8, 1, 0, 3, 5, 0, 0, 8, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0,
        9, 9, 1, 1, 1, 1, 0, 7, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1])
soft_pseudo_label
tensor([[1.0134e-01, 7.7032e-01, 2.3168e-03,  ..., 2.5296e-03, 8.5450e-03,
         2.1679e-03],
        [5.3826e-03, 5.1620e-02, 3.2483e-04,  ..., 7.3765e-05, 3.8436e-02,
         3.8590e-03],
        [9.9686e-01, 2.1135e-03, 1.8179e-05,  ..., 1.0940e-05, 6.7553e-04,
         1.3695e-05],
        ...,
        [3.5955e-01, 1.6961e-02, 3.5745e-01,  ..., 4.1284e-04, 5.2094e-03,
         1.0873e-02],
        [9.6813e-01, 4.6409e-03, 6.6509e-04,  ..., 1.6206e-03, 1.1674e-02,
         1.1525e-03],
        [9.5935e-01, 2.8454e-03, 3.8933e-04,  ..., 2.2379e-04, 4.7039e-03,
         3.4058e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 5, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 9, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 5, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 5, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 9, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 5, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 9, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 5, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 5, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 9, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]
original label
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 7, 1, 9, 0, 8, 0, 1, 8, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 4, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 5, 3, 1, 0, 1, 0, 1, 0,
        0, 1, 2, 0, 1, 0, 0, 1, 0, 4, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 6, 0, 6,
        1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 7, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 3, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0])
soft_pseudo_label
tensor([[1.9878e-04, 9.6181e-01, 5.3196e-04,  ..., 6.3730e-04, 9.7652e-04,
         9.9093e-04],
        [1.8011e-01, 6.4544e-01, 4.2447e-02,  ..., 5.4336e-03, 1.0122e-02,
         1.5608e-02],
        [9.1164e-01, 3.8109e-02, 1.6793e-03,  ..., 3.6093e-03, 1.1897e-02,
         3.0201e-03],
        ...,
        [9.9702e-01, 8.3961e-04, 1.5120e-04,  ..., 3.6191e-04, 5.7488e-04,
         1.3021e-04],
        [4.1571e-01, 5.1736e-01, 2.0533e-03,  ..., 1.3387e-03, 5.2024e-03,
         1.2213e-03],
        [3.0144e-01, 1.6751e-02, 3.7589e-01,  ..., 5.5104e-03, 8.0883e-03,
         4.2183e-02]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 7, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 9, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 7,
        1, 0, 0, 0, 9, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 2], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 7, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 9, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 7, 1, 0, 0, 0, 9, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2]
original label
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 7, 1, 1, 0, 0, 9, 0, 1, 0, 1, 0, 1,
        1, 0, 3, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 3, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 9, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        8, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 6, 1, 0, 7, 1, 1, 0, 0, 1, 5,
        0, 1, 1, 1, 1, 0, 1, 0])
soft_pseudo_label
tensor([[5.7662e-03, 1.2994e-02, 8.1221e-01,  ..., 3.2546e-04, 2.2133e-03,
         4.8492e-02],
        [9.1057e-01, 1.5746e-02, 4.6199e-03,  ..., 2.4168e-03, 2.7807e-02,
         4.2085e-03],
        [9.3194e-01, 7.8060e-04, 2.3270e-03,  ..., 3.8230e-04, 4.6763e-02,
         1.5892e-03],
        ...,
        [9.9311e-01, 4.5963e-07, 1.9380e-04,  ..., 1.1374e-04, 4.9533e-03,
         1.8561e-04],
        [4.1061e-07, 9.9751e-01, 2.0771e-06,  ..., 1.0547e-06, 1.9746e-05,
         1.2177e-05],
        [9.1588e-01, 2.9317e-04, 5.8620e-03,  ..., 2.1223e-03, 5.1168e-02,
         5.8313e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 0, 0, 1, 0, 0, 1, 0, 9, 1, 2, 0, 0, 1, 0, 0, 0, 7, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        5, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 5,
        0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[2, 0, 0, 1, 0, 0, 1, 0, 9, 1, 2, 0, 0, 1, 0, 0, 0, 7, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 5, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 5, 0, 1, 0, 1, 0, 0, 1, 0]
original label
tensor([2, 8, 0, 0, 0, 0, 1, 4, 1, 0, 2, 0, 0, 9, 0, 0, 1, 1, 3, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 5, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 9, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 8, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 4, 7, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0])
soft_pseudo_label
tensor([[9.8058e-01, 1.6736e-04, 8.5657e-04,  ..., 6.5675e-04, 1.0641e-02,
         7.4748e-04],
        [9.3961e-05, 9.8337e-01, 2.4467e-04,  ..., 1.3381e-04, 3.7527e-04,
         2.7242e-04],
        [2.9190e-01, 5.7487e-01, 1.7803e-03,  ..., 3.8533e-02, 1.3343e-02,
         8.1164e-03],
        ...,
        [8.5988e-01, 5.6329e-02, 3.6577e-03,  ..., 8.8691e-03, 2.2937e-02,
         6.7242e-03],
        [3.2783e-01, 1.0264e-02, 1.0776e-03,  ..., 2.4783e-04, 2.7285e-01,
         3.8036e-03],
        [8.3910e-01, 1.5461e-01, 1.0587e-03,  ..., 4.3216e-04, 1.5731e-03,
         4.7048e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 0, 1, 0, 9, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 7, 0,
        5, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 5, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 7, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 5, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 0, 1, 0, 9, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 7, 0, 5, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 5, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 7, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 5, 0]
original label
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 6, 1, 0, 1, 0, 1, 1, 0, 0, 1, 7, 0,
        5, 0, 0, 0, 1, 1, 0, 6, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 9, 1, 0,
        8, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 6, 1, 1, 1, 3, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 3, 1, 0, 1, 1, 1, 1, 0, 0, 5, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        6, 1, 1, 0, 1, 0, 1, 1, 8, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 8, 0, 4])
soft_pseudo_label
tensor([[6.4934e-01, 1.9727e-01, 1.0232e-02,  ..., 1.9257e-02, 2.2624e-02,
         2.3919e-02],
        [5.6148e-04, 9.9167e-01, 2.4746e-04,  ..., 9.7762e-05, 3.0677e-04,
         2.0296e-04],
        [2.6139e-04, 9.6217e-01, 3.7957e-04,  ..., 6.8464e-04, 3.1963e-04,
         5.4691e-04],
        ...,
        [5.9138e-01, 2.6489e-02, 1.0779e-02,  ..., 2.0098e-02, 1.5563e-01,
         4.0841e-02],
        [7.9523e-04, 5.2714e-01, 8.9266e-05,  ..., 8.0018e-05, 1.0781e-02,
         1.6475e-03],
        [9.4097e-05, 7.3041e-01, 1.0592e-03,  ..., 7.2511e-04, 2.8702e-04,
         9.2924e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 5, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 5, 0,
        0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 5, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 5, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 6, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 3, 8, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1])
soft_pseudo_label
tensor([[6.9735e-01, 2.9993e-01, 6.5855e-05,  ..., 1.8524e-04, 7.8581e-04,
         1.2769e-04],
        [9.7071e-01, 1.1377e-03, 1.0941e-03,  ..., 3.8206e-03, 7.7671e-03,
         1.8260e-03],
        [4.9917e-01, 3.2418e-01, 4.9466e-03,  ..., 6.8009e-03, 4.8033e-02,
         1.5022e-02],
        ...,
        [8.8421e-01, 6.5814e-03, 1.6528e-03,  ..., 5.1251e-04, 7.1316e-02,
         1.6674e-03],
        [2.0843e-04, 9.6987e-01, 6.6045e-04,  ..., 7.5573e-04, 1.2780e-03,
         1.2912e-03],
        [9.3928e-01, 3.8392e-02, 7.1774e-04,  ..., 1.1973e-03, 8.0828e-03,
         8.7853e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 2, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 5, 1, 1, 0, 1, 0, 9, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 5, 1, 1, 0, 1, 0, 9, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0]
original label
tensor([1, 0, 0, 0, 0, 0, 6, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 3, 9, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 8, 1, 1, 1, 1,
        6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 6, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 1, 0, 1, 0, 9, 1, 0, 1, 1, 4, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 7, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 6, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0])
soft_pseudo_label
tensor([[9.4114e-01, 4.6239e-03, 9.6680e-04,  ..., 3.2765e-04, 2.5130e-02,
         8.6917e-04],
        [9.8708e-01, 4.1365e-05, 2.9422e-04,  ..., 1.4186e-04, 9.1441e-03,
         2.5991e-04],
        [3.5361e-01, 5.7733e-01, 1.5748e-03,  ..., 3.5627e-03, 1.6078e-02,
         4.2557e-03],
        ...,
        [6.4126e-02, 8.5466e-01, 1.7597e-03,  ..., 2.9441e-03, 4.6317e-03,
         1.8750e-03],
        [8.8213e-01, 8.5821e-02, 2.3185e-03,  ..., 1.3327e-03, 8.8976e-03,
         1.7467e-03],
        [9.5781e-01, 6.0593e-03, 1.2820e-03,  ..., 1.3462e-03, 1.6114e-02,
         1.4218e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 9, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        4, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 9, 0, 1, 0, 0, 9, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 9, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 9, 0, 1, 0, 0, 9, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 1, 3, 1, 1, 1, 1, 0, 1, 0, 0, 7, 1, 0, 1, 0, 1, 9, 0,
        2, 0, 7, 1, 0, 0, 0, 0, 1, 1, 5, 0, 0, 0, 1, 6, 1, 1, 1, 0, 0, 1, 0, 1,
        3, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 3, 1,
        0, 5, 1, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 8, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 4, 0, 1, 1, 0, 0, 0, 1, 7, 6, 1, 0, 0, 9, 0, 1, 0, 0, 7, 0, 0,
        0, 0, 0, 2, 1, 1, 6, 0])
soft_pseudo_label
tensor([[8.4459e-01, 6.6282e-02, 3.9806e-03,  ..., 6.0351e-02, 1.6888e-03,
         8.8940e-03],
        [3.9936e-02, 8.7669e-01, 1.2024e-03,  ..., 6.6936e-03, 9.7988e-03,
         3.9196e-03],
        [1.3754e-03, 9.9063e-01, 1.7667e-04,  ..., 3.2017e-05, 5.5789e-04,
         1.2466e-04],
        ...,
        [1.6028e-04, 9.9821e-01, 3.0189e-05,  ..., 1.0914e-04, 5.8361e-05,
         5.2571e-05],
        [1.5669e-03, 9.6203e-01, 9.9504e-04,  ..., 3.8101e-04, 2.0911e-03,
         1.3016e-03],
        [8.6049e-01, 1.8835e-02, 5.6360e-03,  ..., 1.4774e-03, 6.3130e-02,
         6.9664e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 7, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 9, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 5, 1, 1, 1, 7,
        0, 0, 1, 9, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 7, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 9, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 5, 1, 1, 1, 7, 0, 0, 1, 9, 0, 1, 1, 0]
original label
tensor([0, 1, 1, 0, 1, 5, 0, 3, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        2, 0, 1, 0, 1, 1, 1, 7, 0, 1, 0, 1, 1, 0, 1, 0, 1, 5, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 5, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 5, 1, 1, 0, 1,
        0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 5, 0, 0, 1, 1, 1, 0, 5, 1, 1, 1, 6, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 8])
soft_pseudo_label
tensor([[9.4657e-01, 2.5295e-03, 2.0889e-03,  ..., 2.2128e-03, 2.2002e-02,
         2.4422e-03],
        [3.7237e-02, 8.7016e-01, 1.5131e-03,  ..., 1.7196e-03, 3.2221e-03,
         1.4242e-03],
        [3.2165e-02, 9.4737e-01, 1.3738e-03,  ..., 1.6555e-03, 1.6914e-03,
         1.8199e-03],
        ...,
        [4.1097e-01, 3.2958e-01, 3.7708e-02,  ..., 4.0211e-03, 2.5470e-02,
         1.1399e-02],
        [9.9059e-01, 4.8532e-05, 1.2988e-04,  ..., 9.7847e-05, 6.4063e-03,
         1.4447e-04],
        [9.5608e-01, 1.2440e-03, 9.7927e-04,  ..., 3.4088e-03, 2.0979e-02,
         2.7897e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0]
original label
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 0, 2, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3,
        0, 0, 0, 8, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 7, 8, 8, 4,
        1, 1, 0, 0, 0, 2, 0, 0])
soft_pseudo_label
tensor([[8.9746e-01, 6.8265e-02, 5.0419e-04,  ..., 7.1310e-04, 1.5247e-02,
         8.6017e-04],
        [6.9624e-02, 4.3811e-01, 9.7932e-03,  ..., 5.9341e-03, 9.3188e-02,
         4.6652e-02],
        [9.7108e-01, 5.1242e-05, 9.1273e-04,  ..., 1.0567e-03, 1.7890e-02,
         1.9572e-03],
        ...,
        [8.9275e-03, 9.3999e-01, 4.9835e-03,  ..., 2.6480e-03, 3.6406e-03,
         3.7397e-03],
        [8.5293e-03, 9.4416e-01, 5.3146e-04,  ..., 8.9876e-04, 4.6862e-03,
         2.2419e-03],
        [7.4936e-01, 2.0890e-01, 2.6143e-03,  ..., 3.0955e-03, 6.3827e-03,
         2.5240e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 0, 9, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 5, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 7, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 7,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 9, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 9, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 0, 9, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1, 0, 0, 1, 1, 1, 1, 5, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 7, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 7, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 9, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 9, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0]
original label
tensor([0, 0, 0, 0, 2, 1, 9, 1, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 5, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 7, 0, 0, 1, 1, 3,
        1, 1, 7, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 8, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 4, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 8, 1, 5, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0])
soft_pseudo_label
tensor([[7.0090e-01, 2.7394e-01, 3.7707e-03,  ..., 2.8939e-03, 2.1791e-03,
         2.3310e-03],
        [1.6897e-01, 7.4993e-01, 1.7497e-03,  ..., 1.0968e-02, 5.6591e-03,
         4.1728e-03],
        [9.8658e-01, 3.3716e-04, 7.6427e-04,  ..., 4.2207e-04, 6.4745e-03,
         5.7241e-04],
        ...,
        [9.8637e-01, 1.4736e-06, 8.7165e-05,  ..., 1.2732e-04, 1.1551e-02,
         1.9816e-04],
        [9.3071e-01, 7.8121e-03, 8.9168e-03,  ..., 1.9985e-03, 2.1771e-02,
         4.3758e-03],
        [5.4547e-03, 9.7459e-01, 2.3966e-04,  ..., 6.3266e-04, 1.0259e-03,
         6.7544e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 9, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 9, 1, 1, 2, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 9, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 9, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]
original label
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 8, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 9, 1, 1, 7, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 9, 0, 8, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 5, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 5, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 8, 9, 1, 4, 0, 6, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[2.7504e-03, 9.6072e-01, 9.8499e-04,  ..., 8.8986e-04, 1.3582e-03,
         1.4873e-03],
        [9.7663e-01, 3.0178e-05, 2.1761e-04,  ..., 2.9947e-04, 1.7993e-02,
         5.3569e-04],
        [9.8958e-01, 9.3833e-04, 8.5812e-04,  ..., 7.4482e-04, 4.9357e-03,
         6.7717e-04],
        ...,
        [6.8247e-02, 9.2389e-01, 1.1181e-04,  ..., 5.0561e-03, 1.7539e-04,
         5.6727e-04],
        [2.1358e-01, 5.2398e-01, 1.6373e-02,  ..., 2.8708e-02, 2.6968e-02,
         4.1303e-02],
        [1.0540e-02, 9.6046e-01, 3.2383e-03,  ..., 5.1993e-04, 2.5109e-03,
         2.1858e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 9, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 7, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 2, 0, 0, 1, 7, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        2, 9, 0, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 9, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 7, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 7, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 9, 0, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 7, 0, 0, 0, 1, 4, 4, 1, 1, 1, 0, 1, 1,
        1, 1, 4, 1, 8, 1, 0, 1, 0, 0, 0, 1, 1, 7, 1, 7, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 7, 1, 0, 1, 1, 5, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 2, 0, 3, 1, 1, 0, 1, 3, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        4, 7, 0, 1, 1, 8, 0, 1])
soft_pseudo_label
tensor([[9.5918e-01, 2.5913e-02, 9.7005e-04,  ..., 2.1110e-03, 3.1535e-03,
         1.0448e-03],
        [1.1930e-02, 7.4536e-01, 1.2294e-02,  ..., 4.1839e-03, 2.0712e-03,
         4.5283e-03],
        [9.7059e-01, 8.2752e-03, 9.3573e-04,  ..., 2.4916e-04, 1.2817e-02,
         5.8556e-04],
        ...,
        [4.4780e-02, 8.6582e-01, 3.2312e-03,  ..., 6.2925e-03, 3.8862e-03,
         5.1913e-03],
        [9.0232e-03, 9.7594e-01, 3.7756e-04,  ..., 1.7940e-04, 6.4855e-04,
         2.5004e-04],
        [3.7233e-04, 9.6617e-01, 3.3051e-04,  ..., 1.0170e-03, 8.6399e-04,
         8.5143e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 7, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 7, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]
original label
tensor([0, 1, 0, 6, 1, 0, 6, 0, 1, 0, 1, 1, 5, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 5,
        1, 1, 1, 0, 0, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 3, 0, 0, 0, 9, 0, 1, 3, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 5, 5, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 4, 1, 1, 1])
soft_pseudo_label
tensor([[9.6536e-01, 4.7119e-04, 2.2534e-03,  ..., 5.9506e-04, 2.1369e-02,
         1.3681e-03],
        [4.9703e-01, 3.4027e-01, 1.1960e-02,  ..., 3.1155e-03, 4.6096e-02,
         8.1206e-03],
        [9.4949e-01, 8.2559e-03, 1.4331e-03,  ..., 1.3782e-03, 1.7812e-02,
         1.7731e-03],
        ...,
        [6.9224e-01, 1.3212e-01, 3.3959e-03,  ..., 2.1147e-03, 6.4200e-02,
         5.8446e-03],
        [3.9366e-01, 3.9986e-01, 8.9557e-03,  ..., 5.4063e-02, 6.0420e-03,
         1.5414e-02],
        [9.7005e-01, 2.3800e-03, 7.4634e-04,  ..., 1.3436e-03, 1.3770e-02,
         1.3305e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 7, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 9, 0,
        9, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 7, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 7, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 9, 0, 9, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 7, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0]
original label
tensor([0, 1, 0, 5, 1, 1, 1, 1, 0, 1, 0, 9, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 3, 0, 8, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 5, 0, 1, 6, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 8, 0, 0, 1, 9, 0,
        4, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 9, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 5, 7, 7, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 8, 3, 0])
soft_pseudo_label
tensor([[5.4709e-02, 6.3536e-01, 1.9754e-03,  ..., 1.1613e-03, 4.5801e-02,
         7.1239e-03],
        [8.8469e-01, 1.4697e-02, 5.2120e-03,  ..., 2.2625e-03, 4.7625e-02,
         5.4461e-03],
        [3.5916e-04, 9.2025e-01, 6.8792e-04,  ..., 3.8264e-05, 1.0830e-03,
         1.3497e-03],
        ...,
        [9.4556e-01, 1.4551e-04, 8.2840e-04,  ..., 1.6015e-03, 3.7753e-02,
         1.8990e-03],
        [9.9709e-01, 5.5825e-04, 1.1938e-04,  ..., 6.8421e-05, 1.6416e-03,
         6.9363e-05],
        [4.8823e-01, 6.5368e-02, 2.9373e-03,  ..., 2.0148e-03, 1.6195e-01,
         1.0676e-02]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 9, 5, 0, 9, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 2, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 9, 5, 0, 9, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]
original label
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 8, 1, 4, 2, 0, 0, 1, 0, 0, 1, 5, 0, 9, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 4, 1, 0, 1, 0, 1, 1, 1, 0,
        9, 0, 0, 1, 1, 1, 4, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 8, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 5, 0, 5, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 8, 9, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.4215e-01, 4.9917e-04, 1.8601e-03,  ..., 4.5291e-03, 2.8702e-02,
         3.6401e-03],
        [2.0158e-03, 9.8266e-01, 3.7647e-04,  ..., 1.3688e-04, 7.3135e-04,
         4.0547e-04],
        [6.1890e-01, 3.3913e-01, 1.8907e-03,  ..., 2.5516e-03, 9.6181e-03,
         3.2036e-03],
        ...,
        [1.9050e-02, 9.7898e-01, 1.1087e-04,  ..., 2.2070e-04, 1.8422e-04,
         1.4502e-04],
        [8.3639e-01, 1.0510e-01, 1.2164e-03,  ..., 2.5701e-03, 2.2062e-02,
         2.3909e-03],
        [1.0997e-02, 7.4260e-01, 5.3237e-03,  ..., 3.0796e-03, 2.3612e-03,
         4.0719e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 7, 1, 0, 1, 9, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 5, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 7, 1, 0, 1, 9, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 5, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1]
original label
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 8, 0, 4, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 7, 1, 0, 1, 7, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 3, 1, 1, 0, 7, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 4, 8, 1,
        0, 0, 0, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.0457e-01, 3.4195e-02, 7.7349e-03,  ..., 1.0676e-02, 1.2535e-02,
         6.6581e-03],
        [9.9670e-01, 1.8537e-04, 4.1443e-05,  ..., 7.9805e-05, 2.2017e-03,
         6.5135e-05],
        [8.9666e-01, 2.0781e-02, 1.9367e-03,  ..., 2.6731e-03, 3.9861e-02,
         4.6075e-03],
        ...,
        [1.2048e-01, 8.1374e-01, 2.5950e-03,  ..., 2.7063e-03, 6.8436e-03,
         4.1834e-03],
        [3.2833e-05, 9.7126e-01, 1.6609e-04,  ..., 3.7796e-04, 5.3666e-04,
         4.7778e-04],
        [5.8581e-01, 2.9572e-01, 2.4485e-03,  ..., 1.1309e-03, 3.9497e-02,
         2.4725e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 9, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 7,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 9, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 7, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]
original label
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 8, 0, 0, 0, 7, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 9, 1, 0, 1, 1,
        1, 5, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 9, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 8, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 4,
        1, 5, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2,
        1, 0, 1, 1, 0, 1, 1, 1])
soft_pseudo_label
tensor([[9.6727e-01, 3.1384e-05, 4.1691e-03,  ..., 5.7004e-04, 1.8566e-02,
         1.4233e-03],
        [3.1113e-04, 9.9212e-01, 4.6388e-04,  ..., 1.1547e-04, 3.6840e-04,
         8.0524e-04],
        [9.9688e-01, 7.8967e-05, 2.5437e-05,  ..., 5.4326e-05, 2.4758e-03,
         5.4752e-05],
        ...,
        [8.1969e-02, 8.0317e-01, 2.3642e-03,  ..., 2.9137e-03, 2.0839e-02,
         4.9515e-03],
        [4.2064e-01, 5.6163e-01, 3.3314e-03,  ..., 4.6230e-03, 9.9492e-04,
         3.4498e-03],
        [4.3749e-02, 9.5384e-01, 5.5713e-05,  ..., 8.7619e-04, 1.6391e-04,
         2.2186e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 9, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 5, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 7, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        7, 1, 0, 0, 1, 5, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 1, 0, 1, 0, 0, 1, 9, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 5, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 7, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 7, 1, 0, 0, 1, 5, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
original label
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 9, 7, 0, 4, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 6,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 4, 1, 6, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 4, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 7, 0, 0, 0, 1, 6, 1, 1, 1, 0, 4, 7, 1, 1, 0, 0, 0, 1, 0, 0,
        9, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 5, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 2, 0, 1])
soft_pseudo_label
tensor([[1.2772e-02, 9.5130e-01, 6.3095e-04,  ..., 2.2622e-03, 3.8172e-03,
         1.6902e-03],
        [1.5685e-03, 9.3151e-01, 1.9105e-03,  ..., 2.0597e-03, 3.1010e-03,
         2.2821e-03],
        [9.5757e-01, 1.1342e-04, 1.5347e-03,  ..., 3.0236e-04, 3.0541e-02,
         1.2272e-03],
        ...,
        [5.7571e-01, 1.0510e-01, 3.6180e-02,  ..., 4.2667e-02, 4.8263e-02,
         8.5551e-02],
        [1.0684e-02, 9.5144e-01, 3.1607e-04,  ..., 5.7795e-04, 3.9567e-04,
         2.4092e-04],
        [9.2305e-01, 2.7335e-02, 8.3516e-04,  ..., 2.1939e-03, 1.0533e-02,
         1.3945e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 9, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 7,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 5, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 9, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 7, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 5, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0]
original label
tensor([1, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 6, 0, 1, 1, 1, 0,
        7, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 3, 0, 0, 1, 0, 2, 1, 1, 0, 0, 1, 0, 6, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 2, 0, 0, 0, 0, 1, 5, 1, 0, 1, 1, 0, 1, 7, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 6, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 9, 1, 1])
soft_pseudo_label
tensor([[3.4718e-01, 5.9520e-01, 3.1663e-03,  ..., 1.5103e-03, 1.2401e-02,
         2.3165e-03],
        [9.3006e-01, 7.6903e-03, 1.1558e-03,  ..., 6.9627e-04, 2.2678e-02,
         1.3726e-03],
        [6.5532e-04, 9.9464e-01, 3.3755e-05,  ..., 7.9252e-05, 2.3248e-04,
         1.5761e-04],
        ...,
        [3.7127e-03, 5.9578e-01, 5.4905e-02,  ..., 3.2830e-02, 5.9328e-03,
         1.4990e-01],
        [9.1923e-01, 3.2991e-03, 2.8635e-03,  ..., 9.5780e-03, 3.1578e-02,
         6.5595e-03],
        [9.7261e-01, 4.7096e-05, 4.2058e-04,  ..., 5.4801e-04, 1.9795e-02,
         6.6167e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 7, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 5, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 7, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 5, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0]
original label
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 7, 1, 6, 2, 1, 2, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 6,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 8, 1, 0, 7, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 6, 3, 1, 1, 1, 0, 1, 5, 0, 1, 0, 1, 0, 1, 9, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 8, 3, 0, 1, 1, 1, 6,
        0, 1, 0, 1, 1, 7, 0, 0])
soft_pseudo_label
tensor([[9.7453e-01, 2.6680e-03, 1.7154e-03,  ..., 9.3312e-04, 7.9631e-03,
         1.2804e-03],
        [9.1121e-01, 1.2786e-02, 2.4565e-03,  ..., 1.4128e-02, 1.9926e-02,
         5.8642e-03],
        [9.4752e-01, 2.7992e-05, 7.1211e-04,  ..., 2.9587e-03, 3.1671e-02,
         2.4138e-03],
        ...,
        [8.6264e-03, 9.4309e-01, 7.8609e-04,  ..., 1.3945e-03, 2.6348e-03,
         2.1662e-03],
        [9.6748e-01, 4.3573e-03, 1.9336e-03,  ..., 1.9813e-03, 9.1393e-03,
         1.7900e-03],
        [1.4697e-01, 7.9139e-01, 1.0812e-03,  ..., 1.5513e-02, 6.6557e-03,
         3.5625e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 7, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 7, 0, 1, 0, 2, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 9, 0, 1, 1, 1, 0, 1, 1, 1, 1, 9, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 7, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 9, 0, 1, 1, 1, 0, 1, 1, 1, 1, 9, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]
original label
tensor([0, 6, 3, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 4, 0, 1, 1, 3, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 3, 1, 0, 0,
        1, 0, 0, 1, 0, 6, 0, 1, 1, 0, 1, 8, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 6, 0, 2, 0, 1, 1, 3, 0, 1, 1, 1, 1, 7, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0])
soft_pseudo_label
tensor([[6.2556e-02, 8.4520e-01, 1.0368e-02,  ..., 6.5805e-03, 1.1849e-02,
         1.9304e-02],
        [6.7935e-01, 1.7618e-01, 1.6872e-03,  ..., 2.3655e-03, 6.3499e-02,
         4.9013e-03],
        [2.5639e-01, 4.2025e-01, 1.2192e-02,  ..., 1.0676e-02, 7.2530e-02,
         3.6797e-02],
        ...,
        [3.0898e-04, 9.5774e-01, 3.0740e-03,  ..., 1.0804e-04, 1.9710e-03,
         1.8711e-03],
        [1.1453e-01, 7.4247e-01, 1.0925e-03,  ..., 6.8235e-04, 9.0245e-03,
         1.3153e-03],
        [4.7487e-03, 8.7027e-01, 4.1662e-03,  ..., 7.8434e-04, 6.7000e-03,
         5.0525e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 1, 1, 1, 5, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        9, 0, 7, 4, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 9, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 1, 1, 1, 5, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 9, 0, 7, 4, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 9, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 1, 0, 6, 1, 1, 5, 1, 0, 0, 1, 5, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 8, 7, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 3, 0, 0, 1, 0, 0, 1,
        1, 6, 7, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 7, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 5, 0, 0, 1, 0, 1, 0, 0, 1, 4, 3, 1, 0, 1, 0, 0,
        8, 3, 0, 1, 0, 1, 0, 3, 0, 0, 0, 7, 0, 1, 0, 0, 3, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1])
soft_pseudo_label
tensor([[4.6528e-04, 9.8447e-01, 4.3157e-04,  ..., 5.5145e-04, 2.4518e-04,
         3.3284e-04],
        [2.4319e-05, 9.9295e-01, 3.6507e-05,  ..., 5.9663e-05, 4.9801e-05,
         6.1617e-05],
        [9.8119e-01, 8.5472e-03, 1.8464e-04,  ..., 1.1184e-03, 2.7452e-03,
         3.0234e-04],
        ...,
        [5.5137e-04, 9.3286e-01, 1.1480e-03,  ..., 5.5001e-03, 7.7000e-04,
         1.7712e-03],
        [8.0985e-01, 1.0296e-01, 2.3885e-03,  ..., 1.8748e-03, 3.0960e-02,
         4.3632e-03],
        [9.7015e-01, 3.6878e-04, 7.1642e-04,  ..., 1.1538e-03, 1.7391e-02,
         1.1178e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 9, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 5, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 9, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 9, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 5, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 9, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]
original label
tensor([1, 1, 0, 0, 7, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 8, 1,
        0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 3, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 8, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 3, 1, 1, 1, 0])
soft_pseudo_label
tensor([[1.0447e-05, 9.9764e-01, 6.3988e-06,  ..., 2.0316e-05, 5.3360e-06,
         4.2694e-05],
        [1.7841e-03, 6.9490e-01, 2.7098e-03,  ..., 4.3622e-03, 2.2182e-03,
         1.8918e-03],
        [2.7430e-01, 6.7493e-01, 4.5923e-03,  ..., 1.1933e-03, 6.7243e-03,
         2.8963e-03],
        ...,
        [9.8990e-01, 3.9122e-05, 6.8437e-04,  ..., 2.7692e-04, 4.5931e-03,
         5.1709e-04],
        [4.5724e-03, 9.6262e-01, 4.5185e-04,  ..., 1.9513e-03, 2.3954e-03,
         2.6066e-03],
        [7.4484e-02, 9.2169e-01, 1.1025e-04,  ..., 4.6144e-04, 3.0799e-04,
         1.1920e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 5, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 5, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1]
original label
tensor([1, 1, 1, 1, 1, 0, 0, 1, 6, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 4, 1, 9, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 6, 1, 0, 0, 1, 1, 1, 0, 5, 1, 0,
        0, 1, 1, 0, 0, 6, 1, 0, 2, 1, 1, 4, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 5, 0, 0, 0,
        3, 0, 1, 1, 1, 1, 1, 1, 0, 7, 1, 0, 1, 5, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 3])
soft_pseudo_label
tensor([[5.3506e-04, 9.4499e-01, 7.2984e-03,  ..., 5.3089e-04, 8.3440e-04,
         1.2404e-03],
        [9.2530e-04, 8.9986e-01, 1.2175e-03,  ..., 6.1638e-04, 6.2854e-04,
         5.4983e-04],
        [8.9711e-02, 3.3127e-03, 7.3088e-01,  ..., 1.2960e-03, 5.8768e-03,
         9.6676e-03],
        ...,
        [2.1913e-01, 6.5547e-01, 1.9370e-03,  ..., 3.5315e-03, 2.4376e-02,
         4.9948e-03],
        [1.7354e-02, 9.3738e-01, 2.7320e-04,  ..., 1.1833e-03, 1.5569e-03,
         1.0181e-03],
        [2.5780e-01, 6.3805e-01, 2.0871e-03,  ..., 3.6599e-02, 9.0305e-03,
         6.5362e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 9, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 7, 1, 1, 0, 1, 1, 9, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 7, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 9, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 7, 1, 1, 0, 1, 1, 9, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 7, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]
original label
tensor([1, 1, 2, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 6, 0, 0, 1, 0, 1, 0, 4, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 7, 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 0, 0, 7,
        3, 1, 0, 0, 1, 0, 7, 1, 0, 1, 7, 1, 0, 7, 0, 0, 1, 1, 0, 1, 1, 0, 0, 4,
        3, 0, 1, 0, 0, 0, 2, 0, 3, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1])
soft_pseudo_label
tensor([[3.0469e-03, 9.5219e-01, 2.0347e-03,  ..., 8.9497e-04, 1.9682e-03,
         1.5784e-03],
        [3.4245e-03, 9.5138e-01, 2.9724e-03,  ..., 6.4723e-04, 3.2518e-03,
         3.8512e-03],
        [8.7057e-01, 2.9156e-02, 4.6812e-03,  ..., 3.2937e-03, 3.3789e-02,
         5.7862e-03],
        ...,
        [6.0125e-01, 1.3973e-03, 8.6492e-02,  ..., 3.7034e-02, 4.3152e-02,
         9.7103e-02],
        [5.0369e-04, 1.0704e-02, 3.0504e-01,  ..., 3.2866e-02, 2.7443e-03,
         5.5128e-01],
        [9.1806e-01, 2.5362e-03, 1.0237e-03,  ..., 9.2575e-04, 4.9374e-02,
         2.6411e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 0, 5, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 9, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 6, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 7, 0, 0, 5, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 9, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 0, 5, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 9, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 6, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 7, 0, 0, 5, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 9, 0]
original label
tensor([1, 1, 0, 0, 8, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 4, 1, 0, 0, 0, 0, 1,
        6, 1, 0, 0, 7, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 8, 1, 1, 0, 1, 1, 1, 6, 1, 1, 1,
        1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 7, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 9, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0])
soft_pseudo_label
tensor([[1.4426e-04, 9.7102e-01, 4.6660e-04,  ..., 6.4403e-04, 3.4405e-04,
         7.4636e-04],
        [1.8657e-01, 5.2174e-02, 2.6439e-01,  ..., 6.0089e-03, 6.5636e-02,
         2.0451e-01],
        [8.0121e-01, 3.5548e-02, 5.8011e-02,  ..., 1.0442e-02, 1.3907e-02,
         1.9912e-02],
        ...,
        [7.7528e-05, 4.3092e-01, 4.5934e-05,  ..., 3.9752e-05, 6.0456e-03,
         4.7499e-03],
        [8.4236e-03, 9.6136e-01, 9.0447e-04,  ..., 4.0348e-03, 1.4710e-03,
         2.7414e-03],
        [6.0042e-04, 9.9741e-01, 3.3259e-05,  ..., 1.0508e-04, 5.9522e-05,
         7.0066e-05]], device='cuda:0')
hard_pseudo_label
tensor([1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 7, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 5,
        1, 0, 0, 1, 0, 5, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 7, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 5, 1, 0, 0, 1, 0, 5, 1, 1]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 1, 0, 8, 1, 0, 8, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 1, 0, 7, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 7, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 8, 0, 0, 1, 1, 1, 0, 0, 0, 3, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1])
soft_pseudo_label
tensor([[2.7223e-03, 9.2166e-01, 3.3813e-03,  ..., 3.1816e-02, 7.3019e-04,
         2.1029e-02],
        [9.8775e-01, 2.0117e-04, 7.4818e-04,  ..., 8.4449e-04, 7.1612e-03,
         8.8545e-04],
        [2.0780e-01, 7.3241e-01, 9.1078e-03,  ..., 2.4216e-03, 9.1345e-03,
         3.9964e-03],
        ...,
        [1.9062e-03, 2.3362e-01, 2.0694e-02,  ..., 1.4463e-01, 7.9784e-03,
         4.9651e-01],
        [7.8994e-01, 1.5893e-01, 2.9915e-03,  ..., 2.1268e-02, 5.1137e-03,
         1.2781e-02],
        [5.6295e-05, 9.5259e-01, 6.3171e-05,  ..., 4.4446e-05, 1.0066e-03,
         9.7647e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 9, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 9, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 9, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 9, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 9, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 9, 0, 1]
original label
tensor([1, 0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 5, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 4, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 8, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 5, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 9, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 6, 0, 1, 1, 3, 1, 1, 6, 1, 1, 0,
        1, 0, 0, 0, 5, 1, 0, 1])
soft_pseudo_label
tensor([[9.7696e-01, 2.8252e-04, 5.5749e-04,  ..., 4.1187e-04, 1.5822e-02,
         7.4544e-04],
        [5.3391e-01, 3.5915e-01, 1.3548e-03,  ..., 4.0251e-03, 1.4911e-02,
         2.3848e-03],
        [9.9578e-01, 1.9507e-03, 8.6126e-05,  ..., 9.5612e-05, 8.8268e-04,
         7.3884e-05],
        ...,
        [9.3046e-02, 7.6923e-01, 5.2493e-03,  ..., 8.8038e-03, 6.0833e-03,
         7.0224e-03],
        [9.8582e-01, 5.0609e-06, 1.6260e-04,  ..., 3.0155e-04, 1.0866e-02,
         3.0541e-04],
        [9.6584e-01, 3.6354e-03, 1.1794e-03,  ..., 4.2089e-03, 9.5337e-03,
         2.3512e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 9, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 7, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 9, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 7, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]
original label
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 6, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 5, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 3, 9, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 4, 0, 9, 0, 1, 1, 0, 8, 1, 0, 1, 1, 0, 0, 7, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 7, 0, 1, 1, 0, 1, 0, 1, 8, 1, 0, 0, 0, 0, 0, 4,
        1, 1, 8, 0, 0, 1, 0, 0])
soft_pseudo_label
tensor([[5.5472e-01, 2.7035e-01, 2.4608e-03,  ..., 1.7969e-03, 1.2924e-02,
         2.4971e-03],
        [2.4964e-03, 9.5679e-01, 4.4824e-04,  ..., 4.0101e-04, 3.5421e-03,
         1.1324e-03],
        [2.0565e-02, 9.3633e-01, 1.0060e-03,  ..., 3.2682e-03, 4.0913e-03,
         2.6171e-03],
        ...,
        [9.6659e-01, 8.6945e-04, 7.2009e-04,  ..., 2.1191e-03, 9.5411e-03,
         1.1705e-03],
        [5.6860e-01, 3.6783e-01, 2.3882e-03,  ..., 8.7590e-03, 7.9267e-03,
         7.2562e-03],
        [9.6764e-02, 8.8636e-01, 7.8021e-05,  ..., 2.8104e-03, 6.8328e-04,
         5.1476e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 5, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 5, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 5, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 5, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]
original label
tensor([6, 1, 0, 1, 1, 0, 1, 1, 0, 9, 4, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 4, 0, 5,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 4,
        0, 6, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1])
soft_pseudo_label
tensor([[1.6838e-02, 9.2069e-01, 1.9350e-03,  ..., 1.7009e-03, 3.2818e-03,
         1.9964e-03],
        [3.4556e-01, 5.3208e-01, 1.6672e-03,  ..., 1.0702e-03, 1.3249e-02,
         1.7235e-03],
        [6.6399e-01, 2.4332e-01, 5.4977e-03,  ..., 1.0180e-03, 1.6297e-02,
         1.9280e-03],
        ...,
        [9.6726e-01, 5.8062e-05, 9.1716e-04,  ..., 1.3631e-03, 2.2439e-02,
         2.6829e-03],
        [9.9678e-01, 6.0586e-04, 3.4073e-04,  ..., 8.7656e-05, 1.5045e-03,
         1.3249e-04],
        [3.9741e-02, 9.5908e-01, 5.3767e-05,  ..., 1.2860e-04, 9.0572e-05,
         1.0253e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 7, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 9, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 5, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 9, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 7, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 9, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 5, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 5, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 9, 0, 0, 1]
original label
tensor([1, 1, 0, 0, 9, 5, 0, 8, 1, 1, 0, 1, 4, 1, 1, 1, 1, 0, 0, 0, 0, 7, 0, 0,
        0, 0, 1, 1, 0, 3, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 5, 0, 1, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 5, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 9, 1, 9, 0, 0, 1])
soft_pseudo_label
tensor([[1.5731e-03, 9.8153e-01, 1.8434e-04,  ..., 6.4719e-04, 5.7563e-04,
         4.6298e-04],
        [4.9610e-01, 3.8485e-01, 2.9155e-03,  ..., 2.8564e-02, 1.9046e-02,
         7.1668e-03],
        [1.6934e-01, 8.0003e-01, 7.7054e-04,  ..., 8.7827e-04, 4.8368e-03,
         1.0866e-03],
        ...,
        [9.5268e-01, 4.0135e-03, 8.1610e-04,  ..., 1.4281e-03, 2.5289e-02,
         1.3964e-03],
        [1.9316e-01, 4.0812e-01, 5.2474e-02,  ..., 4.9113e-02, 2.9723e-02,
         1.4999e-01],
        [9.8740e-01, 8.7141e-04, 3.8877e-04,  ..., 4.4182e-04, 5.8484e-03,
         4.8762e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 1, 0, 1, 7, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 7, 0, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 9, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 5, 1, 1, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 1, 0, 1, 7, 0, 1, 1, 0, 1, 1, 1, 1, 1, 7, 0, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 9, 1, 0, 1, 0, 0, 1, 0, 1, 0, 5, 1, 1, 0, 0, 1, 0]
original label
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 7, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 5, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 4, 1, 0, 1, 0, 0, 1, 1, 0, 1, 9, 0, 0, 1, 7, 1,
        1, 6, 0, 8, 0, 0, 0, 4, 9, 0, 0, 1, 0, 0, 0, 1, 1, 6, 1, 0, 0, 0, 0, 1,
        3, 6, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 3, 1, 6, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 1, 1, 1, 0, 5, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0])
soft_pseudo_label
tensor([[2.6304e-01, 6.8361e-01, 6.1973e-04,  ..., 1.0089e-03, 3.8093e-03,
         7.4317e-04],
        [5.9005e-02, 1.6846e-01, 1.0633e-02,  ..., 2.1178e-02, 1.5950e-01,
         1.9618e-01],
        [9.8111e-01, 3.5488e-03, 1.3870e-03,  ..., 3.9584e-04, 7.2109e-03,
         7.3880e-04],
        ...,
        [4.9818e-02, 8.1151e-01, 3.4688e-03,  ..., 7.3471e-03, 9.5358e-03,
         5.8206e-03],
        [3.4269e-01, 6.4778e-01, 1.0296e-03,  ..., 1.8736e-03, 1.0749e-03,
         1.4606e-03],
        [9.5827e-01, 1.5291e-03, 2.7675e-03,  ..., 5.5339e-03, 1.4144e-02,
         4.2376e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 5, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 9, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 7, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 5, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 9, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 7, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0]
original label
tensor([0, 4, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 9, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 3, 1, 0, 6, 1, 0, 2, 2, 9, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 9, 1,
        0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 5, 0, 0, 1, 0, 1, 6, 1, 9, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 5, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 9,
        0, 1, 0, 0, 1, 1, 1, 0])
soft_pseudo_label
tensor([[3.9144e-04, 9.5422e-01, 6.5170e-04,  ..., 1.0893e-03, 2.2581e-03,
         3.3914e-03],
        [9.8464e-01, 1.3740e-05, 2.1181e-04,  ..., 1.4280e-03, 7.1177e-03,
         8.5384e-04],
        [9.4668e-01, 8.4326e-04, 9.7121e-03,  ..., 4.0923e-03, 1.4715e-02,
         5.8185e-03],
        ...,
        [4.7084e-02, 8.7294e-01, 1.7658e-04,  ..., 2.6352e-04, 8.0867e-03,
         7.2195e-04],
        [6.0085e-06, 2.2077e-01, 1.2233e-05,  ..., 1.1158e-06, 2.2594e-03,
         2.5418e-04],
        [1.2760e-03, 9.8959e-01, 1.0756e-04,  ..., 1.5154e-04, 2.5083e-04,
         1.8897e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        5, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 5, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 9, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1,
        1, 0, 1, 1, 1, 1, 5, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 5, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 5, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 9, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 5, 1]
original label
tensor([1, 0, 0, 1, 0, 4, 1, 0, 0, 1, 1, 0, 1, 1, 7, 1, 5, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 8, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 9, 0, 1, 0, 6, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 9, 1, 2, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[1.8272e-01, 5.5734e-01, 1.0911e-01,  ..., 2.2822e-03, 1.4312e-02,
         2.4874e-02],
        [7.8149e-01, 1.3318e-01, 2.5044e-03,  ..., 9.4039e-04, 3.0287e-02,
         2.1822e-03],
        [9.1257e-01, 2.0569e-02, 3.6253e-03,  ..., 1.9348e-03, 2.7868e-02,
         3.4627e-03],
        ...,
        [2.2275e-01, 1.7899e-01, 2.3712e-01,  ..., 1.3301e-01, 3.0557e-03,
         1.8776e-01],
        [8.6525e-01, 4.4446e-02, 2.4374e-03,  ..., 4.4868e-04, 3.6775e-02,
         1.9662e-03],
        [4.5939e-01, 4.4265e-01, 2.1906e-03,  ..., 1.8813e-02, 1.4910e-02,
         5.5940e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 1, 1, 1, 5, 1, 1, 1, 7, 1, 0, 1, 1, 0, 2, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 7, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 7, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 1, 1, 1, 5, 1, 1, 1, 7, 1, 0, 1, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 7, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 7, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0]
original label
tensor([0, 0, 0, 0, 0, 1, 5, 1, 1, 1, 0, 1, 9, 1, 0, 0, 1, 0, 7, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 4, 0, 1, 0, 1, 0, 0, 1, 6, 0, 1, 0, 1, 0, 2, 2, 1,
        1, 1, 1, 0, 6, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 6, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 4, 0, 1, 1, 0, 0, 0, 4, 0, 1, 6, 1, 1, 1,
        1, 1, 1, 5, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 5, 1, 0,
        1, 0, 0, 0, 0, 4, 1, 0])
soft_pseudo_label
tensor([[2.3834e-03, 9.1884e-01, 7.6363e-04,  ..., 9.2472e-04, 9.3745e-04,
         8.5273e-04],
        [5.1849e-03, 8.5343e-01, 2.5894e-03,  ..., 5.5898e-03, 1.4961e-02,
         2.6942e-02],
        [1.1833e-01, 8.7096e-01, 5.9194e-04,  ..., 3.1346e-04, 1.6863e-03,
         3.9163e-04],
        ...,
        [9.9269e-01, 2.6447e-04, 1.4864e-04,  ..., 2.4387e-04, 3.6939e-03,
         2.0617e-04],
        [9.8473e-01, 1.6321e-04, 1.5223e-03,  ..., 1.7019e-03, 4.2735e-03,
         1.7090e-03],
        [5.5917e-01, 3.5474e-01, 5.7900e-03,  ..., 2.4927e-03, 1.2003e-02,
         2.9892e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 5, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 9, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 5, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 9, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]
original label
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 1, 0, 0, 1, 1, 6,
        0, 1, 1, 8, 0, 1, 1, 1, 0, 5, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 5, 0, 0,
        1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 4,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 6, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 6, 2, 1, 1, 8, 0, 1, 0, 1, 1, 0, 0, 0, 1, 9, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[7.7067e-01, 1.5146e-01, 5.6920e-03,  ..., 1.7224e-03, 2.0598e-02,
         4.9792e-03],
        [9.9348e-01, 1.1891e-03, 2.7576e-04,  ..., 1.5559e-04, 3.1713e-03,
         2.2906e-04],
        [4.4516e-02, 8.8891e-01, 7.8411e-04,  ..., 1.1801e-02, 8.3460e-03,
         5.7235e-03],
        ...,
        [2.0998e-03, 5.5223e-02, 1.5160e-02,  ..., 1.9674e-01, 2.6726e-03,
         7.0573e-01],
        [2.5687e-01, 4.8177e-01, 2.0234e-03,  ..., 3.8624e-03, 6.4440e-02,
         7.4449e-03],
        [5.1423e-03, 9.6240e-01, 5.2814e-04,  ..., 5.0593e-04, 2.0375e-03,
         6.4018e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 7, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 9, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 9, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 9, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 7, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 9, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 7, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 9, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1]
original label
tensor([5, 0, 1, 0, 0, 9, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 5, 1, 3,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 6, 0, 1, 0, 0,
        0, 0, 0, 9, 0, 0, 0, 0, 1, 1, 0, 1, 6, 5, 1, 0, 0, 1, 9, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 5, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 7, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 8, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        4, 1, 3, 0, 1, 7, 1, 1])
soft_pseudo_label
tensor([[9.7473e-01, 1.5186e-03, 2.3134e-03,  ..., 3.4885e-03, 6.4564e-03,
         2.2118e-03],
        [6.7296e-02, 8.3763e-01, 4.5885e-03,  ..., 2.2275e-03, 1.3672e-02,
         4.7019e-03],
        [2.3463e-03, 9.4888e-01, 9.8047e-04,  ..., 9.7094e-04, 3.9582e-03,
         2.9703e-03],
        ...,
        [8.0471e-07, 9.9847e-01, 5.8539e-06,  ..., 3.5924e-06, 1.0715e-05,
         1.6418e-05],
        [3.8952e-02, 8.3284e-01, 1.5311e-03,  ..., 1.4328e-03, 2.0129e-02,
         4.8683e-03],
        [1.5938e-01, 7.5446e-01, 1.0823e-03,  ..., 2.1189e-02, 1.1670e-02,
         5.8140e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 6, 1, 1, 0, 1, 0, 2, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 7, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 7, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 9, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 5, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 6, 1, 1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 7, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 7, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 9, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 5, 1, 1, 1]
original label
tensor([0, 1, 1, 3, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 7, 0, 4, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 4, 0, 1, 0, 1, 4, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 6, 1, 1,
        0, 5, 0, 1, 0, 1, 1, 6, 7, 0, 1, 0, 1, 7, 0, 0, 1, 0, 1, 0, 0, 7, 5, 1,
        1, 5, 0, 1, 0, 0, 1, 1, 1, 1, 0, 3, 1, 1, 0, 0, 1, 6, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 5, 1, 1, 0])
soft_pseudo_label
tensor([[9.6199e-01, 3.6420e-05, 4.2213e-04,  ..., 6.4746e-04, 2.8936e-02,
         6.9666e-04],
        [5.2231e-01, 1.2389e-02, 1.2721e-03,  ..., 1.5523e-04, 1.8051e-01,
         1.7269e-03],
        [8.2727e-03, 9.5993e-01, 4.3164e-04,  ..., 1.0134e-03, 1.7080e-03,
         1.1065e-03],
        ...,
        [9.8762e-01, 2.2590e-05, 2.2113e-04,  ..., 1.9306e-04, 9.1134e-03,
         2.6673e-04],
        [2.9975e-05, 9.2927e-01, 4.5796e-05,  ..., 2.5418e-04, 2.3029e-03,
         3.0860e-03],
        [9.8819e-01, 8.1238e-05, 1.8741e-04,  ..., 1.5177e-04, 8.0630e-03,
         2.8272e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 9, 7, 1, 1, 1, 0, 1, 0, 9,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 5, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 9, 7, 1, 1, 1, 0, 1, 0, 9, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 5, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 0, 8, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 9, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 7, 1, 9, 1, 0, 0, 1, 0, 2,
        0, 0, 1, 1, 1, 0, 4, 8, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 5, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        3, 0, 1, 0, 0, 5, 2, 9, 0, 1, 6, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 4, 0, 0, 1, 0])
soft_pseudo_label
tensor([[8.9592e-01, 8.4443e-05, 2.3137e-03,  ..., 4.3811e-03, 6.6830e-02,
         7.8445e-03],
        [9.9415e-01, 2.4108e-06, 5.3129e-05,  ..., 2.2500e-04, 3.7065e-03,
         1.3929e-04],
        [7.7045e-01, 2.0256e-01, 2.3857e-04,  ..., 2.9651e-03, 3.1908e-03,
         8.3350e-04],
        ...,
        [8.1171e-01, 1.2286e-02, 2.4386e-02,  ..., 2.9104e-03, 6.6629e-02,
         2.4008e-02],
        [9.2983e-03, 9.7284e-01, 1.1695e-03,  ..., 1.3079e-03, 4.6068e-04,
         9.4249e-04],
        [3.0384e-01, 4.2591e-02, 4.3523e-01,  ..., 3.9199e-02, 2.5678e-03,
         1.4006e-01]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        5, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 5, 0, 1, 1, 1, 1, 5, 1, 5, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 2], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 5, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 5, 0, 1, 1, 1, 1, 5, 1, 5, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2]
original label
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 9, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        5, 1, 0, 1, 1, 1, 1, 0, 1, 0, 3, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 3,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 4, 0, 1, 0, 1, 1, 1, 1, 1, 1, 5, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[9.8482e-01, 1.0962e-02, 1.7997e-04,  ..., 1.5530e-04, 2.1407e-03,
         1.4167e-04],
        [8.6877e-01, 1.4797e-02, 2.0092e-03,  ..., 2.0408e-03, 6.5185e-02,
         4.4882e-03],
        [9.7094e-01, 1.8881e-03, 7.1701e-04,  ..., 1.6301e-03, 1.3280e-02,
         1.6509e-03],
        ...,
        [9.7791e-01, 4.9439e-04, 3.9224e-04,  ..., 5.7462e-04, 1.3854e-02,
         7.0198e-04],
        [1.2285e-03, 9.7274e-01, 5.1287e-04,  ..., 8.7669e-04, 9.7135e-04,
         7.7594e-04],
        [3.5299e-04, 9.9576e-01, 1.0511e-04,  ..., 3.2077e-04, 1.6424e-04,
         2.3071e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 7, 1, 0, 1, 0, 0, 0, 0, 1, 0, 7, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 9, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 5, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 9, 1, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 7, 1, 0, 1, 0, 0, 0, 0, 1, 0, 7, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 9, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 9, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 5, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 9, 1, 0, 1, 1]
original label
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 6, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 8,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 5, 1, 0, 7, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 9, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 8, 0, 1, 9, 0,
        0, 1, 0, 8, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 4, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 3])
soft_pseudo_label
tensor([[2.0689e-06, 9.9941e-01, 2.5103e-06,  ..., 3.2997e-06, 4.4057e-06,
         6.1346e-06],
        [9.5521e-01, 1.4557e-04, 6.3798e-03,  ..., 9.1373e-04, 2.2377e-02,
         2.3140e-03],
        [8.0485e-01, 6.2550e-02, 3.4036e-03,  ..., 2.5219e-03, 1.9774e-02,
         2.8466e-03],
        ...,
        [3.0337e-01, 8.9587e-02, 2.9280e-03,  ..., 1.6880e-03, 1.3754e-01,
         1.0705e-02],
        [9.7858e-01, 5.7606e-05, 5.1595e-04,  ..., 8.7766e-04, 1.3011e-02,
         7.6514e-04],
        [9.2778e-05, 9.5220e-01, 3.1142e-04,  ..., 2.5368e-04, 4.8234e-04,
         4.4262e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 7, 1, 1, 0, 7, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 2,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 7, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 7, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 5, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 7, 1, 1, 0, 7, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 7, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 7, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 5, 0, 1]
original label
tensor([1, 0, 1, 1, 1, 1, 5, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 7, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 8, 1, 0, 0, 1, 4,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 1, 0, 1, 1, 1, 1, 5, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 8, 1, 0, 1, 1, 1, 7, 1, 1, 6, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 4, 1, 0, 0, 1, 4, 1, 0, 1, 1,
        1, 0, 7, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[3.6585e-01, 6.2721e-01, 8.5607e-04,  ..., 1.3182e-03, 9.3563e-04,
         8.2007e-04],
        [9.8030e-01, 5.3571e-06, 7.2959e-04,  ..., 5.5532e-04, 1.4120e-02,
         7.4909e-04],
        [4.0809e-02, 2.1717e-01, 3.3899e-01,  ..., 3.3085e-02, 8.7388e-03,
         1.1410e-01],
        ...,
        [2.0842e-04, 3.1552e-03, 2.6968e-03,  ..., 3.8727e-01, 1.0011e-03,
         5.9748e-01],
        [3.2168e-01, 5.7234e-01, 1.7281e-03,  ..., 2.8377e-02, 1.1151e-02,
         6.0848e-03],
        [1.2006e-02, 9.6686e-01, 1.5160e-03,  ..., 4.3137e-04, 1.4708e-03,
         8.1940e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 9, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 2, 1, 7, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 7, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 4, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 9, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 9, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 7, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 7, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 4, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 9, 1, 1]
original label
tensor([0, 0, 1, 0, 8, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 8, 1, 1, 0, 9, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 4, 0, 0, 8, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 5, 1, 0, 0, 4, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 8, 0,
        0, 1, 0, 0, 1, 9, 1, 1])
soft_pseudo_label
tensor([[9.8777e-01, 8.8115e-04, 2.9907e-04,  ..., 1.2503e-04, 6.5908e-03,
         2.4673e-04],
        [2.3329e-03, 8.8588e-01, 4.5138e-04,  ..., 9.5594e-03, 6.4793e-03,
         2.4003e-02],
        [1.5486e-04, 2.7742e-04, 9.1269e-01,  ..., 3.9816e-04, 2.8316e-04,
         9.8656e-03],
        ...,
        [3.8282e-02, 9.3295e-01, 4.5626e-04,  ..., 8.1099e-04, 4.2327e-03,
         1.5077e-03],
        [9.2508e-01, 1.3104e-03, 2.0515e-03,  ..., 1.1860e-02, 2.6630e-02,
         7.1359e-03],
        [3.0552e-01, 4.1394e-01, 3.0502e-02,  ..., 8.5075e-03, 4.8547e-02,
         1.5092e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 9, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 7, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        2, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 9, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 7, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1]
original label
tensor([0, 3, 2, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 6, 1, 1, 1, 0, 1, 7, 0,
        0, 0, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 6, 0, 0,
        0, 1, 9, 5, 1, 0, 0, 1, 8, 1, 7, 3, 4, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 6, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 3, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 8, 0, 0, 0, 1, 0, 2])
soft_pseudo_label
tensor([[4.3670e-02, 6.6274e-01, 1.4273e-03,  ..., 2.1349e-01, 3.3576e-03,
         4.3457e-02],
        [3.9806e-01, 5.4408e-01, 2.9175e-02,  ..., 4.8024e-03, 1.8506e-03,
         1.2301e-02],
        [9.6605e-04, 9.4779e-01, 7.1093e-04,  ..., 1.3127e-03, 1.2526e-03,
         8.5004e-04],
        ...,
        [2.1939e-03, 9.5047e-01, 1.4458e-03,  ..., 6.8295e-04, 4.0668e-03,
         3.4129e-03],
        [7.8461e-01, 2.1450e-01, 5.3258e-06,  ..., 5.7998e-04, 2.4650e-05,
         3.6502e-05],
        [9.6468e-01, 1.6561e-04, 2.6436e-03,  ..., 9.7847e-04, 1.8355e-02,
         2.0785e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 5, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 7, 0, 0, 0, 1, 0, 0,
        1, 5, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        9, 0, 0, 2, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 5, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 7, 0, 0, 0, 1, 0, 0, 1, 5, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 9, 0, 0, 2, 0, 1, 0, 0]
original label
tensor([1, 0, 1, 0, 0, 1, 9, 7, 0, 1, 1, 6, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 4, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 5, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        2, 1, 0, 4, 0, 1, 0, 0])
soft_pseudo_label
tensor([[3.9845e-01, 2.9066e-01, 4.2976e-02,  ..., 8.7258e-03, 7.0674e-02,
         3.3918e-02],
        [2.7589e-01, 3.3344e-01, 3.6219e-03,  ..., 4.4674e-04, 8.8265e-02,
         3.6933e-03],
        [3.1315e-05, 9.8997e-01, 5.5391e-05,  ..., 9.4592e-05, 4.7224e-04,
         5.7704e-04],
        ...,
        [9.8938e-01, 3.5672e-05, 1.3822e-04,  ..., 4.7985e-04, 6.1474e-03,
         2.6826e-04],
        [7.9448e-01, 2.0245e-01, 9.7283e-05,  ..., 7.6508e-05, 7.7971e-04,
         5.0372e-05],
        [1.5484e-04, 2.1165e-04, 1.7657e-01,  ..., 1.7114e-01, 2.5780e-04,
         6.3586e-01]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 7, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 7, 4, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 9, 0, 1, 1, 0, 0, 1, 5, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 9], device='cuda:0')
hard_pseudo_label
[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 7, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 7, 4, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 9, 0, 1, 1, 0, 0, 1, 5, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 9]
original label
tensor([3, 1, 5, 1, 1, 0, 0, 0, 0, 1, 9, 0, 1, 0, 0, 1, 4, 0, 9, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 5, 1, 0, 0, 1, 0, 0, 0, 0, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 9, 4, 4, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 5, 1, 1, 1, 0, 1, 0, 1, 5,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 7, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 7])
soft_pseudo_label
tensor([[0.7497, 0.0328, 0.0280,  ..., 0.0078, 0.0688, 0.0257],
        [0.6370, 0.0083, 0.0024,  ..., 0.0035, 0.2092, 0.0101],
        [0.0498, 0.8943, 0.0020,  ..., 0.0026, 0.0062, 0.0022],
        ...,
        [0.9327, 0.0034, 0.0023,  ..., 0.0012, 0.0376, 0.0030],
        [0.9027, 0.0534, 0.0010,  ..., 0.0047, 0.0081, 0.0025],
        [0.3610, 0.5367, 0.0021,  ..., 0.0051, 0.0289, 0.0088]],
       device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 7, 1, 1, 1, 0, 0, 0, 1, 1, 7, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 4, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 7, 1, 1, 1, 0, 0, 0, 1, 1, 7, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 4, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]
original label
tensor([0, 0, 1, 0, 8, 0, 0, 1, 1, 8, 1, 2, 1, 0, 3, 0, 0, 0, 1, 1, 7, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 6, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 6, 0, 1, 8])
soft_pseudo_label
tensor([[1.1760e-02, 9.5218e-01, 8.6827e-04,  ..., 2.6524e-03, 2.8721e-03,
         3.7679e-03],
        [8.8824e-01, 5.2876e-02, 1.0622e-02,  ..., 4.5531e-03, 1.4753e-02,
         6.3709e-03],
        [3.4985e-02, 6.4161e-01, 1.1296e-02,  ..., 5.0100e-03, 1.6204e-02,
         7.4406e-03],
        ...,
        [3.7188e-05, 9.6894e-01, 5.1833e-05,  ..., 3.6971e-05, 5.1619e-04,
         3.6367e-04],
        [1.5504e-02, 8.0302e-01, 1.0540e-03,  ..., 2.7848e-04, 7.9439e-03,
         2.4080e-03],
        [9.7912e-01, 6.9797e-05, 3.8176e-04,  ..., 9.5999e-04, 1.4354e-02,
         9.1648e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 9, 1, 5, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 9, 1, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 5, 1, 1, 1, 0, 1, 1, 0, 0, 0, 7, 0, 1, 0, 1, 2, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 9, 1, 5, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 9, 1, 1, 0, 0, 1, 9, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 5, 1, 1, 1, 0, 1, 1, 0, 0, 0, 7, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]
original label
tensor([1, 3, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 9, 1, 4, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 7, 1, 0, 7, 0, 1, 1, 1, 1, 0, 0, 1, 0, 7, 0, 0, 0, 0, 0,
        0, 9, 1, 0, 1, 1, 0, 5, 0, 0, 1, 1, 0, 4, 0, 3, 0, 1, 0, 1, 2, 1, 1, 1,
        1, 1, 1, 1, 1, 7, 9, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 8, 1, 1, 0, 0,
        0, 1, 0, 4, 1, 1, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[9.1273e-01, 1.0894e-02, 1.9238e-03,  ..., 5.9831e-04, 4.1497e-02,
         2.3320e-03],
        [9.6250e-01, 1.0465e-02, 2.4954e-03,  ..., 8.7341e-04, 9.0856e-03,
         1.3514e-03],
        [7.0747e-01, 1.0291e-02, 1.3190e-01,  ..., 3.4435e-03, 1.1701e-02,
         1.7452e-02],
        ...,
        [1.4890e-02, 9.7678e-01, 1.6476e-04,  ..., 1.4371e-04, 4.0423e-04,
         1.2857e-04],
        [9.7517e-01, 2.3508e-03, 2.0415e-03,  ..., 1.0295e-03, 7.3015e-03,
         1.2491e-03],
        [9.0422e-01, 1.3720e-02, 1.8654e-03,  ..., 2.8444e-03, 4.2415e-02,
         3.9012e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 9, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 7, 0,
        0, 1, 1, 0, 1, 0, 9, 0, 1, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 0, 9, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 7, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 9, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 7, 0, 0, 1, 1, 0, 1, 0, 9, 0, 1, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 0, 9, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 7, 0, 1, 0, 0]
original label
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 3, 0, 1, 8, 0, 1, 1, 0, 0, 1, 0, 6, 1,
        0, 0, 0, 1, 0, 1, 1, 9, 1, 1, 1, 1, 0, 0, 1, 8, 8, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 4, 1, 1, 1, 0, 1, 2, 1, 1, 4, 1, 1, 1, 0, 7, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 4, 1, 0, 0, 0, 0, 1, 0,
        1, 8, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 2, 0, 1, 0, 0])
soft_pseudo_label
tensor([[4.5678e-04, 9.3951e-01, 3.5853e-04,  ..., 2.9453e-03, 3.4208e-03,
         1.4113e-02],
        [1.5401e-03, 9.9240e-01, 1.5138e-04,  ..., 4.1996e-05, 2.6568e-04,
         1.1195e-04],
        [1.1741e-01, 6.7959e-01, 9.4864e-04,  ..., 9.3695e-02, 6.3110e-03,
         1.4262e-02],
        ...,
        [1.8920e-01, 7.8573e-01, 1.2356e-03,  ..., 8.2872e-04, 3.2158e-03,
         9.3358e-04],
        [5.1993e-01, 3.2791e-01, 3.8251e-03,  ..., 3.6109e-03, 3.3960e-02,
         5.5982e-03],
        [3.3273e-01, 5.6821e-01, 2.0613e-03,  ..., 3.3688e-03, 2.4044e-02,
         4.1967e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 9, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 9, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1]
original label
tensor([1, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 7, 1, 1, 1, 0, 1, 8, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 6, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 1, 7, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 9, 1, 0,
        0, 0, 0, 0, 9, 0, 0, 1])
soft_pseudo_label
tensor([[9.9446e-01, 4.8263e-03, 2.4836e-05,  ..., 7.3930e-05, 3.1516e-04,
         3.3193e-05],
        [8.7273e-01, 3.0497e-02, 3.0358e-03,  ..., 1.2205e-02, 2.8817e-02,
         8.2081e-03],
        [1.7527e-03, 9.8384e-01, 4.8303e-04,  ..., 3.0793e-04, 3.9967e-04,
         6.0055e-04],
        ...,
        [2.3061e-02, 9.0225e-01, 3.9213e-03,  ..., 5.3571e-03, 7.1213e-03,
         8.0145e-03],
        [5.5981e-05, 9.9564e-01, 7.3216e-06,  ..., 3.3460e-05, 1.2921e-04,
         3.5693e-04],
        [1.3797e-03, 9.2216e-01, 6.3723e-04,  ..., 1.0193e-03, 2.3503e-03,
         1.5482e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1]
original label
tensor([1, 0, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 8, 0, 0, 0, 2,
        0, 1, 0, 0, 0, 4, 1, 2, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 7, 1, 1, 0, 1, 1,
        1, 0, 4, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 7, 0, 1, 9,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 5, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0,
        0, 2, 1, 1, 1, 0, 1, 1])
soft_pseudo_label
tensor([[1.6770e-04, 9.7876e-01, 1.2221e-04,  ..., 8.7512e-05, 1.0437e-03,
         3.1300e-04],
        [9.9780e-01, 6.5654e-05, 1.3697e-04,  ..., 7.8119e-05, 1.3672e-03,
         7.7890e-05],
        [9.6676e-01, 1.1230e-04, 1.3930e-03,  ..., 3.3997e-03, 1.4353e-02,
         2.4967e-03],
        ...,
        [9.3218e-01, 1.1763e-02, 1.6178e-03,  ..., 1.2093e-03, 3.3625e-02,
         2.0471e-03],
        [9.3027e-01, 1.8768e-02, 1.1516e-03,  ..., 1.0386e-02, 8.9093e-03,
         3.5489e-03],
        [3.9360e-02, 9.1976e-01, 1.0769e-03,  ..., 1.4142e-03, 4.4096e-03,
         1.5746e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 5, 1, 2, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 9, 2, 1, 9, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 2, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 5, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 9, 2, 1, 9, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]
original label
tensor([1, 0, 0, 9, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 8, 1, 1, 8, 1, 1, 8, 1, 0, 1, 0, 1, 5, 1, 4, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 4, 1, 1, 1, 5, 9, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 4, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 6, 0, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 3, 9, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1])
soft_pseudo_label
tensor([[1.2220e-01, 8.0150e-01, 1.3628e-03,  ..., 2.5551e-02, 7.0436e-03,
         7.6271e-03],
        [2.1057e-01, 7.2923e-01, 6.3576e-04,  ..., 4.7352e-02, 9.0713e-04,
         7.3401e-03],
        [9.7626e-01, 1.8473e-03, 6.3436e-04,  ..., 1.1210e-03, 2.9522e-03,
         8.0034e-04],
        ...,
        [1.0426e-02, 9.8351e-01, 1.5135e-04,  ..., 3.9759e-04, 2.0327e-04,
         1.7505e-04],
        [8.3700e-01, 1.0202e-02, 1.1040e-03,  ..., 6.1689e-04, 1.0055e-01,
         2.0727e-03],
        [6.4430e-01, 2.2884e-01, 1.5091e-03,  ..., 2.6772e-03, 4.0136e-02,
         4.6438e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]
original label
tensor([1, 1, 0, 0, 1, 8, 1, 0, 0, 0, 6, 1, 0, 1, 8, 5, 0, 0, 1, 0, 1, 0, 5, 1,
        0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 9, 1, 0, 1,
        6, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 5, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 9, 0, 1,
        8, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 7, 0, 1, 0,
        5, 0, 0, 4, 0, 1, 0, 1])
soft_pseudo_label
tensor([[1.2898e-03, 9.7547e-01, 6.5781e-04,  ..., 7.9193e-04, 1.0359e-03,
         1.2575e-03],
        [7.5966e-03, 9.6082e-01, 9.8798e-04,  ..., 3.9569e-04, 2.7372e-03,
         9.7838e-04],
        [7.8240e-01, 1.4643e-01, 3.2824e-04,  ..., 6.3599e-02, 3.7928e-04,
         4.5601e-03],
        ...,
        [3.2222e-01, 5.9497e-01, 2.3938e-03,  ..., 1.3885e-02, 9.3986e-03,
         7.6709e-03],
        [3.3970e-03, 9.0139e-01, 1.8041e-03,  ..., 4.0386e-02, 1.6299e-03,
         1.7034e-02],
        [2.4061e-02, 9.2385e-01, 2.0348e-03,  ..., 3.6345e-03, 2.9577e-03,
         3.7134e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 9, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 5,
        1, 1, 1, 2, 0, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 9, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 5, 1, 1, 1, 2, 0, 1, 1, 1]
original label
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 6, 1, 1, 1, 0, 1, 0, 1, 1, 6, 1, 0, 0, 9, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 8, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 7, 1])
soft_pseudo_label
tensor([[6.7954e-04, 9.6342e-01, 2.8079e-04,  ..., 1.2148e-04, 2.0142e-03,
         1.0266e-03],
        [9.5607e-01, 1.8671e-04, 1.9309e-03,  ..., 5.8818e-04, 3.1585e-02,
         1.5466e-03],
        [1.0588e-01, 8.4592e-01, 4.7709e-04,  ..., 1.3597e-02, 4.1823e-03,
         4.3787e-03],
        ...,
        [2.7820e-03, 9.8996e-01, 2.4703e-04,  ..., 8.2677e-04, 4.8985e-04,
         1.6899e-03],
        [1.0290e-02, 7.2553e-01, 1.0951e-03,  ..., 6.9879e-04, 2.3288e-02,
         7.0086e-03],
        [4.6447e-03, 9.5077e-01, 1.5291e-03,  ..., 8.4031e-04, 2.5757e-03,
         1.4662e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 9, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 9, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1]
original label
tensor([7, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 8, 6, 0, 1, 1, 1,
        0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 7, 0, 6, 2, 1, 0, 1, 1, 0, 1, 1, 6, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 7, 0, 1, 0, 4, 0, 0, 1, 1, 1, 0, 0, 1, 4,
        1, 1, 0, 6, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1])
soft_pseudo_label
tensor([[9.3120e-01, 7.6571e-03, 1.2660e-03,  ..., 2.6545e-02, 7.4315e-03,
         4.6276e-03],
        [9.4007e-01, 1.0881e-02, 7.7527e-03,  ..., 4.2092e-03, 9.1257e-03,
         4.6026e-03],
        [9.8880e-01, 1.5226e-03, 2.0356e-04,  ..., 6.9540e-04, 5.4431e-03,
         5.8701e-04],
        ...,
        [2.4897e-02, 9.4408e-03, 1.1826e-04,  ..., 5.0469e-05, 8.8788e-02,
         1.6452e-03],
        [6.3612e-04, 8.7156e-01, 2.8118e-04,  ..., 4.9012e-04, 3.1398e-04,
         3.3423e-04],
        [4.6230e-01, 5.3003e-01, 1.1685e-03,  ..., 2.1593e-03, 6.5357e-04,
         1.1611e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 7, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 9, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 9, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 9, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 2, 5, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 0, 1, 1, 1, 0, 1, 1, 7, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 9, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 9, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 9, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 5, 1, 1]
original label
tensor([8, 9, 0, 0, 1, 1, 6, 1, 1, 9, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 5, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 3, 9, 0,
        0, 0, 1, 1, 0, 0, 8, 0, 1, 1, 1, 0, 6, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 9, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 8, 0, 2, 0, 1, 0])
soft_pseudo_label
tensor([[1.0384e-01, 8.4431e-01, 9.1787e-04,  ..., 2.2607e-03, 3.1741e-03,
         1.4783e-03],
        [7.2010e-01, 2.4500e-01, 7.2119e-04,  ..., 1.8798e-03, 5.9209e-03,
         1.1424e-03],
        [3.5797e-03, 5.1167e-01, 3.7564e-04,  ..., 1.8451e-04, 2.5549e-02,
         5.8017e-03],
        ...,
        [9.5086e-01, 5.8886e-03, 2.4176e-03,  ..., 2.0279e-03, 1.8712e-02,
         3.4512e-03],
        [9.5181e-01, 8.5770e-05, 9.4860e-04,  ..., 1.9945e-03, 3.1629e-02,
         1.7748e-03],
        [6.5856e-01, 3.3928e-02, 2.8123e-03,  ..., 1.6356e-03, 1.4766e-01,
         7.3518e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 9, 0, 0, 1, 1,
        1, 0, 7, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 7, 1, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 9, 0, 0, 1, 1, 1, 0, 7, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 0, 1, 0, 0, 1, 1, 1, 0, 7, 1, 0, 1, 0, 0, 0]
original label
tensor([1, 6, 1, 1, 1, 1, 0, 1, 1, 1, 0, 4, 0, 3, 1, 0, 3, 9, 1, 9, 0, 0, 1, 1,
        1, 0, 9, 1, 0, 1, 8, 0, 1, 1, 5, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        7, 4, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 4, 1, 1, 5, 0, 4, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 3, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 2, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 7, 4, 3,
        0, 7, 1, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[3.6655e-03, 9.6402e-01, 7.2523e-04,  ..., 3.6077e-04, 2.9623e-03,
         1.0287e-03],
        [8.2255e-01, 5.7415e-02, 2.5571e-02,  ..., 4.9536e-03, 2.1588e-02,
         1.2165e-02],
        [9.6558e-01, 6.6295e-03, 8.2232e-04,  ..., 1.8649e-03, 1.1564e-02,
         1.6563e-03],
        ...,
        [9.8053e-01, 1.4572e-02, 1.0452e-04,  ..., 1.0250e-04, 2.9605e-03,
         1.2101e-04],
        [9.6630e-01, 4.6212e-03, 1.0505e-03,  ..., 9.8684e-04, 1.4319e-02,
         1.7696e-03],
        [4.7077e-04, 8.2177e-01, 5.1863e-03,  ..., 5.9627e-04, 9.5109e-03,
         1.6195e-02]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 9, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 9, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 7, 1, 1, 1, 1, 1, 7, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 9, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 9, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 7, 1, 1, 1, 1, 1, 7, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 1, 0, 1, 0, 9, 1, 0, 1, 6, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 8, 1, 1, 1, 0,
        1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 6, 0, 1, 0, 0, 0, 3,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 7, 5, 1,
        1, 1, 0, 1, 0, 0, 0, 0])
soft_pseudo_label
tensor([[3.0539e-01, 6.8820e-01, 2.0255e-04,  ..., 1.3383e-03, 8.7682e-04,
         4.3047e-04],
        [9.5991e-01, 2.9124e-03, 5.3679e-03,  ..., 1.4305e-03, 1.5493e-02,
         2.2949e-03],
        [9.0235e-01, 3.1481e-03, 4.5738e-03,  ..., 1.3337e-02, 3.3256e-02,
         1.0123e-02],
        ...,
        [6.9842e-04, 9.5506e-01, 3.9951e-04,  ..., 2.8907e-03, 2.7395e-03,
         4.6362e-03],
        [6.4573e-03, 8.9502e-01, 2.1190e-03,  ..., 1.1614e-02, 2.8431e-03,
         6.3634e-03],
        [9.7473e-01, 2.0825e-04, 6.6441e-04,  ..., 6.9358e-04, 1.5740e-02,
         9.6813e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 5, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 5, 0, 1, 1, 1, 0, 0, 1, 0, 7, 1, 1, 0, 0, 1, 7, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 5, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 5, 0, 1, 1, 1, 0, 0, 1, 0, 7, 1, 1, 0, 0, 1, 7, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0]
original label
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 9, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 9, 1, 0, 2, 1, 1, 9, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 5, 0, 1, 1, 1, 0, 0, 1, 0, 1, 3, 1, 0, 8, 1, 7, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 4, 1, 6, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0])
soft_pseudo_label
tensor([[1.3052e-03, 4.8224e-01, 3.4114e-04,  ..., 7.9940e-04, 3.8732e-04,
         4.6857e-04],
        [9.0086e-01, 1.0675e-02, 1.0968e-03,  ..., 7.3200e-03, 3.9005e-02,
         5.3672e-03],
        [9.8252e-01, 1.8668e-05, 6.6160e-04,  ..., 3.8011e-04, 1.2200e-02,
         5.7565e-04],
        ...,
        [4.9141e-01, 9.8577e-02, 1.6189e-01,  ..., 1.5398e-03, 4.2997e-02,
         9.9822e-03],
        [4.2306e-04, 9.8240e-01, 1.5416e-04,  ..., 4.5126e-05, 9.1336e-05,
         9.4882e-05],
        [2.0521e-02, 9.6396e-01, 4.4852e-04,  ..., 1.5092e-03, 1.1805e-03,
         1.3208e-03]], device='cuda:0')
hard_pseudo_label
tensor([6, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 5, 1, 1, 0, 0, 7, 0,
        1, 0, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 9, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[6, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 5, 1, 1, 0, 0, 7, 0, 1, 0, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 9, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
original label
tensor([1, 0, 0, 1, 1, 1, 6, 1, 1, 0, 1, 1, 1, 0, 0, 1, 3, 1, 3, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 3, 1, 1, 1, 0, 1, 1, 1, 0, 8, 0, 1, 0, 0,
        1, 5, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 4, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 5, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        9, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 5, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 2])
soft_pseudo_label
tensor([[8.4627e-01, 1.4821e-01, 1.7524e-04,  ..., 1.1517e-03, 8.7231e-04,
         2.5004e-04],
        [9.6048e-01, 3.5784e-03, 9.8570e-04,  ..., 2.1300e-03, 1.5150e-02,
         1.8852e-03],
        [2.0886e-01, 6.6504e-01, 5.1475e-03,  ..., 5.9306e-03, 1.2378e-02,
         8.1697e-03],
        ...,
        [9.7095e-01, 1.5653e-03, 3.9908e-04,  ..., 2.4620e-03, 1.3023e-02,
         9.2789e-04],
        [2.5337e-03, 9.1716e-01, 3.1409e-03,  ..., 3.0953e-03, 5.4669e-03,
         5.7405e-03],
        [3.8429e-01, 4.9056e-01, 1.3363e-02,  ..., 1.2102e-02, 2.0367e-02,
         1.8422e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 5, 1, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 7, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 5, 1, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 7, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]
original label
tensor([0, 1, 1, 1, 0, 1, 0, 0, 7, 1, 0, 0, 1, 0, 1, 0, 6, 1, 1, 0, 2, 1, 0, 4,
        1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        9, 0, 0, 1, 1, 1, 8, 1, 0, 0, 5, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 1, 0,
        1, 1, 0, 0, 1, 5, 8, 1, 0, 0, 6, 7, 1, 1, 1, 0, 0, 1, 0, 7, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 6, 0, 0, 1, 1])
soft_pseudo_label
tensor([[1.6890e-04, 9.4984e-01, 3.3004e-04,  ..., 3.6425e-04, 4.9641e-04,
         6.4998e-04],
        [9.8510e-01, 4.6778e-05, 8.6207e-05,  ..., 1.4997e-04, 1.1247e-02,
         1.7077e-04],
        [9.7498e-01, 3.1179e-04, 5.3662e-04,  ..., 3.0516e-04, 1.7718e-02,
         6.8301e-04],
        ...,
        [8.5563e-02, 8.6416e-01, 7.3738e-04,  ..., 3.7957e-04, 5.8719e-03,
         1.0908e-03],
        [9.6846e-01, 6.1112e-04, 5.5372e-04,  ..., 2.0558e-03, 1.6517e-02,
         1.9853e-03],
        [8.4518e-01, 9.2449e-02, 3.9756e-03,  ..., 1.2574e-02, 4.7582e-03,
         5.6892e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
        1, 1, 0, 1, 0, 0, 4, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 9,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 6,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 5, 1,
        0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 4, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 9, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 6, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 5, 1, 0, 1, 0, 1, 1, 1, 0, 0]
original label
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 2, 6, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 9,
        0, 0, 0, 6, 0, 5, 8, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 5, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 6, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0])
soft_pseudo_label
tensor([[1.7298e-03, 9.7998e-01, 1.0820e-04,  ..., 3.4689e-04, 1.8010e-03,
         1.3973e-03],
        [4.7536e-01, 4.1300e-01, 1.7705e-03,  ..., 7.7123e-04, 3.3506e-02,
         2.0201e-03],
        [1.6005e-02, 9.7102e-01, 1.4258e-04,  ..., 1.9040e-03, 1.0357e-03,
         1.1464e-03],
        ...,
        [8.1364e-02, 8.0822e-01, 2.1514e-03,  ..., 1.9023e-03, 2.2006e-02,
         6.8538e-03],
        [8.4231e-01, 1.3224e-01, 1.1132e-03,  ..., 8.6612e-04, 3.8252e-03,
         7.9868e-04],
        [5.2710e-01, 4.6881e-01, 9.6136e-05,  ..., 2.5477e-04, 9.2241e-04,
         1.2937e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 9, 1, 0, 1, 5, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 5, 1, 0, 1, 9, 0, 9, 1, 2, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 5, 0, 1, 1, 0, 1,
        0, 0, 1, 9, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 9, 1, 0, 1, 5, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 5, 1, 0, 1, 9, 0, 9, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 5, 0, 1, 1, 0, 1, 0, 0, 1, 9, 1, 1, 0, 0]
original label
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 9, 2, 1, 0, 0, 4, 0, 1, 2, 1, 1, 5, 1,
        1, 8, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 9, 0, 0, 1,
        0, 0, 0, 2, 3, 1, 8, 1, 5, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 5, 1, 7, 1, 1, 1, 9, 1, 4, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1])
soft_pseudo_label
tensor([[9.8308e-01, 3.0976e-05, 3.8406e-04,  ..., 1.0398e-04, 1.3833e-02,
         2.4748e-04],
        [9.8347e-01, 4.6617e-04, 6.7234e-04,  ..., 1.0913e-03, 8.0323e-03,
         7.0667e-04],
        [2.4679e-01, 6.9080e-01, 2.3200e-03,  ..., 1.7910e-03, 1.0813e-02,
         2.7984e-03],
        ...,
        [2.9560e-04, 9.5837e-01, 4.0089e-04,  ..., 5.3182e-03, 1.3055e-03,
         2.8887e-03],
        [9.9303e-01, 4.2462e-04, 1.5751e-04,  ..., 1.0847e-04, 3.6130e-03,
         1.1890e-04],
        [9.8755e-01, 1.9080e-04, 2.5800e-04,  ..., 1.3973e-04, 8.6955e-03,
         1.8583e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        7, 1, 1, 5, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        5, 9, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 5, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        2, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 7, 1, 1, 5, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 5, 9, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 5, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]
original label
tensor([0, 0, 1, 1, 0, 0, 6, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        7, 1, 1, 5, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 3, 1, 5, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        2, 1, 1, 1, 0, 0, 0, 1, 1, 9, 1, 6, 0, 0, 1, 0, 0, 1, 9, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0])
soft_pseudo_label
tensor([[9.5553e-01, 3.6170e-06, 7.6036e-04,  ..., 1.3209e-03, 3.3734e-02,
         2.0656e-03],
        [9.9359e-01, 2.9641e-05, 1.8407e-04,  ..., 8.5185e-05, 4.7196e-03,
         1.2131e-04],
        [1.1758e-03, 9.7566e-01, 2.1785e-03,  ..., 6.2536e-04, 5.6331e-04,
         9.1613e-04],
        ...,
        [5.7255e-05, 9.9812e-01, 7.5433e-06,  ..., 6.0731e-06, 6.6450e-05,
         2.7727e-05],
        [6.8659e-01, 8.4592e-03, 1.0456e-02,  ..., 8.6611e-02, 5.9353e-02,
         6.1416e-02],
        [6.7764e-01, 3.0783e-01, 2.4995e-03,  ..., 2.0661e-03, 8.7227e-04,
         1.3431e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 0, 1, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 5, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 7, 7, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 9, 1, 0, 1, 1, 1, 7, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1,
        7, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[0, 0, 1, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 5, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 7, 7, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 9, 1, 0, 1, 1, 1, 7, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 7, 1, 1, 0, 0, 1, 0, 0]
original label
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0,
        0, 0, 5, 1, 0, 0, 0, 1, 1, 3, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 4, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 9, 1, 0, 2, 9, 1, 9, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 1,
        7, 0, 1, 0, 0, 1, 8, 0])
soft_pseudo_label
tensor([[1.7013e-03, 9.6130e-01, 6.4321e-04,  ..., 6.8202e-04, 1.3577e-03,
         8.8866e-04],
        [9.7587e-01, 2.4302e-03, 8.3190e-04,  ..., 1.3049e-03, 9.7845e-03,
         1.0882e-03],
        [5.1703e-01, 4.7261e-01, 1.8484e-03,  ..., 2.5669e-03, 9.2671e-04,
         1.8403e-03],
        ...,
        [7.5970e-01, 6.9094e-02, 3.2953e-03,  ..., 1.5339e-03, 7.2057e-02,
         4.4300e-03],
        [9.2942e-01, 4.9274e-03, 4.4953e-03,  ..., 5.8031e-03, 2.0334e-02,
         6.3054e-03],
        [9.4819e-01, 6.4421e-03, 9.7213e-04,  ..., 4.4334e-04, 2.6429e-02,
         1.0604e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 9, 1, 0, 1,
        0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 9, 1, 1, 0, 0, 1, 7, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        9, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 9, 1, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 9, 1, 1, 0, 0, 1, 7, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 9, 1, 1, 0, 1, 0, 0, 0]
original label
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 4, 7, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1,
        0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 3, 0, 1, 0, 0, 1, 1, 8,
        1, 1, 6, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 7, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0])
soft_pseudo_label
tensor([[2.0908e-05, 9.9611e-01, 2.5868e-05,  ..., 1.8130e-05, 1.3561e-04,
         9.3246e-05],
        [8.5823e-01, 8.0611e-02, 4.0926e-03,  ..., 2.6244e-03, 2.0936e-02,
         5.6377e-03],
        [9.6642e-01, 1.9631e-02, 3.2866e-04,  ..., 1.6752e-03, 3.5620e-03,
         9.1547e-04],
        ...,
        [9.5475e-01, 1.2867e-03, 1.0289e-03,  ..., 6.9752e-04, 1.2184e-02,
         1.1006e-03],
        [9.8004e-01, 2.4572e-05, 1.1976e-03,  ..., 3.5203e-04, 1.3871e-02,
         5.5653e-04],
        [7.6218e-01, 1.0852e-01, 2.1206e-02,  ..., 4.0192e-03, 2.9792e-02,
         7.9076e-03]], device='cuda:0')
hard_pseudo_label
tensor([1, 0, 0, 0, 0, 9, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 9, 0, 0, 1, 1, 2, 0, 0,
        0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 9, 1, 0, 9, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 5, 1, 0, 0, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 0, 0, 0, 0, 9, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 9, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 9, 1, 0, 9, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 5, 1, 0, 0, 0, 0]
original label
tensor([1, 0, 0, 0, 0, 7, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 9, 0, 0, 1, 7, 2, 0, 0,
        0, 3, 4, 0, 2, 4, 1, 0, 0, 0, 1, 1, 4, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 9, 1, 0, 9, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 5, 1, 0, 1, 1, 8, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1])
soft_pseudo_label
tensor([[9.7156e-01, 2.8650e-04, 9.2575e-04,  ..., 8.9202e-04, 1.9091e-02,
         1.0666e-03],
        [6.5601e-04, 9.6853e-01, 1.6274e-04,  ..., 1.4418e-04, 1.2226e-03,
         8.0141e-04],
        [9.8305e-01, 9.9017e-05, 2.4103e-04,  ..., 5.6786e-04, 9.8468e-03,
         5.6124e-04],
        ...,
        [2.2610e-03, 9.4020e-01, 7.4560e-04,  ..., 5.7223e-04, 1.4316e-03,
         5.9852e-04],
        [9.7180e-02, 8.1686e-01, 2.0850e-03,  ..., 6.9781e-03, 1.2340e-02,
         4.4399e-03],
        [9.7581e-01, 2.1468e-04, 1.4528e-03,  ..., 2.5001e-04, 1.5035e-02,
         1.0608e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 1, 0, 0, 9, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 9, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 9, 1, 1, 1, 0, 0, 1, 0, 1, 0, 7, 0, 0, 0, 0, 5, 1, 1, 0, 1,
        1, 7, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
hard_pseudo_label
[0, 1, 0, 0, 9, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 9, 0, 0, 0, 0, 1, 1, 0, 1, 1, 9, 1, 1, 1, 0, 0, 1, 0, 1, 0, 7, 0, 0, 0, 0, 5, 1, 1, 0, 1, 1, 7, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]
original label
tensor([0, 1, 0, 0, 5, 1, 1, 8, 0, 1, 0, 0, 1, 6, 0, 0, 1, 1, 2, 0, 0, 0, 0, 8,
        1, 0, 1, 1, 4, 1, 5, 5, 0, 0, 1, 0, 1, 0, 9, 0, 1, 3, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 9, 0, 1, 1, 0, 1, 0, 0, 1, 1, 5, 0, 0, 1, 0, 7, 6,
        0, 0, 4, 0, 0, 0, 1, 0, 1, 0, 1, 1, 7, 8, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 8, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 8, 1, 0, 0])
soft_pseudo_label
tensor([[3.1819e-04, 9.9209e-01, 1.2806e-04,  ..., 2.6071e-04, 2.7216e-04,
         4.7278e-04],
        [2.5448e-03, 9.6393e-01, 2.7497e-04,  ..., 2.2181e-04, 2.1153e-03,
         7.3874e-04],
        [9.6524e-01, 4.0376e-04, 6.1088e-04,  ..., 4.0813e-04, 2.3929e-02,
         7.2120e-04],
        ...,
        [8.6854e-04, 9.3496e-01, 1.2272e-03,  ..., 1.2874e-03, 1.7752e-03,
         1.2000e-03],
        [9.7748e-01, 2.2669e-04, 5.2914e-04,  ..., 5.7158e-04, 1.4959e-02,
         6.5660e-04],
        [7.6683e-01, 2.2403e-01, 8.7365e-04,  ..., 5.7295e-04, 2.2677e-03,
         4.3716e-04]], device='cuda:0')
hard_pseudo_label
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 5, 2, 1, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 7, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 9, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 9, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0], device='cuda:0')
hard_pseudo_label
[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 5, 2, 1, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 7, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 9, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 9, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]
original label
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 8, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 8, 2, 1, 0, 0, 1, 1, 0, 1, 0, 8, 1, 0, 1, 0, 0, 1, 0, 7, 7,
        0, 0, 1, 0, 1, 9, 1, 0, 0, 0, 0, 1, 1, 9, 0, 1, 1, 0, 1, 0, 5, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 4, 1, 0, 1, 0, 0, 4, 1, 7, 1, 1, 5, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1])
[INFO] main.py:340 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:348 > [2-3] Start to train under online
[INFO] main.py:363 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42
[INFO] rainbow_memory.py:119 > Streamed samples: 800
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 9984
[INFO] rainbow_memory.py:127 > Train samples: 11284
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 1/1 | train_loss 1.9035 | train_acc 0.5122 | test_loss 1.1928 | test_acc 0.8070 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
dog           173
deer          156
airplane       26
cat            26
ship           25
automobile     22
frog           22
bird           17
horse          17
truck          16
Name: klass, dtype: int64
[INFO] main.py:379 > Train over memory
batch_size : 64 stream_batch_size : 22 memory_batch_size : 21
[INFO] rainbow_memory.py:119 > Streamed samples: 0
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 0
[INFO] rainbow_memory.py:127 > Train samples: 500
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 1/256 | train_loss 1.7302 | train_acc 0.5120 | test_loss 0.6386 | test_acc 0.8410 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 2/256 | train_loss 1.8109 | train_acc 0.4400 | test_loss 0.8276 | test_acc 0.7095 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 3/256 | train_loss 1.7320 | train_acc 0.4300 | test_loss 0.9215 | test_acc 0.6890 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 4/256 | train_loss 1.6662 | train_acc 0.4740 | test_loss 0.7706 | test_acc 0.7260 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 5/256 | train_loss 1.6512 | train_acc 0.4720 | test_loss 0.6889 | test_acc 0.7695 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 6/256 | train_loss 1.6766 | train_acc 0.4700 | test_loss 0.8353 | test_acc 0.7685 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 7/256 | train_loss 1.6105 | train_acc 0.5100 | test_loss 0.6330 | test_acc 0.8100 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 8/256 | train_loss 1.5318 | train_acc 0.5100 | test_loss 0.6510 | test_acc 0.7870 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 9/256 | train_loss 1.5942 | train_acc 0.4920 | test_loss 0.6228 | test_acc 0.8065 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 10/256 | train_loss 1.6055 | train_acc 0.4720 | test_loss 0.6953 | test_acc 0.8060 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 11/256 | train_loss 1.5560 | train_acc 0.5000 | test_loss 0.5762 | test_acc 0.8065 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 12/256 | train_loss 1.6000 | train_acc 0.4760 | test_loss 0.7255 | test_acc 0.7720 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 13/256 | train_loss 1.5875 | train_acc 0.4980 | test_loss 0.7549 | test_acc 0.7730 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 14/256 | train_loss 1.5232 | train_acc 0.5240 | test_loss 0.5955 | test_acc 0.8055 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 15/256 | train_loss 1.4616 | train_acc 0.5320 | test_loss 0.6199 | test_acc 0.8015 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 16/256 | train_loss 1.4441 | train_acc 0.5500 | test_loss 0.5817 | test_acc 0.8100 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 17/256 | train_loss 1.5613 | train_acc 0.5000 | test_loss 0.8614 | test_acc 0.7600 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 18/256 | train_loss 1.4848 | train_acc 0.5260 | test_loss 0.6546 | test_acc 0.7920 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 19/256 | train_loss 1.5450 | train_acc 0.5160 | test_loss 0.5841 | test_acc 0.7885 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 20/256 | train_loss 1.5516 | train_acc 0.5000 | test_loss 0.7577 | test_acc 0.6775 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 21/256 | train_loss 1.5826 | train_acc 0.4620 | test_loss 0.5929 | test_acc 0.7830 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 22/256 | train_loss 1.5339 | train_acc 0.5060 | test_loss 0.5133 | test_acc 0.8190 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 23/256 | train_loss 1.4150 | train_acc 0.5440 | test_loss 0.6183 | test_acc 0.8115 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 24/256 | train_loss 1.5081 | train_acc 0.5260 | test_loss 0.7333 | test_acc 0.7440 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 25/256 | train_loss 1.4301 | train_acc 0.5540 | test_loss 0.5773 | test_acc 0.8030 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 26/256 | train_loss 1.4750 | train_acc 0.5220 | test_loss 0.5177 | test_acc 0.8035 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 27/256 | train_loss 1.4240 | train_acc 0.5460 | test_loss 0.5352 | test_acc 0.8180 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 28/256 | train_loss 1.3475 | train_acc 0.5460 | test_loss 0.6882 | test_acc 0.7720 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 29/256 | train_loss 1.3503 | train_acc 0.5640 | test_loss 0.6218 | test_acc 0.7940 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 30/256 | train_loss 1.3115 | train_acc 0.5740 | test_loss 0.6088 | test_acc 0.8085 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 31/256 | train_loss 1.4414 | train_acc 0.5480 | test_loss 0.5965 | test_acc 0.8035 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 32/256 | train_loss 1.2963 | train_acc 0.5780 | test_loss 0.6008 | test_acc 0.8045 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 33/256 | train_loss 1.4678 | train_acc 0.5340 | test_loss 0.5542 | test_acc 0.7855 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 34/256 | train_loss 1.4952 | train_acc 0.5180 | test_loss 0.6509 | test_acc 0.8175 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 35/256 | train_loss 1.5698 | train_acc 0.4880 | test_loss 0.7379 | test_acc 0.7585 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 36/256 | train_loss 1.5135 | train_acc 0.5380 | test_loss 0.8123 | test_acc 0.7265 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 37/256 | train_loss 1.4474 | train_acc 0.5320 | test_loss 0.7474 | test_acc 0.7935 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 38/256 | train_loss 1.4721 | train_acc 0.5420 | test_loss 0.6284 | test_acc 0.8090 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 39/256 | train_loss 1.4812 | train_acc 0.5160 | test_loss 0.7408 | test_acc 0.7680 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 40/256 | train_loss 1.4854 | train_acc 0.4940 | test_loss 0.5559 | test_acc 0.8120 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 41/256 | train_loss 1.4938 | train_acc 0.5160 | test_loss 0.6753 | test_acc 0.7690 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 42/256 | train_loss 1.4104 | train_acc 0.5540 | test_loss 0.6184 | test_acc 0.8090 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 43/256 | train_loss 1.3882 | train_acc 0.5400 | test_loss 0.7648 | test_acc 0.7580 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 44/256 | train_loss 1.4490 | train_acc 0.5420 | test_loss 0.5963 | test_acc 0.7945 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 45/256 | train_loss 1.4732 | train_acc 0.5160 | test_loss 0.7297 | test_acc 0.7835 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 46/256 | train_loss 1.4255 | train_acc 0.5440 | test_loss 0.7069 | test_acc 0.7865 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 47/256 | train_loss 1.3789 | train_acc 0.5660 | test_loss 0.6388 | test_acc 0.7895 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 48/256 | train_loss 1.3632 | train_acc 0.5700 | test_loss 0.6128 | test_acc 0.8125 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 49/256 | train_loss 1.3528 | train_acc 0.5580 | test_loss 0.5339 | test_acc 0.8350 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 50/256 | train_loss 1.2767 | train_acc 0.6080 | test_loss 0.6150 | test_acc 0.8080 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 51/256 | train_loss 1.3965 | train_acc 0.5380 | test_loss 0.6664 | test_acc 0.8030 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 52/256 | train_loss 1.2508 | train_acc 0.5980 | test_loss 0.6526 | test_acc 0.7970 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 53/256 | train_loss 1.3823 | train_acc 0.5360 | test_loss 0.7757 | test_acc 0.7450 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 54/256 | train_loss 1.2646 | train_acc 0.6160 | test_loss 0.5179 | test_acc 0.8260 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 55/256 | train_loss 1.3663 | train_acc 0.5580 | test_loss 0.5112 | test_acc 0.8355 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 56/256 | train_loss 1.2964 | train_acc 0.5780 | test_loss 0.5225 | test_acc 0.8270 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 57/256 | train_loss 1.3101 | train_acc 0.5720 | test_loss 0.4858 | test_acc 0.8390 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 58/256 | train_loss 1.2121 | train_acc 0.6060 | test_loss 0.5685 | test_acc 0.8145 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 59/256 | train_loss 1.1924 | train_acc 0.6280 | test_loss 0.5430 | test_acc 0.8185 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 60/256 | train_loss 1.2609 | train_acc 0.5800 | test_loss 0.6075 | test_acc 0.7990 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 61/256 | train_loss 1.1465 | train_acc 0.6320 | test_loss 0.5538 | test_acc 0.8125 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 62/256 | train_loss 1.2267 | train_acc 0.6140 | test_loss 0.5843 | test_acc 0.8075 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 63/256 | train_loss 1.1706 | train_acc 0.6300 | test_loss 0.5527 | test_acc 0.8180 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 64/256 | train_loss 1.2167 | train_acc 0.5920 | test_loss 0.5657 | test_acc 0.8115 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 65/256 | train_loss 1.4992 | train_acc 0.5140 | test_loss 0.5382 | test_acc 0.8180 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 66/256 | train_loss 1.3740 | train_acc 0.5280 | test_loss 0.6881 | test_acc 0.7695 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 67/256 | train_loss 1.4677 | train_acc 0.5200 | test_loss 0.5065 | test_acc 0.8170 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 68/256 | train_loss 1.3809 | train_acc 0.5420 | test_loss 0.7812 | test_acc 0.7770 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 69/256 | train_loss 1.4854 | train_acc 0.5180 | test_loss 0.6912 | test_acc 0.8005 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 70/256 | train_loss 1.3492 | train_acc 0.5340 | test_loss 0.8400 | test_acc 0.7180 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 71/256 | train_loss 1.4571 | train_acc 0.5400 | test_loss 0.5928 | test_acc 0.8100 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 72/256 | train_loss 1.4062 | train_acc 0.5460 | test_loss 0.4828 | test_acc 0.8330 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 73/256 | train_loss 1.3865 | train_acc 0.5340 | test_loss 0.6593 | test_acc 0.7675 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 74/256 | train_loss 1.3432 | train_acc 0.5680 | test_loss 0.6518 | test_acc 0.7975 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 75/256 | train_loss 1.3944 | train_acc 0.5380 | test_loss 0.9350 | test_acc 0.6700 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 76/256 | train_loss 1.3607 | train_acc 0.5420 | test_loss 0.8815 | test_acc 0.7110 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 77/256 | train_loss 1.4091 | train_acc 0.5280 | test_loss 0.7907 | test_acc 0.7505 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 78/256 | train_loss 1.2822 | train_acc 0.5920 | test_loss 0.5303 | test_acc 0.8355 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 79/256 | train_loss 1.3620 | train_acc 0.5380 | test_loss 0.5456 | test_acc 0.8105 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 80/256 | train_loss 1.2090 | train_acc 0.6140 | test_loss 0.8179 | test_acc 0.7360 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 81/256 | train_loss 1.4601 | train_acc 0.5500 | test_loss 0.6935 | test_acc 0.7625 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 82/256 | train_loss 1.3218 | train_acc 0.5840 | test_loss 0.5872 | test_acc 0.8050 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 83/256 | train_loss 1.1555 | train_acc 0.6080 | test_loss 0.7897 | test_acc 0.7390 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 84/256 | train_loss 1.2650 | train_acc 0.6040 | test_loss 0.6308 | test_acc 0.8170 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 85/256 | train_loss 1.3246 | train_acc 0.5520 | test_loss 0.5926 | test_acc 0.8050 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 86/256 | train_loss 1.2413 | train_acc 0.5800 | test_loss 0.7222 | test_acc 0.7415 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 87/256 | train_loss 1.3399 | train_acc 0.5780 | test_loss 0.5557 | test_acc 0.8175 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 88/256 | train_loss 1.3255 | train_acc 0.5700 | test_loss 0.5485 | test_acc 0.8370 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 89/256 | train_loss 1.1887 | train_acc 0.6260 | test_loss 0.5826 | test_acc 0.8080 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 90/256 | train_loss 1.2730 | train_acc 0.5820 | test_loss 0.5630 | test_acc 0.8300 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 91/256 | train_loss 1.2016 | train_acc 0.6060 | test_loss 0.5269 | test_acc 0.8255 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 92/256 | train_loss 1.1744 | train_acc 0.5980 | test_loss 0.6992 | test_acc 0.7655 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 93/256 | train_loss 1.1565 | train_acc 0.6020 | test_loss 0.5845 | test_acc 0.8045 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 94/256 | train_loss 1.3429 | train_acc 0.5500 | test_loss 0.6214 | test_acc 0.8045 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 95/256 | train_loss 1.2475 | train_acc 0.5900 | test_loss 0.5333 | test_acc 0.8395 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 96/256 | train_loss 1.1930 | train_acc 0.6000 | test_loss 0.5518 | test_acc 0.8165 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 97/256 | train_loss 1.1090 | train_acc 0.6500 | test_loss 0.4435 | test_acc 0.8495 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 98/256 | train_loss 1.2225 | train_acc 0.5760 | test_loss 0.5610 | test_acc 0.8055 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 99/256 | train_loss 1.1378 | train_acc 0.6180 | test_loss 0.5550 | test_acc 0.8220 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 100/256 | train_loss 1.1227 | train_acc 0.6380 | test_loss 0.6701 | test_acc 0.7780 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 101/256 | train_loss 1.1757 | train_acc 0.6380 | test_loss 0.5979 | test_acc 0.8065 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 102/256 | train_loss 1.0717 | train_acc 0.6540 | test_loss 0.4894 | test_acc 0.8320 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 103/256 | train_loss 1.1211 | train_acc 0.6200 | test_loss 0.6929 | test_acc 0.7705 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 104/256 | train_loss 1.1184 | train_acc 0.6120 | test_loss 0.5242 | test_acc 0.8275 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 105/256 | train_loss 1.0527 | train_acc 0.6640 | test_loss 0.4966 | test_acc 0.8300 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 106/256 | train_loss 1.1000 | train_acc 0.6640 | test_loss 0.5193 | test_acc 0.8235 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 107/256 | train_loss 1.1117 | train_acc 0.6280 | test_loss 0.5644 | test_acc 0.8100 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 108/256 | train_loss 1.0794 | train_acc 0.6480 | test_loss 0.5798 | test_acc 0.8045 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 109/256 | train_loss 1.0739 | train_acc 0.6640 | test_loss 0.5917 | test_acc 0.7930 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 110/256 | train_loss 0.9461 | train_acc 0.6780 | test_loss 0.6025 | test_acc 0.7910 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 111/256 | train_loss 1.1374 | train_acc 0.6480 | test_loss 0.5924 | test_acc 0.7900 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 112/256 | train_loss 1.0886 | train_acc 0.6780 | test_loss 0.5994 | test_acc 0.7900 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 113/256 | train_loss 1.0324 | train_acc 0.6600 | test_loss 0.4636 | test_acc 0.8305 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 114/256 | train_loss 1.0722 | train_acc 0.6100 | test_loss 0.5403 | test_acc 0.8150 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 115/256 | train_loss 0.9680 | train_acc 0.6840 | test_loss 0.5092 | test_acc 0.8160 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 116/256 | train_loss 0.9119 | train_acc 0.6980 | test_loss 0.5146 | test_acc 0.8175 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 117/256 | train_loss 1.0451 | train_acc 0.6860 | test_loss 0.5004 | test_acc 0.8315 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 118/256 | train_loss 1.0475 | train_acc 0.6920 | test_loss 0.5007 | test_acc 0.8240 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 119/256 | train_loss 0.9029 | train_acc 0.7240 | test_loss 0.4946 | test_acc 0.8305 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 120/256 | train_loss 0.9619 | train_acc 0.7120 | test_loss 0.5381 | test_acc 0.8135 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 121/256 | train_loss 0.9954 | train_acc 0.6820 | test_loss 0.5613 | test_acc 0.8040 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 122/256 | train_loss 0.9230 | train_acc 0.7180 | test_loss 0.5484 | test_acc 0.8080 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 123/256 | train_loss 0.9924 | train_acc 0.7080 | test_loss 0.5959 | test_acc 0.7910 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 124/256 | train_loss 1.1443 | train_acc 0.6200 | test_loss 0.5797 | test_acc 0.7935 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 125/256 | train_loss 0.9918 | train_acc 0.6660 | test_loss 0.5613 | test_acc 0.7990 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 126/256 | train_loss 0.9198 | train_acc 0.7260 | test_loss 0.5492 | test_acc 0.8055 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 127/256 | train_loss 0.9036 | train_acc 0.7580 | test_loss 0.5493 | test_acc 0.8025 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 128/256 | train_loss 0.9720 | train_acc 0.7180 | test_loss 0.5525 | test_acc 0.8040 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 129/256 | train_loss 1.1393 | train_acc 0.6340 | test_loss 1.0886 | test_acc 0.6440 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 130/256 | train_loss 1.3385 | train_acc 0.5780 | test_loss 0.6136 | test_acc 0.7940 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 131/256 | train_loss 1.1559 | train_acc 0.6400 | test_loss 0.6159 | test_acc 0.8155 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 132/256 | train_loss 1.2419 | train_acc 0.6000 | test_loss 0.5396 | test_acc 0.8155 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 133/256 | train_loss 1.0685 | train_acc 0.6580 | test_loss 0.5296 | test_acc 0.8120 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 134/256 | train_loss 1.2952 | train_acc 0.5560 | test_loss 0.6224 | test_acc 0.8030 | lr 0.0498
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 135/256 | train_loss 1.2404 | train_acc 0.5820 | test_loss 0.5033 | test_acc 0.8410 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 136/256 | train_loss 1.2116 | train_acc 0.6100 | test_loss 0.7415 | test_acc 0.7350 | lr 0.0496
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 137/256 | train_loss 1.1926 | train_acc 0.6180 | test_loss 0.5869 | test_acc 0.8020 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 138/256 | train_loss 1.1481 | train_acc 0.6660 | test_loss 0.5386 | test_acc 0.8145 | lr 0.0494
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 139/256 | train_loss 1.0761 | train_acc 0.6640 | test_loss 0.6036 | test_acc 0.8025 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 140/256 | train_loss 1.2052 | train_acc 0.5980 | test_loss 0.5760 | test_acc 0.8055 | lr 0.0491
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 141/256 | train_loss 1.2434 | train_acc 0.5920 | test_loss 0.6709 | test_acc 0.7770 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 142/256 | train_loss 1.1439 | train_acc 0.6160 | test_loss 0.6747 | test_acc 0.7770 | lr 0.0488
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 143/256 | train_loss 1.1742 | train_acc 0.6260 | test_loss 0.7358 | test_acc 0.7600 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 144/256 | train_loss 1.2186 | train_acc 0.6000 | test_loss 0.6453 | test_acc 0.7780 | lr 0.0483
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 145/256 | train_loss 1.0873 | train_acc 0.6620 | test_loss 0.8468 | test_acc 0.7295 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 146/256 | train_loss 1.2907 | train_acc 0.5660 | test_loss 0.5720 | test_acc 0.8090 | lr 0.0479
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 147/256 | train_loss 1.2461 | train_acc 0.5860 | test_loss 0.7671 | test_acc 0.7580 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 148/256 | train_loss 1.1237 | train_acc 0.6520 | test_loss 0.6022 | test_acc 0.7865 | lr 0.0474
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 149/256 | train_loss 1.0374 | train_acc 0.6340 | test_loss 0.7991 | test_acc 0.7450 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 150/256 | train_loss 1.1416 | train_acc 0.6220 | test_loss 0.8449 | test_acc 0.7205 | lr 0.0468
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 151/256 | train_loss 1.3005 | train_acc 0.5780 | test_loss 0.6225 | test_acc 0.7980 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 152/256 | train_loss 0.9229 | train_acc 0.6700 | test_loss 0.7049 | test_acc 0.7650 | lr 0.0462
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 153/256 | train_loss 1.0087 | train_acc 0.6720 | test_loss 0.4990 | test_acc 0.8230 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 154/256 | train_loss 1.1297 | train_acc 0.6600 | test_loss 0.6415 | test_acc 0.7775 | lr 0.0455
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 155/256 | train_loss 1.0933 | train_acc 0.6520 | test_loss 0.8423 | test_acc 0.6865 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 156/256 | train_loss 1.0688 | train_acc 0.6560 | test_loss 0.6232 | test_acc 0.7865 | lr 0.0448
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 157/256 | train_loss 1.1456 | train_acc 0.6140 | test_loss 0.8150 | test_acc 0.7475 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 158/256 | train_loss 1.0314 | train_acc 0.6620 | test_loss 0.7576 | test_acc 0.7545 | lr 0.0440
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 159/256 | train_loss 0.9336 | train_acc 0.6980 | test_loss 0.6472 | test_acc 0.7850 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 160/256 | train_loss 1.2164 | train_acc 0.6500 | test_loss 0.4845 | test_acc 0.8400 | lr 0.0432
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 161/256 | train_loss 1.0137 | train_acc 0.6680 | test_loss 0.6204 | test_acc 0.7980 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 162/256 | train_loss 1.1514 | train_acc 0.6660 | test_loss 0.7678 | test_acc 0.7455 | lr 0.0423
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 163/256 | train_loss 1.0590 | train_acc 0.6820 | test_loss 0.5449 | test_acc 0.7970 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 164/256 | train_loss 1.0173 | train_acc 0.6780 | test_loss 0.5117 | test_acc 0.8210 | lr 0.0414
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 165/256 | train_loss 1.0546 | train_acc 0.6860 | test_loss 0.4701 | test_acc 0.8410 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 166/256 | train_loss 1.0554 | train_acc 0.6460 | test_loss 0.5880 | test_acc 0.7915 | lr 0.0405
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 167/256 | train_loss 1.0822 | train_acc 0.6800 | test_loss 0.5679 | test_acc 0.8045 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 168/256 | train_loss 1.1227 | train_acc 0.6480 | test_loss 0.5200 | test_acc 0.8260 | lr 0.0395
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 169/256 | train_loss 1.0017 | train_acc 0.6840 | test_loss 0.4721 | test_acc 0.8290 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 170/256 | train_loss 1.0547 | train_acc 0.6300 | test_loss 0.6648 | test_acc 0.7795 | lr 0.0385
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 171/256 | train_loss 0.9467 | train_acc 0.7180 | test_loss 0.6241 | test_acc 0.7920 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 172/256 | train_loss 1.0342 | train_acc 0.6720 | test_loss 0.8306 | test_acc 0.7310 | lr 0.0374
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 173/256 | train_loss 1.0701 | train_acc 0.6680 | test_loss 0.7556 | test_acc 0.7340 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 174/256 | train_loss 1.0821 | train_acc 0.6740 | test_loss 0.6377 | test_acc 0.7815 | lr 0.0364
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 175/256 | train_loss 0.9598 | train_acc 0.7080 | test_loss 0.6784 | test_acc 0.7830 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 176/256 | train_loss 0.9416 | train_acc 0.7080 | test_loss 0.6807 | test_acc 0.7735 | lr 0.0353
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 177/256 | train_loss 1.1395 | train_acc 0.6320 | test_loss 0.4421 | test_acc 0.8425 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 178/256 | train_loss 1.0157 | train_acc 0.6820 | test_loss 0.6428 | test_acc 0.7910 | lr 0.0342
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 179/256 | train_loss 0.9736 | train_acc 0.6520 | test_loss 0.8825 | test_acc 0.7085 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 180/256 | train_loss 0.9483 | train_acc 0.7260 | test_loss 0.5062 | test_acc 0.8240 | lr 0.0330
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 181/256 | train_loss 0.9164 | train_acc 0.7200 | test_loss 0.6351 | test_acc 0.7820 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 182/256 | train_loss 0.7914 | train_acc 0.7840 | test_loss 0.5542 | test_acc 0.8150 | lr 0.0319
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 183/256 | train_loss 1.0617 | train_acc 0.6380 | test_loss 0.7688 | test_acc 0.7415 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 184/256 | train_loss 0.8090 | train_acc 0.7500 | test_loss 0.7659 | test_acc 0.7385 | lr 0.0307
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 185/256 | train_loss 0.9929 | train_acc 0.6680 | test_loss 0.5784 | test_acc 0.8075 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 186/256 | train_loss 0.9117 | train_acc 0.7500 | test_loss 0.7082 | test_acc 0.7515 | lr 0.0295
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 187/256 | train_loss 0.9894 | train_acc 0.7180 | test_loss 0.5555 | test_acc 0.8020 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 188/256 | train_loss 0.8367 | train_acc 0.7300 | test_loss 0.6501 | test_acc 0.7905 | lr 0.0283
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 189/256 | train_loss 0.9613 | train_acc 0.6700 | test_loss 0.5552 | test_acc 0.8065 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 190/256 | train_loss 0.8918 | train_acc 0.7240 | test_loss 0.6522 | test_acc 0.7790 | lr 0.0271
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 191/256 | train_loss 0.9302 | train_acc 0.7180 | test_loss 1.1065 | test_acc 0.6305 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 192/256 | train_loss 0.7407 | train_acc 0.7580 | test_loss 0.4071 | test_acc 0.8580 | lr 0.0259
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 193/256 | train_loss 0.7942 | train_acc 0.7620 | test_loss 0.5296 | test_acc 0.8220 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 194/256 | train_loss 0.8639 | train_acc 0.7040 | test_loss 0.5588 | test_acc 0.8080 | lr 0.0246
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 195/256 | train_loss 0.7726 | train_acc 0.7780 | test_loss 0.5041 | test_acc 0.8230 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 196/256 | train_loss 0.7015 | train_acc 0.8000 | test_loss 0.4913 | test_acc 0.8360 | lr 0.0234
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 197/256 | train_loss 0.9295 | train_acc 0.7100 | test_loss 0.8711 | test_acc 0.7250 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 198/256 | train_loss 0.9586 | train_acc 0.7100 | test_loss 0.6227 | test_acc 0.7900 | lr 0.0222
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 199/256 | train_loss 0.8647 | train_acc 0.7060 | test_loss 0.4528 | test_acc 0.8505 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 200/256 | train_loss 0.8108 | train_acc 0.7740 | test_loss 0.7393 | test_acc 0.7560 | lr 0.0210
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 201/256 | train_loss 0.8276 | train_acc 0.7460 | test_loss 0.4928 | test_acc 0.8355 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 202/256 | train_loss 0.8228 | train_acc 0.7460 | test_loss 0.4890 | test_acc 0.8220 | lr 0.0198
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 203/256 | train_loss 0.9350 | train_acc 0.7160 | test_loss 0.5465 | test_acc 0.8090 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 204/256 | train_loss 0.8026 | train_acc 0.7680 | test_loss 0.6091 | test_acc 0.7990 | lr 0.0186
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 205/256 | train_loss 1.1212 | train_acc 0.6120 | test_loss 0.5248 | test_acc 0.8215 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 206/256 | train_loss 0.7947 | train_acc 0.7860 | test_loss 0.5576 | test_acc 0.8140 | lr 0.0175
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 207/256 | train_loss 0.7471 | train_acc 0.7720 | test_loss 0.5078 | test_acc 0.8225 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 208/256 | train_loss 0.7089 | train_acc 0.7980 | test_loss 0.5547 | test_acc 0.8095 | lr 0.0163
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 209/256 | train_loss 0.9080 | train_acc 0.7200 | test_loss 0.5563 | test_acc 0.8185 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 210/256 | train_loss 0.6886 | train_acc 0.8160 | test_loss 0.4389 | test_acc 0.8565 | lr 0.0152
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 211/256 | train_loss 0.7251 | train_acc 0.7840 | test_loss 0.5561 | test_acc 0.8080 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 212/256 | train_loss 0.6666 | train_acc 0.7920 | test_loss 0.6419 | test_acc 0.7780 | lr 0.0141
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 213/256 | train_loss 0.7237 | train_acc 0.8000 | test_loss 0.5319 | test_acc 0.8155 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 214/256 | train_loss 0.7160 | train_acc 0.7900 | test_loss 0.5709 | test_acc 0.8060 | lr 0.0131
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 215/256 | train_loss 0.8781 | train_acc 0.7640 | test_loss 0.5782 | test_acc 0.8030 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 216/256 | train_loss 0.6883 | train_acc 0.8380 | test_loss 0.5848 | test_acc 0.8030 | lr 0.0120
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 217/256 | train_loss 0.6175 | train_acc 0.8060 | test_loss 0.5432 | test_acc 0.8195 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 218/256 | train_loss 0.8285 | train_acc 0.7060 | test_loss 0.6312 | test_acc 0.7890 | lr 0.0110
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 219/256 | train_loss 0.5972 | train_acc 0.8280 | test_loss 0.4837 | test_acc 0.8295 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 220/256 | train_loss 0.7621 | train_acc 0.7600 | test_loss 0.4673 | test_acc 0.8415 | lr 0.0100
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 221/256 | train_loss 0.8708 | train_acc 0.7360 | test_loss 0.4861 | test_acc 0.8255 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 222/256 | train_loss 0.7570 | train_acc 0.7700 | test_loss 0.4445 | test_acc 0.8415 | lr 0.0091
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 223/256 | train_loss 0.8207 | train_acc 0.7180 | test_loss 0.5027 | test_acc 0.8260 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 224/256 | train_loss 0.7291 | train_acc 0.8040 | test_loss 0.5350 | test_acc 0.8185 | lr 0.0082
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 225/256 | train_loss 0.8410 | train_acc 0.7760 | test_loss 0.5278 | test_acc 0.8185 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 226/256 | train_loss 0.7729 | train_acc 0.7700 | test_loss 0.4758 | test_acc 0.8385 | lr 0.0073
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 227/256 | train_loss 0.6247 | train_acc 0.8440 | test_loss 0.4233 | test_acc 0.8590 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 228/256 | train_loss 0.7281 | train_acc 0.8080 | test_loss 0.5989 | test_acc 0.8025 | lr 0.0065
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 229/256 | train_loss 0.6758 | train_acc 0.8020 | test_loss 0.5591 | test_acc 0.8125 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 230/256 | train_loss 0.6956 | train_acc 0.8020 | test_loss 0.5064 | test_acc 0.8275 | lr 0.0057
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 231/256 | train_loss 0.6459 | train_acc 0.8200 | test_loss 0.4839 | test_acc 0.8335 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 232/256 | train_loss 0.7758 | train_acc 0.7720 | test_loss 0.5007 | test_acc 0.8285 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 233/256 | train_loss 0.6681 | train_acc 0.7880 | test_loss 0.5250 | test_acc 0.8215 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 234/256 | train_loss 0.6675 | train_acc 0.8400 | test_loss 0.4965 | test_acc 0.8250 | lr 0.0043
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 235/256 | train_loss 0.8273 | train_acc 0.8020 | test_loss 0.5271 | test_acc 0.8190 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 236/256 | train_loss 0.8139 | train_acc 0.7720 | test_loss 0.4735 | test_acc 0.8380 | lr 0.0037
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 237/256 | train_loss 0.9219 | train_acc 0.7260 | test_loss 0.4406 | test_acc 0.8515 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 238/256 | train_loss 0.6887 | train_acc 0.7960 | test_loss 0.4718 | test_acc 0.8355 | lr 0.0031
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 239/256 | train_loss 0.7840 | train_acc 0.7720 | test_loss 0.4695 | test_acc 0.8420 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 240/256 | train_loss 0.6108 | train_acc 0.8160 | test_loss 0.4562 | test_acc 0.8390 | lr 0.0026
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 241/256 | train_loss 0.7844 | train_acc 0.7380 | test_loss 0.4803 | test_acc 0.8365 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 242/256 | train_loss 0.6024 | train_acc 0.8420 | test_loss 0.4781 | test_acc 0.8365 | lr 0.0022
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 243/256 | train_loss 0.7849 | train_acc 0.7960 | test_loss 0.4949 | test_acc 0.8340 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 244/256 | train_loss 0.5083 | train_acc 0.8640 | test_loss 0.4887 | test_acc 0.8365 | lr 0.0017
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 245/256 | train_loss 0.4784 | train_acc 0.8900 | test_loss 0.4866 | test_acc 0.8415 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 246/256 | train_loss 0.7543 | train_acc 0.7060 | test_loss 0.4710 | test_acc 0.8445 | lr 0.0014
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 247/256 | train_loss 0.7315 | train_acc 0.8140 | test_loss 0.5077 | test_acc 0.8285 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 248/256 | train_loss 0.8713 | train_acc 0.7500 | test_loss 0.4792 | test_acc 0.8405 | lr 0.0011
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 249/256 | train_loss 0.6008 | train_acc 0.8440 | test_loss 0.4975 | test_acc 0.8275 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 250/256 | train_loss 0.6344 | train_acc 0.8360 | test_loss 0.4702 | test_acc 0.8405 | lr 0.0009
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 251/256 | train_loss 0.7000 | train_acc 0.7860 | test_loss 0.4697 | test_acc 0.8430 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 252/256 | train_loss 0.7627 | train_acc 0.7500 | test_loss 0.4569 | test_acc 0.8445 | lr 0.0007
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 253/256 | train_loss 0.6081 | train_acc 0.8080 | test_loss 0.4859 | test_acc 0.8365 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 254/256 | train_loss 0.7960 | train_acc 0.7860 | test_loss 0.4750 | test_acc 0.8410 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 255/256 | train_loss 0.8230 | train_acc 0.7620 | test_loss 0.4822 | test_acc 0.8375 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 1 | Epoch 256/256 | train_loss 0.6650 | train_acc 0.7900 | test_loss 0.4794 | test_acc 0.8380 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (1)
[INFO] main.py:389 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (1)
[INFO] main.py:396 > [2-5] Report task result

##################################################
# Task 2 iteration
##################################################

[INFO] main.py:308 > [2-1] Prepare a datalist for the current task
meta_pseudo_init
total : 5000  current step :  0
total : 5000  current step :  1
total : 5000  current step :  2
total : 5000  current step :  3
total : 5000  current step :  4
total : 5000  current step :  5
total : 5000  current step :  6
total : 5000  current step :  7
total : 5000  current step :  8
total : 5000  current step :  9
total : 5000  current step :  10
total : 5000  current step :  11
total : 5000  current step :  12
total : 5000  current step :  13
total : 5000  current step :  14
total : 5000  current step :  15
total : 5000  current step :  16
total : 5000  current step :  17
total : 5000  current step :  18
total : 5000  current step :  19
total : 5000  current step :  20
total : 5000  current step :  21
total : 5000  current step :  22
total : 5000  current step :  23
total : 5000  current step :  24
total : 5000  current step :  25
total : 5000  current step :  26
total : 5000  current step :  27
total : 5000  current step :  28
total : 5000  current step :  29
total : 5000  current step :  30
total : 5000  current step :  31
total : 5000  current step :  32
total : 5000  current step :  33
total : 5000  current step :  34
total : 5000  current step :  35
total : 5000  current step :  36
total : 5000  current step :  37
total : 5000  current step :  38
total : 5000  current step :  39
total : 5000  current step :  40
total : 5000  current step :  41
total : 5000  current step :  42
total : 5000  current step :  43
total : 5000  current step :  44
total : 5000  current step :  45
total : 5000  current step :  46
total : 5000  current step :  47
total : 5000  current step :  48
total : 5000  current step :  49
total : 5000  current step :  50
total : 5000  current step :  51
total : 5000  current step :  52
total : 5000  current step :  53
total : 5000  current step :  54
total : 5000  current step :  55
total : 5000  current step :  56
total : 5000  current step :  57
total : 5000  current step :  58
total : 5000  current step :  59
total : 5000  current step :  60
total : 5000  current step :  61
total : 5000  current step :  62
total : 5000  current step :  63
total : 5000  current step :  64
total : 5000  current step :  65
total : 5000  current step :  66
total : 5000  current step :  67
total : 5000  current step :  68
total : 5000  current step :  69
total : 5000  current step :  70
total : 5000  current step :  71
total : 5000  current step :  72
total : 5000  current step :  73
total : 5000  current step :  74
total : 5000  current step :  75
total : 5000  current step :  76
total : 5000  current step :  77
total : 5000  current step :  78
total : 5000  current step :  79
total : 5000  current step :  80
total : 5000  current step :  81
total : 5000  current step :  82
total : 5000  current step :  83
total : 5000  current step :  84
total : 5000  current step :  85
total : 5000  current step :  86
total : 5000  current step :  87
total : 5000  current step :  88
total : 5000  current step :  89
total : 5000  current step :  90
total : 5000  current step :  91
total : 5000  current step :  92
total : 5000  current step :  93
total : 5000  current step :  94
total : 5000  current step :  95
total : 5000  current step :  96
total : 5000  current step :  97
total : 5000  current step :  98
total : 5000  current step :  99
total : 5000  current step :  100
total : 5000  current step :  101
total : 5000  current step :  102
total : 5000  current step :  103
total : 5000  current step :  104
total : 5000  current step :  105
total : 5000  current step :  106
total : 5000  current step :  107
total : 5000  current step :  108
total : 5000  current step :  109
total : 5000  current step :  110
total : 5000  current step :  111
total : 5000  current step :  112
total : 5000  current step :  113
total : 5000  current step :  114
total : 5000  current step :  115
total : 5000  current step :  116
total : 5000  current step :  117
total : 5000  current step :  118
total : 5000  current step :  119
total : 5000  current step :  120
total : 5000  current step :  121
total : 5000  current step :  122
total : 5000  current step :  123
total : 5000  current step :  124
total : 5000  current step :  125
total : 5000  current step :  126
total : 5000  current step :  127
total : 5000  current step :  128
total : 5000  current step :  129
total : 5000  current step :  130
total : 5000  current step :  131
total : 5000  current step :  132
total : 5000  current step :  133
total : 5000  current step :  134
total : 5000  current step :  135
total : 5000  current step :  136
total : 5000  current step :  137
total : 5000  current step :  138
total : 5000  current step :  139
total : 5000  current step :  140
total : 5000  current step :  141
total : 5000  current step :  142
total : 5000  current step :  143
total : 5000  current step :  144
total : 5000  current step :  145
total : 5000  current step :  146
total : 5000  current step :  147
total : 5000  current step :  148
total : 5000  current step :  149
total : 5000  current step :  150
total : 5000  current step :  151
total : 5000  current step :  152
total : 5000  current step :  153
total : 5000  current step :  154
total : 5000  current step :  155
total : 5000  current step :  156
total : 5000  current step :  157
total : 5000  current step :  158
total : 5000  current step :  159
total : 5000  current step :  160
total : 5000  current step :  161
total : 5000  current step :  162
total : 5000  current step :  163
total : 5000  current step :  164
total : 5000  current step :  165
total : 5000  current step :  166
total : 5000  current step :  167
total : 5000  current step :  168
total : 5000  current step :  169
total : 5000  current step :  170
total : 5000  current step :  171
total : 5000  current step :  172
total : 5000  current step :  173
total : 5000  current step :  174
total : 5000  current step :  175
total : 5000  current step :  176
total : 5000  current step :  177
total : 5000  current step :  178
total : 5000  current step :  179
total : 5000  current step :  180
total : 5000  current step :  181
total : 5000  current step :  182
total : 5000  current step :  183
total : 5000  current step :  184
total : 5000  current step :  185
total : 5000  current step :  186
total : 5000  current step :  187
total : 5000  current step :  188
total : 5000  current step :  189
total : 5000  current step :  190
total : 5000  current step :  191
total : 5000  current step :  192
total : 5000  current step :  193
total : 5000  current step :  194
total : 5000  current step :  195
total : 5000  current step :  196
total : 5000  current step :  197
total : 5000  current step :  198
total : 5000  current step :  199
total : 5000  current step :  200
total : 5000  current step :  201
total : 5000  current step :  202
total : 5000  current step :  203
total : 5000  current step :  204
total : 5000  current step :  205
total : 5000  current step :  206
total : 5000  current step :  207
total : 5000  current step :  208
total : 5000  current step :  209
total : 5000  current step :  210
total : 5000  current step :  211
total : 5000  current step :  212
total : 5000  current step :  213
total : 5000  current step :  214
total : 5000  current step :  215
total : 5000  current step :  216
total : 5000  current step :  217
total : 5000  current step :  218
total : 5000  current step :  219
total : 5000  current step :  220
total : 5000  current step :  221
total : 5000  current step :  222
total : 5000  current step :  223
total : 5000  current step :  224
total : 5000  current step :  225
total : 5000  current step :  226
total : 5000  current step :  227
total : 5000  current step :  228
total : 5000  current step :  229
total : 5000  current step :  230
total : 5000  current step :  231
total : 5000  current step :  232
total : 5000  current step :  233
total : 5000  current step :  234
total : 5000  current step :  235
total : 5000  current step :  236
total : 5000  current step :  237
total : 5000  current step :  238
total : 5000  current step :  239
total : 5000  current step :  240
total : 5000  current step :  241
total : 5000  current step :  242
total : 5000  current step :  243
total : 5000  current step :  244
total : 5000  current step :  245
total : 5000  current step :  246
total : 5000  current step :  247
total : 5000  current step :  248
total : 5000  current step :  249
total : 5000  current step :  250
total : 5000  current step :  251
total : 5000  current step :  252
total : 5000  current step :  253
total : 5000  current step :  254
total : 5000  current step :  255
total : 5000  current step :  256
total : 5000  current step :  257
total : 5000  current step :  258
total : 5000  current step :  259
total : 5000  current step :  260
total : 5000  current step :  261
total : 5000  current step :  262
total : 5000  current step :  263
total : 5000  current step :  264
total : 5000  current step :  265
total : 5000  current step :  266
total : 5000  current step :  267
total : 5000  current step :  268
total : 5000  current step :  269
total : 5000  current step :  270
total : 5000  current step :  271
total : 5000  current step :  272
total : 5000  current step :  273
total : 5000  current step :  274
total : 5000  current step :  275
total : 5000  current step :  276
total : 5000  current step :  277
total : 5000  current step :  278
total : 5000  current step :  279
total : 5000  current step :  280
total : 5000  current step :  281
total : 5000  current step :  282
total : 5000  current step :  283
total : 5000  current step :  284
total : 5000  current step :  285
total : 5000  current step :  286
total : 5000  current step :  287
total : 5000  current step :  288
total : 5000  current step :  289
total : 5000  current step :  290
total : 5000  current step :  291
total : 5000  current step :  292
total : 5000  current step :  293
total : 5000  current step :  294
total : 5000  current step :  295
total : 5000  current step :  296
total : 5000  current step :  297
total : 5000  current step :  298
total : 5000  current step :  299
total : 5000  current step :  300
total : 5000  current step :  301
total : 5000  current step :  302
total : 5000  current step :  303
total : 5000  current step :  304
total : 5000  current step :  305
total : 5000  current step :  306
total : 5000  current step :  307
total : 5000  current step :  308
total : 5000  current step :  309
total : 5000  current step :  310
total : 5000  current step :  311
total : 5000  current step :  312
total : 5000  current step :  313
total : 5000  current step :  314
total : 5000  current step :  315
total : 5000  current step :  316
total : 5000  current step :  317
total : 5000  current step :  318
total : 5000  current step :  319
total : 5000  current step :  320
total : 5000  current step :  321
total : 5000  current step :  322
total : 5000  current step :  323
total : 5000  current step :  324
total : 5000  current step :  325
total : 5000  current step :  326
total : 5000  current step :  327
total : 5000  current step :  328
total : 5000  current step :  329
total : 5000  current step :  330
total : 5000  current step :  331
total : 5000  current step :  332
total : 5000  current step :  333
total : 5000  current step :  334
total : 5000  current step :  335
total : 5000  current step :  336
total : 5000  current step :  337
total : 5000  current step :  338
total : 5000  current step :  339
total : 5000  current step :  340
total : 5000  current step :  341
total : 5000  current step :  342
total : 5000  current step :  343
total : 5000  current step :  344
total : 5000  current step :  345
total : 5000  current step :  346
total : 5000  current step :  347
total : 5000  current step :  348
total : 5000  current step :  349
total : 5000  current step :  350
total : 5000  current step :  351
total : 5000  current step :  352
total : 5000  current step :  353
total : 5000  current step :  354
total : 5000  current step :  355
total : 5000  current step :  356
total : 5000  current step :  357
total : 5000  current step :  358
total : 5000  current step :  359
total : 5000  current step :  360
total : 5000  current step :  361
total : 5000  current step :  362
total : 5000  current step :  363
total : 5000  current step :  364
total : 5000  current step :  365
total : 5000  current step :  366
total : 5000  current step :  367
total : 5000  current step :  368
total : 5000  current step :  369
total : 5000  current step :  370
total : 5000  current step :  371
total : 5000  current step :  372
total : 5000  current step :  373
total : 5000  current step :  374
total : 5000  current step :  375
total : 5000  current step :  376
total : 5000  current step :  377
total : 5000  current step :  378
total : 5000  current step :  379
total : 5000  current step :  380
total : 5000  current step :  381
total : 5000  current step :  382
total : 5000  current step :  383
total : 5000  current step :  384
total : 5000  current step :  385
total : 5000  current step :  386
total : 5000  current step :  387
total : 5000  current step :  388
total : 5000  current step :  389
total : 5000  current step :  390
total : 5000  current step :  391
total : 5000  current step :  392
total : 5000  current step :  393
total : 5000  current step :  394
total : 5000  current step :  395
total : 5000  current step :  396
total : 5000  current step :  397
total : 5000  current step :  398
total : 5000  current step :  399
total : 5000  current step :  400
total : 5000  current step :  401
total : 5000  current step :  402
total : 5000  current step :  403
total : 5000  current step :  404
total : 5000  current step :  405
total : 5000  current step :  406
total : 5000  current step :  407
total : 5000  current step :  408
total : 5000  current step :  409
total : 5000  current step :  410
total : 5000  current step :  411
total : 5000  current step :  412
total : 5000  current step :  413
total : 5000  current step :  414
total : 5000  current step :  415
total : 5000  current step :  416
total : 5000  current step :  417
total : 5000  current step :  418
total : 5000  current step :  419
total : 5000  current step :  420
total : 5000  current step :  421
total : 5000  current step :  422
total : 5000  current step :  423
total : 5000  current step :  424
total : 5000  current step :  425
total : 5000  current step :  426
total : 5000  current step :  427
total : 5000  current step :  428
total : 5000  current step :  429
total : 5000  current step :  430
total : 5000  current step :  431
total : 5000  current step :  432
total : 5000  current step :  433
total : 5000  current step :  434
total : 5000  current step :  435
total : 5000  current step :  436
total : 5000  current step :  437
total : 5000  current step :  438
total : 5000  current step :  439
total : 5000  current step :  440
total : 5000  current step :  441
total : 5000  current step :  442
total : 5000  current step :  443
total : 5000  current step :  444
total : 5000  current step :  445
total : 5000  current step :  446
total : 5000  current step :  447
total : 5000  current step :  448
total : 5000  current step :  449
total : 5000  current step :  450
total : 5000  current step :  451
total : 5000  current step :  452
total : 5000  current step :  453
total : 5000  current step :  454
total : 5000  current step :  455
total : 5000  current step :  456
total : 5000  current step :  457
total : 5000  current step :  458
total : 5000  current step :  459
total : 5000  current step :  460
total : 5000  current step :  461
total : 5000  current step :  462
total : 5000  current step :  463
total : 5000  current step :  464
total : 5000  current step :  465
total : 5000  current step :  466
total : 5000  current step :  467
total : 5000  current step :  468
total : 5000  current step :  469
total : 5000  current step :  470
total : 5000  current step :  471
total : 5000  current step :  472
total : 5000  current step :  473
total : 5000  current step :  474
total : 5000  current step :  475
total : 5000  current step :  476
total : 5000  current step :  477
total : 5000  current step :  478
total : 5000  current step :  479
total : 5000  current step :  480
total : 5000  current step :  481
total : 5000  current step :  482
total : 5000  current step :  483
total : 5000  current step :  484
total : 5000  current step :  485
total : 5000  current step :  486
total : 5000  current step :  487
total : 5000  current step :  488
total : 5000  current step :  489
total : 5000  current step :  490
total : 5000  current step :  491
total : 5000  current step :  492
total : 5000  current step :  493
total : 5000  current step :  494
total : 5000  current step :  495
total : 5000  current step :  496
total : 5000  current step :  497
total : 5000  current step :  498
total : 5000  current step :  499
total : 5000  current step :  500
total : 5000  current step :  501
total : 5000  current step :  502
total : 5000  current step :  503
total : 5000  current step :  504
total : 5000  current step :  505
total : 5000  current step :  506
total : 5000  current step :  507
total : 5000  current step :  508
total : 5000  current step :  509
total : 5000  current step :  510
total : 5000  current step :  511
total : 5000  current step :  512
total : 5000  current step :  513
total : 5000  current step :  514
total : 5000  current step :  515
total : 5000  current step :  516
total : 5000  current step :  517
total : 5000  current step :  518
total : 5000  current step :  519
total : 5000  current step :  520
total : 5000  current step :  521
total : 5000  current step :  522
total : 5000  current step :  523
total : 5000  current step :  524
total : 5000  current step :  525
total : 5000  current step :  526
total : 5000  current step :  527
total : 5000  current step :  528
total : 5000  current step :  529
total : 5000  current step :  530
total : 5000  current step :  531
total : 5000  current step :  532
total : 5000  current step :  533
total : 5000  current step :  534
total : 5000  current step :  535
total : 5000  current step :  536
total : 5000  current step :  537
total : 5000  current step :  538
total : 5000  current step :  539
total : 5000  current step :  540
total : 5000  current step :  541
total : 5000  current step :  542
total : 5000  current step :  543
total : 5000  current step :  544
total : 5000  current step :  545
total : 5000  current step :  546
total : 5000  current step :  547
total : 5000  current step :  548
total : 5000  current step :  549
total : 5000  current step :  550
total : 5000  current step :  551
total : 5000  current step :  552
total : 5000  current step :  553
total : 5000  current step :  554
total : 5000  current step :  555
total : 5000  current step :  556
total : 5000  current step :  557
total : 5000  current step :  558
total : 5000  current step :  559
total : 5000  current step :  560
total : 5000  current step :  561
total : 5000  current step :  562
total : 5000  current step :  563
total : 5000  current step :  564
total : 5000  current step :  565
total : 5000  current step :  566
total : 5000  current step :  567
total : 5000  current step :  568
total : 5000  current step :  569
total : 5000  current step :  570
total : 5000  current step :  571
total : 5000  current step :  572
total : 5000  current step :  573
total : 5000  current step :  574
total : 5000  current step :  575
total : 5000  current step :  576
total : 5000  current step :  577
total : 5000  current step :  578
total : 5000  current step :  579
total : 5000  current step :  580
total : 5000  current step :  581
total : 5000  current step :  582
total : 5000  current step :  583
total : 5000  current step :  584
total : 5000  current step :  585
total : 5000  current step :  586
total : 5000  current step :  587
total : 5000  current step :  588
total : 5000  current step :  589
total : 5000  current step :  590
total : 5000  current step :  591
total : 5000  current step :  592
total : 5000  current step :  593
total : 5000  current step :  594
total : 5000  current step :  595
total : 5000  current step :  596
total : 5000  current step :  597
total : 5000  current step :  598
total : 5000  current step :  599
total : 5000  current step :  600
total : 5000  current step :  601
total : 5000  current step :  602
total : 5000  current step :  603
total : 5000  current step :  604
total : 5000  current step :  605
total : 5000  current step :  606
total : 5000  current step :  607
total : 5000  current step :  608
total : 5000  current step :  609
total : 5000  current step :  610
total : 5000  current step :  611
total : 5000  current step :  612
total : 5000  current step :  613
total : 5000  current step :  614
total : 5000  current step :  615
total : 5000  current step :  616
total : 5000  current step :  617
total : 5000  current step :  618
total : 5000  current step :  619
total : 5000  current step :  620
total : 5000  current step :  621
total : 5000  current step :  622
total : 5000  current step :  623
total : 5000  current step :  624
total : 5000  current step :  625
total : 5000  current step :  626
total : 5000  current step :  627
total : 5000  current step :  628
total : 5000  current step :  629
total : 5000  current step :  630
total : 5000  current step :  631
total : 5000  current step :  632
total : 5000  current step :  633
total : 5000  current step :  634
total : 5000  current step :  635
total : 5000  current step :  636
total : 5000  current step :  637
total : 5000  current step :  638
total : 5000  current step :  639
total : 5000  current step :  640
total : 5000  current step :  641
total : 5000  current step :  642
total : 5000  current step :  643
total : 5000  current step :  644
total : 5000  current step :  645
total : 5000  current step :  646
total : 5000  current step :  647
total : 5000  current step :  648
total : 5000  current step :  649
total : 5000  current step :  650
total : 5000  current step :  651
total : 5000  current step :  652
total : 5000  current step :  653
total : 5000  current step :  654
total : 5000  current step :  655
total : 5000  current step :  656
total : 5000  current step :  657
total : 5000  current step :  658
total : 5000  current step :  659
total : 5000  current step :  660
total : 5000  current step :  661
total : 5000  current step :  662
total : 5000  current step :  663
total : 5000  current step :  664
total : 5000  current step :  665
total : 5000  current step :  666
total : 5000  current step :  667
total : 5000  current step :  668
total : 5000  current step :  669
total : 5000  current step :  670
total : 5000  current step :  671
total : 5000  current step :  672
total : 5000  current step :  673
total : 5000  current step :  674
total : 5000  current step :  675
total : 5000  current step :  676
total : 5000  current step :  677
total : 5000  current step :  678
total : 5000  current step :  679
total : 5000  current step :  680
total : 5000  current step :  681
total : 5000  current step :  682
total : 5000  current step :  683
total : 5000  current step :  684
total : 5000  current step :  685
total : 5000  current step :  686
total : 5000  current step :  687
total : 5000  current step :  688
total : 5000  current step :  689
total : 5000  current step :  690
total : 5000  current step :  691
total : 5000  current step :  692
total : 5000  current step :  693
total : 5000  current step :  694
total : 5000  current step :  695
total : 5000  current step :  696
total : 5000  current step :  697
total : 5000  current step :  698
total : 5000  current step :  699
total : 5000  current step :  700
total : 5000  current step :  701
total : 5000  current step :  702
total : 5000  current step :  703
total : 5000  current step :  704
total : 5000  current step :  705
total : 5000  current step :  706
total : 5000  current step :  707
total : 5000  current step :  708
total : 5000  current step :  709
total : 5000  current step :  710
total : 5000  current step :  711
total : 5000  current step :  712
total : 5000  current step :  713
total : 5000  current step :  714
total : 5000  current step :  715
total : 5000  current step :  716
total : 5000  current step :  717
total : 5000  current step :  718
total : 5000  current step :  719
total : 5000  current step :  720
total : 5000  current step :  721
total : 5000  current step :  722
total : 5000  current step :  723
total : 5000  current step :  724
total : 5000  current step :  725
total : 5000  current step :  726
total : 5000  current step :  727
total : 5000  current step :  728
total : 5000  current step :  729
total : 5000  current step :  730
total : 5000  current step :  731
total : 5000  current step :  732
total : 5000  current step :  733
total : 5000  current step :  734
total : 5000  current step :  735
total : 5000  current step :  736
total : 5000  current step :  737
total : 5000  current step :  738
total : 5000  current step :  739
total : 5000  current step :  740
total : 5000  current step :  741
total : 5000  current step :  742
total : 5000  current step :  743
total : 5000  current step :  744
total : 5000  current step :  745
total : 5000  current step :  746
total : 5000  current step :  747
total : 5000  current step :  748
total : 5000  current step :  749
total : 5000  current step :  750
total : 5000  current step :  751
total : 5000  current step :  752
total : 5000  current step :  753
total : 5000  current step :  754
total : 5000  current step :  755
total : 5000  current step :  756
total : 5000  current step :  757
total : 5000  current step :  758
total : 5000  current step :  759
total : 5000  current step :  760
total : 5000  current step :  761
total : 5000  current step :  762
total : 5000  current step :  763
total : 5000  current step :  764
total : 5000  current step :  765
total : 5000  current step :  766
total : 5000  current step :  767
total : 5000  current step :  768
total : 5000  current step :  769
total : 5000  current step :  770
total : 5000  current step :  771
total : 5000  current step :  772
total : 5000  current step :  773
total : 5000  current step :  774
total : 5000  current step :  775
total : 5000  current step :  776
total : 5000  current step :  777
total : 5000  current step :  778
total : 5000  current step :  779
total : 5000  current step :  780
total : 5000  current step :  781
total : 5000  current step :  782
total : 5000  current step :  783
total : 5000  current step :  784
total : 5000  current step :  785
total : 5000  current step :  786
total : 5000  current step :  787
total : 5000  current step :  788
total : 5000  current step :  789
total : 5000  current step :  790
total : 5000  current step :  791
total : 5000  current step :  792
total : 5000  current step :  793
total : 5000  current step :  794
total : 5000  current step :  795
total : 5000  current step :  796
total : 5000  current step :  797
total : 5000  current step :  798
total : 5000  current step :  799
total : 5000  current step :  800
total : 5000  current step :  801
total : 5000  current step :  802
total : 5000  current step :  803
total : 5000  current step :  804
total : 5000  current step :  805
total : 5000  current step :  806
total : 5000  current step :  807
total : 5000  current step :  808
total : 5000  current step :  809
total : 5000  current step :  810
total : 5000  current step :  811
total : 5000  current step :  812
total : 5000  current step :  813
total : 5000  current step :  814
total : 5000  current step :  815
total : 5000  current step :  816
total : 5000  current step :  817
total : 5000  current step :  818
total : 5000  current step :  819
total : 5000  current step :  820
total : 5000  current step :  821
total : 5000  current step :  822
total : 5000  current step :  823
total : 5000  current step :  824
total : 5000  current step :  825
total : 5000  current step :  826
total : 5000  current step :  827
total : 5000  current step :  828
total : 5000  current step :  829
total : 5000  current step :  830
total : 5000  current step :  831
total : 5000  current step :  832
total : 5000  current step :  833
total : 5000  current step :  834
total : 5000  current step :  835
total : 5000  current step :  836
total : 5000  current step :  837
total : 5000  current step :  838
total : 5000  current step :  839
total : 5000  current step :  840
total : 5000  current step :  841
total : 5000  current step :  842
total : 5000  current step :  843
total : 5000  current step :  844
total : 5000  current step :  845
total : 5000  current step :  846
total : 5000  current step :  847
total : 5000  current step :  848
total : 5000  current step :  849
total : 5000  current step :  850
total : 5000  current step :  851
total : 5000  current step :  852
total : 5000  current step :  853
total : 5000  current step :  854
total : 5000  current step :  855
total : 5000  current step :  856
total : 5000  current step :  857
total : 5000  current step :  858
total : 5000  current step :  859
total : 5000  current step :  860
total : 5000  current step :  861
total : 5000  current step :  862
total : 5000  current step :  863
total : 5000  current step :  864
total : 5000  current step :  865
total : 5000  current step :  866
total : 5000  current step :  867
total : 5000  current step :  868
total : 5000  current step :  869
total : 5000  current step :  870
total : 5000  current step :  871
total : 5000  current step :  872
total : 5000  current step :  873
total : 5000  current step :  874
total : 5000  current step :  875
total : 5000  current step :  876
total : 5000  current step :  877
total : 5000  current step :  878
total : 5000  current step :  879
total : 5000  current step :  880
total : 5000  current step :  881
total : 5000  current step :  882
total : 5000  current step :  883
total : 5000  current step :  884
total : 5000  current step :  885
total : 5000  current step :  886
total : 5000  current step :  887
total : 5000  current step :  888
total : 5000  current step :  889
total : 5000  current step :  890
total : 5000  current step :  891
total : 5000  current step :  892
total : 5000  current step :  893
total : 5000  current step :  894
total : 5000  current step :  895
total : 5000  current step :  896
total : 5000  current step :  897
total : 5000  current step :  898
total : 5000  current step :  899
total : 5000  current step :  900
total : 5000  current step :  901
total : 5000  current step :  902
total : 5000  current step :  903
total : 5000  current step :  904
total : 5000  current step :  905
total : 5000  current step :  906
total : 5000  current step :  907
total : 5000  current step :  908
total : 5000  current step :  909
total : 5000  current step :  910
total : 5000  current step :  911
total : 5000  current step :  912
total : 5000  current step :  913
total : 5000  current step :  914
total : 5000  current step :  915
total : 5000  current step :  916
total : 5000  current step :  917
total : 5000  current step :  918
total : 5000  current step :  919
total : 5000  current step :  920
total : 5000  current step :  921
total : 5000  current step :  922
total : 5000  current step :  923
total : 5000  current step :  924
total : 5000  current step :  925
total : 5000  current step :  926
total : 5000  current step :  927
total : 5000  current step :  928
total : 5000  current step :  929
total : 5000  current step :  930
total : 5000  current step :  931
total : 5000  current step :  932
total : 5000  current step :  933
total : 5000  current step :  934
total : 5000  current step :  935
total : 5000  current step :  936
total : 5000  current step :  937
total : 5000  current step :  938
total : 5000  current step :  939
total : 5000  current step :  940
total : 5000  current step :  941
total : 5000  current step :  942
total : 5000  current step :  943
total : 5000  current step :  944
total : 5000  current step :  945
total : 5000  current step :  946
total : 5000  current step :  947
total : 5000  current step :  948
total : 5000  current step :  949
total : 5000  current step :  950
total : 5000  current step :  951
total : 5000  current step :  952
total : 5000  current step :  953
total : 5000  current step :  954
total : 5000  current step :  955
total : 5000  current step :  956
total : 5000  current step :  957
total : 5000  current step :  958
total : 5000  current step :  959
total : 5000  current step :  960
total : 5000  current step :  961
total : 5000  current step :  962
total : 5000  current step :  963
total : 5000  current step :  964
total : 5000  current step :  965
total : 5000  current step :  966
total : 5000  current step :  967
total : 5000  current step :  968
total : 5000  current step :  969
total : 5000  current step :  970
total : 5000  current step :  971
total : 5000  current step :  972
total : 5000  current step :  973
total : 5000  current step :  974
total : 5000  current step :  975
total : 5000  current step :  976
total : 5000  current step :  977
total : 5000  current step :  978
total : 5000  current step :  979
total : 5000  current step :  980
total : 5000  current step :  981
total : 5000  current step :  982
total : 5000  current step :  983
total : 5000  current step :  984
total : 5000  current step :  985
total : 5000  current step :  986
total : 5000  current step :  987
total : 5000  current step :  988
total : 5000  current step :  989
total : 5000  current step :  990
total : 5000  current step :  991
total : 5000  current step :  992
total : 5000  current step :  993
total : 5000  current step :  994
total : 5000  current step :  995
total : 5000  current step :  996
total : 5000  current step :  997
total : 5000  current step :  998
total : 5000  current step :  999
total : 5000  current step :  1000
total : 5000  current step :  1001
total : 5000  current step :  1002
total : 5000  current step :  1003
total : 5000  current step :  1004
total : 5000  current step :  1005
total : 5000  current step :  1006
total : 5000  current step :  1007
total : 5000  current step :  1008
total : 5000  current step :  1009
total : 5000  current step :  1010
total : 5000  current step :  1011
total : 5000  current step :  1012
total : 5000  current step :  1013
total : 5000  current step :  1014
total : 5000  current step :  1015
total : 5000  current step :  1016
total : 5000  current step :  1017
total : 5000  current step :  1018
total : 5000  current step :  1019
total : 5000  current step :  1020
total : 5000  current step :  1021
total : 5000  current step :  1022
total : 5000  current step :  1023
total : 5000  current step :  1024
total : 5000  current step :  1025
total : 5000  current step :  1026
total : 5000  current step :  1027
total : 5000  current step :  1028
total : 5000  current step :  1029
total : 5000  current step :  1030
total : 5000  current step :  1031
total : 5000  current step :  1032
total : 5000  current step :  1033
total : 5000  current step :  1034
total : 5000  current step :  1035
total : 5000  current step :  1036
total : 5000  current step :  1037
total : 5000  current step :  1038
total : 5000  current step :  1039
total : 5000  current step :  1040
total : 5000  current step :  1041
total : 5000  current step :  1042
total : 5000  current step :  1043
total : 5000  current step :  1044
total : 5000  current step :  1045
total : 5000  current step :  1046
total : 5000  current step :  1047
total : 5000  current step :  1048
total : 5000  current step :  1049
total : 5000  current step :  1050
total : 5000  current step :  1051
total : 5000  current step :  1052
total : 5000  current step :  1053
total : 5000  current step :  1054
total : 5000  current step :  1055
total : 5000  current step :  1056
total : 5000  current step :  1057
total : 5000  current step :  1058
total : 5000  current step :  1059
total : 5000  current step :  1060
total : 5000  current step :  1061
total : 5000  current step :  1062
total : 5000  current step :  1063
total : 5000  current step :  1064
total : 5000  current step :  1065
total : 5000  current step :  1066
total : 5000  current step :  1067
total : 5000  current step :  1068
total : 5000  current step :  1069
total : 5000  current step :  1070
total : 5000  current step :  1071
total : 5000  current step :  1072
total : 5000  current step :  1073
total : 5000  current step :  1074
total : 5000  current step :  1075
total : 5000  current step :  1076
total : 5000  current step :  1077
total : 5000  current step :  1078
total : 5000  current step :  1079
total : 5000  current step :  1080
total : 5000  current step :  1081
total : 5000  current step :  1082
total : 5000  current step :  1083
total : 5000  current step :  1084
total : 5000  current step :  1085
total : 5000  current step :  1086
total : 5000  current step :  1087
total : 5000  current step :  1088
total : 5000  current step :  1089
total : 5000  current step :  1090
total : 5000  current step :  1091
total : 5000  current step :  1092
total : 5000  current step :  1093
total : 5000  current step :  1094
total : 5000  current step :  1095
total : 5000  current step :  1096
total : 5000  current step :  1097
total : 5000  current step :  1098
total : 5000  current step :  1099
total : 5000  current step :  1100
total : 5000  current step :  1101
total : 5000  current step :  1102
total : 5000  current step :  1103
total : 5000  current step :  1104
total : 5000  current step :  1105
total : 5000  current step :  1106
total : 5000  current step :  1107
total : 5000  current step :  1108
total : 5000  current step :  1109
total : 5000  current step :  1110
total : 5000  current step :  1111
total : 5000  current step :  1112
total : 5000  current step :  1113
total : 5000  current step :  1114
total : 5000  current step :  1115
total : 5000  current step :  1116
total : 5000  current step :  1117
total : 5000  current step :  1118
total : 5000  current step :  1119
total : 5000  current step :  1120
total : 5000  current step :  1121
total : 5000  current step :  1122
total : 5000  current step :  1123
total : 5000  current step :  1124
total : 5000  current step :  1125
total : 5000  current step :  1126
total : 5000  current step :  1127
total : 5000  current step :  1128
total : 5000  current step :  1129
total : 5000  current step :  1130
total : 5000  current step :  1131
total : 5000  current step :  1132
total : 5000  current step :  1133
total : 5000  current step :  1134
total : 5000  current step :  1135
total : 5000  current step :  1136
total : 5000  current step :  1137
total : 5000  current step :  1138
total : 5000  current step :  1139
total : 5000  current step :  1140
total : 5000  current step :  1141
total : 5000  current step :  1142
total : 5000  current step :  1143
total : 5000  current step :  1144
total : 5000  current step :  1145
total : 5000  current step :  1146
total : 5000  current step :  1147
total : 5000  current step :  1148
total : 5000  current step :  1149
total : 5000  current step :  1150
total : 5000  current step :  1151
total : 5000  current step :  1152
total : 5000  current step :  1153
total : 5000  current step :  1154
total : 5000  current step :  1155
total : 5000  current step :  1156
total : 5000  current step :  1157
total : 5000  current step :  1158
total : 5000  current step :  1159
total : 5000  current step :  1160
total : 5000  current step :  1161
total : 5000  current step :  1162
total : 5000  current step :  1163
total : 5000  current step :  1164
total : 5000  current step :  1165
total : 5000  current step :  1166
total : 5000  current step :  1167
total : 5000  current step :  1168
total : 5000  current step :  1169
total : 5000  current step :  1170
total : 5000  current step :  1171
total : 5000  current step :  1172
total : 5000  current step :  1173
total : 5000  current step :  1174
total : 5000  current step :  1175
total : 5000  current step :  1176
total : 5000  current step :  1177
total : 5000  current step :  1178
total : 5000  current step :  1179
total : 5000  current step :  1180
total : 5000  current step :  1181
total : 5000  current step :  1182
total : 5000  current step :  1183
total : 5000  current step :  1184
total : 5000  current step :  1185
total : 5000  current step :  1186
total : 5000  current step :  1187
total : 5000  current step :  1188
total : 5000  current step :  1189
total : 5000  current step :  1190
total : 5000  current step :  1191
total : 5000  current step :  1192
total : 5000  current step :  1193
total : 5000  current step :  1194
total : 5000  current step :  1195
total : 5000  current step :  1196
total : 5000  current step :  1197
total : 5000  current step :  1198
total : 5000  current step :  1199
total : 5000  current step :  1200
total : 5000  current step :  1201
total : 5000  current step :  1202
total : 5000  current step :  1203
total : 5000  current step :  1204
total : 5000  current step :  1205
total : 5000  current step :  1206
total : 5000  current step :  1207
total : 5000  current step :  1208
total : 5000  current step :  1209
total : 5000  current step :  1210
total : 5000  current step :  1211
total : 5000  current step :  1212
total : 5000  current step :  1213
total : 5000  current step :  1214
total : 5000  current step :  1215
total : 5000  current step :  1216
total : 5000  current step :  1217
total : 5000  current step :  1218
total : 5000  current step :  1219
total : 5000  current step :  1220
total : 5000  current step :  1221
total : 5000  current step :  1222
total : 5000  current step :  1223
total : 5000  current step :  1224
total : 5000  current step :  1225
total : 5000  current step :  1226
total : 5000  current step :  1227
total : 5000  current step :  1228
total : 5000  current step :  1229
total : 5000  current step :  1230
total : 5000  current step :  1231
total : 5000  current step :  1232
total : 5000  current step :  1233
total : 5000  current step :  1234
total : 5000  current step :  1235
total : 5000  current step :  1236
total : 5000  current step :  1237
total : 5000  current step :  1238
total : 5000  current step :  1239
total : 5000  current step :  1240
total : 5000  current step :  1241
total : 5000  current step :  1242
total : 5000  current step :  1243
total : 5000  current step :  1244
total : 5000  current step :  1245
total : 5000  current step :  1246
total : 5000  current step :  1247
total : 5000  current step :  1248
total : 5000  current step :  1249
total : 5000  current step :  1250
total : 5000  current step :  1251
total : 5000  current step :  1252
total : 5000  current step :  1253
total : 5000  current step :  1254
total : 5000  current step :  1255
total : 5000  current step :  1256
total : 5000  current step :  1257
total : 5000  current step :  1258
total : 5000  current step :  1259
total : 5000  current step :  1260
total : 5000  current step :  1261
total : 5000  current step :  1262
total : 5000  current step :  1263
total : 5000  current step :  1264
total : 5000  current step :  1265
total : 5000  current step :  1266
total : 5000  current step :  1267
total : 5000  current step :  1268
total : 5000  current step :  1269
total : 5000  current step :  1270
total : 5000  current step :  1271
total : 5000  current step :  1272
total : 5000  current step :  1273
total : 5000  current step :  1274
total : 5000  current step :  1275
total : 5000  current step :  1276
total : 5000  current step :  1277
total : 5000  current step :  1278
total : 5000  current step :  1279
total : 5000  current step :  1280
total : 5000  current step :  1281
total : 5000  current step :  1282
total : 5000  current step :  1283
total : 5000  current step :  1284
total : 5000  current step :  1285
total : 5000  current step :  1286
total : 5000  current step :  1287
total : 5000  current step :  1288
total : 5000  current step :  1289
total : 5000  current step :  1290
total : 5000  current step :  1291
total : 5000  current step :  1292
total : 5000  current step :  1293
total : 5000  current step :  1294
total : 5000  current step :  1295
total : 5000  current step :  1296
total : 5000  current step :  1297
total : 5000  current step :  1298
total : 5000  current step :  1299
total : 5000  current step :  1300
total : 5000  current step :  1301
total : 5000  current step :  1302
total : 5000  current step :  1303
total : 5000  current step :  1304
total : 5000  current step :  1305
total : 5000  current step :  1306
total : 5000  current step :  1307
total : 5000  current step :  1308
total : 5000  current step :  1309
total : 5000  current step :  1310
total : 5000  current step :  1311
total : 5000  current step :  1312
total : 5000  current step :  1313
total : 5000  current step :  1314
total : 5000  current step :  1315
total : 5000  current step :  1316
total : 5000  current step :  1317
total : 5000  current step :  1318
total : 5000  current step :  1319
total : 5000  current step :  1320
total : 5000  current step :  1321
total : 5000  current step :  1322
total : 5000  current step :  1323
total : 5000  current step :  1324
total : 5000  current step :  1325
total : 5000  current step :  1326
total : 5000  current step :  1327
total : 5000  current step :  1328
total : 5000  current step :  1329
total : 5000  current step :  1330
total : 5000  current step :  1331
total : 5000  current step :  1332
total : 5000  current step :  1333
total : 5000  current step :  1334
total : 5000  current step :  1335
total : 5000  current step :  1336
total : 5000  current step :  1337
total : 5000  current step :  1338
total : 5000  current step :  1339
total : 5000  current step :  1340
total : 5000  current step :  1341
total : 5000  current step :  1342
total : 5000  current step :  1343
total : 5000  current step :  1344
total : 5000  current step :  1345
total : 5000  current step :  1346
total : 5000  current step :  1347
total : 5000  current step :  1348
total : 5000  current step :  1349
total : 5000  current step :  1350
total : 5000  current step :  1351
total : 5000  current step :  1352
total : 5000  current step :  1353
total : 5000  current step :  1354
total : 5000  current step :  1355
total : 5000  current step :  1356
total : 5000  current step :  1357
total : 5000  current step :  1358
total : 5000  current step :  1359
total : 5000  current step :  1360
total : 5000  current step :  1361
total : 5000  current step :  1362
total : 5000  current step :  1363
total : 5000  current step :  1364
total : 5000  current step :  1365
total : 5000  current step :  1366
total : 5000  current step :  1367
total : 5000  current step :  1368
total : 5000  current step :  1369
total : 5000  current step :  1370
total : 5000  current step :  1371
total : 5000  current step :  1372
total : 5000  current step :  1373
total : 5000  current step :  1374
total : 5000  current step :  1375
total : 5000  current step :  1376
total : 5000  current step :  1377
total : 5000  current step :  1378
total : 5000  current step :  1379
total : 5000  current step :  1380
total : 5000  current step :  1381
total : 5000  current step :  1382
total : 5000  current step :  1383
total : 5000  current step :  1384
total : 5000  current step :  1385
total : 5000  current step :  1386
total : 5000  current step :  1387
total : 5000  current step :  1388
total : 5000  current step :  1389
total : 5000  current step :  1390
total : 5000  current step :  1391
total : 5000  current step :  1392
total : 5000  current step :  1393
total : 5000  current step :  1394
total : 5000  current step :  1395
total : 5000  current step :  1396
total : 5000  current step :  1397
total : 5000  current step :  1398
total : 5000  current step :  1399
total : 5000  current step :  1400
total : 5000  current step :  1401
total : 5000  current step :  1402
total : 5000  current step :  1403
total : 5000  current step :  1404
total : 5000  current step :  1405
total : 5000  current step :  1406
total : 5000  current step :  1407
total : 5000  current step :  1408
total : 5000  current step :  1409
total : 5000  current step :  1410
total : 5000  current step :  1411
total : 5000  current step :  1412
total : 5000  current step :  1413
total : 5000  current step :  1414
total : 5000  current step :  1415
total : 5000  current step :  1416
total : 5000  current step :  1417
total : 5000  current step :  1418
total : 5000  current step :  1419
total : 5000  current step :  1420
total : 5000  current step :  1421
total : 5000  current step :  1422
total : 5000  current step :  1423
total : 5000  current step :  1424
total : 5000  current step :  1425
total : 5000  current step :  1426
total : 5000  current step :  1427
total : 5000  current step :  1428
total : 5000  current step :  1429
total : 5000  current step :  1430
total : 5000  current step :  1431
total : 5000  current step :  1432
total : 5000  current step :  1433
total : 5000  current step :  1434
total : 5000  current step :  1435
total : 5000  current step :  1436
total : 5000  current step :  1437
total : 5000  current step :  1438
total : 5000  current step :  1439
total : 5000  current step :  1440
total : 5000  current step :  1441
total : 5000  current step :  1442
total : 5000  current step :  1443
total : 5000  current step :  1444
total : 5000  current step :  1445
total : 5000  current step :  1446
total : 5000  current step :  1447
total : 5000  current step :  1448
total : 5000  current step :  1449
total : 5000  current step :  1450
total : 5000  current step :  1451
total : 5000  current step :  1452
total : 5000  current step :  1453
total : 5000  current step :  1454
total : 5000  current step :  1455
total : 5000  current step :  1456
total : 5000  current step :  1457
total : 5000  current step :  1458
total : 5000  current step :  1459
total : 5000  current step :  1460
total : 5000  current step :  1461
total : 5000  current step :  1462
total : 5000  current step :  1463
total : 5000  current step :  1464
total : 5000  current step :  1465
total : 5000  current step :  1466
total : 5000  current step :  1467
total : 5000  current step :  1468
total : 5000  current step :  1469
total : 5000  current step :  1470
total : 5000  current step :  1471
total : 5000  current step :  1472
total : 5000  current step :  1473
total : 5000  current step :  1474
total : 5000  current step :  1475
total : 5000  current step :  1476
total : 5000  current step :  1477
total : 5000  current step :  1478
total : 5000  current step :  1479
total : 5000  current step :  1480
total : 5000  current step :  1481
total : 5000  current step :  1482
total : 5000  current step :  1483
total : 5000  current step :  1484
total : 5000  current step :  1485
total : 5000  current step :  1486
total : 5000  current step :  1487
total : 5000  current step :  1488
total : 5000  current step :  1489
total : 5000  current step :  1490
total : 5000  current step :  1491
total : 5000  current step :  1492
total : 5000  current step :  1493
total : 5000  current step :  1494
total : 5000  current step :  1495
total : 5000  current step :  1496
total : 5000  current step :  1497
total : 5000  current step :  1498
total : 5000  current step :  1499
total : 5000  current step :  1500
total : 5000  current step :  1501
total : 5000  current step :  1502
total : 5000  current step :  1503
total : 5000  current step :  1504
total : 5000  current step :  1505
total : 5000  current step :  1506
total : 5000  current step :  1507
total : 5000  current step :  1508
total : 5000  current step :  1509
total : 5000  current step :  1510
total : 5000  current step :  1511
total : 5000  current step :  1512
total : 5000  current step :  1513
total : 5000  current step :  1514
total : 5000  current step :  1515
total : 5000  current step :  1516
total : 5000  current step :  1517
total : 5000  current step :  1518
total : 5000  current step :  1519
total : 5000  current step :  1520
total : 5000  current step :  1521
total : 5000  current step :  1522
total : 5000  current step :  1523
total : 5000  current step :  1524
total : 5000  current step :  1525
total : 5000  current step :  1526
total : 5000  current step :  1527
total : 5000  current step :  1528
total : 5000  current step :  1529
total : 5000  current step :  1530
total : 5000  current step :  1531
total : 5000  current step :  1532
total : 5000  current step :  1533
total : 5000  current step :  1534
total : 5000  current step :  1535
total : 5000  current step :  1536
total : 5000  current step :  1537
total : 5000  current step :  1538
total : 5000  current step :  1539
total : 5000  current step :  1540
total : 5000  current step :  1541
total : 5000  current step :  1542
total : 5000  current step :  1543
total : 5000  current step :  1544
total : 5000  current step :  1545
total : 5000  current step :  1546
total : 5000  current step :  1547
total : 5000  current step :  1548
total : 5000  current step :  1549
total : 5000  current step :  1550
total : 5000  current step :  1551
total : 5000  current step :  1552
total : 5000  current step :  1553
total : 5000  current step :  1554
total : 5000  current step :  1555
total : 5000  current step :  1556
total : 5000  current step :  1557
total : 5000  current step :  1558
total : 5000  current step :  1559
total : 5000  current step :  1560
total : 5000  current step :  1561
total : 5000  current step :  1562
total : 5000  current step :  1563
total : 5000  current step :  1564
total : 5000  current step :  1565
total : 5000  current step :  1566
total : 5000  current step :  1567
total : 5000  current step :  1568
total : 5000  current step :  1569
total : 5000  current step :  1570
total : 5000  current step :  1571
total : 5000  current step :  1572
total : 5000  current step :  1573
total : 5000  current step :  1574
total : 5000  current step :  1575
total : 5000  current step :  1576
total : 5000  current step :  1577
total : 5000  current step :  1578
total : 5000  current step :  1579
total : 5000  current step :  1580
total : 5000  current step :  1581
total : 5000  current step :  1582
total : 5000  current step :  1583
total : 5000  current step :  1584
total : 5000  current step :  1585
total : 5000  current step :  1586
total : 5000  current step :  1587
total : 5000  current step :  1588
total : 5000  current step :  1589
total : 5000  current step :  1590
total : 5000  current step :  1591
total : 5000  current step :  1592
total : 5000  current step :  1593
total : 5000  current step :  1594
total : 5000  current step :  1595
total : 5000  current step :  1596
total : 5000  current step :  1597
total : 5000  current step :  1598
total : 5000  current step :  1599
total : 5000  current step :  1600
total : 5000  current step :  1601
total : 5000  current step :  1602
total : 5000  current step :  1603
total : 5000  current step :  1604
total : 5000  current step :  1605
total : 5000  current step :  1606
total : 5000  current step :  1607
total : 5000  current step :  1608
total : 5000  current step :  1609
total : 5000  current step :  1610
total : 5000  current step :  1611
total : 5000  current step :  1612
total : 5000  current step :  1613
total : 5000  current step :  1614
total : 5000  current step :  1615
total : 5000  current step :  1616
total : 5000  current step :  1617
total : 5000  current step :  1618
total : 5000  current step :  1619
total : 5000  current step :  1620
total : 5000  current step :  1621
total : 5000  current step :  1622
total : 5000  current step :  1623
total : 5000  current step :  1624
total : 5000  current step :  1625
total : 5000  current step :  1626
total : 5000  current step :  1627
total : 5000  current step :  1628
total : 5000  current step :  1629
total : 5000  current step :  1630
total : 5000  current step :  1631
total : 5000  current step :  1632
total : 5000  current step :  1633
total : 5000  current step :  1634
total : 5000  current step :  1635
total : 5000  current step :  1636
total : 5000  current step :  1637
total : 5000  current step :  1638
total : 5000  current step :  1639
total : 5000  current step :  1640
total : 5000  current step :  1641
total : 5000  current step :  1642
total : 5000  current step :  1643
total : 5000  current step :  1644
total : 5000  current step :  1645
total : 5000  current step :  1646
total : 5000  current step :  1647
total : 5000  current step :  1648
total : 5000  current step :  1649
total : 5000  current step :  1650
total : 5000  current step :  1651
total : 5000  current step :  1652
total : 5000  current step :  1653
total : 5000  current step :  1654
total : 5000  current step :  1655
total : 5000  current step :  1656
total : 5000  current step :  1657
total : 5000  current step :  1658
total : 5000  current step :  1659
total : 5000  current step :  1660
total : 5000  current step :  1661
total : 5000  current step :  1662
total : 5000  current step :  1663
total : 5000  current step :  1664
total : 5000  current step :  1665
total : 5000  current step :  1666
total : 5000  current step :  1667
total : 5000  current step :  1668
total : 5000  current step :  1669
total : 5000  current step :  1670
total : 5000  current step :  1671
total : 5000  current step :  1672
total : 5000  current step :  1673
total : 5000  current step :  1674
total : 5000  current step :  1675
total : 5000  current step :  1676
total : 5000  current step :  1677
total : 5000  current step :  1678
total : 5000  current step :  1679
total : 5000  current step :  1680
total : 5000  current step :  1681
total : 5000  current step :  1682
total : 5000  current step :  1683
total : 5000  current step :  1684
total : 5000  current step :  1685
total : 5000  current step :  1686
total : 5000  current step :  1687
total : 5000  current step :  1688
total : 5000  current step :  1689
total : 5000  current step :  1690
total : 5000  current step :  1691
total : 5000  current step :  1692
total : 5000  current step :  1693
total : 5000  current step :  1694
total : 5000  current step :  1695
total : 5000  current step :  1696
total : 5000  current step :  1697
total : 5000  current step :  1698
total : 5000  current step :  1699
total : 5000  current step :  1700
total : 5000  current step :  1701
total : 5000  current step :  1702
total : 5000  current step :  1703
total : 5000  current step :  1704
total : 5000  current step :  1705
total : 5000  current step :  1706
total : 5000  current step :  1707
total : 5000  current step :  1708
total : 5000  current step :  1709
total : 5000  current step :  1710
total : 5000  current step :  1711
total : 5000  current step :  1712
total : 5000  current step :  1713
total : 5000  current step :  1714
total : 5000  current step :  1715
total : 5000  current step :  1716
total : 5000  current step :  1717
total : 5000  current step :  1718
total : 5000  current step :  1719
total : 5000  current step :  1720
total : 5000  current step :  1721
total : 5000  current step :  1722
total : 5000  current step :  1723
total : 5000  current step :  1724
total : 5000  current step :  1725
total : 5000  current step :  1726
total : 5000  current step :  1727
total : 5000  current step :  1728
total : 5000  current step :  1729
total : 5000  current step :  1730
total : 5000  current step :  1731
total : 5000  current step :  1732
total : 5000  current step :  1733
total : 5000  current step :  1734
total : 5000  current step :  1735
total : 5000  current step :  1736
total : 5000  current step :  1737
total : 5000  current step :  1738
total : 5000  current step :  1739
total : 5000  current step :  1740
total : 5000  current step :  1741
total : 5000  current step :  1742
total : 5000  current step :  1743
total : 5000  current step :  1744
total : 5000  current step :  1745
total : 5000  current step :  1746
total : 5000  current step :  1747
total : 5000  current step :  1748
total : 5000  current step :  1749
total : 5000  current step :  1750
total : 5000  current step :  1751
total : 5000  current step :  1752
total : 5000  current step :  1753
total : 5000  current step :  1754
total : 5000  current step :  1755
total : 5000  current step :  1756
total : 5000  current step :  1757
total : 5000  current step :  1758
total : 5000  current step :  1759
total : 5000  current step :  1760
total : 5000  current step :  1761
total : 5000  current step :  1762
total : 5000  current step :  1763
total : 5000  current step :  1764
total : 5000  current step :  1765
total : 5000  current step :  1766
total : 5000  current step :  1767
total : 5000  current step :  1768
total : 5000  current step :  1769
total : 5000  current step :  1770
total : 5000  current step :  1771
total : 5000  current step :  1772
total : 5000  current step :  1773
total : 5000  current step :  1774
total : 5000  current step :  1775
total : 5000  current step :  1776
total : 5000  current step :  1777
total : 5000  current step :  1778
total : 5000  current step :  1779
total : 5000  current step :  1780
total : 5000  current step :  1781
total : 5000  current step :  1782
total : 5000  current step :  1783
total : 5000  current step :  1784
total : 5000  current step :  1785
total : 5000  current step :  1786
total : 5000  current step :  1787
total : 5000  current step :  1788
total : 5000  current step :  1789
total : 5000  current step :  1790
total : 5000  current step :  1791
total : 5000  current step :  1792
total : 5000  current step :  1793
total : 5000  current step :  1794
total : 5000  current step :  1795
total : 5000  current step :  1796
total : 5000  current step :  1797
total : 5000  current step :  1798
total : 5000  current step :  1799
total : 5000  current step :  1800
total : 5000  current step :  1801
total : 5000  current step :  1802
total : 5000  current step :  1803
total : 5000  current step :  1804
total : 5000  current step :  1805
total : 5000  current step :  1806
total : 5000  current step :  1807
total : 5000  current step :  1808
total : 5000  current step :  1809
total : 5000  current step :  1810
total : 5000  current step :  1811
total : 5000  current step :  1812
total : 5000  current step :  1813
total : 5000  current step :  1814
total : 5000  current step :  1815
total : 5000  current step :  1816
total : 5000  current step :  1817
total : 5000  current step :  1818
total : 5000  current step :  1819
total : 5000  current step :  1820
total : 5000  current step :  1821
total : 5000  current step :  1822
total : 5000  current step :  1823
total : 5000  current step :  1824
total : 5000  current step :  1825
total : 5000  current step :  1826
total : 5000  current step :  1827
total : 5000  current step :  1828
total : 5000  current step :  1829
total : 5000  current step :  1830
total : 5000  current step :  1831
total : 5000  current step :  1832
total : 5000  current step :  1833
total : 5000  current step :  1834
total : 5000  current step :  1835
total : 5000  current step :  1836
total : 5000  current step :  1837
total : 5000  current step :  1838
total : 5000  current step :  1839
total : 5000  current step :  1840
total : 5000  current step :  1841
total : 5000  current step :  1842
total : 5000  current step :  1843
total : 5000  current step :  1844
total : 5000  current step :  1845
total : 5000  current step :  1846
total : 5000  current step :  1847
total : 5000  current step :  1848
total : 5000  current step :  1849
total : 5000  current step :  1850
total : 5000  current step :  1851
total : 5000  current step :  1852
total : 5000  current step :  1853
total : 5000  current step :  1854
total : 5000  current step :  1855
total : 5000  current step :  1856
total : 5000  current step :  1857
total : 5000  current step :  1858
total : 5000  current step :  1859
total : 5000  current step :  1860
total : 5000  current step :  1861
total : 5000  current step :  1862
total : 5000  current step :  1863
total : 5000  current step :  1864
total : 5000  current step :  1865
total : 5000  current step :  1866
total : 5000  current step :  1867
total : 5000  current step :  1868
total : 5000  current step :  1869
total : 5000  current step :  1870
total : 5000  current step :  1871
total : 5000  current step :  1872
total : 5000  current step :  1873
total : 5000  current step :  1874
total : 5000  current step :  1875
total : 5000  current step :  1876
total : 5000  current step :  1877
total : 5000  current step :  1878
total : 5000  current step :  1879
total : 5000  current step :  1880
total : 5000  current step :  1881
total : 5000  current step :  1882
total : 5000  current step :  1883
total : 5000  current step :  1884
total : 5000  current step :  1885
total : 5000  current step :  1886
total : 5000  current step :  1887
total : 5000  current step :  1888
total : 5000  current step :  1889
total : 5000  current step :  1890
total : 5000  current step :  1891
total : 5000  current step :  1892
total : 5000  current step :  1893
total : 5000  current step :  1894
total : 5000  current step :  1895
total : 5000  current step :  1896
total : 5000  current step :  1897
total : 5000  current step :  1898
total : 5000  current step :  1899
total : 5000  current step :  1900
total : 5000  current step :  1901
total : 5000  current step :  1902
total : 5000  current step :  1903
total : 5000  current step :  1904
total : 5000  current step :  1905
total : 5000  current step :  1906
total : 5000  current step :  1907
total : 5000  current step :  1908
total : 5000  current step :  1909
total : 5000  current step :  1910
total : 5000  current step :  1911
total : 5000  current step :  1912
total : 5000  current step :  1913
total : 5000  current step :  1914
total : 5000  current step :  1915
total : 5000  current step :  1916
total : 5000  current step :  1917
total : 5000  current step :  1918
total : 5000  current step :  1919
total : 5000  current step :  1920
total : 5000  current step :  1921
total : 5000  current step :  1922
total : 5000  current step :  1923
total : 5000  current step :  1924
total : 5000  current step :  1925
total : 5000  current step :  1926
total : 5000  current step :  1927
total : 5000  current step :  1928
total : 5000  current step :  1929
total : 5000  current step :  1930
total : 5000  current step :  1931
total : 5000  current step :  1932
total : 5000  current step :  1933
total : 5000  current step :  1934
total : 5000  current step :  1935
total : 5000  current step :  1936
total : 5000  current step :  1937
total : 5000  current step :  1938
total : 5000  current step :  1939
total : 5000  current step :  1940
total : 5000  current step :  1941
total : 5000  current step :  1942
total : 5000  current step :  1943
total : 5000  current step :  1944
total : 5000  current step :  1945
total : 5000  current step :  1946
total : 5000  current step :  1947
total : 5000  current step :  1948
total : 5000  current step :  1949
total : 5000  current step :  1950
total : 5000  current step :  1951
total : 5000  current step :  1952
total : 5000  current step :  1953
total : 5000  current step :  1954
total : 5000  current step :  1955
total : 5000  current step :  1956
total : 5000  current step :  1957
total : 5000  current step :  1958
total : 5000  current step :  1959
total : 5000  current step :  1960
total : 5000  current step :  1961
total : 5000  current step :  1962
total : 5000  current step :  1963
total : 5000  current step :  1964
total : 5000  current step :  1965
total : 5000  current step :  1966
total : 5000  current step :  1967
total : 5000  current step :  1968
total : 5000  current step :  1969
total : 5000  current step :  1970
total : 5000  current step :  1971
total : 5000  current step :  1972
total : 5000  current step :  1973
total : 5000  current step :  1974
total : 5000  current step :  1975
total : 5000  current step :  1976
total : 5000  current step :  1977
total : 5000  current step :  1978
total : 5000  current step :  1979
total : 5000  current step :  1980
total : 5000  current step :  1981
total : 5000  current step :  1982
total : 5000  current step :  1983
total : 5000  current step :  1984
total : 5000  current step :  1985
total : 5000  current step :  1986
total : 5000  current step :  1987
total : 5000  current step :  1988
total : 5000  current step :  1989
total : 5000  current step :  1990
total : 5000  current step :  1991
total : 5000  current step :  1992
total : 5000  current step :  1993
total : 5000  current step :  1994
total : 5000  current step :  1995
total : 5000  current step :  1996
total : 5000  current step :  1997
total : 5000  current step :  1998
total : 5000  current step :  1999
total : 5000  current step :  2000
total : 5000  current step :  2001
total : 5000  current step :  2002
total : 5000  current step :  2003
total : 5000  current step :  2004
total : 5000  current step :  2005
total : 5000  current step :  2006
total : 5000  current step :  2007
total : 5000  current step :  2008
total : 5000  current step :  2009
total : 5000  current step :  2010
total : 5000  current step :  2011
total : 5000  current step :  2012
total : 5000  current step :  2013
total : 5000  current step :  2014
total : 5000  current step :  2015
total : 5000  current step :  2016
total : 5000  current step :  2017
total : 5000  current step :  2018
total : 5000  current step :  2019
total : 5000  current step :  2020
total : 5000  current step :  2021
total : 5000  current step :  2022
total : 5000  current step :  2023
total : 5000  current step :  2024
total : 5000  current step :  2025
total : 5000  current step :  2026
total : 5000  current step :  2027
total : 5000  current step :  2028
total : 5000  current step :  2029
total : 5000  current step :  2030
total : 5000  current step :  2031
total : 5000  current step :  2032
total : 5000  current step :  2033
total : 5000  current step :  2034
total : 5000  current step :  2035
total : 5000  current step :  2036
total : 5000  current step :  2037
total : 5000  current step :  2038
total : 5000  current step :  2039
total : 5000  current step :  2040
total : 5000  current step :  2041
total : 5000  current step :  2042
total : 5000  current step :  2043
total : 5000  current step :  2044
total : 5000  current step :  2045
total : 5000  current step :  2046
total : 5000  current step :  2047
total : 5000  current step :  2048
total : 5000  current step :  2049
total : 5000  current step :  2050
total : 5000  current step :  2051
total : 5000  current step :  2052
total : 5000  current step :  2053
total : 5000  current step :  2054
total : 5000  current step :  2055
total : 5000  current step :  2056
total : 5000  current step :  2057
total : 5000  current step :  2058
total : 5000  current step :  2059
total : 5000  current step :  2060
total : 5000  current step :  2061
total : 5000  current step :  2062
total : 5000  current step :  2063
total : 5000  current step :  2064
total : 5000  current step :  2065
total : 5000  current step :  2066
total : 5000  current step :  2067
total : 5000  current step :  2068
total : 5000  current step :  2069
total : 5000  current step :  2070
total : 5000  current step :  2071
total : 5000  current step :  2072
total : 5000  current step :  2073
total : 5000  current step :  2074
total : 5000  current step :  2075
total : 5000  current step :  2076
total : 5000  current step :  2077
total : 5000  current step :  2078
total : 5000  current step :  2079
total : 5000  current step :  2080
total : 5000  current step :  2081
total : 5000  current step :  2082
total : 5000  current step :  2083
total : 5000  current step :  2084
total : 5000  current step :  2085
total : 5000  current step :  2086
total : 5000  current step :  2087
total : 5000  current step :  2088
total : 5000  current step :  2089
total : 5000  current step :  2090
total : 5000  current step :  2091
total : 5000  current step :  2092
total : 5000  current step :  2093
total : 5000  current step :  2094
total : 5000  current step :  2095
total : 5000  current step :  2096
total : 5000  current step :  2097
total : 5000  current step :  2098
total : 5000  current step :  2099
total : 5000  current step :  2100
total : 5000  current step :  2101
total : 5000  current step :  2102
total : 5000  current step :  2103
total : 5000  current step :  2104
total : 5000  current step :  2105
total : 5000  current step :  2106
total : 5000  current step :  2107
total : 5000  current step :  2108
total : 5000  current step :  2109
total : 5000  current step :  2110
total : 5000  current step :  2111
total : 5000  current step :  2112
total : 5000  current step :  2113
total : 5000  current step :  2114
total : 5000  current step :  2115
total : 5000  current step :  2116
total : 5000  current step :  2117
total : 5000  current step :  2118
total : 5000  current step :  2119
total : 5000  current step :  2120
total : 5000  current step :  2121
total : 5000  current step :  2122
total : 5000  current step :  2123
total : 5000  current step :  2124
total : 5000  current step :  2125
total : 5000  current step :  2126
total : 5000  current step :  2127
total : 5000  current step :  2128
total : 5000  current step :  2129
total : 5000  current step :  2130
total : 5000  current step :  2131
total : 5000  current step :  2132
total : 5000  current step :  2133
total : 5000  current step :  2134
total : 5000  current step :  2135
total : 5000  current step :  2136
total : 5000  current step :  2137
total : 5000  current step :  2138
total : 5000  current step :  2139
total : 5000  current step :  2140
total : 5000  current step :  2141
total : 5000  current step :  2142
total : 5000  current step :  2143
total : 5000  current step :  2144
total : 5000  current step :  2145
total : 5000  current step :  2146
total : 5000  current step :  2147
total : 5000  current step :  2148
total : 5000  current step :  2149
total : 5000  current step :  2150
total : 5000  current step :  2151
total : 5000  current step :  2152
total : 5000  current step :  2153
total : 5000  current step :  2154
total : 5000  current step :  2155
total : 5000  current step :  2156
total : 5000  current step :  2157
total : 5000  current step :  2158
total : 5000  current step :  2159
total : 5000  current step :  2160
total : 5000  current step :  2161
total : 5000  current step :  2162
total : 5000  current step :  2163
total : 5000  current step :  2164
total : 5000  current step :  2165
total : 5000  current step :  2166
total : 5000  current step :  2167
total : 5000  current step :  2168
total : 5000  current step :  2169
total : 5000  current step :  2170
total : 5000  current step :  2171
total : 5000  current step :  2172
total : 5000  current step :  2173
total : 5000  current step :  2174
total : 5000  current step :  2175
total : 5000  current step :  2176
total : 5000  current step :  2177
total : 5000  current step :  2178
total : 5000  current step :  2179
total : 5000  current step :  2180
total : 5000  current step :  2181
total : 5000  current step :  2182
total : 5000  current step :  2183
total : 5000  current step :  2184
total : 5000  current step :  2185
total : 5000  current step :  2186
total : 5000  current step :  2187
total : 5000  current step :  2188
total : 5000  current step :  2189
total : 5000  current step :  2190
total : 5000  current step :  2191
total : 5000  current step :  2192
total : 5000  current step :  2193
total : 5000  current step :  2194
total : 5000  current step :  2195
total : 5000  current step :  2196
total : 5000  current step :  2197
total : 5000  current step :  2198
total : 5000  current step :  2199
total : 5000  current step :  2200
total : 5000  current step :  2201
total : 5000  current step :  2202
total : 5000  current step :  2203
total : 5000  current step :  2204
total : 5000  current step :  2205
total : 5000  current step :  2206
total : 5000  current step :  2207
total : 5000  current step :  2208
total : 5000  current step :  2209
total : 5000  current step :  2210
total : 5000  current step :  2211
total : 5000  current step :  2212
total : 5000  current step :  2213
total : 5000  current step :  2214
total : 5000  current step :  2215
total : 5000  current step :  2216
total : 5000  current step :  2217
total : 5000  current step :  2218
total : 5000  current step :  2219
total : 5000  current step :  2220
total : 5000  current step :  2221
total : 5000  current step :  2222
total : 5000  current step :  2223
total : 5000  current step :  2224
total : 5000  current step :  2225
total : 5000  current step :  2226
total : 5000  current step :  2227
total : 5000  current step :  2228
total : 5000  current step :  2229
total : 5000  current step :  2230
total : 5000  current step :  2231
total : 5000  current step :  2232
total : 5000  current step :  2233
total : 5000  current step :  2234
total : 5000  current step :  2235
total : 5000  current step :  2236
total : 5000  current step :  2237
total : 5000  current step :  2238
total : 5000  current step :  2239
total : 5000  current step :  2240
total : 5000  current step :  2241
total : 5000  current step :  2242
total : 5000  current step :  2243
total : 5000  current step :  2244
total : 5000  current step :  2245
total : 5000  current step :  2246
total : 5000  current step :  2247
total : 5000  current step :  2248
total : 5000  current step :  2249
total : 5000  current step :  2250
total : 5000  current step :  2251
total : 5000  current step :  2252
total : 5000  current step :  2253
total : 5000  current step :  2254
total : 5000  current step :  2255
total : 5000  current step :  2256
total : 5000  current step :  2257
total : 5000  current step :  2258
total : 5000  current step :  2259
total : 5000  current step :  2260
total : 5000  current step :  2261
total : 5000  current step :  2262
total : 5000  current step :  2263
total : 5000  current step :  2264
total : 5000  current step :  2265
total : 5000  current step :  2266
total : 5000  current step :  2267
total : 5000  current step :  2268
total : 5000  current step :  2269
total : 5000  current step :  2270
total : 5000  current step :  2271
total : 5000  current step :  2272
total : 5000  current step :  2273
total : 5000  current step :  2274
total : 5000  current step :  2275
total : 5000  current step :  2276
total : 5000  current step :  2277
total : 5000  current step :  2278
total : 5000  current step :  2279
total : 5000  current step :  2280
total : 5000  current step :  2281
total : 5000  current step :  2282
total : 5000  current step :  2283
total : 5000  current step :  2284
total : 5000  current step :  2285
total : 5000  current step :  2286
total : 5000  current step :  2287
total : 5000  current step :  2288
total : 5000  current step :  2289
total : 5000  current step :  2290
total : 5000  current step :  2291
total : 5000  current step :  2292
total : 5000  current step :  2293
total : 5000  current step :  2294
total : 5000  current step :  2295
total : 5000  current step :  2296
total : 5000  current step :  2297
total : 5000  current step :  2298
total : 5000  current step :  2299
total : 5000  current step :  2300
total : 5000  current step :  2301
total : 5000  current step :  2302
total : 5000  current step :  2303
total : 5000  current step :  2304
total : 5000  current step :  2305
total : 5000  current step :  2306
total : 5000  current step :  2307
total : 5000  current step :  2308
total : 5000  current step :  2309
total : 5000  current step :  2310
total : 5000  current step :  2311
total : 5000  current step :  2312
total : 5000  current step :  2313
total : 5000  current step :  2314
total : 5000  current step :  2315
total : 5000  current step :  2316
total : 5000  current step :  2317
total : 5000  current step :  2318
total : 5000  current step :  2319
total : 5000  current step :  2320
total : 5000  current step :  2321
total : 5000  current step :  2322
total : 5000  current step :  2323
total : 5000  current step :  2324
total : 5000  current step :  2325
total : 5000  current step :  2326
total : 5000  current step :  2327
total : 5000  current step :  2328
total : 5000  current step :  2329
total : 5000  current step :  2330
total : 5000  current step :  2331
total : 5000  current step :  2332
total : 5000  current step :  2333
total : 5000  current step :  2334
total : 5000  current step :  2335
total : 5000  current step :  2336
total : 5000  current step :  2337
total : 5000  current step :  2338
total : 5000  current step :  2339
total : 5000  current step :  2340
total : 5000  current step :  2341
total : 5000  current step :  2342
total : 5000  current step :  2343
total : 5000  current step :  2344
total : 5000  current step :  2345
total : 5000  current step :  2346
total : 5000  current step :  2347
total : 5000  current step :  2348
total : 5000  current step :  2349
total : 5000  current step :  2350
total : 5000  current step :  2351
total : 5000  current step :  2352
total : 5000  current step :  2353
total : 5000  current step :  2354
total : 5000  current step :  2355
total : 5000  current step :  2356
total : 5000  current step :  2357
total : 5000  current step :  2358
total : 5000  current step :  2359
total : 5000  current step :  2360
total : 5000  current step :  2361
total : 5000  current step :  2362
total : 5000  current step :  2363
total : 5000  current step :  2364
total : 5000  current step :  2365
total : 5000  current step :  2366
total : 5000  current step :  2367
total : 5000  current step :  2368
total : 5000  current step :  2369
total : 5000  current step :  2370
total : 5000  current step :  2371
total : 5000  current step :  2372
total : 5000  current step :  2373
total : 5000  current step :  2374
total : 5000  current step :  2375
total : 5000  current step :  2376
total : 5000  current step :  2377
total : 5000  current step :  2378
total : 5000  current step :  2379
total : 5000  current step :  2380
total : 5000  current step :  2381
total : 5000  current step :  2382
total : 5000  current step :  2383
total : 5000  current step :  2384
total : 5000  current step :  2385
total : 5000  current step :  2386
total : 5000  current step :  2387
total : 5000  current step :  2388
total : 5000  current step :  2389
total : 5000  current step :  2390
total : 5000  current step :  2391
total : 5000  current step :  2392
total : 5000  current step :  2393
total : 5000  current step :  2394
total : 5000  current step :  2395
total : 5000  current step :  2396
total : 5000  current step :  2397
total : 5000  current step :  2398
total : 5000  current step :  2399
total : 5000  current step :  2400
total : 5000  current step :  2401
total : 5000  current step :  2402
total : 5000  current step :  2403
total : 5000  current step :  2404
total : 5000  current step :  2405
total : 5000  current step :  2406
total : 5000  current step :  2407
total : 5000  current step :  2408
total : 5000  current step :  2409
total : 5000  current step :  2410
total : 5000  current step :  2411
total : 5000  current step :  2412
total : 5000  current step :  2413
total : 5000  current step :  2414
total : 5000  current step :  2415
total : 5000  current step :  2416
total : 5000  current step :  2417
total : 5000  current step :  2418
total : 5000  current step :  2419
total : 5000  current step :  2420
total : 5000  current step :  2421
total : 5000  current step :  2422
total : 5000  current step :  2423
total : 5000  current step :  2424
total : 5000  current step :  2425
total : 5000  current step :  2426
total : 5000  current step :  2427
total : 5000  current step :  2428
total : 5000  current step :  2429
total : 5000  current step :  2430
total : 5000  current step :  2431
total : 5000  current step :  2432
total : 5000  current step :  2433
total : 5000  current step :  2434
total : 5000  current step :  2435
total : 5000  current step :  2436
total : 5000  current step :  2437
total : 5000  current step :  2438
total : 5000  current step :  2439
total : 5000  current step :  2440
total : 5000  current step :  2441
total : 5000  current step :  2442
total : 5000  current step :  2443
total : 5000  current step :  2444
total : 5000  current step :  2445
total : 5000  current step :  2446
total : 5000  current step :  2447
total : 5000  current step :  2448
total : 5000  current step :  2449
total : 5000  current step :  2450
total : 5000  current step :  2451
total : 5000  current step :  2452
total : 5000  current step :  2453
total : 5000  current step :  2454
total : 5000  current step :  2455
total : 5000  current step :  2456
total : 5000  current step :  2457
total : 5000  current step :  2458
total : 5000  current step :  2459
total : 5000  current step :  2460
total : 5000  current step :  2461
total : 5000  current step :  2462
total : 5000  current step :  2463
total : 5000  current step :  2464
total : 5000  current step :  2465
total : 5000  current step :  2466
total : 5000  current step :  2467
total : 5000  current step :  2468
total : 5000  current step :  2469
total : 5000  current step :  2470
total : 5000  current step :  2471
total : 5000  current step :  2472
total : 5000  current step :  2473
total : 5000  current step :  2474
total : 5000  current step :  2475
total : 5000  current step :  2476
total : 5000  current step :  2477
total : 5000  current step :  2478
total : 5000  current step :  2479
total : 5000  current step :  2480
total : 5000  current step :  2481
total : 5000  current step :  2482
total : 5000  current step :  2483
total : 5000  current step :  2484
total : 5000  current step :  2485
total : 5000  current step :  2486
total : 5000  current step :  2487
total : 5000  current step :  2488
total : 5000  current step :  2489
total : 5000  current step :  2490
total : 5000  current step :  2491
total : 5000  current step :  2492
total : 5000  current step :  2493
total : 5000  current step :  2494
total : 5000  current step :  2495
total : 5000  current step :  2496
total : 5000  current step :  2497
total : 5000  current step :  2498
total : 5000  current step :  2499
total : 5000  current step :  2500
total : 5000  current step :  2501
total : 5000  current step :  2502
total : 5000  current step :  2503
total : 5000  current step :  2504
total : 5000  current step :  2505
total : 5000  current step :  2506
total : 5000  current step :  2507
total : 5000  current step :  2508
total : 5000  current step :  2509
total : 5000  current step :  2510
total : 5000  current step :  2511
total : 5000  current step :  2512
total : 5000  current step :  2513
total : 5000  current step :  2514
total : 5000  current step :  2515
total : 5000  current step :  2516
total : 5000  current step :  2517
total : 5000  current step :  2518
total : 5000  current step :  2519
total : 5000  current step :  2520
total : 5000  current step :  2521
total : 5000  current step :  2522
total : 5000  current step :  2523
total : 5000  current step :  2524
total : 5000  current step :  2525
total : 5000  current step :  2526
total : 5000  current step :  2527
total : 5000  current step :  2528
total : 5000  current step :  2529
total : 5000  current step :  2530
total : 5000  current step :  2531
total : 5000  current step :  2532
total : 5000  current step :  2533
total : 5000  current step :  2534
total : 5000  current step :  2535
total : 5000  current step :  2536
total : 5000  current step :  2537
total : 5000  current step :  2538
total : 5000  current step :  2539
total : 5000  current step :  2540
total : 5000  current step :  2541
total : 5000  current step :  2542
total : 5000  current step :  2543
total : 5000  current step :  2544
total : 5000  current step :  2545
total : 5000  current step :  2546
total : 5000  current step :  2547
total : 5000  current step :  2548
total : 5000  current step :  2549
total : 5000  current step :  2550
total : 5000  current step :  2551
total : 5000  current step :  2552
total : 5000  current step :  2553
total : 5000  current step :  2554
total : 5000  current step :  2555
total : 5000  current step :  2556
total : 5000  current step :  2557
total : 5000  current step :  2558
total : 5000  current step :  2559
total : 5000  current step :  2560
total : 5000  current step :  2561
total : 5000  current step :  2562
total : 5000  current step :  2563
total : 5000  current step :  2564
total : 5000  current step :  2565
total : 5000  current step :  2566
total : 5000  current step :  2567
total : 5000  current step :  2568
total : 5000  current step :  2569
total : 5000  current step :  2570
total : 5000  current step :  2571
total : 5000  current step :  2572
total : 5000  current step :  2573
total : 5000  current step :  2574
total : 5000  current step :  2575
total : 5000  current step :  2576
total : 5000  current step :  2577
total : 5000  current step :  2578
total : 5000  current step :  2579
total : 5000  current step :  2580
total : 5000  current step :  2581
total : 5000  current step :  2582
total : 5000  current step :  2583
total : 5000  current step :  2584
total : 5000  current step :  2585
total : 5000  current step :  2586
total : 5000  current step :  2587
total : 5000  current step :  2588
total : 5000  current step :  2589
total : 5000  current step :  2590
total : 5000  current step :  2591
total : 5000  current step :  2592
total : 5000  current step :  2593
total : 5000  current step :  2594
total : 5000  current step :  2595
total : 5000  current step :  2596
total : 5000  current step :  2597
total : 5000  current step :  2598
total : 5000  current step :  2599
total : 5000  current step :  2600
total : 5000  current step :  2601
total : 5000  current step :  2602
total : 5000  current step :  2603
total : 5000  current step :  2604
total : 5000  current step :  2605
total : 5000  current step :  2606
total : 5000  current step :  2607
total : 5000  current step :  2608
total : 5000  current step :  2609
total : 5000  current step :  2610
total : 5000  current step :  2611
total : 5000  current step :  2612
total : 5000  current step :  2613
total : 5000  current step :  2614
total : 5000  current step :  2615
total : 5000  current step :  2616
total : 5000  current step :  2617
total : 5000  current step :  2618
total : 5000  current step :  2619
total : 5000  current step :  2620
total : 5000  current step :  2621
total : 5000  current step :  2622
total : 5000  current step :  2623
total : 5000  current step :  2624
total : 5000  current step :  2625
total : 5000  current step :  2626
total : 5000  current step :  2627
total : 5000  current step :  2628
total : 5000  current step :  2629
total : 5000  current step :  2630
total : 5000  current step :  2631
total : 5000  current step :  2632
total : 5000  current step :  2633
total : 5000  current step :  2634
total : 5000  current step :  2635
total : 5000  current step :  2636
total : 5000  current step :  2637
total : 5000  current step :  2638
total : 5000  current step :  2639
total : 5000  current step :  2640
total : 5000  current step :  2641
total : 5000  current step :  2642
total : 5000  current step :  2643
total : 5000  current step :  2644
total : 5000  current step :  2645
total : 5000  current step :  2646
total : 5000  current step :  2647
total : 5000  current step :  2648
total : 5000  current step :  2649
total : 5000  current step :  2650
total : 5000  current step :  2651
total : 5000  current step :  2652
total : 5000  current step :  2653
total : 5000  current step :  2654
total : 5000  current step :  2655
total : 5000  current step :  2656
total : 5000  current step :  2657
total : 5000  current step :  2658
total : 5000  current step :  2659
total : 5000  current step :  2660
total : 5000  current step :  2661
total : 5000  current step :  2662
total : 5000  current step :  2663
total : 5000  current step :  2664
total : 5000  current step :  2665
total : 5000  current step :  2666
total : 5000  current step :  2667
total : 5000  current step :  2668
total : 5000  current step :  2669
total : 5000  current step :  2670
total : 5000  current step :  2671
total : 5000  current step :  2672
total : 5000  current step :  2673
total : 5000  current step :  2674
total : 5000  current step :  2675
total : 5000  current step :  2676
total : 5000  current step :  2677
total : 5000  current step :  2678
total : 5000  current step :  2679
total : 5000  current step :  2680
total : 5000  current step :  2681
total : 5000  current step :  2682
total : 5000  current step :  2683
total : 5000  current step :  2684
total : 5000  current step :  2685
total : 5000  current step :  2686
total : 5000  current step :  2687
total : 5000  current step :  2688
total : 5000  current step :  2689
total : 5000  current step :  2690
total : 5000  current step :  2691
total : 5000  current step :  2692
total : 5000  current step :  2693
total : 5000  current step :  2694
total : 5000  current step :  2695
total : 5000  current step :  2696
total : 5000  current step :  2697
total : 5000  current step :  2698
total : 5000  current step :  2699
total : 5000  current step :  2700
total : 5000  current step :  2701
total : 5000  current step :  2702
total : 5000  current step :  2703
total : 5000  current step :  2704
total : 5000  current step :  2705
total : 5000  current step :  2706
total : 5000  current step :  2707
total : 5000  current step :  2708
total : 5000  current step :  2709
total : 5000  current step :  2710
total : 5000  current step :  2711
total : 5000  current step :  2712
total : 5000  current step :  2713
total : 5000  current step :  2714
total : 5000  current step :  2715
total : 5000  current step :  2716
total : 5000  current step :  2717
total : 5000  current step :  2718
total : 5000  current step :  2719
total : 5000  current step :  2720
total : 5000  current step :  2721
total : 5000  current step :  2722
total : 5000  current step :  2723
total : 5000  current step :  2724
total : 5000  current step :  2725
total : 5000  current step :  2726
total : 5000  current step :  2727
total : 5000  current step :  2728
total : 5000  current step :  2729
total : 5000  current step :  2730
total : 5000  current step :  2731
total : 5000  current step :  2732
total : 5000  current step :  2733
total : 5000  current step :  2734
total : 5000  current step :  2735
total : 5000  current step :  2736
total : 5000  current step :  2737
total : 5000  current step :  2738
total : 5000  current step :  2739
total : 5000  current step :  2740
total : 5000  current step :  2741
total : 5000  current step :  2742
total : 5000  current step :  2743
total : 5000  current step :  2744
total : 5000  current step :  2745
total : 5000  current step :  2746
total : 5000  current step :  2747
total : 5000  current step :  2748
total : 5000  current step :  2749
total : 5000  current step :  2750
total : 5000  current step :  2751
total : 5000  current step :  2752
total : 5000  current step :  2753
total : 5000  current step :  2754
total : 5000  current step :  2755
total : 5000  current step :  2756
total : 5000  current step :  2757
total : 5000  current step :  2758
total : 5000  current step :  2759
total : 5000  current step :  2760
total : 5000  current step :  2761
total : 5000  current step :  2762
total : 5000  current step :  2763
total : 5000  current step :  2764
total : 5000  current step :  2765
total : 5000  current step :  2766
total : 5000  current step :  2767
total : 5000  current step :  2768
total : 5000  current step :  2769
total : 5000  current step :  2770
total : 5000  current step :  2771
total : 5000  current step :  2772
total : 5000  current step :  2773
total : 5000  current step :  2774
total : 5000  current step :  2775
total : 5000  current step :  2776
total : 5000  current step :  2777
total : 5000  current step :  2778
total : 5000  current step :  2779
total : 5000  current step :  2780
total : 5000  current step :  2781
total : 5000  current step :  2782
total : 5000  current step :  2783
total : 5000  current step :  2784
total : 5000  current step :  2785
total : 5000  current step :  2786
total : 5000  current step :  2787
total : 5000  current step :  2788
total : 5000  current step :  2789
total : 5000  current step :  2790
total : 5000  current step :  2791
total : 5000  current step :  2792
total : 5000  current step :  2793
total : 5000  current step :  2794
total : 5000  current step :  2795
total : 5000  current step :  2796
total : 5000  current step :  2797
total : 5000  current step :  2798
total : 5000  current step :  2799
total : 5000  current step :  2800
total : 5000  current step :  2801
total : 5000  current step :  2802
total : 5000  current step :  2803
total : 5000  current step :  2804
total : 5000  current step :  2805
total : 5000  current step :  2806
total : 5000  current step :  2807
total : 5000  current step :  2808
total : 5000  current step :  2809
total : 5000  current step :  2810
total : 5000  current step :  2811
total : 5000  current step :  2812
total : 5000  current step :  2813
total : 5000  current step :  2814
total : 5000  current step :  2815
total : 5000  current step :  2816
total : 5000  current step :  2817
total : 5000  current step :  2818
total : 5000  current step :  2819
total : 5000  current step :  2820
total : 5000  current step :  2821
total : 5000  current step :  2822
total : 5000  current step :  2823
total : 5000  current step :  2824
total : 5000  current step :  2825
total : 5000  current step :  2826
total : 5000  current step :  2827
total : 5000  current step :  2828
total : 5000  current step :  2829
total : 5000  current step :  2830
total : 5000  current step :  2831
total : 5000  current step :  2832
total : 5000  current step :  2833
total : 5000  current step :  2834
total : 5000  current step :  2835
total : 5000  current step :  2836
total : 5000  current step :  2837
total : 5000  current step :  2838
total : 5000  current step :  2839
total : 5000  current step :  2840
total : 5000  current step :  2841
total : 5000  current step :  2842
total : 5000  current step :  2843
total : 5000  current step :  2844
total : 5000  current step :  2845
total : 5000  current step :  2846
total : 5000  current step :  2847
total : 5000  current step :  2848
total : 5000  current step :  2849
total : 5000  current step :  2850
total : 5000  current step :  2851
total : 5000  current step :  2852
total : 5000  current step :  2853
total : 5000  current step :  2854
total : 5000  current step :  2855
total : 5000  current step :  2856
total : 5000  current step :  2857
total : 5000  current step :  2858
total : 5000  current step :  2859
total : 5000  current step :  2860
total : 5000  current step :  2861
total : 5000  current step :  2862
total : 5000  current step :  2863
total : 5000  current step :  2864
total : 5000  current step :  2865
total : 5000  current step :  2866
total : 5000  current step :  2867
total : 5000  current step :  2868
total : 5000  current step :  2869
total : 5000  current step :  2870
total : 5000  current step :  2871
total : 5000  current step :  2872
total : 5000  current step :  2873
total : 5000  current step :  2874
total : 5000  current step :  2875
total : 5000  current step :  2876
total : 5000  current step :  2877
total : 5000  current step :  2878
total : 5000  current step :  2879
total : 5000  current step :  2880
total : 5000  current step :  2881
total : 5000  current step :  2882
total : 5000  current step :  2883
total : 5000  current step :  2884
total : 5000  current step :  2885
total : 5000  current step :  2886
total : 5000  current step :  2887
total : 5000  current step :  2888
total : 5000  current step :  2889
total : 5000  current step :  2890
total : 5000  current step :  2891
total : 5000  current step :  2892
total : 5000  current step :  2893
total : 5000  current step :  2894
total : 5000  current step :  2895
total : 5000  current step :  2896
total : 5000  current step :  2897
total : 5000  current step :  2898
total : 5000  current step :  2899
total : 5000  current step :  2900
total : 5000  current step :  2901
total : 5000  current step :  2902
total : 5000  current step :  2903
total : 5000  current step :  2904
total : 5000  current step :  2905
total : 5000  current step :  2906
total : 5000  current step :  2907
total : 5000  current step :  2908
total : 5000  current step :  2909
total : 5000  current step :  2910
total : 5000  current step :  2911
total : 5000  current step :  2912
total : 5000  current step :  2913
total : 5000  current step :  2914
total : 5000  current step :  2915
total : 5000  current step :  2916
total : 5000  current step :  2917
total : 5000  current step :  2918
total : 5000  current step :  2919
total : 5000  current step :  2920
total : 5000  current step :  2921
total : 5000  current step :  2922
total : 5000  current step :  2923
total : 5000  current step :  2924
total : 5000  current step :  2925
total : 5000  current step :  2926
total : 5000  current step :  2927
total : 5000  current step :  2928
total : 5000  current step :  2929
total : 5000  current step :  2930
total : 5000  current step :  2931
total : 5000  current step :  2932
total : 5000  current step :  2933
total : 5000  current step :  2934
total : 5000  current step :  2935
total : 5000  current step :  2936
total : 5000  current step :  2937
total : 5000  current step :  2938
total : 5000  current step :  2939
total : 5000  current step :  2940
total : 5000  current step :  2941
total : 5000  current step :  2942
total : 5000  current step :  2943
total : 5000  current step :  2944
total : 5000  current step :  2945
total : 5000  current step :  2946
total : 5000  current step :  2947
total : 5000  current step :  2948
total : 5000  current step :  2949
total : 5000  current step :  2950
total : 5000  current step :  2951
total : 5000  current step :  2952
total : 5000  current step :  2953
total : 5000  current step :  2954
total : 5000  current step :  2955
total : 5000  current step :  2956
total : 5000  current step :  2957
total : 5000  current step :  2958
total : 5000  current step :  2959
total : 5000  current step :  2960
total : 5000  current step :  2961
total : 5000  current step :  2962
total : 5000  current step :  2963
total : 5000  current step :  2964
total : 5000  current step :  2965
total : 5000  current step :  2966
total : 5000  current step :  2967
total : 5000  current step :  2968
total : 5000  current step :  2969
total : 5000  current step :  2970
total : 5000  current step :  2971
total : 5000  current step :  2972
total : 5000  current step :  2973
total : 5000  current step :  2974
total : 5000  current step :  2975
total : 5000  current step :  2976
total : 5000  current step :  2977
total : 5000  current step :  2978
total : 5000  current step :  2979
total : 5000  current step :  2980
total : 5000  current step :  2981
total : 5000  current step :  2982
total : 5000  current step :  2983
total : 5000  current step :  2984
total : 5000  current step :  2985
total : 5000  current step :  2986
total : 5000  current step :  2987
total : 5000  current step :  2988
total : 5000  current step :  2989
total : 5000  current step :  2990
total : 5000  current step :  2991
total : 5000  current step :  2992
total : 5000  current step :  2993
total : 5000  current step :  2994
total : 5000  current step :  2995
total : 5000  current step :  2996
total : 5000  current step :  2997
total : 5000  current step :  2998
total : 5000  current step :  2999
total : 5000  current step :  3000
total : 5000  current step :  3001
total : 5000  current step :  3002
total : 5000  current step :  3003
total : 5000  current step :  3004
total : 5000  current step :  3005
total : 5000  current step :  3006
total : 5000  current step :  3007
total : 5000  current step :  3008
total : 5000  current step :  3009
total : 5000  current step :  3010
total : 5000  current step :  3011
total : 5000  current step :  3012
total : 5000  current step :  3013
total : 5000  current step :  3014
total : 5000  current step :  3015
total : 5000  current step :  3016
total : 5000  current step :  3017
total : 5000  current step :  3018
total : 5000  current step :  3019
total : 5000  current step :  3020
total : 5000  current step :  3021
total : 5000  current step :  3022
total : 5000  current step :  3023
total : 5000  current step :  3024
total : 5000  current step :  3025
total : 5000  current step :  3026
total : 5000  current step :  3027
total : 5000  current step :  3028
total : 5000  current step :  3029
total : 5000  current step :  3030
total : 5000  current step :  3031
total : 5000  current step :  3032
total : 5000  current step :  3033
total : 5000  current step :  3034
total : 5000  current step :  3035
total : 5000  current step :  3036
total : 5000  current step :  3037
total : 5000  current step :  3038
total : 5000  current step :  3039
total : 5000  current step :  3040
total : 5000  current step :  3041
total : 5000  current step :  3042
total : 5000  current step :  3043
total : 5000  current step :  3044
total : 5000  current step :  3045
total : 5000  current step :  3046
total : 5000  current step :  3047
total : 5000  current step :  3048
total : 5000  current step :  3049
total : 5000  current step :  3050
total : 5000  current step :  3051
total : 5000  current step :  3052
total : 5000  current step :  3053
total : 5000  current step :  3054
total : 5000  current step :  3055
total : 5000  current step :  3056
total : 5000  current step :  3057
total : 5000  current step :  3058
total : 5000  current step :  3059
total : 5000  current step :  3060
total : 5000  current step :  3061
total : 5000  current step :  3062
total : 5000  current step :  3063
total : 5000  current step :  3064
total : 5000  current step :  3065
total : 5000  current step :  3066
total : 5000  current step :  3067
total : 5000  current step :  3068
total : 5000  current step :  3069
total : 5000  current step :  3070
total : 5000  current step :  3071
total : 5000  current step :  3072
total : 5000  current step :  3073
total : 5000  current step :  3074
total : 5000  current step :  3075
total : 5000  current step :  3076
total : 5000  current step :  3077
total : 5000  current step :  3078
total : 5000  current step :  3079
total : 5000  current step :  3080
total : 5000  current step :  3081
total : 5000  current step :  3082
total : 5000  current step :  3083
total : 5000  current step :  3084
total : 5000  current step :  3085
total : 5000  current step :  3086
total : 5000  current step :  3087
total : 5000  current step :  3088
total : 5000  current step :  3089
total : 5000  current step :  3090
total : 5000  current step :  3091
total : 5000  current step :  3092
total : 5000  current step :  3093
total : 5000  current step :  3094
total : 5000  current step :  3095
total : 5000  current step :  3096
total : 5000  current step :  3097
total : 5000  current step :  3098
total : 5000  current step :  3099
total : 5000  current step :  3100
total : 5000  current step :  3101
total : 5000  current step :  3102
total : 5000  current step :  3103
total : 5000  current step :  3104
total : 5000  current step :  3105
total : 5000  current step :  3106
total : 5000  current step :  3107
total : 5000  current step :  3108
total : 5000  current step :  3109
total : 5000  current step :  3110
total : 5000  current step :  3111
total : 5000  current step :  3112
total : 5000  current step :  3113
total : 5000  current step :  3114
total : 5000  current step :  3115
total : 5000  current step :  3116
total : 5000  current step :  3117
total : 5000  current step :  3118
total : 5000  current step :  3119
total : 5000  current step :  3120
total : 5000  current step :  3121
total : 5000  current step :  3122
total : 5000  current step :  3123
total : 5000  current step :  3124
total : 5000  current step :  3125
total : 5000  current step :  3126
total : 5000  current step :  3127
total : 5000  current step :  3128
total : 5000  current step :  3129
total : 5000  current step :  3130
total : 5000  current step :  3131
total : 5000  current step :  3132
total : 5000  current step :  3133
total : 5000  current step :  3134
total : 5000  current step :  3135
total : 5000  current step :  3136
total : 5000  current step :  3137
total : 5000  current step :  3138
total : 5000  current step :  3139
total : 5000  current step :  3140
total : 5000  current step :  3141
total : 5000  current step :  3142
total : 5000  current step :  3143
total : 5000  current step :  3144
total : 5000  current step :  3145
total : 5000  current step :  3146
total : 5000  current step :  3147
total : 5000  current step :  3148
total : 5000  current step :  3149
total : 5000  current step :  3150
total : 5000  current step :  3151
total : 5000  current step :  3152
total : 5000  current step :  3153
total : 5000  current step :  3154
total : 5000  current step :  3155
total : 5000  current step :  3156
total : 5000  current step :  3157
total : 5000  current step :  3158
total : 5000  current step :  3159
total : 5000  current step :  3160
total : 5000  current step :  3161
total : 5000  current step :  3162
total : 5000  current step :  3163
total : 5000  current step :  3164
total : 5000  current step :  3165
total : 5000  current step :  3166
total : 5000  current step :  3167
total : 5000  current step :  3168
total : 5000  current step :  3169
total : 5000  current step :  3170
total : 5000  current step :  3171
total : 5000  current step :  3172
total : 5000  current step :  3173
total : 5000  current step :  3174
total : 5000  current step :  3175
total : 5000  current step :  3176
total : 5000  current step :  3177
total : 5000  current step :  3178
total : 5000  current step :  3179
total : 5000  current step :  3180
total : 5000  current step :  3181
total : 5000  current step :  3182
total : 5000  current step :  3183
total : 5000  current step :  3184
total : 5000  current step :  3185
total : 5000  current step :  3186
total : 5000  current step :  3187
total : 5000  current step :  3188
total : 5000  current step :  3189
total : 5000  current step :  3190
total : 5000  current step :  3191
total : 5000  current step :  3192
total : 5000  current step :  3193
total : 5000  current step :  3194
total : 5000  current step :  3195
total : 5000  current step :  3196
total : 5000  current step :  3197
total : 5000  current step :  3198
total : 5000  current step :  3199
total : 5000  current step :  3200
total : 5000  current step :  3201
total : 5000  current step :  3202
total : 5000  current step :  3203
total : 5000  current step :  3204
total : 5000  current step :  3205
total : 5000  current step :  3206
total : 5000  current step :  3207
total : 5000  current step :  3208
total : 5000  current step :  3209
total : 5000  current step :  3210
total : 5000  current step :  3211
total : 5000  current step :  3212
total : 5000  current step :  3213
total : 5000  current step :  3214
total : 5000  current step :  3215
total : 5000  current step :  3216
total : 5000  current step :  3217
total : 5000  current step :  3218
total : 5000  current step :  3219
total : 5000  current step :  3220
total : 5000  current step :  3221
total : 5000  current step :  3222
total : 5000  current step :  3223
total : 5000  current step :  3224
total : 5000  current step :  3225
total : 5000  current step :  3226
total : 5000  current step :  3227
total : 5000  current step :  3228
total : 5000  current step :  3229
total : 5000  current step :  3230
total : 5000  current step :  3231
total : 5000  current step :  3232
total : 5000  current step :  3233
total : 5000  current step :  3234
total : 5000  current step :  3235
total : 5000  current step :  3236
total : 5000  current step :  3237
total : 5000  current step :  3238
total : 5000  current step :  3239
total : 5000  current step :  3240
total : 5000  current step :  3241
total : 5000  current step :  3242
total : 5000  current step :  3243
total : 5000  current step :  3244
total : 5000  current step :  3245
total : 5000  current step :  3246
total : 5000  current step :  3247
total : 5000  current step :  3248
total : 5000  current step :  3249
total : 5000  current step :  3250
total : 5000  current step :  3251
total : 5000  current step :  3252
total : 5000  current step :  3253
total : 5000  current step :  3254
total : 5000  current step :  3255
total : 5000  current step :  3256
total : 5000  current step :  3257
total : 5000  current step :  3258
total : 5000  current step :  3259
total : 5000  current step :  3260
total : 5000  current step :  3261
total : 5000  current step :  3262
total : 5000  current step :  3263
total : 5000  current step :  3264
total : 5000  current step :  3265
total : 5000  current step :  3266
total : 5000  current step :  3267
total : 5000  current step :  3268
total : 5000  current step :  3269
total : 5000  current step :  3270
total : 5000  current step :  3271
total : 5000  current step :  3272
total : 5000  current step :  3273
total : 5000  current step :  3274
total : 5000  current step :  3275
total : 5000  current step :  3276
total : 5000  current step :  3277
total : 5000  current step :  3278
total : 5000  current step :  3279
total : 5000  current step :  3280
total : 5000  current step :  3281
total : 5000  current step :  3282
total : 5000  current step :  3283
total : 5000  current step :  3284
total : 5000  current step :  3285
total : 5000  current step :  3286
total : 5000  current step :  3287
total : 5000  current step :  3288
total : 5000  current step :  3289
total : 5000  current step :  3290
total : 5000  current step :  3291
total : 5000  current step :  3292
total : 5000  current step :  3293
total : 5000  current step :  3294
total : 5000  current step :  3295
total : 5000  current step :  3296
total : 5000  current step :  3297
total : 5000  current step :  3298
total : 5000  current step :  3299
total : 5000  current step :  3300
total : 5000  current step :  3301
total : 5000  current step :  3302
total : 5000  current step :  3303
total : 5000  current step :  3304
total : 5000  current step :  3305
total : 5000  current step :  3306
total : 5000  current step :  3307
total : 5000  current step :  3308
total : 5000  current step :  3309
total : 5000  current step :  3310
total : 5000  current step :  3311
total : 5000  current step :  3312
total : 5000  current step :  3313
total : 5000  current step :  3314
total : 5000  current step :  3315
total : 5000  current step :  3316
total : 5000  current step :  3317
total : 5000  current step :  3318
total : 5000  current step :  3319
total : 5000  current step :  3320
total : 5000  current step :  3321
total : 5000  current step :  3322
total : 5000  current step :  3323
total : 5000  current step :  3324
total : 5000  current step :  3325
total : 5000  current step :  3326
total : 5000  current step :  3327
total : 5000  current step :  3328
total : 5000  current step :  3329
total : 5000  current step :  3330
total : 5000  current step :  3331
total : 5000  current step :  3332
total : 5000  current step :  3333
total : 5000  current step :  3334
total : 5000  current step :  3335
total : 5000  current step :  3336
total : 5000  current step :  3337
total : 5000  current step :  3338
total : 5000  current step :  3339
total : 5000  current step :  3340
total : 5000  current step :  3341
total : 5000  current step :  3342
total : 5000  current step :  3343
total : 5000  current step :  3344
total : 5000  current step :  3345
total : 5000  current step :  3346
total : 5000  current step :  3347
total : 5000  current step :  3348
total : 5000  current step :  3349
total : 5000  current step :  3350
total : 5000  current step :  3351
total : 5000  current step :  3352
total : 5000  current step :  3353
total : 5000  current step :  3354
total : 5000  current step :  3355
total : 5000  current step :  3356
total : 5000  current step :  3357
total : 5000  current step :  3358
total : 5000  current step :  3359
total : 5000  current step :  3360
total : 5000  current step :  3361
total : 5000  current step :  3362
total : 5000  current step :  3363
total : 5000  current step :  3364
total : 5000  current step :  3365
total : 5000  current step :  3366
total : 5000  current step :  3367
total : 5000  current step :  3368
total : 5000  current step :  3369
total : 5000  current step :  3370
total : 5000  current step :  3371
total : 5000  current step :  3372
total : 5000  current step :  3373
total : 5000  current step :  3374
total : 5000  current step :  3375
total : 5000  current step :  3376
total : 5000  current step :  3377
total : 5000  current step :  3378
total : 5000  current step :  3379
total : 5000  current step :  3380
total : 5000  current step :  3381
total : 5000  current step :  3382
total : 5000  current step :  3383
total : 5000  current step :  3384
total : 5000  current step :  3385
total : 5000  current step :  3386
total : 5000  current step :  3387
total : 5000  current step :  3388
total : 5000  current step :  3389
total : 5000  current step :  3390
total : 5000  current step :  3391
total : 5000  current step :  3392
total : 5000  current step :  3393
total : 5000  current step :  3394
total : 5000  current step :  3395
total : 5000  current step :  3396
total : 5000  current step :  3397
total : 5000  current step :  3398
total : 5000  current step :  3399
total : 5000  current step :  3400
total : 5000  current step :  3401
total : 5000  current step :  3402
total : 5000  current step :  3403
total : 5000  current step :  3404
total : 5000  current step :  3405
total : 5000  current step :  3406
total : 5000  current step :  3407
total : 5000  current step :  3408
total : 5000  current step :  3409
total : 5000  current step :  3410
total : 5000  current step :  3411
total : 5000  current step :  3412
total : 5000  current step :  3413
total : 5000  current step :  3414
total : 5000  current step :  3415
total : 5000  current step :  3416
total : 5000  current step :  3417
total : 5000  current step :  3418
total : 5000  current step :  3419
total : 5000  current step :  3420
total : 5000  current step :  3421
total : 5000  current step :  3422
total : 5000  current step :  3423
total : 5000  current step :  3424
total : 5000  current step :  3425
total : 5000  current step :  3426
total : 5000  current step :  3427
total : 5000  current step :  3428
total : 5000  current step :  3429
total : 5000  current step :  3430
total : 5000  current step :  3431
total : 5000  current step :  3432
total : 5000  current step :  3433
total : 5000  current step :  3434
total : 5000  current step :  3435
total : 5000  current step :  3436
total : 5000  current step :  3437
total : 5000  current step :  3438
total : 5000  current step :  3439
total : 5000  current step :  3440
total : 5000  current step :  3441
total : 5000  current step :  3442
total : 5000  current step :  3443
total : 5000  current step :  3444
total : 5000  current step :  3445
total : 5000  current step :  3446
total : 5000  current step :  3447
total : 5000  current step :  3448
total : 5000  current step :  3449
total : 5000  current step :  3450
total : 5000  current step :  3451
total : 5000  current step :  3452
total : 5000  current step :  3453
total : 5000  current step :  3454
total : 5000  current step :  3455
total : 5000  current step :  3456
total : 5000  current step :  3457
total : 5000  current step :  3458
total : 5000  current step :  3459
total : 5000  current step :  3460
total : 5000  current step :  3461
total : 5000  current step :  3462
total : 5000  current step :  3463
total : 5000  current step :  3464
total : 5000  current step :  3465
total : 5000  current step :  3466
total : 5000  current step :  3467
total : 5000  current step :  3468
total : 5000  current step :  3469
total : 5000  current step :  3470
total : 5000  current step :  3471
total : 5000  current step :  3472
total : 5000  current step :  3473
total : 5000  current step :  3474
total : 5000  current step :  3475
total : 5000  current step :  3476
total : 5000  current step :  3477
total : 5000  current step :  3478
total : 5000  current step :  3479
total : 5000  current step :  3480
total : 5000  current step :  3481
total : 5000  current step :  3482
total : 5000  current step :  3483
total : 5000  current step :  3484
total : 5000  current step :  3485
total : 5000  current step :  3486
total : 5000  current step :  3487
total : 5000  current step :  3488
total : 5000  current step :  3489
total : 5000  current step :  3490
total : 5000  current step :  3491
total : 5000  current step :  3492
total : 5000  current step :  3493
total : 5000  current step :  3494
total : 5000  current step :  3495
total : 5000  current step :  3496
total : 5000  current step :  3497
total : 5000  current step :  3498
total : 5000  current step :  3499
total : 5000  current step :  3500
total : 5000  current step :  3501
total : 5000  current step :  3502
total : 5000  current step :  3503
total : 5000  current step :  3504
total : 5000  current step :  3505
total : 5000  current step :  3506
total : 5000  current step :  3507
total : 5000  current step :  3508
total : 5000  current step :  3509
total : 5000  current step :  3510
total : 5000  current step :  3511
total : 5000  current step :  3512
total : 5000  current step :  3513
total : 5000  current step :  3514
total : 5000  current step :  3515
total : 5000  current step :  3516
total : 5000  current step :  3517
total : 5000  current step :  3518
total : 5000  current step :  3519
total : 5000  current step :  3520
total : 5000  current step :  3521
total : 5000  current step :  3522
total : 5000  current step :  3523
total : 5000  current step :  3524
total : 5000  current step :  3525
total : 5000  current step :  3526
total : 5000  current step :  3527
total : 5000  current step :  3528
total : 5000  current step :  3529
total : 5000  current step :  3530
total : 5000  current step :  3531
total : 5000  current step :  3532
total : 5000  current step :  3533
total : 5000  current step :  3534
total : 5000  current step :  3535
total : 5000  current step :  3536
total : 5000  current step :  3537
total : 5000  current step :  3538
total : 5000  current step :  3539
total : 5000  current step :  3540
total : 5000  current step :  3541
total : 5000  current step :  3542
total : 5000  current step :  3543
total : 5000  current step :  3544
total : 5000  current step :  3545
total : 5000  current step :  3546
total : 5000  current step :  3547
total : 5000  current step :  3548
total : 5000  current step :  3549
total : 5000  current step :  3550
total : 5000  current step :  3551
total : 5000  current step :  3552
total : 5000  current step :  3553
total : 5000  current step :  3554
total : 5000  current step :  3555
total : 5000  current step :  3556
total : 5000  current step :  3557
total : 5000  current step :  3558
total : 5000  current step :  3559
total : 5000  current step :  3560
total : 5000  current step :  3561
total : 5000  current step :  3562
total : 5000  current step :  3563
total : 5000  current step :  3564
total : 5000  current step :  3565
total : 5000  current step :  3566
total : 5000  current step :  3567
total : 5000  current step :  3568
total : 5000  current step :  3569
total : 5000  current step :  3570
total : 5000  current step :  3571
total : 5000  current step :  3572
total : 5000  current step :  3573
total : 5000  current step :  3574
total : 5000  current step :  3575
total : 5000  current step :  3576
total : 5000  current step :  3577
total : 5000  current step :  3578
total : 5000  current step :  3579
total : 5000  current step :  3580
total : 5000  current step :  3581
total : 5000  current step :  3582
total : 5000  current step :  3583
total : 5000  current step :  3584
total : 5000  current step :  3585
total : 5000  current step :  3586
total : 5000  current step :  3587
total : 5000  current step :  3588
total : 5000  current step :  3589
total : 5000  current step :  3590
total : 5000  current step :  3591
total : 5000  current step :  3592
total : 5000  current step :  3593
total : 5000  current step :  3594
total : 5000  current step :  3595
total : 5000  current step :  3596
total : 5000  current step :  3597
total : 5000  current step :  3598
total : 5000  current step :  3599
total : 5000  current step :  3600
total : 5000  current step :  3601
total : 5000  current step :  3602
total : 5000  current step :  3603
total : 5000  current step :  3604
total : 5000  current step :  3605
total : 5000  current step :  3606
total : 5000  current step :  3607
total : 5000  current step :  3608
total : 5000  current step :  3609
total : 5000  current step :  3610
total : 5000  current step :  3611
total : 5000  current step :  3612
total : 5000  current step :  3613
total : 5000  current step :  3614
total : 5000  current step :  3615
total : 5000  current step :  3616
total : 5000  current step :  3617
total : 5000  current step :  3618
total : 5000  current step :  3619
total : 5000  current step :  3620
total : 5000  current step :  3621
total : 5000  current step :  3622
total : 5000  current step :  3623
total : 5000  current step :  3624
total : 5000  current step :  3625
total : 5000  current step :  3626
total : 5000  current step :  3627
total : 5000  current step :  3628
total : 5000  current step :  3629
total : 5000  current step :  3630
total : 5000  current step :  3631
total : 5000  current step :  3632
total : 5000  current step :  3633
total : 5000  current step :  3634
total : 5000  current step :  3635
total : 5000  current step :  3636
total : 5000  current step :  3637
total : 5000  current step :  3638
total : 5000  current step :  3639
total : 5000  current step :  3640
total : 5000  current step :  3641
total : 5000  current step :  3642
total : 5000  current step :  3643
total : 5000  current step :  3644
total : 5000  current step :  3645
total : 5000  current step :  3646
total : 5000  current step :  3647
total : 5000  current step :  3648
total : 5000  current step :  3649
total : 5000  current step :  3650
total : 5000  current step :  3651
total : 5000  current step :  3652
total : 5000  current step :  3653
total : 5000  current step :  3654
total : 5000  current step :  3655
total : 5000  current step :  3656
total : 5000  current step :  3657
total : 5000  current step :  3658
total : 5000  current step :  3659
total : 5000  current step :  3660
total : 5000  current step :  3661
total : 5000  current step :  3662
total : 5000  current step :  3663
total : 5000  current step :  3664
total : 5000  current step :  3665
total : 5000  current step :  3666
total : 5000  current step :  3667
total : 5000  current step :  3668
total : 5000  current step :  3669
total : 5000  current step :  3670
total : 5000  current step :  3671
total : 5000  current step :  3672
total : 5000  current step :  3673
total : 5000  current step :  3674
total : 5000  current step :  3675
total : 5000  current step :  3676
total : 5000  current step :  3677
total : 5000  current step :  3678
total : 5000  current step :  3679
total : 5000  current step :  3680
total : 5000  current step :  3681
total : 5000  current step :  3682
total : 5000  current step :  3683
total : 5000  current step :  3684
total : 5000  current step :  3685
total : 5000  current step :  3686
total : 5000  current step :  3687
total : 5000  current step :  3688
total : 5000  current step :  3689
total : 5000  current step :  3690
total : 5000  current step :  3691
total : 5000  current step :  3692
total : 5000  current step :  3693
total : 5000  current step :  3694
total : 5000  current step :  3695
total : 5000  current step :  3696
total : 5000  current step :  3697
total : 5000  current step :  3698
total : 5000  current step :  3699
total : 5000  current step :  3700
total : 5000  current step :  3701
total : 5000  current step :  3702
total : 5000  current step :  3703
total : 5000  current step :  3704
total : 5000  current step :  3705
total : 5000  current step :  3706
total : 5000  current step :  3707
total : 5000  current step :  3708
total : 5000  current step :  3709
total : 5000  current step :  3710
total : 5000  current step :  3711
total : 5000  current step :  3712
total : 5000  current step :  3713
total : 5000  current step :  3714
total : 5000  current step :  3715
total : 5000  current step :  3716
total : 5000  current step :  3717
total : 5000  current step :  3718
total : 5000  current step :  3719
total : 5000  current step :  3720
total : 5000  current step :  3721
total : 5000  current step :  3722
total : 5000  current step :  3723
total : 5000  current step :  3724
total : 5000  current step :  3725
total : 5000  current step :  3726
total : 5000  current step :  3727
total : 5000  current step :  3728
total : 5000  current step :  3729
total : 5000  current step :  3730
total : 5000  current step :  3731
total : 5000  current step :  3732
total : 5000  current step :  3733
total : 5000  current step :  3734
total : 5000  current step :  3735
total : 5000  current step :  3736
total : 5000  current step :  3737
total : 5000  current step :  3738
total : 5000  current step :  3739
total : 5000  current step :  3740
total : 5000  current step :  3741
total : 5000  current step :  3742
total : 5000  current step :  3743
total : 5000  current step :  3744
total : 5000  current step :  3745
total : 5000  current step :  3746
total : 5000  current step :  3747
total : 5000  current step :  3748
total : 5000  current step :  3749
total : 5000  current step :  3750
total : 5000  current step :  3751
total : 5000  current step :  3752
total : 5000  current step :  3753
total : 5000  current step :  3754
total : 5000  current step :  3755
total : 5000  current step :  3756
total : 5000  current step :  3757
total : 5000  current step :  3758
total : 5000  current step :  3759
total : 5000  current step :  3760
total : 5000  current step :  3761
total : 5000  current step :  3762
total : 5000  current step :  3763
total : 5000  current step :  3764
total : 5000  current step :  3765
total : 5000  current step :  3766
total : 5000  current step :  3767
total : 5000  current step :  3768
total : 5000  current step :  3769
total : 5000  current step :  3770
total : 5000  current step :  3771
total : 5000  current step :  3772
total : 5000  current step :  3773
total : 5000  current step :  3774
total : 5000  current step :  3775
total : 5000  current step :  3776
total : 5000  current step :  3777
total : 5000  current step :  3778
total : 5000  current step :  3779
total : 5000  current step :  3780
total : 5000  current step :  3781
total : 5000  current step :  3782
total : 5000  current step :  3783
total : 5000  current step :  3784
total : 5000  current step :  3785
total : 5000  current step :  3786
total : 5000  current step :  3787
total : 5000  current step :  3788
total : 5000  current step :  3789
total : 5000  current step :  3790
total : 5000  current step :  3791
total : 5000  current step :  3792
total : 5000  current step :  3793
total : 5000  current step :  3794
total : 5000  current step :  3795
total : 5000  current step :  3796
total : 5000  current step :  3797
total : 5000  current step :  3798
total : 5000  current step :  3799
total : 5000  current step :  3800
total : 5000  current step :  3801
total : 5000  current step :  3802
total : 5000  current step :  3803
total : 5000  current step :  3804
total : 5000  current step :  3805
total : 5000  current step :  3806
total : 5000  current step :  3807
total : 5000  current step :  3808
total : 5000  current step :  3809
total : 5000  current step :  3810
total : 5000  current step :  3811
total : 5000  current step :  3812
total : 5000  current step :  3813
total : 5000  current step :  3814
total : 5000  current step :  3815
total : 5000  current step :  3816
total : 5000  current step :  3817
total : 5000  current step :  3818
total : 5000  current step :  3819
total : 5000  current step :  3820
total : 5000  current step :  3821
total : 5000  current step :  3822
total : 5000  current step :  3823
total : 5000  current step :  3824
total : 5000  current step :  3825
total : 5000  current step :  3826
total : 5000  current step :  3827
total : 5000  current step :  3828
total : 5000  current step :  3829
total : 5000  current step :  3830
total : 5000  current step :  3831
total : 5000  current step :  3832
total : 5000  current step :  3833
total : 5000  current step :  3834
total : 5000  current step :  3835
total : 5000  current step :  3836
total : 5000  current step :  3837
total : 5000  current step :  3838
total : 5000  current step :  3839
total : 5000  current step :  3840
total : 5000  current step :  3841
total : 5000  current step :  3842
total : 5000  current step :  3843
total : 5000  current step :  3844
total : 5000  current step :  3845
total : 5000  current step :  3846
total : 5000  current step :  3847
total : 5000  current step :  3848
total : 5000  current step :  3849
total : 5000  current step :  3850
total : 5000  current step :  3851
total : 5000  current step :  3852
total : 5000  current step :  3853
total : 5000  current step :  3854
total : 5000  current step :  3855
total : 5000  current step :  3856
total : 5000  current step :  3857
total : 5000  current step :  3858
total : 5000  current step :  3859
total : 5000  current step :  3860
total : 5000  current step :  3861
total : 5000  current step :  3862
total : 5000  current step :  3863
total : 5000  current step :  3864
total : 5000  current step :  3865
total : 5000  current step :  3866
total : 5000  current step :  3867
total : 5000  current step :  3868
total : 5000  current step :  3869
total : 5000  current step :  3870
total : 5000  current step :  3871
total : 5000  current step :  3872
total : 5000  current step :  3873
total : 5000  current step :  3874
total : 5000  current step :  3875
total : 5000  current step :  3876
total : 5000  current step :  3877
total : 5000  current step :  3878
total : 5000  current step :  3879
total : 5000  current step :  3880
total : 5000  current step :  3881
total : 5000  current step :  3882
total : 5000  current step :  3883
total : 5000  current step :  3884
total : 5000  current step :  3885
total : 5000  current step :  3886
total : 5000  current step :  3887
total : 5000  current step :  3888
total : 5000  current step :  3889
total : 5000  current step :  3890
total : 5000  current step :  3891
total : 5000  current step :  3892
total : 5000  current step :  3893
total : 5000  current step :  3894
total : 5000  current step :  3895
total : 5000  current step :  3896
total : 5000  current step :  3897
total : 5000  current step :  3898
total : 5000  current step :  3899
total : 5000  current step :  3900
total : 5000  current step :  3901
total : 5000  current step :  3902
total : 5000  current step :  3903
total : 5000  current step :  3904
total : 5000  current step :  3905
total : 5000  current step :  3906
total : 5000  current step :  3907
total : 5000  current step :  3908
total : 5000  current step :  3909
total : 5000  current step :  3910
total : 5000  current step :  3911
total : 5000  current step :  3912
total : 5000  current step :  3913
total : 5000  current step :  3914
total : 5000  current step :  3915
total : 5000  current step :  3916
total : 5000  current step :  3917
total : 5000  current step :  3918
total : 5000  current step :  3919
total : 5000  current step :  3920
total : 5000  current step :  3921
total : 5000  current step :  3922
total : 5000  current step :  3923
total : 5000  current step :  3924
total : 5000  current step :  3925
total : 5000  current step :  3926
total : 5000  current step :  3927
total : 5000  current step :  3928
total : 5000  current step :  3929
total : 5000  current step :  3930
total : 5000  current step :  3931
total : 5000  current step :  3932
total : 5000  current step :  3933
total : 5000  current step :  3934
total : 5000  current step :  3935
total : 5000  current step :  3936
total : 5000  current step :  3937
total : 5000  current step :  3938
total : 5000  current step :  3939
total : 5000  current step :  3940
total : 5000  current step :  3941
total : 5000  current step :  3942
total : 5000  current step :  3943
total : 5000  current step :  3944
total : 5000  current step :  3945
total : 5000  current step :  3946
total : 5000  current step :  3947
total : 5000  current step :  3948
total : 5000  current step :  3949
total : 5000  current step :  3950
total : 5000  current step :  3951
total : 5000  current step :  3952
total : 5000  current step :  3953
total : 5000  current step :  3954
total : 5000  current step :  3955
total : 5000  current step :  3956
total : 5000  current step :  3957
total : 5000  current step :  3958
total : 5000  current step :  3959
total : 5000  current step :  3960
total : 5000  current step :  3961
total : 5000  current step :  3962
total : 5000  current step :  3963
total : 5000  current step :  3964
total : 5000  current step :  3965
total : 5000  current step :  3966
total : 5000  current step :  3967
total : 5000  current step :  3968
total : 5000  current step :  3969
total : 5000  current step :  3970
total : 5000  current step :  3971
total : 5000  current step :  3972
total : 5000  current step :  3973
total : 5000  current step :  3974
total : 5000  current step :  3975
total : 5000  current step :  3976
total : 5000  current step :  3977
total : 5000  current step :  3978
total : 5000  current step :  3979
total : 5000  current step :  3980
total : 5000  current step :  3981
total : 5000  current step :  3982
total : 5000  current step :  3983
total : 5000  current step :  3984
total : 5000  current step :  3985
total : 5000  current step :  3986
total : 5000  current step :  3987
total : 5000  current step :  3988
total : 5000  current step :  3989
total : 5000  current step :  3990
total : 5000  current step :  3991
total : 5000  current step :  3992
total : 5000  current step :  3993
total : 5000  current step :  3994
total : 5000  current step :  3995
total : 5000  current step :  3996
total : 5000  current step :  3997
total : 5000  current step :  3998
total : 5000  current step :  3999
total : 5000  current step :  4000
total : 5000  current step :  4001
total : 5000  current step :  4002
total : 5000  current step :  4003
total : 5000  current step :  4004
total : 5000  current step :  4005
total : 5000  current step :  4006
total : 5000  current step :  4007
total : 5000  current step :  4008
total : 5000  current step :  4009
total : 5000  current step :  4010
total : 5000  current step :  4011
total : 5000  current step :  4012
total : 5000  current step :  4013
total : 5000  current step :  4014
total : 5000  current step :  4015
total : 5000  current step :  4016
total : 5000  current step :  4017
total : 5000  current step :  4018
total : 5000  current step :  4019
total : 5000  current step :  4020
total : 5000  current step :  4021
total : 5000  current step :  4022
total : 5000  current step :  4023
total : 5000  current step :  4024
total : 5000  current step :  4025
total : 5000  current step :  4026
total : 5000  current step :  4027
total : 5000  current step :  4028
total : 5000  current step :  4029
total : 5000  current step :  4030
total : 5000  current step :  4031
total : 5000  current step :  4032
total : 5000  current step :  4033
total : 5000  current step :  4034
total : 5000  current step :  4035
total : 5000  current step :  4036
total : 5000  current step :  4037
total : 5000  current step :  4038
total : 5000  current step :  4039
total : 5000  current step :  4040
total : 5000  current step :  4041
total : 5000  current step :  4042
total : 5000  current step :  4043
total : 5000  current step :  4044
total : 5000  current step :  4045
total : 5000  current step :  4046
total : 5000  current step :  4047
total : 5000  current step :  4048
total : 5000  current step :  4049
total : 5000  current step :  4050
total : 5000  current step :  4051
total : 5000  current step :  4052
total : 5000  current step :  4053
total : 5000  current step :  4054
total : 5000  current step :  4055
total : 5000  current step :  4056
total : 5000  current step :  4057
total : 5000  current step :  4058
total : 5000  current step :  4059
total : 5000  current step :  4060
total : 5000  current step :  4061
total : 5000  current step :  4062
total : 5000  current step :  4063
total : 5000  current step :  4064
total : 5000  current step :  4065
total : 5000  current step :  4066
total : 5000  current step :  4067
total : 5000  current step :  4068
total : 5000  current step :  4069
total : 5000  current step :  4070
total : 5000  current step :  4071
total : 5000  current step :  4072
total : 5000  current step :  4073
total : 5000  current step :  4074
total : 5000  current step :  4075
total : 5000  current step :  4076
total : 5000  current step :  4077
total : 5000  current step :  4078
total : 5000  current step :  4079
total : 5000  current step :  4080
total : 5000  current step :  4081
total : 5000  current step :  4082
total : 5000  current step :  4083
total : 5000  current step :  4084
total : 5000  current step :  4085
total : 5000  current step :  4086
total : 5000  current step :  4087
total : 5000  current step :  4088
total : 5000  current step :  4089
total : 5000  current step :  4090
total : 5000  current step :  4091
total : 5000  current step :  4092
total : 5000  current step :  4093
total : 5000  current step :  4094
total : 5000  current step :  4095
total : 5000  current step :  4096
total : 5000  current step :  4097
total : 5000  current step :  4098
total : 5000  current step :  4099
total : 5000  current step :  4100
total : 5000  current step :  4101
total : 5000  current step :  4102
total : 5000  current step :  4103
total : 5000  current step :  4104
total : 5000  current step :  4105
total : 5000  current step :  4106
total : 5000  current step :  4107
total : 5000  current step :  4108
total : 5000  current step :  4109
total : 5000  current step :  4110
total : 5000  current step :  4111
total : 5000  current step :  4112
total : 5000  current step :  4113
total : 5000  current step :  4114
total : 5000  current step :  4115
total : 5000  current step :  4116
total : 5000  current step :  4117
total : 5000  current step :  4118
total : 5000  current step :  4119
total : 5000  current step :  4120
total : 5000  current step :  4121
total : 5000  current step :  4122
total : 5000  current step :  4123
total : 5000  current step :  4124
total : 5000  current step :  4125
total : 5000  current step :  4126
total : 5000  current step :  4127
total : 5000  current step :  4128
total : 5000  current step :  4129
total : 5000  current step :  4130
total : 5000  current step :  4131
total : 5000  current step :  4132
total : 5000  current step :  4133
total : 5000  current step :  4134
total : 5000  current step :  4135
total : 5000  current step :  4136
total : 5000  current step :  4137
total : 5000  current step :  4138
total : 5000  current step :  4139
total : 5000  current step :  4140
total : 5000  current step :  4141
total : 5000  current step :  4142
total : 5000  current step :  4143
total : 5000  current step :  4144
total : 5000  current step :  4145
total : 5000  current step :  4146
total : 5000  current step :  4147
total : 5000  current step :  4148
total : 5000  current step :  4149
total : 5000  current step :  4150
total : 5000  current step :  4151
total : 5000  current step :  4152
total : 5000  current step :  4153
total : 5000  current step :  4154
total : 5000  current step :  4155
total : 5000  current step :  4156
total : 5000  current step :  4157
total : 5000  current step :  4158
total : 5000  current step :  4159
total : 5000  current step :  4160
total : 5000  current step :  4161
total : 5000  current step :  4162
total : 5000  current step :  4163
total : 5000  current step :  4164
total : 5000  current step :  4165
total : 5000  current step :  4166
total : 5000  current step :  4167
total : 5000  current step :  4168
total : 5000  current step :  4169
total : 5000  current step :  4170
total : 5000  current step :  4171
total : 5000  current step :  4172
total : 5000  current step :  4173
total : 5000  current step :  4174
total : 5000  current step :  4175
total : 5000  current step :  4176
total : 5000  current step :  4177
total : 5000  current step :  4178
total : 5000  current step :  4179
total : 5000  current step :  4180
total : 5000  current step :  4181
total : 5000  current step :  4182
total : 5000  current step :  4183
total : 5000  current step :  4184
total : 5000  current step :  4185
total : 5000  current step :  4186
total : 5000  current step :  4187
total : 5000  current step :  4188
total : 5000  current step :  4189
total : 5000  current step :  4190
total : 5000  current step :  4191
total : 5000  current step :  4192
total : 5000  current step :  4193
total : 5000  current step :  4194
total : 5000  current step :  4195
total : 5000  current step :  4196
total : 5000  current step :  4197
total : 5000  current step :  4198
total : 5000  current step :  4199
total : 5000  current step :  4200
total : 5000  current step :  4201
total : 5000  current step :  4202
total : 5000  current step :  4203
total : 5000  current step :  4204
total : 5000  current step :  4205
total : 5000  current step :  4206
total : 5000  current step :  4207
total : 5000  current step :  4208
total : 5000  current step :  4209
total : 5000  current step :  4210
total : 5000  current step :  4211
total : 5000  current step :  4212
total : 5000  current step :  4213
total : 5000  current step :  4214
total : 5000  current step :  4215
total : 5000  current step :  4216
total : 5000  current step :  4217
total : 5000  current step :  4218
total : 5000  current step :  4219
total : 5000  current step :  4220
total : 5000  current step :  4221
total : 5000  current step :  4222
total : 5000  current step :  4223
total : 5000  current step :  4224
total : 5000  current step :  4225
total : 5000  current step :  4226
total : 5000  current step :  4227
total : 5000  current step :  4228
total : 5000  current step :  4229
total : 5000  current step :  4230
total : 5000  current step :  4231
total : 5000  current step :  4232
total : 5000  current step :  4233
total : 5000  current step :  4234
total : 5000  current step :  4235
total : 5000  current step :  4236
total : 5000  current step :  4237
total : 5000  current step :  4238
total : 5000  current step :  4239
total : 5000  current step :  4240
total : 5000  current step :  4241
total : 5000  current step :  4242
total : 5000  current step :  4243
total : 5000  current step :  4244
total : 5000  current step :  4245
total : 5000  current step :  4246
total : 5000  current step :  4247
total : 5000  current step :  4248
total : 5000  current step :  4249
total : 5000  current step :  4250
total : 5000  current step :  4251
total : 5000  current step :  4252
total : 5000  current step :  4253
total : 5000  current step :  4254
total : 5000  current step :  4255
total : 5000  current step :  4256
total : 5000  current step :  4257
total : 5000  current step :  4258
total : 5000  current step :  4259
total : 5000  current step :  4260
total : 5000  current step :  4261
total : 5000  current step :  4262
total : 5000  current step :  4263
total : 5000  current step :  4264
total : 5000  current step :  4265
total : 5000  current step :  4266
total : 5000  current step :  4267
total : 5000  current step :  4268
total : 5000  current step :  4269
total : 5000  current step :  4270
total : 5000  current step :  4271
total : 5000  current step :  4272
total : 5000  current step :  4273
total : 5000  current step :  4274
total : 5000  current step :  4275
total : 5000  current step :  4276
total : 5000  current step :  4277
total : 5000  current step :  4278
total : 5000  current step :  4279
total : 5000  current step :  4280
total : 5000  current step :  4281
total : 5000  current step :  4282
total : 5000  current step :  4283
total : 5000  current step :  4284
total : 5000  current step :  4285
total : 5000  current step :  4286
total : 5000  current step :  4287
total : 5000  current step :  4288
total : 5000  current step :  4289
total : 5000  current step :  4290
total : 5000  current step :  4291
total : 5000  current step :  4292
total : 5000  current step :  4293
total : 5000  current step :  4294
total : 5000  current step :  4295
total : 5000  current step :  4296
total : 5000  current step :  4297
total : 5000  current step :  4298
total : 5000  current step :  4299
total : 5000  current step :  4300
total : 5000  current step :  4301
total : 5000  current step :  4302
total : 5000  current step :  4303
total : 5000  current step :  4304
total : 5000  current step :  4305
total : 5000  current step :  4306
total : 5000  current step :  4307
total : 5000  current step :  4308
total : 5000  current step :  4309
total : 5000  current step :  4310
total : 5000  current step :  4311
total : 5000  current step :  4312
total : 5000  current step :  4313
total : 5000  current step :  4314
total : 5000  current step :  4315
total : 5000  current step :  4316
total : 5000  current step :  4317
total : 5000  current step :  4318
total : 5000  current step :  4319
total : 5000  current step :  4320
total : 5000  current step :  4321
total : 5000  current step :  4322
total : 5000  current step :  4323
total : 5000  current step :  4324
total : 5000  current step :  4325
total : 5000  current step :  4326
total : 5000  current step :  4327
total : 5000  current step :  4328
total : 5000  current step :  4329
total : 5000  current step :  4330
total : 5000  current step :  4331
total : 5000  current step :  4332
total : 5000  current step :  4333
total : 5000  current step :  4334
total : 5000  current step :  4335
total : 5000  current step :  4336
total : 5000  current step :  4337
total : 5000  current step :  4338
total : 5000  current step :  4339
total : 5000  current step :  4340
total : 5000  current step :  4341
total : 5000  current step :  4342
total : 5000  current step :  4343
total : 5000  current step :  4344
total : 5000  current step :  4345
total : 5000  current step :  4346
total : 5000  current step :  4347
total : 5000  current step :  4348
total : 5000  current step :  4349
total : 5000  current step :  4350
total : 5000  current step :  4351
total : 5000  current step :  4352
total : 5000  current step :  4353
total : 5000  current step :  4354
total : 5000  current step :  4355
total : 5000  current step :  4356
total : 5000  current step :  4357
total : 5000  current step :  4358
total : 5000  current step :  4359
total : 5000  current step :  4360
total : 5000  current step :  4361
total : 5000  current step :  4362
total : 5000  current step :  4363
total : 5000  current step :  4364
total : 5000  current step :  4365
total : 5000  current step :  4366
total : 5000  current step :  4367
total : 5000  current step :  4368
total : 5000  current step :  4369
total : 5000  current step :  4370
total : 5000  current step :  4371
total : 5000  current step :  4372
total : 5000  current step :  4373
total : 5000  current step :  4374
total : 5000  current step :  4375
total : 5000  current step :  4376
total : 5000  current step :  4377
total : 5000  current step :  4378
total : 5000  current step :  4379
total : 5000  current step :  4380
total : 5000  current step :  4381
total : 5000  current step :  4382
total : 5000  current step :  4383
total : 5000  current step :  4384
total : 5000  current step :  4385
total : 5000  current step :  4386
total : 5000  current step :  4387
total : 5000  current step :  4388
total : 5000  current step :  4389
total : 5000  current step :  4390
total : 5000  current step :  4391
total : 5000  current step :  4392
total : 5000  current step :  4393
total : 5000  current step :  4394
total : 5000  current step :  4395
total : 5000  current step :  4396
total : 5000  current step :  4397
total : 5000  current step :  4398
total : 5000  current step :  4399
total : 5000  current step :  4400
total : 5000  current step :  4401
total : 5000  current step :  4402
total : 5000  current step :  4403
total : 5000  current step :  4404
total : 5000  current step :  4405
total : 5000  current step :  4406
total : 5000  current step :  4407
total : 5000  current step :  4408
total : 5000  current step :  4409
total : 5000  current step :  4410
total : 5000  current step :  4411
total : 5000  current step :  4412
total : 5000  current step :  4413
total : 5000  current step :  4414
total : 5000  current step :  4415
total : 5000  current step :  4416
total : 5000  current step :  4417
total : 5000  current step :  4418
total : 5000  current step :  4419
total : 5000  current step :  4420
total : 5000  current step :  4421
total : 5000  current step :  4422
total : 5000  current step :  4423
total : 5000  current step :  4424
total : 5000  current step :  4425
total : 5000  current step :  4426
total : 5000  current step :  4427
total : 5000  current step :  4428
total : 5000  current step :  4429
total : 5000  current step :  4430
total : 5000  current step :  4431
total : 5000  current step :  4432
total : 5000  current step :  4433
total : 5000  current step :  4434
total : 5000  current step :  4435
total : 5000  current step :  4436
total : 5000  current step :  4437
total : 5000  current step :  4438
total : 5000  current step :  4439
total : 5000  current step :  4440
total : 5000  current step :  4441
total : 5000  current step :  4442
total : 5000  current step :  4443
total : 5000  current step :  4444
total : 5000  current step :  4445
total : 5000  current step :  4446
total : 5000  current step :  4447
total : 5000  current step :  4448
total : 5000  current step :  4449
total : 5000  current step :  4450
total : 5000  current step :  4451
total : 5000  current step :  4452
total : 5000  current step :  4453
total : 5000  current step :  4454
total : 5000  current step :  4455
total : 5000  current step :  4456
total : 5000  current step :  4457
total : 5000  current step :  4458
total : 5000  current step :  4459
total : 5000  current step :  4460
total : 5000  current step :  4461
total : 5000  current step :  4462
total : 5000  current step :  4463
total : 5000  current step :  4464
total : 5000  current step :  4465
total : 5000  current step :  4466
total : 5000  current step :  4467
total : 5000  current step :  4468
total : 5000  current step :  4469
total : 5000  current step :  4470
total : 5000  current step :  4471
total : 5000  current step :  4472
total : 5000  current step :  4473
total : 5000  current step :  4474
total : 5000  current step :  4475
total : 5000  current step :  4476
total : 5000  current step :  4477
total : 5000  current step :  4478
total : 5000  current step :  4479
total : 5000  current step :  4480
total : 5000  current step :  4481
total : 5000  current step :  4482
total : 5000  current step :  4483
total : 5000  current step :  4484
total : 5000  current step :  4485
total : 5000  current step :  4486
total : 5000  current step :  4487
total : 5000  current step :  4488
total : 5000  current step :  4489
total : 5000  current step :  4490
total : 5000  current step :  4491
total : 5000  current step :  4492
total : 5000  current step :  4493
total : 5000  current step :  4494
total : 5000  current step :  4495
total : 5000  current step :  4496
total : 5000  current step :  4497
total : 5000  current step :  4498
total : 5000  current step :  4499
total : 5000  current step :  4500
total : 5000  current step :  4501
total : 5000  current step :  4502
total : 5000  current step :  4503
total : 5000  current step :  4504
total : 5000  current step :  4505
total : 5000  current step :  4506
total : 5000  current step :  4507
total : 5000  current step :  4508
total : 5000  current step :  4509
total : 5000  current step :  4510
total : 5000  current step :  4511
total : 5000  current step :  4512
total : 5000  current step :  4513
total : 5000  current step :  4514
total : 5000  current step :  4515
total : 5000  current step :  4516
total : 5000  current step :  4517
total : 5000  current step :  4518
total : 5000  current step :  4519
total : 5000  current step :  4520
total : 5000  current step :  4521
total : 5000  current step :  4522
total : 5000  current step :  4523
total : 5000  current step :  4524
total : 5000  current step :  4525
total : 5000  current step :  4526
total : 5000  current step :  4527
total : 5000  current step :  4528
total : 5000  current step :  4529
total : 5000  current step :  4530
total : 5000  current step :  4531
total : 5000  current step :  4532
total : 5000  current step :  4533
total : 5000  current step :  4534
total : 5000  current step :  4535
total : 5000  current step :  4536
total : 5000  current step :  4537
total : 5000  current step :  4538
total : 5000  current step :  4539
total : 5000  current step :  4540
total : 5000  current step :  4541
total : 5000  current step :  4542
total : 5000  current step :  4543
total : 5000  current step :  4544
total : 5000  current step :  4545
total : 5000  current step :  4546
total : 5000  current step :  4547
total : 5000  current step :  4548
total : 5000  current step :  4549
total : 5000  current step :  4550
total : 5000  current step :  4551
total : 5000  current step :  4552
total : 5000  current step :  4553
total : 5000  current step :  4554
total : 5000  current step :  4555
total : 5000  current step :  4556
total : 5000  current step :  4557
total : 5000  current step :  4558
total : 5000  current step :  4559
total : 5000  current step :  4560
total : 5000  current step :  4561
total : 5000  current step :  4562
total : 5000  current step :  4563
total : 5000  current step :  4564
total : 5000  current step :  4565
total : 5000  current step :  4566
total : 5000  current step :  4567
total : 5000  current step :  4568
total : 5000  current step :  4569
total : 5000  current step :  4570
total : 5000  current step :  4571
total : 5000  current step :  4572
total : 5000  current step :  4573
total : 5000  current step :  4574
total : 5000  current step :  4575
total : 5000  current step :  4576
total : 5000  current step :  4577
total : 5000  current step :  4578
total : 5000  current step :  4579
total : 5000  current step :  4580
total : 5000  current step :  4581
total : 5000  current step :  4582
total : 5000  current step :  4583
total : 5000  current step :  4584
total : 5000  current step :  4585
total : 5000  current step :  4586
total : 5000  current step :  4587
total : 5000  current step :  4588
total : 5000  current step :  4589
total : 5000  current step :  4590
total : 5000  current step :  4591
total : 5000  current step :  4592
total : 5000  current step :  4593
total : 5000  current step :  4594
total : 5000  current step :  4595
total : 5000  current step :  4596
total : 5000  current step :  4597
total : 5000  current step :  4598
total : 5000  current step :  4599
total : 5000  current step :  4600
total : 5000  current step :  4601
total : 5000  current step :  4602
total : 5000  current step :  4603
total : 5000  current step :  4604
total : 5000  current step :  4605
total : 5000  current step :  4606
total : 5000  current step :  4607
total : 5000  current step :  4608
total : 5000  current step :  4609
total : 5000  current step :  4610
total : 5000  current step :  4611
total : 5000  current step :  4612
total : 5000  current step :  4613
total : 5000  current step :  4614
total : 5000  current step :  4615
total : 5000  current step :  4616
total : 5000  current step :  4617
total : 5000  current step :  4618
total : 5000  current step :  4619
total : 5000  current step :  4620
total : 5000  current step :  4621
total : 5000  current step :  4622
total : 5000  current step :  4623
total : 5000  current step :  4624
total : 5000  current step :  4625
total : 5000  current step :  4626
total : 5000  current step :  4627
total : 5000  current step :  4628
total : 5000  current step :  4629
total : 5000  current step :  4630
total : 5000  current step :  4631
total : 5000  current step :  4632
total : 5000  current step :  4633
total : 5000  current step :  4634
total : 5000  current step :  4635
total : 5000  current step :  4636
total : 5000  current step :  4637
total : 5000  current step :  4638
total : 5000  current step :  4639
total : 5000  current step :  4640
total : 5000  current step :  4641
total : 5000  current step :  4642
total : 5000  current step :  4643
total : 5000  current step :  4644
total : 5000  current step :  4645
total : 5000  current step :  4646
total : 5000  current step :  4647
total : 5000  current step :  4648
total : 5000  current step :  4649
total : 5000  current step :  4650
total : 5000  current step :  4651
total : 5000  current step :  4652
total : 5000  current step :  4653
total : 5000  current step :  4654
total : 5000  current step :  4655
total : 5000  current step :  4656
total : 5000  current step :  4657
total : 5000  current step :  4658
total : 5000  current step :  4659
total : 5000  current step :  4660
total : 5000  current step :  4661
total : 5000  current step :  4662
total : 5000  current step :  4663
total : 5000  current step :  4664
total : 5000  current step :  4665
total : 5000  current step :  4666
total : 5000  current step :  4667
total : 5000  current step :  4668
total : 5000  current step :  4669
total : 5000  current step :  4670
total : 5000  current step :  4671
total : 5000  current step :  4672
total : 5000  current step :  4673
total : 5000  current step :  4674
total : 5000  current step :  4675
total : 5000  current step :  4676
total : 5000  current step :  4677
total : 5000  current step :  4678
total : 5000  current step :  4679
total : 5000  current step :  4680
total : 5000  current step :  4681
total : 5000  current step :  4682
total : 5000  current step :  4683
total : 5000  current step :  4684
total : 5000  current step :  4685
total : 5000  current step :  4686
total : 5000  current step :  4687
total : 5000  current step :  4688
total : 5000  current step :  4689
total : 5000  current step :  4690
total : 5000  current step :  4691
total : 5000  current step :  4692
total : 5000  current step :  4693
total : 5000  current step :  4694
total : 5000  current step :  4695
total : 5000  current step :  4696
total : 5000  current step :  4697
total : 5000  current step :  4698
total : 5000  current step :  4699
total : 5000  current step :  4700
total : 5000  current step :  4701
total : 5000  current step :  4702
total : 5000  current step :  4703
total : 5000  current step :  4704
total : 5000  current step :  4705
total : 5000  current step :  4706
total : 5000  current step :  4707
total : 5000  current step :  4708
total : 5000  current step :  4709
total : 5000  current step :  4710
total : 5000  current step :  4711
total : 5000  current step :  4712
total : 5000  current step :  4713
total : 5000  current step :  4714
total : 5000  current step :  4715
total : 5000  current step :  4716
total : 5000  current step :  4717
total : 5000  current step :  4718
total : 5000  current step :  4719
total : 5000  current step :  4720
total : 5000  current step :  4721
total : 5000  current step :  4722
total : 5000  current step :  4723
total : 5000  current step :  4724
total : 5000  current step :  4725
total : 5000  current step :  4726
total : 5000  current step :  4727
total : 5000  current step :  4728
total : 5000  current step :  4729
total : 5000  current step :  4730
total : 5000  current step :  4731
total : 5000  current step :  4732
total : 5000  current step :  4733
total : 5000  current step :  4734
total : 5000  current step :  4735
total : 5000  current step :  4736
total : 5000  current step :  4737
total : 5000  current step :  4738
total : 5000  current step :  4739
total : 5000  current step :  4740
total : 5000  current step :  4741
total : 5000  current step :  4742
total : 5000  current step :  4743
total : 5000  current step :  4744
total : 5000  current step :  4745
total : 5000  current step :  4746
total : 5000  current step :  4747
total : 5000  current step :  4748
total : 5000  current step :  4749
total : 5000  current step :  4750
total : 5000  current step :  4751
total : 5000  current step :  4752
total : 5000  current step :  4753
total : 5000  current step :  4754
total : 5000  current step :  4755
total : 5000  current step :  4756
total : 5000  current step :  4757
total : 5000  current step :  4758
total : 5000  current step :  4759
total : 5000  current step :  4760
total : 5000  current step :  4761
total : 5000  current step :  4762
total : 5000  current step :  4763
total : 5000  current step :  4764
total : 5000  current step :  4765
total : 5000  current step :  4766
total : 5000  current step :  4767
total : 5000  current step :  4768
total : 5000  current step :  4769
total : 5000  current step :  4770
total : 5000  current step :  4771
total : 5000  current step :  4772
total : 5000  current step :  4773
total : 5000  current step :  4774
total : 5000  current step :  4775
total : 5000  current step :  4776
total : 5000  current step :  4777
total : 5000  current step :  4778
total : 5000  current step :  4779
total : 5000  current step :  4780
total : 5000  current step :  4781
total : 5000  current step :  4782
total : 5000  current step :  4783
total : 5000  current step :  4784
total : 5000  current step :  4785
total : 5000  current step :  4786
total : 5000  current step :  4787
total : 5000  current step :  4788
total : 5000  current step :  4789
total : 5000  current step :  4790
total : 5000  current step :  4791
total : 5000  current step :  4792
total : 5000  current step :  4793
total : 5000  current step :  4794
total : 5000  current step :  4795
total : 5000  current step :  4796
total : 5000  current step :  4797
total : 5000  current step :  4798
total : 5000  current step :  4799
total : 5000  current step :  4800
total : 5000  current step :  4801
total : 5000  current step :  4802
total : 5000  current step :  4803
total : 5000  current step :  4804
total : 5000  current step :  4805
total : 5000  current step :  4806
total : 5000  current step :  4807
total : 5000  current step :  4808
total : 5000  current step :  4809
total : 5000  current step :  4810
total : 5000  current step :  4811
total : 5000  current step :  4812
total : 5000  current step :  4813
total : 5000  current step :  4814
total : 5000  current step :  4815
total : 5000  current step :  4816
total : 5000  current step :  4817
total : 5000  current step :  4818
total : 5000  current step :  4819
total : 5000  current step :  4820
total : 5000  current step :  4821
total : 5000  current step :  4822
total : 5000  current step :  4823
total : 5000  current step :  4824
total : 5000  current step :  4825
total : 5000  current step :  4826
total : 5000  current step :  4827
total : 5000  current step :  4828
total : 5000  current step :  4829
total : 5000  current step :  4830
total : 5000  current step :  4831
total : 5000  current step :  4832
total : 5000  current step :  4833
total : 5000  current step :  4834
total : 5000  current step :  4835
total : 5000  current step :  4836
total : 5000  current step :  4837
total : 5000  current step :  4838
total : 5000  current step :  4839
total : 5000  current step :  4840
total : 5000  current step :  4841
total : 5000  current step :  4842
total : 5000  current step :  4843
total : 5000  current step :  4844
total : 5000  current step :  4845
total : 5000  current step :  4846
total : 5000  current step :  4847
total : 5000  current step :  4848
total : 5000  current step :  4849
total : 5000  current step :  4850
total : 5000  current step :  4851
total : 5000  current step :  4852
total : 5000  current step :  4853
total : 5000  current step :  4854
total : 5000  current step :  4855
total : 5000  current step :  4856
total : 5000  current step :  4857
total : 5000  current step :  4858
total : 5000  current step :  4859
total : 5000  current step :  4860
total : 5000  current step :  4861
total : 5000  current step :  4862
total : 5000  current step :  4863
total : 5000  current step :  4864
total : 5000  current step :  4865
total : 5000  current step :  4866
total : 5000  current step :  4867
total : 5000  current step :  4868
total : 5000  current step :  4869
total : 5000  current step :  4870
total : 5000  current step :  4871
total : 5000  current step :  4872
total : 5000  current step :  4873
total : 5000  current step :  4874
total : 5000  current step :  4875
total : 5000  current step :  4876
total : 5000  current step :  4877
total : 5000  current step :  4878
total : 5000  current step :  4879
total : 5000  current step :  4880
total : 5000  current step :  4881
total : 5000  current step :  4882
total : 5000  current step :  4883
total : 5000  current step :  4884
total : 5000  current step :  4885
total : 5000  current step :  4886
total : 5000  current step :  4887
total : 5000  current step :  4888
total : 5000  current step :  4889
total : 5000  current step :  4890
total : 5000  current step :  4891
total : 5000  current step :  4892
total : 5000  current step :  4893
total : 5000  current step :  4894
total : 5000  current step :  4895
total : 5000  current step :  4896
total : 5000  current step :  4897
total : 5000  current step :  4898
total : 5000  current step :  4899
total : 5000  current step :  4900
total : 5000  current step :  4901
total : 5000  current step :  4902
total : 5000  current step :  4903
total : 5000  current step :  4904
total : 5000  current step :  4905
total : 5000  current step :  4906
total : 5000  current step :  4907
total : 5000  current step :  4908
total : 5000  current step :  4909
total : 5000  current step :  4910
total : 5000  current step :  4911
total : 5000  current step :  4912
total : 5000  current step :  4913
total : 5000  current step :  4914
total : 5000  current step :  4915
total : 5000  current step :  4916
total : 5000  current step :  4917
total : 5000  current step :  4918
total : 5000  current step :  4919
total : 5000  current step :  4920
total : 5000  current step :  4921
total : 5000  current step :  4922
total : 5000  current step :  4923
total : 5000  current step :  4924
total : 5000  current step :  4925
total : 5000  current step :  4926
total : 5000  current step :  4927
total : 5000  current step :  4928
total : 5000  current step :  4929
total : 5000  current step :  4930
total : 5000  current step :  4931
total : 5000  current step :  4932
total : 5000  current step :  4933
total : 5000  current step :  4934
total : 5000  current step :  4935
total : 5000  current step :  4936
total : 5000  current step :  4937
total : 5000  current step :  4938
total : 5000  current step :  4939
total : 5000  current step :  4940
total : 5000  current step :  4941
total : 5000  current step :  4942
total : 5000  current step :  4943
total : 5000  current step :  4944
total : 5000  current step :  4945
total : 5000  current step :  4946
total : 5000  current step :  4947
total : 5000  current step :  4948
total : 5000  current step :  4949
total : 5000  current step :  4950
total : 5000  current step :  4951
total : 5000  current step :  4952
total : 5000  current step :  4953
total : 5000  current step :  4954
total : 5000  current step :  4955
total : 5000  current step :  4956
total : 5000  current step :  4957
total : 5000  current step :  4958
total : 5000  current step :  4959
total : 5000  current step :  4960
total : 5000  current step :  4961
total : 5000  current step :  4962
total : 5000  current step :  4963
total : 5000  current step :  4964
total : 5000  current step :  4965
total : 5000  current step :  4966
total : 5000  current step :  4967
total : 5000  current step :  4968
total : 5000  current step :  4969
total : 5000  current step :  4970
total : 5000  current step :  4971
total : 5000  current step :  4972
total : 5000  current step :  4973
total : 5000  current step :  4974
total : 5000  current step :  4975
total : 5000  current step :  4976
total : 5000  current step :  4977
total : 5000  current step :  4978
total : 5000  current step :  4979
total : 5000  current step :  4980
total : 5000  current step :  4981
total : 5000  current step :  4982
total : 5000  current step :  4983
total : 5000  current step :  4984
total : 5000  current step :  4985
total : 5000  current step :  4986
total : 5000  current step :  4987
total : 5000  current step :  4988
total : 5000  current step :  4989
total : 5000  current step :  4990
total : 5000  current step :  4991
total : 5000  current step :  4992
total : 5000  current step :  4993
total : 5000  current step :  4994
total : 5000  current step :  4995
total : 5000  current step :  4996
total : 5000  current step :  4997
total : 5000  current step :  4998
total : 5000  current step :  4999
soft_pseudo_label
tensor([[3.8562e-03, 3.8694e-03, 3.3694e-01,  ..., 3.9036e-03, 3.9903e-03,
         3.9651e-03],
        [2.0854e-03, 1.9860e-03, 9.8358e-01,  ..., 1.8974e-03, 2.2329e-03,
         2.1711e-03],
        [2.6934e-03, 2.7185e-03, 9.6832e-01,  ..., 2.7318e-03, 2.6960e-03,
         2.7979e-03],
        ...,
        [1.3382e-04, 1.3369e-04, 6.8858e-01,  ..., 1.2113e-04, 1.3820e-04,
         1.2869e-04],
        [4.8037e-04, 4.9416e-04, 3.8674e-04,  ..., 4.8295e-04, 4.3313e-04,
         4.9537e-04],
        [1.3307e-03, 1.8973e-03, 9.8689e-01,  ..., 1.7255e-03, 1.7042e-03,
         1.7537e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3,
        3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2,
        3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        2, 3, 2, 3, 2, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2]
original label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 6, 2, 3, 3, 3, 2, 2, 3, 2, 3,
        7, 2, 2, 3, 3, 3, 2, 3, 8, 3, 3, 2, 2, 3, 3, 2, 6, 2, 2, 2, 3, 2, 2, 3,
        2, 8, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3, 3, 3, 3, 5, 3, 2, 2,
        3, 2, 3, 2, 5, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        9, 3, 4, 3, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.2960e-04, 1.3056e-04, 2.9916e-04,  ..., 1.2834e-04, 1.1910e-04,
         1.2960e-04],
        [1.8334e-03, 1.6298e-03, 9.8514e-01,  ..., 1.7614e-03, 1.5882e-03,
         1.6093e-03],
        [3.8178e-03, 4.1205e-03, 9.6797e-01,  ..., 3.8884e-03, 3.9937e-03,
         3.4102e-03],
        ...,
        [1.7566e-03, 1.6788e-03, 9.8609e-01,  ..., 1.8576e-03, 1.7600e-03,
         1.5510e-03],
        [1.1179e-03, 1.1289e-03, 9.9035e-01,  ..., 1.1894e-03, 1.2081e-03,
         1.0502e-03],
        [4.1518e-03, 4.6316e-03, 9.4462e-01,  ..., 4.1094e-03, 4.5687e-03,
         4.5023e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3,
        3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3,
        3, 2, 2, 3, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2]
original label
tensor([3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 6, 3,
        7, 2, 2, 3, 3, 2, 3, 3, 1, 3, 3, 2, 2, 0, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 7, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4,
        7, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 4, 2, 2, 2, 4, 3, 2,
        5, 3, 2, 3, 2, 3, 5, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3,
        2, 2, 2, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[6.4269e-03, 6.7288e-03, 7.2070e-01,  ..., 6.4995e-03, 6.3489e-03,
         6.4269e-03],
        [4.7430e-03, 4.3556e-03, 9.6284e-01,  ..., 4.5092e-03, 4.6649e-03,
         4.6263e-03],
        [1.1661e-03, 1.3066e-03, 9.8864e-01,  ..., 1.2401e-03, 1.2203e-03,
         1.1627e-03],
        ...,
        [2.2576e-03, 2.8096e-03, 9.7856e-01,  ..., 2.4428e-03, 2.6138e-03,
         2.9137e-03],
        [2.0081e-03, 1.8919e-03, 1.0105e-04,  ..., 2.0906e-03, 2.0627e-03,
         2.1410e-03],
        [9.5792e-04, 1.0337e-03, 6.4962e-02,  ..., 1.0327e-03, 9.3939e-04,
         9.8352e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3,
        3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3]
original label
tensor([2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 0, 9, 2, 2, 3, 3,
        1, 3, 3, 2, 3, 3, 9, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 0, 0, 2, 3,
        2, 3, 3, 2, 2, 1, 3, 1, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 0, 0, 2, 3, 2, 2, 3, 9, 2, 3, 3, 2,
        3, 3, 3, 6, 3, 2, 3, 3])
soft_pseudo_label
tensor([[1.1114e-03, 1.3105e-03, 9.8894e-01,  ..., 1.5202e-03, 1.4042e-03,
         1.3995e-03],
        [2.0838e-04, 2.0575e-04, 3.8798e-04,  ..., 1.9758e-04, 2.2020e-04,
         2.1521e-04],
        [5.3637e-03, 5.7798e-03, 4.4112e-01,  ..., 6.2220e-03, 5.9053e-03,
         5.8651e-03],
        ...,
        [5.4756e-03, 5.5347e-03, 3.0920e-01,  ..., 5.8259e-03, 5.5808e-03,
         5.9379e-03],
        [1.5110e-03, 1.4556e-03, 9.8778e-01,  ..., 1.5440e-03, 1.5853e-03,
         1.4133e-03],
        [1.4835e-03, 1.6174e-03, 9.7237e-01,  ..., 1.6389e-03, 1.6549e-03,
         1.7463e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2,
        3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 2, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2]
original label
tensor([2, 6, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 0, 3,
        3, 3, 2, 3, 9, 3, 6, 9, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 5, 7, 2, 2, 3, 2,
        3, 2, 2, 8, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 8, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 7, 3, 3, 3, 3, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 1, 2, 3, 0, 3, 8, 3, 2, 3, 8, 3,
        3, 2, 2, 2, 3, 2, 2, 2])
soft_pseudo_label
tensor([[9.9562e-04, 9.3919e-04, 9.9194e-01,  ..., 9.3325e-04, 1.0493e-03,
         9.9368e-04],
        [8.3893e-05, 7.8734e-05, 2.6647e-01,  ..., 7.7211e-05, 8.0522e-05,
         7.7211e-05],
        [7.9584e-04, 9.4002e-04, 6.1648e-04,  ..., 8.4758e-04, 9.1645e-04,
         9.2499e-04],
        ...,
        [8.2984e-04, 8.3717e-04, 2.3713e-02,  ..., 8.4786e-04, 8.6628e-04,
         7.6523e-04],
        [2.7140e-03, 2.6100e-03, 9.7812e-01,  ..., 2.6427e-03, 2.5847e-03,
         2.5390e-03],
        [2.5234e-03, 2.2686e-03, 3.8303e-04,  ..., 2.4285e-03, 2.3372e-03,
         2.4667e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3,
        2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3,
        3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,
        3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        3, 3, 2, 3, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3]
original label
tensor([2, 3, 3, 2, 3, 1, 3, 2, 4, 3, 2, 3, 3, 3, 1, 3, 2, 2, 3, 5, 3, 8, 3, 6,
        3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 3, 2, 3,
        4, 2, 2, 2, 1, 2, 3, 0, 2, 2, 3, 3, 3, 3, 3, 3, 5, 2, 2, 3, 2, 3, 2, 3,
        3, 3, 3, 0, 3, 3, 7, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3,
        3, 5, 6, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 8, 3,
        7, 3, 3, 3, 3, 3, 2, 3])
soft_pseudo_label
tensor([[9.3376e-04, 8.9908e-04, 9.9273e-01,  ..., 9.0392e-04, 9.1838e-04,
         8.9078e-04],
        [1.8410e-03, 1.7957e-03, 1.1533e-02,  ..., 1.7455e-03, 1.8374e-03,
         1.8151e-03],
        [2.3119e-03, 2.5342e-03, 4.9166e-02,  ..., 2.4924e-03, 2.5665e-03,
         2.4827e-03],
        ...,
        [1.2570e-03, 1.2375e-03, 5.2137e-05,  ..., 1.3088e-03, 1.2725e-03,
         1.1858e-03],
        [1.2719e-03, 1.3379e-03, 9.8943e-01,  ..., 1.3110e-03, 1.3779e-03,
         1.2400e-03],
        [1.8910e-03, 2.0981e-03, 1.4159e-05,  ..., 2.2096e-03, 1.8818e-03,
         2.2100e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 2, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3]
original label
tensor([2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3,
        3, 4, 2, 2, 3, 9, 3, 2, 3, 3, 0, 3, 2, 1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 2, 3, 9, 3, 9, 3, 2, 4, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 8,
        2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 7, 2, 3, 3,
        2, 2, 3, 2, 2, 3, 2, 3])
soft_pseudo_label
tensor([[6.9367e-04, 6.5158e-04, 9.9429e-01,  ..., 7.2430e-04, 7.2923e-04,
         6.6496e-04],
        [1.5019e-03, 1.4953e-03, 1.3527e-04,  ..., 1.4272e-03, 1.4579e-03,
         1.4089e-03],
        [1.4741e-03, 1.4028e-03, 9.8840e-01,  ..., 1.4838e-03, 1.2392e-03,
         1.2898e-03],
        ...,
        [1.6721e-03, 1.8517e-03, 9.8339e-01,  ..., 1.7404e-03, 1.7184e-03,
         1.6729e-03],
        [1.6270e-03, 2.0517e-03, 9.8436e-01,  ..., 1.7635e-03, 1.9180e-03,
         1.8194e-03],
        [2.4026e-03, 2.7332e-03, 9.7784e-01,  ..., 2.5915e-03, 2.6685e-03,
         2.4668e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3,
        2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 3, 2, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2]
original label
tensor([2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 8, 3, 2, 2, 3, 3, 6, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 1, 2, 2, 3, 3, 3, 3, 2, 2, 3,
        3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 9, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3,
        2, 6, 1, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 2, 2, 7, 2])
soft_pseudo_label
tensor([[7.5231e-04, 7.6229e-04, 4.5120e-04,  ..., 7.3920e-04, 6.8766e-04,
         7.1786e-04],
        [1.3244e-03, 1.3370e-03, 9.8668e-05,  ..., 1.3325e-03, 1.3089e-03,
         1.3774e-03],
        [1.4521e-03, 1.4667e-03, 4.2872e-04,  ..., 1.4807e-03, 1.4071e-03,
         1.2963e-03],
        ...,
        [4.8338e-03, 5.0510e-03, 2.2019e-01,  ..., 4.7705e-03, 4.7219e-03,
         4.8386e-03],
        [4.7719e-03, 4.8317e-03, 6.1751e-04,  ..., 4.1302e-03, 4.1708e-03,
         3.9837e-03],
        [1.3277e-03, 1.3540e-03, 9.8995e-01,  ..., 1.2298e-03, 1.2001e-03,
         1.1936e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3,
        3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2,
        3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2,
        3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2]
original label
tensor([3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 5, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3,
        3, 3, 2, 5, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 2, 3, 3, 5, 3, 3, 2, 2, 2, 2, 2, 3, 2,
        3, 3, 2, 2, 9, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 8, 2, 7, 2, 2, 2, 2,
        3, 2, 9, 3, 2, 2, 3, 3, 3, 3, 0, 2, 0, 7, 3, 3, 6, 3, 3, 3, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2])
soft_pseudo_label
tensor([[3.4255e-03, 3.6358e-03, 8.1082e-01,  ..., 3.5672e-03, 3.7311e-03,
         3.5033e-03],
        [9.4034e-04, 8.7564e-04, 9.9219e-01,  ..., 8.9553e-04, 9.1543e-04,
         8.4953e-04],
        [1.8385e-03, 2.0853e-03, 9.7493e-01,  ..., 2.0054e-03, 1.9248e-03,
         2.0271e-03],
        ...,
        [1.6918e-03, 1.7514e-03, 9.8652e-01,  ..., 1.7067e-03, 1.6729e-03,
         1.5978e-03],
        [1.9757e-03, 1.8766e-03, 8.0186e-06,  ..., 1.8392e-03, 1.7861e-03,
         1.7179e-03],
        [3.0191e-04, 3.0280e-04, 4.3393e-01,  ..., 2.8362e-04, 2.9986e-04,
         2.9006e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2,
        3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2,
        2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        3, 3, 2, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3]
original label
tensor([2, 2, 2, 9, 2, 2, 3, 2, 2, 6, 7, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 5, 1, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 7, 4, 1, 3, 2, 2, 2, 4, 3, 2, 2, 4, 3,
        2, 2, 3, 3, 2, 3, 2, 1, 3, 3, 2, 8, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2,
        6, 3, 2, 2, 3, 6, 9, 2, 3, 1, 4, 3, 2, 8, 3, 3, 0, 2, 3, 2, 3, 3, 2, 2,
        9, 8, 2, 3, 3, 2, 3, 7, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 2, 2, 3, 2])
soft_pseudo_label
tensor([[2.2122e-03, 2.2208e-03, 9.8160e-01,  ..., 2.3178e-03, 2.2127e-03,
         2.1641e-03],
        [8.8710e-04, 7.6177e-04, 5.2219e-06,  ..., 8.3125e-04, 8.1906e-04,
         8.0391e-04],
        [1.9294e-05, 2.3273e-05, 9.9975e-01,  ..., 1.8321e-05, 1.7844e-05,
         2.0259e-05],
        ...,
        [1.1592e-03, 1.0842e-03, 9.8713e-01,  ..., 1.1324e-03, 1.0916e-03,
         1.1252e-03],
        [5.1835e-03, 5.4322e-03, 8.4079e-01,  ..., 5.2369e-03, 5.2754e-03,
         5.0981e-03],
        [2.7720e-03, 2.8951e-03, 9.4077e-01,  ..., 2.7477e-03, 2.8824e-03,
         2.9292e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3,
        2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3,
        3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2]
original label
tensor([2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 7, 3, 8, 3, 8, 3, 2, 8, 2, 3, 3, 3, 3, 3,
        3, 2, 2, 5, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 5, 0, 2, 2, 3, 2, 2, 3,
        2, 2, 1, 3, 3, 3, 2, 2, 2, 5, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 2, 6,
        3, 3, 2, 2, 2, 3, 2, 2, 1, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 7, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 0, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 2])
soft_pseudo_label
tensor([[3.7852e-03, 3.8233e-03, 4.0009e-03,  ..., 3.7945e-03, 3.5750e-03,
         3.8476e-03],
        [8.0907e-03, 8.7353e-03, 4.3425e-01,  ..., 8.7910e-03, 8.8254e-03,
         8.2947e-03],
        [1.5169e-03, 1.7880e-03, 6.2230e-05,  ..., 1.7070e-03, 1.5095e-03,
         1.5446e-03],
        ...,
        [1.6952e-04, 1.7576e-04, 9.6741e-01,  ..., 1.7456e-04, 1.7405e-04,
         1.7354e-04],
        [3.9331e-03, 4.3898e-03, 9.6616e-01,  ..., 3.6517e-03, 4.1908e-03,
         3.7585e-03],
        [2.3641e-03, 2.2858e-03, 8.2419e-01,  ..., 2.2525e-03, 2.5425e-03,
         2.2350e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2,
        3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3,
        3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2]
original label
tensor([3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 7, 2, 3, 2, 2, 8, 3, 2, 3, 3, 3, 3,
        3, 2, 0, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 0, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 8, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3,
        8, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 6, 2, 3, 6, 3, 3, 3, 3, 3, 4,
        3, 2, 2, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[1.3719e-03, 1.6268e-03, 1.9022e-02,  ..., 1.6379e-03, 1.4711e-03,
         1.4419e-03],
        [6.4784e-03, 6.5900e-03, 5.7244e-01,  ..., 6.0238e-03, 6.8392e-03,
         7.1116e-03],
        [3.0102e-03, 3.0102e-03, 2.1069e-02,  ..., 2.6102e-03, 3.0726e-03,
         2.6565e-03],
        ...,
        [1.8251e-03, 1.9576e-03, 7.2210e-04,  ..., 1.8136e-03, 1.7458e-03,
         2.0798e-03],
        [2.2435e-03, 2.1851e-03, 7.4666e-03,  ..., 2.0497e-03, 2.2034e-03,
         2.2315e-03],
        [1.8098e-03, 1.9595e-03, 9.8443e-01,  ..., 1.9569e-03, 2.0857e-03,
         1.8789e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3,
        3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2]
original label
tensor([0, 8, 3, 2, 3, 2, 2, 4, 3, 2, 0, 3, 2, 8, 2, 2, 3, 2, 0, 2, 3, 3, 3, 2,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 4, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 8, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 9, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 2, 2, 3, 5, 7, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2])
soft_pseudo_label
tensor([[3.1271e-03, 2.9824e-03, 2.7050e-03,  ..., 3.3013e-03, 3.3215e-03,
         3.0547e-03],
        [1.7641e-03, 1.8380e-03, 9.8423e-01,  ..., 1.7641e-03, 1.7342e-03,
         1.7572e-03],
        [1.9095e-03, 2.0900e-03, 5.9147e-03,  ..., 1.8336e-03, 1.8789e-03,
         1.7251e-03],
        ...,
        [2.4233e-03, 2.3749e-03, 1.3044e-04,  ..., 2.3666e-03, 2.2541e-03,
         2.3965e-03],
        [1.8450e-03, 1.6344e-03, 9.8511e-01,  ..., 1.8800e-03, 1.9599e-03,
         2.0782e-03],
        [5.5211e-05, 5.8201e-05, 5.8578e-01,  ..., 5.5916e-05, 5.4674e-05,
         5.8887e-05]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2,
        2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 6, 3, 3, 3, 3, 3, 3, 2, 2, 3, 7, 2,
        4, 2, 3, 3, 2, 3, 2, 6, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 9, 3, 3,
        8, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 6, 2, 2, 3, 1, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 2, 1, 2, 3, 2, 2, 2, 2, 3, 3, 5, 2, 2, 3, 2, 2, 2, 2, 2, 3,
        6, 2, 2, 3, 2, 3, 3, 2, 8, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 2, 8, 2, 4])
soft_pseudo_label
tensor([[3.0718e-03, 3.0080e-03, 1.1260e-01,  ..., 2.9977e-03, 2.8230e-03,
         3.1370e-03],
        [2.7110e-03, 2.9227e-03, 2.7181e-02,  ..., 2.8301e-03, 3.0392e-03,
         3.0303e-03],
        [9.2598e-04, 9.3136e-04, 9.9248e-01,  ..., 9.1046e-04, 9.6428e-04,
         8.8741e-04],
        ...,
        [2.7203e-03, 2.4738e-03, 3.4220e-03,  ..., 2.4702e-03, 2.3919e-03,
         2.6122e-03],
        [2.9513e-03, 2.9949e-03, 9.7225e-01,  ..., 3.1111e-03, 2.8396e-03,
         2.7902e-03],
        [1.7674e-03, 1.6758e-03, 9.8049e-03,  ..., 1.7391e-03, 1.6750e-03,
         1.7391e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3,
        3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,
        2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3]
original label
tensor([3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2,
        3, 3, 2, 1, 3, 2, 3, 3, 2, 2, 2, 3, 2, 6, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 2, 2, 2, 1, 9, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 3, 2, 3, 3, 2, 3])
soft_pseudo_label
tensor([[1.8643e-03, 1.7690e-03, 1.8912e-06,  ..., 1.8176e-03, 1.7154e-03,
         1.8136e-03],
        [1.9127e-03, 2.0919e-03, 9.8403e-01,  ..., 1.7237e-03, 1.9758e-03,
         1.9201e-03],
        [2.4497e-03, 2.5924e-03, 6.1965e-01,  ..., 2.4046e-03, 2.3558e-03,
         2.4568e-03],
        ...,
        [1.6994e-03, 1.7660e-03, 4.8038e-05,  ..., 1.6425e-03, 1.8325e-03,
         1.8438e-03],
        [2.6112e-03, 2.5069e-03, 1.5339e-03,  ..., 2.4765e-03, 2.6679e-03,
         2.5808e-03],
        [1.3082e-03, 1.1065e-03, 1.5149e-03,  ..., 1.2099e-03, 1.3031e-03,
         1.3101e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2,
        2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3,
        3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 2, 2, 3, 3, 7, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2,
        3, 3, 1, 8, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 8, 3, 2, 2, 3,
        7, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 7, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 2, 5, 2, 3, 3, 2, 3, 2, 9, 3, 2, 2, 2, 4, 2, 3,
        2, 2, 3, 2, 2, 2, 3, 3, 6, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 7, 2, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[3.8419e-03, 3.6678e-03, 9.4792e-01,  ..., 3.8288e-03, 3.8796e-03,
         3.9158e-03],
        [4.6884e-03, 4.4065e-03, 2.5848e-03,  ..., 4.7183e-03, 4.3128e-03,
         4.2449e-03],
        [2.2728e-03, 2.5535e-03, 8.0487e-04,  ..., 2.5053e-03, 2.5035e-03,
         2.4354e-03],
        ...,
        [3.7898e-03, 3.3972e-03, 2.2131e-02,  ..., 3.3168e-03, 3.7475e-03,
         3.7677e-03],
        [3.1850e-03, 3.2320e-03, 2.9399e-03,  ..., 3.4028e-03, 3.5801e-03,
         3.3673e-03],
        [2.0482e-03, 1.9840e-03, 9.8464e-01,  ..., 1.7336e-03, 1.9157e-03,
         1.9516e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2,
        3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2,
        3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]
original label
tensor([2, 3, 3, 2, 3, 3, 2, 1, 2, 3, 3, 3, 3, 3, 3, 2, 7, 2, 3, 3, 3, 3, 9, 2,
        0, 3, 6, 2, 3, 2, 3, 3, 2, 3, 5, 2, 3, 2, 3, 6, 2, 3, 2, 2, 3, 2, 3, 2,
        0, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 3,
        3, 4, 3, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 2, 9, 2, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 2, 2, 4, 2, 2, 2, 2, 3, 2, 2, 6, 6, 3, 3, 2, 8, 3, 2, 3, 3, 6, 3, 3,
        3, 3, 3, 1, 3, 3, 6, 2])
soft_pseudo_label
tensor([[1.2082e-05, 1.2272e-05, 9.9942e-01,  ..., 1.1372e-05, 1.3025e-05,
         1.2861e-05],
        [8.0361e-03, 8.4259e-03, 5.2078e-01,  ..., 8.5086e-03, 7.9970e-03,
         8.3726e-03],
        [3.6056e-03, 3.8250e-03, 2.2736e-01,  ..., 3.5238e-03, 4.0302e-03,
         3.6623e-03],
        ...,
        [8.3534e-05, 9.0321e-05, 3.4465e-02,  ..., 7.5762e-05, 8.0648e-05,
         8.0884e-05],
        [3.6587e-03, 3.6409e-03, 8.6434e-01,  ..., 3.6250e-03, 3.9368e-03,
         3.7382e-03],
        [3.8773e-03, 3.6388e-03, 1.3489e-01,  ..., 3.8509e-03, 3.7893e-03,
         3.9115e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3,
        3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        2, 3, 2, 3, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3]
original label
tensor([2, 2, 3, 2, 3, 4, 2, 0, 2, 3, 0, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 9,
        2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2,
        1, 3, 2, 2, 2, 3, 2, 7, 3, 2, 3, 3, 3, 2, 2, 3, 2, 4, 2, 3, 2, 3, 3, 3,
        2, 3, 3, 2, 2, 4, 2, 3, 4, 2, 2, 2, 3, 2, 3, 3, 3, 3, 7, 5, 2, 3, 2, 3,
        2, 0, 3, 3, 3, 2, 3, 3, 3, 2, 5, 2, 2, 2, 2, 3, 2, 5, 2, 3, 3, 7, 3, 3,
        2, 3, 2, 2, 2, 3, 3, 8])
soft_pseudo_label
tensor([[1.5685e-03, 1.7464e-03, 9.8341e-01,  ..., 1.6984e-03, 1.6819e-03,
         1.8938e-03],
        [2.8357e-04, 3.2492e-04, 9.9748e-01,  ..., 3.2282e-04, 3.0924e-04,
         3.5321e-04],
        [1.7921e-03, 1.6729e-03, 9.8607e-01,  ..., 1.5840e-03, 1.7120e-03,
         1.8398e-03],
        ...,
        [4.2470e-03, 4.2814e-03, 8.6939e-04,  ..., 4.6180e-03, 3.9780e-03,
         4.6724e-03],
        [5.0836e-03, 5.3799e-03, 9.4679e-01,  ..., 5.2399e-03, 5.5942e-03,
         5.2501e-03],
        [1.0992e-03, 1.3331e-03, 9.8915e-01,  ..., 1.0843e-03, 1.2764e-03,
         1.2031e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2,
        2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3,
        3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2,
        3, 3, 2, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2]
original label
tensor([2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 0, 3, 3, 1, 3, 2, 3, 2,
        2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 1,
        3, 2, 3, 9, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 7, 8, 8, 4,
        3, 3, 2, 3, 3, 1, 2, 2])
soft_pseudo_label
tensor([[4.4263e-04, 4.0997e-04, 9.9942e-04,  ..., 4.5247e-04, 4.4263e-04,
         4.6820e-04],
        [1.2110e-03, 1.2834e-03, 3.5518e-04,  ..., 1.2270e-03, 1.2439e-03,
         1.1858e-03],
        [7.8936e-04, 8.8362e-04, 9.9295e-01,  ..., 9.1100e-04, 9.3488e-04,
         8.7931e-04],
        ...,
        [1.8576e-03, 2.0779e-03, 9.8435e-01,  ..., 1.8731e-03, 1.8804e-03,
         1.8975e-03],
        [2.1664e-03, 2.0007e-03, 3.0247e-03,  ..., 2.0382e-03, 2.1214e-03,
         2.0967e-03],
        [1.9785e-03, 2.1102e-03, 9.8227e-01,  ..., 2.0483e-03, 2.0215e-03,
         2.0755e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3,
        3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3,
        3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2]
original label
tensor([3, 3, 2, 3, 0, 3, 8, 2, 0, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 7, 3, 2, 2,
        3, 3, 3, 3, 3, 3, 5, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 6, 2, 3, 3, 2, 1,
        2, 3, 7, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 8, 2, 3, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 0, 2, 2, 2, 3,
        3, 2, 3, 2, 3, 3, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 8, 3, 5, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 2])
soft_pseudo_label
tensor([[1.7332e-04, 1.8577e-04, 4.4584e-01,  ..., 1.7434e-04, 1.7848e-04,
         1.8925e-04],
        [4.7818e-03, 4.1182e-03, 9.6130e-01,  ..., 4.5451e-03, 4.7760e-03,
         4.8773e-03],
        [1.6304e-03, 1.3123e-03, 9.8897e-01,  ..., 1.2280e-03, 1.2637e-03,
         1.5409e-03],
        ...,
        [2.1860e-03, 1.9505e-03, 9.8456e-01,  ..., 1.9097e-03, 1.8744e-03,
         1.6933e-03],
        [3.1250e-03, 3.3519e-03, 9.7184e-01,  ..., 3.1403e-03, 3.0841e-03,
         3.1007e-03],
        [5.0956e-05, 5.4667e-05, 3.3439e-04,  ..., 5.4295e-05, 5.8278e-05,
         5.9631e-05]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2,
        3, 2, 3, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3]
original label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 9, 3,
        3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 9, 2, 3, 6, 3, 2, 3, 3, 2, 3, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 9, 2, 8, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 5, 2, 2, 3, 2, 3, 3,
        3, 3, 2, 4, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 8, 8, 2, 5, 3, 6, 3, 2, 2,
        3, 2, 3, 3, 2, 2, 2, 2])
soft_pseudo_label
tensor([[2.6829e-03, 2.6504e-03, 8.9952e-01,  ..., 2.5928e-03, 2.7385e-03,
         2.6948e-03],
        [2.6821e-03, 2.6509e-03, 1.1079e-02,  ..., 2.6613e-03, 2.7592e-03,
         2.7177e-03],
        [1.8580e-04, 1.6851e-04, 1.3282e-02,  ..., 1.5253e-04, 1.6301e-04,
         1.4512e-04],
        ...,
        [6.6650e-05, 6.1161e-05, 3.0489e-04,  ..., 6.3535e-05, 6.5552e-05,
         6.5297e-05],
        [3.7694e-03, 4.1333e-03, 9.6825e-01,  ..., 4.0692e-03, 3.5100e-03,
         3.8795e-03],
        [1.7295e-03, 1.6812e-03, 2.5461e-03,  ..., 1.8967e-03, 1.7958e-03,
         1.7567e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3]
original label
tensor([3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 6, 3, 2, 3, 2, 4, 5, 3, 3, 2, 2, 2, 3,
        2, 3, 4, 2, 8, 2, 3, 2, 2, 2, 3, 2, 3, 6, 3, 6, 2, 2, 3, 3, 2, 2, 2, 2,
        2, 3, 3, 7, 3, 2, 3, 3, 5, 3, 0, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 8, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 0, 3, 1, 2, 3, 3, 2, 1, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2,
        4, 7, 2, 3, 3, 8, 2, 3])
soft_pseudo_label
tensor([[2.8848e-03, 3.6093e-03, 8.3899e-05,  ..., 3.4514e-03, 2.8806e-03,
         3.5027e-03],
        [3.8123e-03, 4.0503e-03, 9.5729e-01,  ..., 3.7863e-03, 3.8403e-03,
         3.4644e-03],
        [2.4015e-03, 2.2949e-03, 9.7884e-01,  ..., 2.5434e-03, 2.3759e-03,
         2.4192e-03],
        ...,
        [1.9739e-03, 2.0961e-03, 1.3038e-01,  ..., 2.0168e-03, 1.9816e-03,
         2.0168e-03],
        [2.5171e-03, 2.6379e-03, 9.4653e-01,  ..., 2.6123e-03, 2.6418e-03,
         2.3531e-03],
        [1.4305e-03, 1.4680e-03, 9.8817e-01,  ..., 1.4562e-03, 1.3882e-03,
         1.4995e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]
original label
tensor([3, 2, 2, 6, 3, 3, 7, 3, 3, 3, 2, 3, 5, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 3, 3, 1, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 4,
        2, 2, 3, 2, 2, 9, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 0, 2, 2, 3, 8, 3, 3, 1, 3, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 3, 5, 3, 2, 2])
soft_pseudo_label
tensor([[1.8648e-03, 1.9793e-03, 7.9174e-04,  ..., 1.7471e-03, 1.7033e-03,
         1.8772e-03],
        [1.5187e-03, 1.6117e-03, 2.5453e-05,  ..., 1.5343e-03, 1.3368e-03,
         1.5183e-03],
        [1.7443e-03, 1.9139e-03, 9.8418e-01,  ..., 1.8325e-03, 1.8138e-03,
         1.8723e-03],
        ...,
        [4.0420e-04, 4.4894e-04, 5.4028e-03,  ..., 4.2277e-04, 4.2360e-04,
         4.3364e-04],
        [3.6898e-03, 3.5972e-03, 6.4012e-02,  ..., 3.8050e-03, 3.5501e-03,
         3.8953e-03],
        [3.0838e-03, 2.8188e-03, 9.7866e-01,  ..., 2.5212e-03, 2.4822e-03,
         2.4998e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3,
        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3,
        2, 2, 2, 2, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2]
original label
tensor([3, 3, 2, 5, 2, 2, 2, 2, 2, 3, 3, 8, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2,
        2, 1, 2, 9, 3, 3, 2, 3, 0, 2, 2, 3, 3, 3, 3, 2, 3, 2, 4, 3, 2, 7, 3, 3,
        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 9, 2, 3, 2, 8, 3,
        4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 9, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2,
        2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4, 7, 7, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 8, 1, 2])
soft_pseudo_label
tensor([[4.8791e-03, 5.0364e-03, 4.3493e-01,  ..., 4.9366e-03, 5.1157e-03,
         5.2090e-03],
        [2.2648e-03, 2.3804e-03, 9.8024e-01,  ..., 2.2286e-03, 2.2893e-03,
         2.3585e-03],
        [3.1927e-03, 3.1571e-03, 1.5921e-03,  ..., 3.2885e-03, 3.1772e-03,
         3.3788e-03],
        ...,
        [1.5793e-03, 1.8430e-03, 5.2332e-07,  ..., 1.7863e-03, 1.5463e-03,
         1.7509e-03],
        [8.9139e-05, 9.3782e-05, 7.6365e-01,  ..., 7.7976e-05, 9.2238e-05,
         8.3984e-05],
        [2.2463e-03, 1.7896e-03, 9.8320e-01,  ..., 2.2875e-03, 2.0003e-03,
         2.1801e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3,
        3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3,
        3, 3, 2, 2, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2]
original label
tensor([2, 2, 3, 3, 2, 2, 3, 2, 3, 9, 2, 4, 1, 3, 2, 2, 2, 2, 3, 5, 3, 9, 3, 3,
        2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 5, 2, 2, 3, 2, 3, 3, 2, 3,
        9, 2, 3, 2, 2, 3, 4, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 8, 2, 2, 2, 3, 2, 3,
        2, 3, 3, 2, 4, 2, 5, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 8, 9, 2, 3,
        3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 2, 2, 3, 3, 2, 2])
soft_pseudo_label
tensor([[6.6869e-04, 5.7799e-04, 9.9475e-01,  ..., 6.0190e-04, 6.6494e-04,
         6.6673e-04],
        [2.0325e-03, 2.0878e-03, 1.0495e-02,  ..., 2.1731e-03, 2.2608e-03,
         2.0888e-03],
        [2.3014e-03, 1.9839e-03, 9.8015e-01,  ..., 2.4036e-03, 2.2470e-03,
         2.5693e-03],
        ...,
        [1.3791e-04, 1.4284e-04, 9.6601e-02,  ..., 1.2459e-04, 1.3737e-04,
         1.2147e-04],
        [2.3689e-03, 1.8144e-03, 1.0034e-05,  ..., 2.1193e-03, 2.1106e-03,
         2.2050e-03],
        [2.4505e-03, 2.6506e-03, 9.8066e-01,  ..., 2.2372e-03, 2.4800e-03,
         2.0741e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 2, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2]
original label
tensor([2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 1,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 8, 3, 5, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        2, 3, 3, 3, 2, 3, 3, 6, 3, 2, 2, 6, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 2, 1, 2, 3, 2, 6, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 5, 9, 2,
        3, 3, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[2.1459e-03, 1.9812e-03, 4.4662e-04,  ..., 2.1396e-03, 1.9827e-03,
         1.9851e-03],
        [1.7994e-03, 1.8277e-03, 4.0293e-03,  ..., 1.8894e-03, 1.7976e-03,
         1.7095e-03],
        [1.2060e-03, 1.4148e-03, 9.8912e-01,  ..., 1.2510e-03, 1.3395e-03,
         1.2658e-03],
        ...,
        [1.2959e-03, 1.3215e-03, 4.1594e-02,  ..., 1.1447e-03, 1.2846e-03,
         1.1481e-03],
        [6.2710e-04, 5.7490e-04, 7.6441e-04,  ..., 6.0632e-04, 5.7743e-04,
         6.3666e-04],
        [2.1647e-03, 2.3291e-03, 5.5106e-05,  ..., 2.2174e-03, 2.3939e-03,
         2.5646e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2,
        3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3,
        3, 3, 2, 2, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3]
original label
tensor([3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 1, 2, 3, 3, 2, 3, 3, 2,
        2, 3, 2, 9, 2, 3, 3, 7, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 9, 3, 3, 2, 2,
        3, 4, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 8, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 3, 2, 3, 8, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 5,
        2, 5, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 0,
        3, 3, 2, 2, 2, 3, 3, 3])
soft_pseudo_label
tensor([[2.6489e-03, 2.7611e-03, 9.0694e-01,  ..., 2.7077e-03, 2.7733e-03,
         2.7477e-03],
        [9.8603e-04, 1.0084e-03, 9.9130e-01,  ..., 8.8171e-04, 9.8555e-04,
         9.8555e-04],
        [3.6329e-05, 3.6011e-05, 9.9880e-01,  ..., 3.0563e-05, 3.0563e-05,
         2.8073e-05],
        ...,
        [1.6175e-03, 1.5823e-03, 8.6957e-04,  ..., 1.7704e-03, 1.6510e-03,
         1.8328e-03],
        [3.0143e-04, 2.9996e-04, 3.8683e-01,  ..., 2.7073e-04, 2.6240e-04,
         2.6035e-04],
        [3.5160e-03, 3.6632e-03, 9.2934e-01,  ..., 3.7100e-03, 3.7282e-03,
         3.5749e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3,
        3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2,
        3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2,
        3, 3, 3, 3, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2]
original label
tensor([2, 2, 2, 2, 3, 2, 2, 2, 3, 9, 6, 2, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 4, 3, 7, 3, 3, 0, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 4, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3,
        3, 2, 3, 2, 7, 3, 3, 2, 2, 6, 3, 3, 2, 3, 5, 7, 2, 3, 3, 3, 3, 3, 2, 2,
        9, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 4, 2, 2, 3, 2, 3, 2, 2,
        2, 2, 3, 3, 2, 0, 2, 2])
soft_pseudo_label
tensor([[2.9543e-03, 3.0148e-03, 9.9767e-05,  ..., 2.6929e-03, 2.7652e-03,
         2.7736e-03],
        [8.8054e-04, 8.8097e-04, 9.9288e-01,  ..., 8.8658e-04, 8.6774e-04,
         9.1584e-04],
        [1.9760e-03, 2.2314e-03, 9.8100e-01,  ..., 2.3169e-03, 2.1449e-03,
         2.3163e-03],
        ...,
        [2.2904e-04, 2.2748e-04, 2.7798e-02,  ..., 2.3061e-04, 2.2814e-04,
         2.4357e-04],
        [3.5705e-03, 3.6327e-03, 9.7072e-01,  ..., 3.5048e-03, 3.9118e-03,
         3.7023e-03],
        [1.9302e-03, 1.8159e-03, 9.8340e-01,  ..., 1.7825e-03, 1.9644e-03,
         2.0517e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3,
        3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 3, 2, 9, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 7, 2, 3, 3, 3, 2,
        6, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 2, 0, 3, 2, 3, 2, 0, 2, 2, 3, 2, 2, 2, 7, 3, 2, 2, 3, 2, 3, 2, 2,
        2, 1, 3, 3, 3, 2, 3, 4, 3, 2, 2, 2, 3, 3, 7, 2, 3, 2, 2, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 7, 3, 2, 3, 1, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2,
        2, 2, 3, 3, 2, 8, 2, 2])
soft_pseudo_label
tensor([[1.0862e-03, 1.1716e-03, 3.0054e-02,  ..., 1.1383e-03, 1.0958e-03,
         1.2135e-03],
        [1.1436e-03, 1.3462e-03, 2.2744e-03,  ..., 1.1492e-03, 1.1673e-03,
         1.2191e-03],
        [9.7465e-04, 9.4859e-04, 2.0828e-04,  ..., 9.8710e-04, 8.9112e-04,
         9.7942e-04],
        ...,
        [3.5095e-03, 3.6173e-03, 6.0352e-02,  ..., 3.4839e-03, 3.8412e-03,
         3.3341e-03],
        [3.7655e-03, 3.6124e-03, 2.5115e-01,  ..., 3.6336e-03, 3.5598e-03,
         3.8774e-03],
        [1.4490e-03, 1.5121e-03, 8.9133e-07,  ..., 1.5687e-03, 1.6562e-03,
         1.4950e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,
        3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3,
        3, 3, 2, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3]
original label
tensor([3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 6, 3, 7, 1, 3, 1, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 7,
        2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 9, 3, 3, 6, 2, 2, 2, 3, 2, 3, 3, 3,
        2, 3, 3, 7, 0, 3, 3, 2, 3, 2, 5, 3, 2, 3, 2, 3, 3, 9, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 3, 8, 0, 2, 2, 3, 2, 6,
        3, 3, 2, 3, 2, 7, 3, 3])
soft_pseudo_label
tensor([[9.5893e-04, 8.2067e-04, 9.9278e-01,  ..., 9.4547e-04, 8.8957e-04,
         9.5598e-04],
        [4.9355e-03, 4.6626e-03, 3.3208e-03,  ..., 4.7199e-03, 4.2933e-03,
         4.7176e-03],
        [2.1291e-03, 2.0208e-03, 1.1943e-03,  ..., 2.1978e-03, 1.8983e-03,
         2.1291e-03],
        ...,
        [7.4517e-04, 7.8859e-04, 9.9319e-01,  ..., 8.2805e-04, 8.3884e-04,
         1.0241e-03],
        [1.7332e-03, 1.8405e-03, 9.8077e-01,  ..., 1.8906e-03, 1.7961e-03,
         1.9630e-03],
        [2.1905e-03, 2.1948e-03, 5.5821e-03,  ..., 2.3148e-03, 2.1831e-03,
         2.1640e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        2, 3, 2, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3]
original label
tensor([2, 6, 1, 3, 3, 3, 2, 2, 0, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 0, 2, 2, 0, 3, 3, 2,
        3, 2, 3, 3, 2, 6, 2, 3, 3, 3, 3, 8, 2, 2, 3, 2, 2, 3, 3, 3, 3, 0, 3, 2,
        3, 2, 3, 3, 3, 2, 2, 2, 6, 2, 0, 2, 3, 3, 0, 3, 3, 3, 2, 2, 7, 2, 3, 3,
        2, 2, 2, 2, 3, 2, 2, 2])
soft_pseudo_label
tensor([[0.0141, 0.0147, 0.5950,  ..., 0.0154, 0.0146, 0.0141],
        [0.0021, 0.0021, 0.0047,  ..., 0.0020, 0.0023, 0.0022],
        [0.0052, 0.0055, 0.9410,  ..., 0.0056, 0.0055, 0.0052],
        ...,
        [0.0022, 0.0024, 0.9803,  ..., 0.0024, 0.0024, 0.0025],
        [0.0042, 0.0039, 0.0037,  ..., 0.0041, 0.0038, 0.0041],
        [0.0013, 0.0013, 0.0025,  ..., 0.0012, 0.0013, 0.0013]],
       device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2,
        3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3]
original label
tensor([2, 2, 2, 2, 6, 2, 3, 5, 3, 2, 2, 2, 4, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 2, 8, 6, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 0, 2, 3, 3, 3, 2, 2,
        2, 6, 7, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 6, 3, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 3, 2, 2, 4, 2, 3, 2, 3, 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 2,
        9, 0, 3, 2, 2, 2, 3, 1, 2, 3, 2, 6, 3, 2, 3, 2, 0, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 3, 3])
soft_pseudo_label
tensor([[6.0661e-04, 7.1476e-04, 2.9405e-04,  ..., 6.6786e-04, 7.5697e-04,
         6.8637e-04],
        [4.2707e-03, 4.3401e-03, 6.4409e-03,  ..., 4.3274e-03, 4.2375e-03,
         4.0851e-03],
        [6.7814e-03, 7.4225e-03, 3.8033e-01,  ..., 7.0861e-03, 6.9152e-03,
         6.9899e-03],
        ...,
        [1.2009e-03, 1.4475e-03, 9.8807e-01,  ..., 1.3965e-03, 1.5246e-03,
         1.7069e-03],
        [6.2693e-04, 5.9487e-04, 9.9508e-01,  ..., 6.3680e-04, 6.5007e-04,
         5.5522e-04],
        [1.2818e-03, 1.5889e-03, 9.8710e-01,  ..., 1.7753e-03, 1.6661e-03,
         1.6339e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 2, 2, 3, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2]
original label
tensor([2, 3, 2, 2, 6, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 8, 2,
        3, 2, 0, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 9, 3, 3, 2, 2, 2, 3, 2, 2,
        2, 2, 2, 0, 2, 2, 2, 2])
soft_pseudo_label
tensor([[5.5882e-03, 6.1254e-03, 3.5661e-04,  ..., 5.7779e-03, 5.4152e-03,
         6.3617e-03],
        [4.4295e-04, 4.2640e-04, 2.0014e-04,  ..., 4.5390e-04, 4.8246e-04,
         4.3354e-04],
        [9.8688e-04, 1.0498e-03, 9.1943e-04,  ..., 9.5186e-04, 9.3939e-04,
         9.2709e-04],
        ...,
        [4.2574e-04, 4.0883e-04, 9.9370e-01,  ..., 4.2366e-04, 4.0347e-04,
         4.2782e-04],
        [3.9204e-03, 3.6910e-03, 2.1703e-04,  ..., 3.8082e-03, 3.5604e-03,
         4.0943e-03],
        [1.0840e-04, 1.0682e-04, 1.8756e-01,  ..., 1.0882e-04, 1.0808e-04,
         1.0808e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2,
        3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 2, 2, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3]
original label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 7, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 4, 2, 9, 2,
        2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 7, 3, 3, 3, 3, 3, 3, 3, 4, 2, 3,
        2, 3, 2, 3, 2, 6, 2, 2, 0, 3, 3, 4, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 5, 2, 2, 2,
        1, 2, 3, 2, 2, 3, 3, 2, 2, 7, 3, 2, 3, 4, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 2, 2, 2, 3, 0])
soft_pseudo_label
tensor([[0.0032, 0.0032, 0.0769,  ..., 0.0032, 0.0034, 0.0033],
        [0.0031, 0.0031, 0.0957,  ..., 0.0030, 0.0032, 0.0030],
        [0.0009, 0.0010, 0.0133,  ..., 0.0009, 0.0009, 0.0010],
        ...,
        [0.0043, 0.0044, 0.0749,  ..., 0.0041, 0.0040, 0.0043],
        [0.0013, 0.0012, 0.0057,  ..., 0.0013, 0.0012, 0.0013],
        [0.0034, 0.0038, 0.0023,  ..., 0.0035, 0.0031, 0.0032]],
       device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2,
        2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3]
original label
tensor([3, 3, 0, 2, 3, 2, 3, 0, 2, 3, 2, 2, 3, 7, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 0, 2, 3,
        2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 6, 3, 3, 2, 2, 2, 2, 3, 0, 0, 3, 3, 3, 7,
        1, 3, 2, 3, 3, 2, 7, 3, 2, 3, 6, 2, 2, 7, 3, 3, 2, 3, 3, 3, 2, 2, 3, 5,
        1, 3, 2, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 2])
soft_pseudo_label
tensor([[2.1610e-03, 2.3304e-03, 9.8175e-01,  ..., 1.9784e-03, 2.4100e-03,
         2.1554e-03],
        [3.9429e-03, 4.2157e-03, 3.4569e-01,  ..., 3.8403e-03, 4.1281e-03,
         4.2508e-03],
        [1.1468e-03, 1.1457e-03, 4.7171e-02,  ..., 1.1468e-03, 1.1268e-03,
         1.1490e-03],
        ...,
        [2.5141e-03, 2.6187e-03, 9.7850e-01,  ..., 2.8461e-03, 2.5264e-03,
         2.4415e-03],
        [2.3681e-03, 2.3704e-03, 9.7800e-01,  ..., 2.5856e-03, 2.7699e-03,
         2.4588e-03],
        [1.7527e-03, 1.7656e-03, 3.6214e-04,  ..., 1.9434e-03, 1.7786e-03,
         2.0243e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2,
        2, 3, 3, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3]
original label
tensor([2, 3, 3, 2, 9, 2, 3, 2, 2, 2, 3, 3, 3, 1, 2, 3, 2, 4, 2, 3, 3, 3, 3, 2,
        7, 3, 2, 3, 6, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 8, 2, 2, 2, 3, 3, 2, 6, 3, 3, 3,
        3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 6, 3, 3, 3, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 2, 3, 9, 2, 2,
        2, 3, 2, 2, 3, 2, 2, 3])
soft_pseudo_label
tensor([[2.5196e-03, 2.3739e-03, 1.3681e-02,  ..., 2.5705e-03, 2.5122e-03,
         2.3890e-03],
        [1.3502e-03, 1.3917e-03, 3.7017e-03,  ..., 1.3023e-03, 1.2481e-03,
         1.3196e-03],
        [3.6922e-03, 3.8618e-03, 9.6454e-01,  ..., 3.6190e-03, 3.8038e-03,
         3.5594e-03],
        ...,
        [4.4619e-03, 4.1528e-03, 6.2087e-01,  ..., 4.4597e-03, 4.3970e-03,
         4.6238e-03],
        [6.0093e-05, 6.1099e-05, 9.9914e-01,  ..., 5.6729e-05, 6.1278e-05,
         6.3969e-05],
        [7.1578e-05, 6.9715e-05, 1.9692e-04,  ..., 6.9920e-05, 7.2919e-05,
         7.0813e-05]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3,
        2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3]
original label
tensor([3, 3, 2, 2, 3, 2, 2, 3, 2, 9, 3, 3, 3, 8, 2, 3, 8, 3, 3, 3, 2, 3, 2, 3,
        2, 3, 2, 5, 2, 3, 2, 3, 2, 3, 2, 3, 7, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3,
        2, 2, 3, 2, 2, 1, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 2, 3, 3, 3, 2, 6, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 1, 3, 8, 3, 3, 3, 2, 2, 3, 3, 2, 1, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 2, 3])
soft_pseudo_label
tensor([[1.0356e-03, 1.0878e-03, 2.0122e-05,  ..., 1.1529e-03, 1.1402e-03,
         1.1473e-03],
        [2.3569e-03, 2.1872e-03, 9.8006e-01,  ..., 2.2065e-03, 2.1262e-03,
         2.1303e-03],
        [5.8446e-03, 6.3940e-03, 2.3074e-01,  ..., 5.6207e-03, 6.0301e-03,
         6.0007e-03],
        ...,
        [4.0038e-03, 3.9687e-03, 6.2182e-01,  ..., 4.0214e-03, 4.0470e-03,
         3.9053e-03],
        [3.4571e-06, 3.5391e-06, 9.8749e-01,  ..., 3.0390e-06, 3.6729e-06,
         2.9340e-06],
        [2.7474e-03, 2.6057e-03, 2.1486e-05,  ..., 2.6454e-03, 2.6446e-03,
         2.7439e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2,
        3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3,
        3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3,
        2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2,
        3, 2, 2, 2, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3]
original label
tensor([3, 2, 3, 4, 2, 2, 3, 2, 3, 2, 3, 2, 2, 6, 4, 3, 2, 2, 2, 2, 3, 2, 2, 2,
        3, 5, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 9, 3, 3, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 5, 3, 3, 2, 2, 3,
        3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 9, 2, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 7, 2, 2, 2, 0, 3, 2, 6, 2, 3, 2,
        3, 2, 2, 2, 5, 3, 2, 3])
soft_pseudo_label
tensor([[2.2888e-03, 2.6139e-03, 9.1084e-01,  ..., 2.4979e-03, 2.4640e-03,
         2.4010e-03],
        [7.8400e-03, 8.2042e-03, 2.4105e-01,  ..., 7.8285e-03, 8.3987e-03,
         8.7461e-03],
        [1.3314e-04, 1.3301e-04, 5.8257e-01,  ..., 1.3980e-04, 1.3710e-04,
         1.3817e-04],
        ...,
        [1.6872e-03, 1.8336e-03, 9.8551e-01,  ..., 1.6610e-03, 1.6529e-03,
         1.5741e-03],
        [2.9887e-03, 2.8158e-03, 2.7739e-02,  ..., 2.9266e-03, 3.0195e-03,
         3.1737e-03],
        [2.6488e-03, 2.7567e-03, 9.7871e-01,  ..., 2.5982e-03, 2.5288e-03,
         2.5793e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3,
        2, 3, 3, 2, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]
original label
tensor([2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 7, 2, 2, 3, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 2, 2, 3,
        2, 2, 3, 5, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 0, 9, 3, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 5, 3, 9, 2, 2, 2, 3, 8, 3, 2, 3, 2, 2, 3, 7, 3, 3, 3,
        2, 3, 2, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 2, 3, 3, 9, 3, 2, 2, 2, 2, 2, 4,
        2, 3, 8, 2, 3, 2, 3, 2])
soft_pseudo_label
tensor([[1.8515e-03, 1.8807e-03, 3.3049e-01,  ..., 1.8880e-03, 1.9140e-03,
         1.9347e-03],
        [2.0817e-03, 2.0655e-03, 9.8293e-01,  ..., 2.0937e-03, 2.0447e-03,
         2.2125e-03],
        [2.3519e-03, 2.3302e-03, 6.6300e-03,  ..., 2.3948e-03, 2.2651e-03,
         2.4408e-03],
        ...,
        [1.4094e-03, 1.4777e-03, 1.2432e-03,  ..., 1.4864e-03, 1.4712e-03,
         1.5403e-03],
        [3.3769e-03, 3.1503e-03, 7.3260e-05,  ..., 3.3557e-03, 2.8930e-03,
         2.9046e-03],
        [1.7820e-03, 1.9619e-03, 9.8264e-01,  ..., 1.7803e-03, 2.0163e-03,
         1.9505e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3,
        3, 2, 3, 2, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2]
original label
tensor([6, 2, 3, 3, 3, 2, 3, 2, 3, 9, 4, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 5, 3, 4,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4,
        3, 7, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        3, 2, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.9456e-03, 1.7802e-03, 9.8715e-01,  ..., 1.5165e-03, 1.6908e-03,
         1.5255e-03],
        [1.0539e-03, 1.1126e-03, 3.7448e-03,  ..., 9.9688e-04, 1.0003e-03,
         9.6056e-04],
        [2.4843e-03, 2.1312e-03, 2.8699e-03,  ..., 2.3032e-03, 2.0040e-03,
         2.1732e-03],
        ...,
        [2.9683e-03, 3.3308e-03, 9.7117e-01,  ..., 3.3586e-03, 3.3324e-03,
         3.1938e-03],
        [1.1816e-04, 1.1187e-04, 9.7727e-01,  ..., 1.1363e-04, 1.1003e-04,
         1.2072e-04],
        [1.4673e-03, 1.7961e-03, 9.8221e-01,  ..., 1.6914e-03, 1.7485e-03,
         1.7970e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3,
        2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2,
        3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 3, 3, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2]
original label
tensor([2, 3, 3, 3, 9, 5, 2, 8, 2, 3, 3, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 7, 3, 3,
        2, 3, 3, 3, 3, 1, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 4, 2, 2, 3, 3, 2, 2, 2, 3, 3, 5, 3, 3,
        2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 4, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 9, 2, 9, 2, 3, 2])
soft_pseudo_label
tensor([[1.4738e-03, 1.6526e-03, 9.8701e-01,  ..., 1.6222e-03, 1.5785e-03,
         1.6729e-03],
        [3.6742e-03, 4.2269e-03, 2.6249e-02,  ..., 3.7339e-03, 3.8243e-03,
         3.6207e-03],
        [5.6481e-03, 5.7624e-03, 7.1351e-01,  ..., 5.8904e-03, 5.9221e-03,
         5.7064e-03],
        ...,
        [1.5390e-03, 1.6074e-03, 9.0514e-01,  ..., 1.4685e-03, 1.5663e-03,
         1.5763e-03],
        [2.0134e-03, 1.9900e-03, 9.8387e-01,  ..., 1.9269e-03, 1.8512e-03,
         1.9425e-03],
        [1.0223e-03, 8.4548e-04, 1.5541e-04,  ..., 9.5946e-04, 8.8996e-04,
         1.0067e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3,
        3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2,
        3, 2, 2, 2, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3]
original label
tensor([2, 2, 3, 2, 2, 3, 2, 3, 3, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 5, 2, 2, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 2, 7, 2,
        3, 7, 2, 8, 3, 3, 3, 5, 8, 3, 3, 2, 2, 3, 3, 2, 3, 7, 3, 3, 2, 3, 3, 3,
        0, 7, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 7, 2, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 6, 3, 2, 2, 3, 5, 3, 2, 3,
        2, 2, 2, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[2.8203e-03, 2.8900e-03, 3.2820e-03,  ..., 2.8148e-03, 2.7712e-03,
         2.8724e-03],
        [3.1454e-03, 3.1485e-03, 2.7712e-01,  ..., 3.3254e-03, 3.3319e-03,
         3.2947e-03],
        [2.8309e-03, 3.2275e-03, 9.7424e-01,  ..., 3.6496e-03, 3.2086e-03,
         3.0707e-03],
        ...,
        [1.5762e-03, 1.9492e-03, 9.8056e-01,  ..., 1.7303e-03, 1.8837e-03,
         1.5307e-03],
        [4.4223e-05, 5.1551e-05, 1.1794e-01,  ..., 3.5052e-05, 4.2116e-05,
         3.4847e-05],
        [1.1689e-03, 1.4823e-03, 9.8814e-01,  ..., 1.4484e-03, 1.5729e-03,
         1.6316e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2,
        3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3,
        2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2]
original label
tensor([3, 5, 2, 0, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 9, 3, 3, 3, 3, 2, 3, 3, 2,
        3, 1, 2, 2, 7, 3, 3, 0, 1, 8, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 8, 2,
        3, 2, 2, 2, 2, 2, 2, 0, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 4, 2, 3, 3, 3, 2, 6, 2, 8, 2, 3, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 3, 2, 3, 5, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 9,
        3, 3, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[4.6395e-03, 4.4957e-03, 2.3374e-04,  ..., 4.2481e-03, 4.2244e-03,
         4.8592e-03],
        [1.2955e-03, 1.3366e-03, 3.7390e-03,  ..., 1.4166e-03, 1.3810e-03,
         1.4838e-03],
        [1.1342e-03, 1.0247e-03, 1.0781e-03,  ..., 1.1326e-03, 1.0781e-03,
         1.1459e-03],
        ...,
        [1.0991e-03, 1.1734e-03, 9.8519e-01,  ..., 1.1972e-03, 1.2461e-03,
         1.2071e-03],
        [4.5639e-03, 4.6630e-03, 7.6833e-03,  ..., 4.8476e-03, 4.4149e-03,
         4.5728e-03],
        [1.9657e-04, 1.9033e-04, 2.6783e-04,  ..., 1.9590e-04, 1.7229e-04,
         1.8188e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3,
        2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2,
        3, 2, 3, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3]
original label
tensor([3, 3, 3, 3, 2, 4, 3, 3, 2, 3, 3, 3, 3, 2, 7, 2, 5, 3, 2, 2, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 8, 2, 3,
        2, 2, 3, 2, 3, 2, 3, 3, 2, 8, 3, 3, 3, 7, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 1, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 1, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 8, 2, 0, 2, 2,
        2, 2, 3, 3, 2, 2, 3, 3])
soft_pseudo_label
tensor([[2.6695e-03, 2.6813e-03, 4.3699e-02,  ..., 2.7342e-03, 2.6813e-03,
         2.4188e-03],
        [7.1024e-03, 7.1059e-03, 6.6374e-02,  ..., 7.2319e-03, 7.3065e-03,
         7.7360e-03],
        [1.2787e-03, 1.3150e-03, 9.8914e-01,  ..., 1.5296e-03, 1.3624e-03,
         1.3435e-03],
        ...,
        [1.5734e-05, 1.7280e-05, 9.9985e-01,  ..., 1.4214e-05, 1.6424e-05,
         1.5028e-05],
        [2.1593e-03, 2.1805e-03, 6.9958e-03,  ..., 2.1709e-03, 2.1944e-03,
         2.2896e-03],
        [3.6770e-03, 3.7882e-03, 2.6744e-03,  ..., 3.5292e-03, 3.9991e-03,
         3.9506e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2,
        2, 2, 3, 3, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3]
original label
tensor([3, 3, 2, 3, 3, 2, 5, 2, 3, 2, 2, 3, 8, 2, 3, 2, 2, 3, 6, 2, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 2, 4, 3, 3, 2, 2, 3, 2, 2, 6, 2, 2, 2, 3, 3, 1, 0, 3,
        3, 3, 2, 3, 6, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 6, 3, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 5, 2, 3, 3, 3, 3, 3, 4, 2, 2, 6, 2, 3, 3,
        3, 2, 2, 4, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 5, 2, 2, 3, 3, 3, 4, 3, 2,
        2, 2, 3, 3, 3, 4, 3, 3])
soft_pseudo_label
tensor([[5.6881e-03, 5.6632e-03, 4.0856e-02,  ..., 5.5455e-03, 5.4596e-03,
         5.4224e-03],
        [2.5581e-03, 2.4843e-03, 1.0475e-02,  ..., 2.5908e-03, 2.6484e-03,
         2.7863e-03],
        [3.7119e-05, 4.3016e-05, 1.1394e-01,  ..., 3.3665e-05, 4.2184e-05,
         3.6579e-05],
        ...,
        [2.4042e-03, 2.4799e-03, 1.2470e-03,  ..., 2.5555e-03, 2.2901e-03,
         2.4981e-03],
        [1.3150e-03, 1.1719e-03, 9.9014e-01,  ..., 9.7912e-04, 1.2660e-03,
         1.2129e-03],
        [1.1546e-03, 1.4036e-03, 9.8899e-01,  ..., 1.3119e-03, 1.3748e-03,
         1.4219e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3,
        2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3,
        3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        2, 2, 3, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2]
original label
tensor([3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 2, 2, 2, 3, 3, 3, 7,
        2, 3, 2, 9, 2, 3, 2, 2, 3, 5, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 2, 3,
        2, 2, 0, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 0, 3, 2, 3, 2, 3, 3, 2, 2, 5,
        3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 6, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 6, 0, 3, 2, 9, 2, 3, 2, 3, 3, 3, 2, 2, 2, 8, 2, 3, 3,
        2, 3, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[2.0361e-03, 2.1833e-03, 2.3157e-03,  ..., 2.0173e-03, 2.0401e-03,
         2.0631e-03],
        [2.7341e-03, 2.8140e-03, 5.6011e-03,  ..., 2.4484e-03, 2.7301e-03,
         2.6358e-03],
        [5.0868e-03, 4.4079e-03, 1.0859e-05,  ..., 4.1835e-03, 4.2720e-03,
         4.4012e-03],
        ...,
        [4.9178e-03, 5.1262e-03, 3.3738e-02,  ..., 5.2734e-03, 5.1137e-03,
         4.5928e-03],
        [1.5688e-03, 1.4375e-03, 2.7368e-04,  ..., 1.5295e-03, 1.3633e-03,
         1.5284e-03],
        [1.3330e-03, 1.1946e-03, 9.9077e-01,  ..., 1.1024e-03, 1.1446e-03,
         1.0529e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3,
        2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2,
        2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3,
        3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3,
        3, 3, 3, 2, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2]
original label
tensor([5, 3, 3, 3, 2, 9, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 5, 2, 1,
        2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 7, 2, 2, 3, 2,
        2, 2, 3, 8, 3, 3, 3, 3, 3, 2, 3, 3, 6, 5, 2, 2, 3, 2, 8, 2, 2, 3, 3, 2,
        3, 3, 3, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 6, 2,
        3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 9, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        5, 3, 0, 3, 3, 7, 3, 2])
soft_pseudo_label
tensor([[8.8345e-04, 9.3499e-04, 9.9264e-01,  ..., 9.5518e-04, 9.7235e-04,
         9.6771e-04],
        [3.5712e-03, 3.8859e-03, 6.4875e-01,  ..., 3.8974e-03, 3.8444e-03,
         3.6648e-03],
        [1.9947e-03, 2.0006e-03, 2.0169e-01,  ..., 2.0600e-03, 2.0701e-03,
         1.9850e-03],
        ...,
        [1.6900e-03, 1.4949e-03, 9.8777e-01,  ..., 1.3406e-03, 1.5544e-03,
         1.4411e-03],
        [2.2011e-03, 2.5418e-03, 9.8073e-01,  ..., 2.7756e-03, 2.4358e-03,
         2.4792e-03],
        [2.9540e-03, 3.2254e-03, 1.2719e-02,  ..., 3.1816e-03, 3.0493e-03,
         3.2459e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3,
        3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3,
        2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3]
original label
tensor([2, 3, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 2, 3, 3, 2, 3, 1, 2, 3, 2, 2, 6, 3, 5, 2, 3, 2, 2, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 5, 3, 2, 2, 3, 4, 3, 3, 0, 3, 2, 2, 3, 3, 2, 3, 2, 7, 2, 2,
        2, 5, 2, 3, 3, 3, 2, 6, 7, 2, 2, 2, 2, 6, 2, 2, 3, 2, 3, 3, 3, 6, 4, 3,
        2, 5, 3, 3, 3, 2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 7, 2, 3, 3, 2, 3, 3,
        3, 2, 3, 2, 4, 2, 2, 2])
soft_pseudo_label
tensor([[3.0273e-03, 3.1555e-03, 1.6159e-02,  ..., 3.1218e-03, 3.0436e-03,
         3.2605e-03],
        [9.5659e-04, 7.5447e-04, 9.9345e-01,  ..., 7.0807e-04, 8.4605e-04,
         8.3313e-04],
        [2.9836e-03, 2.8251e-03, 2.1149e-05,  ..., 3.3569e-03, 3.1817e-03,
         3.3996e-03],
        ...,
        [3.2150e-03, 3.2961e-03, 9.6199e-01,  ..., 3.2323e-03, 3.3742e-03,
         3.0454e-03],
        [6.3720e-03, 6.8864e-03, 8.8622e-01,  ..., 6.0861e-03, 6.8528e-03,
         6.9302e-03],
        [2.3418e-03, 2.5161e-03, 2.1391e-01,  ..., 2.6510e-03, 2.6240e-03,
         2.5745e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2,
        3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3]
original label
tensor([3, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2,
        3, 3, 9, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 7, 2, 9, 2, 2, 3, 3, 3, 0,
        2, 3, 2, 3, 2, 2, 4, 9, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 5, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2,
        0, 2, 2, 3, 2, 5, 0, 8, 3, 2, 7, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2,
        2, 2, 3, 4, 2, 2, 2, 2])
soft_pseudo_label
tensor([[5.3388e-04, 5.2560e-04, 9.8431e-01,  ..., 5.0993e-04, 5.1093e-04,
         4.9860e-04],
        [3.1625e-03, 3.4734e-03, 9.4306e-01,  ..., 3.3322e-03, 3.4480e-03,
         3.4262e-03],
        [3.1021e-03, 3.0525e-03, 1.9708e-03,  ..., 2.8217e-03, 2.9312e-03,
         3.0749e-03],
        ...,
        [2.4687e-03, 2.6498e-03, 8.7850e-01,  ..., 2.4917e-03, 2.4627e-03,
         2.6292e-03],
        [8.2480e-04, 7.7379e-04, 9.9365e-01,  ..., 7.9466e-04, 7.7049e-04,
         7.8684e-04],
        [1.1798e-04, 1.0997e-04, 4.9875e-03,  ..., 1.0722e-04, 1.0484e-04,
         1.0514e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3,
        3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3,
        2, 3, 3, 2, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3]
original label
tensor([2, 2, 3, 3, 2, 3, 2, 3, 2, 1, 3, 8, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2,
        5, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 1,
        3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 2, 3, 3, 2, 2, 2, 2, 2, 5, 3, 3,
        2, 2, 3, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[8.2654e-05, 8.8243e-05, 9.9467e-01,  ..., 7.0836e-05, 8.2332e-05,
         7.3156e-05],
        [1.1299e-03, 1.1416e-03, 4.1615e-03,  ..., 1.2158e-03, 1.2075e-03,
         1.0761e-03],
        [1.3242e-04, 1.4744e-04, 1.6683e-04,  ..., 1.3281e-04, 1.2810e-04,
         1.2568e-04],
        ...,
        [1.3923e-03, 1.4309e-03, 2.2338e-03,  ..., 1.5246e-03, 1.4288e-03,
         1.3862e-03],
        [3.7950e-03, 3.8548e-03, 6.1019e-01,  ..., 3.9174e-03, 3.7563e-03,
         4.0084e-03],
        [1.6664e-04, 1.6770e-04, 6.9399e-05,  ..., 1.8419e-04, 1.7592e-04,
         1.7143e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3,
        3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2,
        3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3,
        2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        3, 3, 2, 2, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3]
original label
tensor([2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 6, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 8,
        3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 4, 3, 2, 7, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 3, 3, 8, 0, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 8, 2, 2, 8, 2,
        3, 3, 2, 9, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 1, 4, 3, 3, 2, 3,
        2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 2, 2, 3, 3, 3, 0])
soft_pseudo_label
tensor([[6.0263e-05, 5.8580e-05, 6.0831e-01,  ..., 5.5300e-05, 6.3712e-05,
         5.7391e-05],
        [6.7977e-04, 8.3084e-04, 9.9410e-01,  ..., 7.3393e-04, 7.2538e-04,
         6.9283e-04],
        [8.3486e-04, 8.9131e-04, 8.3323e-04,  ..., 8.6854e-04, 7.7779e-04,
         8.9087e-04],
        ...,
        [2.0073e-03, 2.1139e-03, 2.8316e-01,  ..., 1.9570e-03, 1.9820e-03,
         1.9762e-03],
        [1.9949e-03, 1.6906e-03, 1.2508e-03,  ..., 1.8084e-03, 1.8594e-03,
         2.1014e-03],
        [2.8769e-04, 2.5638e-04, 7.7227e-03,  ..., 3.1107e-04, 2.8601e-04,
         2.9857e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 3, 2, 3, 2, 5, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 2, 6, 3, 3, 2, 6, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 9, 2, 2, 3, 3, 4,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 8, 3, 3, 2, 2, 3, 3, 4, 2,
        2, 2, 3, 3, 3, 3, 3, 3, 9, 2, 3, 3, 2, 3, 7, 2, 3, 6, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 4, 2, 3, 3, 2, 4, 3, 2, 3, 2,
        3, 3, 6, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[9.7042e-05, 1.0627e-04, 1.7997e-01,  ..., 8.3901e-05, 1.0130e-04,
         8.4147e-05],
        [1.4988e-03, 1.4176e-03, 5.0120e-02,  ..., 1.4288e-03, 1.4806e-03,
         1.4232e-03],
        [1.6513e-03, 1.7721e-03, 3.6967e-05,  ..., 1.6852e-03, 1.8408e-03,
         1.6535e-03],
        ...,
        [7.9215e-03, 8.4036e-03, 6.5723e-01,  ..., 8.2693e-03, 8.1212e-03,
         8.2290e-03],
        [2.2233e-03, 2.3073e-03, 4.8608e-03,  ..., 2.4039e-03, 2.2671e-03,
         2.2693e-03],
        [1.1351e-03, 1.4068e-03, 9.8902e-01,  ..., 1.3223e-03, 1.3666e-03,
         1.6626e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2,
        2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2,
        3, 2, 2, 2, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2]
original label
tensor([2, 3, 3, 2, 9, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 9, 2, 3, 3, 8, 2,
        2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 5, 2, 3, 9, 3, 2, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 2, 4, 3, 3, 2, 5, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        0, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 9, 2,
        3, 2, 2, 2, 3, 9, 2, 2])
soft_pseudo_label
tensor([[1.5959e-03, 1.6717e-03, 7.7500e-02,  ..., 1.6362e-03, 1.5789e-03,
         1.5243e-03],
        [2.0253e-03, 2.0794e-03, 6.3075e-01,  ..., 2.1163e-03, 2.0774e-03,
         2.0916e-03],
        [4.3631e-03, 4.2662e-03, 1.6679e-01,  ..., 4.4210e-03, 4.3398e-03,
         4.3419e-03],
        ...,
        [1.2440e-03, 1.3013e-03, 9.8970e-01,  ..., 1.1907e-03, 1.3204e-03,
         1.3082e-03],
        [3.6681e-03, 3.6681e-03, 1.5982e-03,  ..., 3.8771e-03, 3.6342e-03,
         3.7845e-03],
        [1.5527e-03, 1.7513e-03, 8.6705e-05,  ..., 1.7091e-03, 1.6703e-03,
         1.6028e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2,
        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3,
        3, 3, 3, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3]
original label
tensor([3, 1, 0, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 6, 2, 3, 2, 3, 3, 7, 2,
        2, 2, 1, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 7, 3, 2,
        2, 3, 8, 4, 2, 2, 2, 3, 9, 3, 6, 0, 4, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 6, 2, 2,
        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3,
        2, 8, 3, 3, 2, 2, 3, 0])
soft_pseudo_label
tensor([[1.5870e-03, 1.6648e-03, 1.1453e-03,  ..., 1.6787e-03, 1.4060e-03,
         1.6112e-03],
        [1.9080e-04, 1.8932e-04, 9.7132e-01,  ..., 1.7647e-04, 1.9744e-04,
         1.7578e-04],
        [2.2305e-03, 1.9761e-03, 2.5413e-04,  ..., 2.1698e-03, 2.2873e-03,
         2.3035e-03],
        ...,
        [3.1984e-03, 3.2077e-03, 9.7184e-01,  ..., 3.6039e-03, 3.5802e-03,
         3.2511e-03],
        [1.4783e-03, 1.5270e-03, 1.5146e-05,  ..., 1.3582e-03, 1.5069e-03,
         1.6733e-03],
        [3.8121e-03, 3.6464e-03, 3.4032e-01,  ..., 3.8140e-03, 3.8439e-03,
         3.8608e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2,
        3, 3, 2, 2, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3]
original label
tensor([3, 2, 3, 2, 3, 3, 9, 6, 2, 2, 3, 7, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 0, 3, 4, 3, 3, 3, 3, 3,
        2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 5, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3,
        1, 2, 2, 4, 2, 2, 3, 3])
soft_pseudo_label
tensor([[1.4002e-03, 1.4538e-03, 9.7906e-03,  ..., 1.3361e-03, 1.2496e-03,
         1.3218e-03],
        [2.3953e-03, 2.2535e-03, 1.8641e-05,  ..., 2.4043e-03, 2.2682e-03,
         2.4499e-03],
        [1.9193e-03, 1.8189e-03, 1.3044e-03,  ..., 1.8675e-03, 1.8413e-03,
         1.8057e-03],
        ...,
        [2.1458e-03, 2.0111e-03, 1.2443e-05,  ..., 2.0578e-03, 1.8737e-03,
         2.2145e-03],
        [5.6020e-05, 5.7911e-05, 9.7692e-02,  ..., 4.9389e-05, 5.9749e-05,
         4.9148e-05],
        [6.6025e-04, 6.5575e-04, 4.2262e-03,  ..., 6.1843e-04, 6.3588e-04,
         6.6445e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2,
        2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3]
original label
tensor([1, 3, 5, 2, 2, 3, 3, 2, 2, 3, 9, 3, 3, 2, 3, 2, 5, 3, 9, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 2, 4, 2, 3, 3, 2, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 4, 5, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 5, 3, 3, 3, 3, 3, 3, 2, 5,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 6, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 6])
soft_pseudo_label
tensor([[2.9847e-03, 2.7760e-03, 9.7656e-01,  ..., 2.8166e-03, 3.2107e-03,
         2.9313e-03],
        [3.6627e-03, 4.3602e-03, 9.4246e-01,  ..., 3.9333e-03, 4.1972e-03,
         3.8761e-03],
        [3.7199e-03, 3.5958e-03, 9.6988e-01,  ..., 3.2256e-03, 3.4420e-03,
         3.4169e-03],
        ...,
        [2.1463e-03, 2.1653e-03, 9.8071e-01,  ..., 2.2080e-03, 2.0742e-03,
         2.0153e-03],
        [3.5891e-03, 2.9577e-03, 2.7577e-06,  ..., 3.8398e-03, 3.2715e-03,
         3.5430e-03],
        [2.2041e-03, 2.1808e-03, 3.8301e-05,  ..., 2.3996e-03, 2.3683e-03,
         2.2611e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2,
        3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2,
        2, 2, 2, 2, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3]
original label
tensor([2, 2, 2, 3, 9, 3, 2, 3, 3, 9, 3, 0, 2, 2, 1, 2, 2, 2, 3, 2, 6, 3, 2, 2,
        3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 7, 2, 2, 2, 3, 2, 2, 2, 3, 2,
        2, 2, 2, 2, 6, 2, 3, 8])
soft_pseudo_label
tensor([[2.4017e-03, 2.1655e-03, 9.8224e-01,  ..., 2.2595e-03, 2.0856e-03,
         2.0167e-03],
        [4.5677e-03, 4.2826e-03, 1.2459e-02,  ..., 4.2826e-03, 4.0845e-03,
         4.5321e-03],
        [1.3793e-03, 1.6611e-03, 9.8775e-01,  ..., 1.5646e-03, 1.3834e-03,
         1.6058e-03],
        ...,
        [2.2894e-03, 2.3893e-03, 3.7263e-04,  ..., 2.3975e-03, 2.3823e-03,
         2.3277e-03],
        [1.5004e-03, 1.6144e-03, 3.7762e-02,  ..., 1.6223e-03, 1.7687e-03,
         1.5693e-03],
        [1.5055e-03, 1.4328e-03, 9.8866e-01,  ..., 1.2760e-03, 1.4268e-03,
         1.3572e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3,
        3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3,
        3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2,
        3, 2, 3, 2, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2]
original label
tensor([2, 1, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 2, 5, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 6, 2, 3, 6, 2, 3, 3, 2, 3, 3, 3, 2, 2, 6, 3, 3, 3, 3, 3,
        3, 9, 3, 2, 2, 3, 3, 5, 3, 2, 3, 3, 3, 5, 3, 1, 2, 3, 3, 3, 0, 2, 3, 3,
        3, 2, 3, 3, 3, 6, 9, 0, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 8, 3, 3, 2, 3,
        3, 2, 2, 5, 2, 2, 4, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3,
        3, 2, 2, 2, 3, 3, 3, 2])
soft_pseudo_label
tensor([[9.7679e-04, 1.0413e-03, 1.4563e-03,  ..., 1.1320e-03, 1.0702e-03,
         1.0603e-03],
        [2.1703e-03, 2.3262e-03, 3.4557e-01,  ..., 2.0954e-03, 2.1938e-03,
         2.2436e-03],
        [3.4112e-03, 3.6508e-03, 9.6637e-01,  ..., 4.0400e-03, 3.8853e-03,
         3.6740e-03],
        ...,
        [1.8397e-03, 2.0057e-03, 4.2117e-02,  ..., 2.2083e-03, 1.9737e-03,
         1.9009e-03],
        [2.5083e-03, 2.2441e-03, 9.7984e-01,  ..., 2.4520e-03, 2.4058e-03,
         2.3598e-03],
        [2.0929e-03, 2.0675e-03, 9.7895e-01,  ..., 2.2685e-03, 2.1227e-03,
         2.0959e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2,
        2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 0, 3, 3, 8, 3, 3, 3, 2, 3, 2, 3, 7, 2,
        2, 3, 3, 2, 2, 3, 3, 9, 3, 3, 3, 2, 3, 2, 2, 9, 8, 2, 2, 3, 3, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 5, 2, 2, 3, 2, 2, 1, 3, 3, 4, 3, 2, 3, 2, 6, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 4, 2, 3, 2, 2, 2, 3, 3,
        3, 9, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2,
        3, 3, 2, 1, 2, 3, 2, 2])
soft_pseudo_label
tensor([[2.2039e-03, 2.1613e-03, 9.7746e-01,  ..., 2.5231e-03, 2.6339e-03,
         2.7214e-03],
        [2.8991e-03, 2.9831e-03, 1.0804e-03,  ..., 2.8126e-03, 2.8409e-03,
         2.9584e-03],
        [3.7826e-03, 4.1382e-03, 9.6450e-03,  ..., 4.1931e-03, 3.9583e-03,
         3.9065e-03],
        ...,
        [1.1269e-03, 1.3784e-03, 9.8910e-01,  ..., 1.4937e-03, 1.4495e-03,
         1.4040e-03],
        [2.0471e-03, 1.8110e-03, 9.8458e-01,  ..., 1.8306e-03, 1.8243e-03,
         1.8013e-03],
        [1.0544e-03, 9.9727e-04, 1.9633e-04,  ..., 1.0580e-03, 9.7226e-04,
         1.0454e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2,
        2, 3, 3, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3]
original label
tensor([2, 3, 2, 3, 3, 2, 3, 5, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3,
        3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 2, 7, 2, 3, 3, 2, 2, 9, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 7, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 0, 2, 3, 3, 2, 3, 6, 3, 2, 2,
        2, 3, 3, 3, 2, 2, 2, 3, 2, 0, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 9, 2, 2,
        2, 3, 3, 2, 8, 2, 2, 3])
soft_pseudo_label
tensor([[1.5435e-05, 1.5300e-05, 9.6565e-01,  ..., 1.5601e-05, 1.4261e-05,
         1.5151e-05],
        [7.7298e-03, 7.8669e-03, 2.6852e-01,  ..., 7.9324e-03, 8.1922e-03,
         7.7260e-03],
        [6.8679e-04, 7.2469e-04, 4.1384e-02,  ..., 7.0034e-04, 7.2186e-04,
         7.0859e-04],
        ...,
        [1.7033e-03, 1.7759e-03, 7.0330e-04,  ..., 1.6336e-03, 1.6999e-03,
         1.9183e-03],
        [3.0429e-05, 3.4548e-05, 1.4006e-02,  ..., 3.0311e-05, 2.8419e-05,
         2.7951e-05],
        [1.0192e-03, 1.0209e-03, 9.9189e-01,  ..., 9.9218e-04, 9.6541e-04,
         8.8138e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3,
        2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3,
        3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3,
        2, 3, 3, 2, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2]
original label
tensor([3, 2, 3, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 8, 3, 3, 2, 0,
        3, 2, 2, 3, 2, 5, 2, 0, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 7, 2, 3, 2, 2, 3,
        2, 3, 4, 2, 2, 3, 2, 2, 0, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 6, 3, 3, 8,
        3, 2, 3, 2, 3, 3, 2, 3, 2, 4, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3,
        2, 0, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[2.7111e-03, 2.3195e-03, 7.5432e-04,  ..., 2.5971e-03, 2.3269e-03,
         2.3235e-03],
        [3.5543e-05, 3.4957e-05, 3.5271e-04,  ..., 3.6208e-05, 3.5473e-05,
         3.3520e-05],
        [3.2229e-03, 3.2864e-03, 9.0150e-01,  ..., 3.0603e-03, 3.0888e-03,
         3.1822e-03],
        ...,
        [1.3527e-03, 1.5260e-03, 9.8814e-01,  ..., 1.3059e-03, 1.3271e-03,
         1.3040e-03],
        [2.1163e-03, 1.9169e-03, 3.3762e-06,  ..., 2.0904e-03, 1.9568e-03,
         2.2331e-03],
        [1.9729e-03, 1.8003e-03, 5.0644e-04,  ..., 2.1176e-03, 1.8858e-03,
         1.7959e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2,
        3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2,
        3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2,
        3, 3, 3, 2, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3]
original label
tensor([3, 3, 2, 8, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 3, 2, 3, 2, 2, 9, 2, 3, 8, 2, 2, 9, 2, 3, 3, 3, 3, 4, 2, 5, 2, 2,
        3, 2, 3, 3, 2, 3, 2, 4, 3, 2, 2, 4, 9, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 4, 3, 3, 3, 3, 2, 2, 2,
        3, 3, 7, 3, 2, 2, 2, 2, 0, 2, 2, 3, 2, 3, 0, 8, 2, 3, 3, 2, 3, 3, 3, 2,
        3, 3, 3, 2, 3, 2, 3, 3])
soft_pseudo_label
tensor([[1.3446e-03, 1.3882e-03, 9.8836e-01,  ..., 1.2417e-03, 1.3896e-03,
         1.3695e-03],
        [3.8386e-05, 4.8194e-05, 4.7593e-04,  ..., 4.1790e-05, 4.8762e-05,
         4.3582e-05],
        [6.0614e-04, 5.8214e-04, 9.9530e-01,  ..., 6.5180e-04, 6.1306e-04,
         5.7775e-04],
        ...,
        [2.1417e-03, 2.2660e-03, 6.1532e-05,  ..., 2.4642e-03, 2.0466e-03,
         2.3695e-03],
        [9.6850e-04, 9.6732e-04, 1.1530e-04,  ..., 8.9179e-04, 8.9966e-04,
         9.1338e-04],
        [2.9957e-03, 2.8501e-03, 9.6967e-01,  ..., 2.7463e-03, 3.0443e-03,
         2.8585e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2,
        3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2,
        3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2]
original label
tensor([2, 3, 2, 3, 2, 8, 2, 2, 2, 2, 6, 2, 3, 2, 9, 5, 3, 2, 3, 3, 2, 3, 5, 2,
        3, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 9, 3, 3, 2,
        7, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 4, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 8, 3, 2,
        8, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 7, 2, 3, 3,
        5, 2, 3, 4, 3, 3, 3, 2])
soft_pseudo_label
tensor([[2.1911e-03, 2.0796e-03, 9.8180e-01,  ..., 2.1943e-03, 2.0553e-03,
         2.2612e-03],
        [1.3130e-03, 1.3142e-03, 1.9225e-03,  ..., 1.3130e-03, 1.3310e-03,
         1.4619e-03],
        [7.4429e-05, 7.8077e-05, 2.7264e-01,  ..., 6.5108e-05, 7.6343e-05,
         7.2989e-05],
        ...,
        [1.8733e-03, 1.9493e-03, 9.8566e-01,  ..., 1.5598e-03, 1.6775e-03,
         1.6133e-03],
        [1.8298e-03, 1.8523e-03, 2.8018e-03,  ..., 1.9834e-03, 1.8316e-03,
         1.8769e-03],
        [3.4176e-03, 3.4578e-03, 1.0554e-01,  ..., 3.5226e-03, 3.4410e-03,
         3.5089e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3,
        2, 2, 3, 2, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3]
original label
tensor([2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 6, 2, 3, 3, 3, 2, 2, 2, 2, 6, 3, 3, 2, 8, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 7, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3,
        2, 2, 3, 2, 2, 2, 7, 3])
soft_pseudo_label
tensor([[8.2104e-03, 8.8171e-03, 6.7458e-01,  ..., 9.2855e-03, 9.3355e-03,
         9.4087e-03],
        [2.5384e-03, 2.2794e-03, 5.3005e-05,  ..., 2.1970e-03, 2.1417e-03,
         2.3039e-03],
        [9.6862e-04, 9.2607e-04, 9.6527e-01,  ..., 9.5547e-04, 8.9933e-04,
         9.6202e-04],
        ...,
        [5.6559e-04, 5.3184e-04, 4.1601e-01,  ..., 5.1047e-04, 5.0501e-04,
         4.9187e-04],
        [3.6319e-03, 3.1321e-03, 1.0838e-04,  ..., 3.2457e-03, 2.8403e-03,
         3.3653e-03],
        [1.6482e-03, 1.6664e-03, 1.7697e-04,  ..., 1.9953e-03, 1.7756e-03,
         1.9340e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2,
        2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2,
        3, 3, 2, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3]
original label
tensor([6, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 8, 6, 3, 3, 2, 2,
        3, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 1, 3, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 7, 2, 6, 0, 3, 2, 3, 2, 2, 3, 3, 6, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 6, 3, 2, 3, 4, 2, 3, 2, 2, 3, 3, 2, 3, 4,
        2, 3, 3, 7, 3, 3, 2, 3, 3, 1, 2, 2, 3, 2, 2, 2, 2, 2, 5, 2, 2, 2, 3, 2,
        3, 2, 3, 2, 3, 3, 3, 3])
soft_pseudo_label
tensor([[2.5878e-03, 2.4203e-03, 9.2718e-01,  ..., 2.6961e-03, 2.7253e-03,
         2.8022e-03],
        [1.9357e-03, 2.0028e-03, 9.8478e-01,  ..., 1.8629e-03, 1.9374e-03,
         1.8282e-03],
        [4.5436e-04, 4.7943e-04, 9.9334e-01,  ..., 5.1010e-04, 4.5614e-04,
         4.7039e-04],
        ...,
        [2.6808e-03, 2.7124e-03, 4.4400e-02,  ..., 2.8246e-03, 2.6599e-03,
         2.8218e-03],
        [6.0612e-04, 5.8604e-04, 4.6496e-04,  ..., 5.8404e-04, 5.5675e-04,
         5.6359e-04],
        [7.2166e-05, 7.9881e-05, 4.1751e-01,  ..., 6.3006e-05, 7.6745e-05,
         7.1256e-05]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2,
        3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3]
original label
tensor([8, 9, 2, 2, 3, 2, 6, 3, 2, 8, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3,
        3, 2, 2, 2, 2, 3, 3, 2, 4, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 1, 8, 2,
        3, 3, 2, 3, 2, 3, 9, 3, 3, 2, 2, 2, 7, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2,
        3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 8, 3, 2, 3, 2, 3, 3, 2,
        3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2,
        2, 3, 9, 3, 1, 3, 3, 2])
soft_pseudo_label
tensor([[1.6108e-03, 1.6532e-03, 9.8655e-01,  ..., 1.6672e-03, 1.6207e-03,
         1.7289e-03],
        [1.3771e-03, 1.3940e-03, 9.8748e-01,  ..., 1.3237e-03, 1.3597e-03,
         1.4992e-03],
        [2.3931e-03, 2.5901e-03, 1.4594e-06,  ..., 2.4643e-03, 2.5209e-03,
         2.3905e-03],
        ...,
        [5.2711e-03, 5.2531e-03, 8.7741e-01,  ..., 5.1944e-03, 5.1014e-03,
         5.1515e-03],
        [1.5180e-03, 1.3348e-03, 1.5425e-04,  ..., 1.4460e-03, 1.2640e-03,
         1.4397e-03],
        [2.2346e-04, 2.3316e-04, 9.9802e-01,  ..., 2.3010e-04, 2.6504e-04,
         2.5014e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3,
        3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2,
        2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 2, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2]
original label
tensor([2, 7, 3, 3, 3, 2, 2, 2, 3, 2, 3, 5, 3, 0, 2, 2, 0, 9, 3, 9, 2, 3, 2, 3,
        3, 3, 8, 3, 3, 2, 8, 3, 2, 3, 4, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2,
        7, 5, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 5, 2, 2, 4, 2, 4, 2,
        2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 0, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        2, 3, 2, 1, 3, 3, 3, 3, 0, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 7, 4, 1,
        3, 6, 3, 3, 2, 2, 3, 2])
soft_pseudo_label
tensor([[1.8820e-03, 1.9107e-03, 9.7967e-01,  ..., 1.8011e-03, 1.9532e-03,
         1.7203e-03],
        [3.3150e-03, 3.2446e-03, 5.8941e-01,  ..., 3.3623e-03, 3.6676e-03,
         3.3378e-03],
        [1.8526e-03, 2.0911e-03, 9.8194e-01,  ..., 2.1116e-03, 1.9673e-03,
         2.1344e-03],
        ...,
        [1.3849e-04, 1.3999e-04, 7.9409e-02,  ..., 1.3035e-04, 1.3228e-04,
         1.3903e-04],
        [3.9352e-03, 4.0404e-03, 1.2284e-01,  ..., 4.2446e-03, 4.1080e-03,
         4.0266e-03],
        [4.9396e-03, 4.9083e-03, 3.2436e-01,  ..., 4.9492e-03, 4.6881e-03,
         4.9637e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2,
        2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3,
        3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3,
        3, 3, 3, 2, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3]
original label
tensor([2, 3, 2, 2, 2, 3, 2, 3, 3, 7, 3, 3, 3, 3, 3, 2, 2, 2, 9, 2, 3, 2, 7, 2,
        2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 9, 3, 2, 2, 2,
        3, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 1, 3, 2, 2, 2, 3,
        2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 2, 2, 2, 6, 2, 2, 3, 3, 3, 0,
        2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 0, 2, 3, 2, 2, 7, 5, 3,
        3, 3, 3, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[6.9154e-05, 6.1929e-05, 6.3727e-03,  ..., 6.3583e-05, 6.6440e-05,
         5.5188e-05],
        [2.0686e-03, 1.9566e-03, 9.8382e-01,  ..., 1.9894e-03, 1.9605e-03,
         1.9207e-03],
        [3.4154e-04, 4.1897e-04, 9.9696e-01,  ..., 2.9804e-04, 3.9735e-04,
         4.0937e-04],
        ...,
        [1.3561e-03, 1.3400e-03, 1.3493e-04,  ..., 1.2109e-03, 1.1052e-03,
         1.3443e-03],
        [6.6291e-04, 6.9541e-04, 3.8535e-04,  ..., 6.9134e-04, 6.5009e-04,
         6.7105e-04],
        [1.3512e-03, 1.3102e-03, 5.3735e-02,  ..., 1.2988e-03, 1.2861e-03,
         1.2687e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3,
        3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 9, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2,
        2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 9, 3, 3, 0, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 5, 2, 2, 3, 3, 3, 3, 3, 3, 2, 1, 2, 3, 8, 3, 6, 2, 2, 2, 3,
        2, 3, 2, 2, 2, 2, 4, 3, 6, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 5, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[5.1260e-04, 4.6947e-04, 9.9238e-01,  ..., 5.0466e-04, 4.8937e-04,
         4.6107e-04],
        [1.8942e-03, 2.0672e-03, 2.5443e-02,  ..., 2.1727e-03, 1.9860e-03,
         2.0451e-03],
        [2.6739e-03, 2.9360e-03, 2.4432e-04,  ..., 2.6114e-03, 2.8287e-03,
         2.8408e-03],
        ...,
        [3.4143e-03, 3.4723e-03, 9.7127e-01,  ..., 3.4919e-03, 3.5287e-03,
         3.2747e-03],
        [4.3963e-03, 4.8002e-03, 5.5048e-03,  ..., 5.1814e-03, 4.5204e-03,
         4.7943e-03],
        [4.1032e-03, 4.1236e-03, 4.4752e-05,  ..., 4.0131e-03, 4.0897e-03,
         4.1675e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3,
        3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2,
        2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3]
original label
tensor([2, 2, 3, 3, 2, 2, 7, 3, 2, 2, 3, 2, 2, 2, 3, 2, 0, 3, 1, 3, 3, 2, 3, 3,
        3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 9, 2, 3, 3, 2,
        2, 5, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 5, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 2, 3, 4, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        9, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 4, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 3, 2, 2, 3, 0])
soft_pseudo_label
tensor([[3.3873e-05, 3.4507e-05, 3.7037e-02,  ..., 3.5016e-05, 3.2959e-05,
         3.3939e-05],
        [1.5196e-03, 1.4422e-03, 2.2502e-03,  ..., 1.5019e-03, 1.5473e-03,
         1.4338e-03],
        [1.9125e-03, 1.9583e-03, 9.8489e-01,  ..., 1.9612e-03, 1.8755e-03,
         1.8550e-03],
        ...,
        [1.7348e-03, 1.9287e-03, 9.8528e-01,  ..., 1.7450e-03, 1.9467e-03,
         1.7175e-03],
        [8.8359e-04, 9.1879e-04, 1.4747e-03,  ..., 9.3234e-04, 8.9794e-04,
         8.1003e-04],
        [3.2883e-04, 3.0785e-04, 9.9785e-01,  ..., 2.5510e-04, 2.9382e-04,
         2.2975e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2,
        2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3,
        3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2]
original label
tensor([3, 3, 2, 2, 2, 3, 3, 3, 6, 2, 3, 3, 3, 3, 2, 2, 6, 3, 3, 2, 0, 2, 3, 5,
        3, 3, 1, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3,
        9, 3, 2, 2, 2, 3, 8, 2, 2, 2, 4, 2, 2, 3, 3, 2, 3, 2, 3, 3, 0, 2, 2, 2,
        2, 2, 2, 3, 2, 5, 9, 3, 3, 2, 6, 6, 2, 3, 2, 3, 2, 2, 3, 7, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,
        2, 2, 2, 7, 3, 2, 3, 2])
soft_pseudo_label
tensor([[1.8059e-03, 1.7426e-03, 2.4812e-04,  ..., 1.7516e-03, 1.8787e-03,
         1.8664e-03],
        [1.9182e-03, 1.9685e-03, 9.9065e-05,  ..., 1.7419e-03, 1.7376e-03,
         1.9159e-03],
        [1.9007e-03, 1.9553e-03, 2.0557e-01,  ..., 2.0743e-03, 1.9533e-03,
         2.0272e-03],
        ...,
        [1.5897e-03, 1.5036e-03, 9.8653e-01,  ..., 1.7502e-03, 1.6143e-03,
         1.8653e-03],
        [2.0925e-03, 2.0894e-03, 9.8261e-01,  ..., 2.2289e-03, 2.1600e-03,
         1.9749e-03],
        [1.1203e-03, 1.3729e-03, 9.9051e-01,  ..., 1.0713e-03, 1.1310e-03,
         1.0813e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2,
        2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2,
        3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3,
        2, 2, 3, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2]
original label
tensor([3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 2, 3, 0, 7, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 8,
        2, 3, 2, 6, 2, 5, 9, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 4, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 6, 3, 2,
        2, 2, 3, 2, 2, 2, 2, 2])
soft_pseudo_label
tensor([[3.3140e-03, 3.5938e-03, 8.9323e-01,  ..., 3.4343e-03, 3.5468e-03,
         3.4343e-03],
        [2.1326e-03, 2.0310e-03, 9.1674e-01,  ..., 2.0711e-03, 2.0201e-03,
         2.1662e-03],
        [3.8714e-03, 3.9593e-03, 1.7056e-02,  ..., 4.0060e-03, 3.8002e-03,
         3.8999e-03],
        ...,
        [1.6665e-03, 1.6779e-03, 1.0658e-01,  ..., 1.6993e-03, 1.5995e-03,
         1.5948e-03],
        [3.5163e-03, 2.8535e-03, 4.2217e-04,  ..., 3.2580e-03, 3.0591e-03,
         3.2177e-03],
        [3.1269e-03, 3.2995e-03, 1.8839e-02,  ..., 3.3646e-03, 3.1392e-03,
         3.3695e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2,
        2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3,
        2, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3]
original label
tensor([2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 9, 0, 2, 2, 2, 4, 3, 3, 1, 3, 2, 4, 2,
        2, 9, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 1, 2, 2, 9, 3, 2, 3,
        2, 2, 3, 0, 1, 2, 9, 3, 5, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 5, 2, 6, 3, 3, 3, 9, 2, 4, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 8, 2, 3, 3, 3, 3, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 3])
soft_pseudo_label
tensor([[1.1725e-03, 1.1031e-03, 9.8204e-01,  ..., 1.1227e-03, 1.3042e-03,
         1.1276e-03],
        [1.6541e-03, 1.6000e-03, 9.8619e-01,  ..., 1.7419e-03, 1.6488e-03,
         1.5969e-03],
        [1.2965e-03, 1.2408e-03, 9.8629e-01,  ..., 1.2658e-03, 1.2826e-03,
         1.3247e-03],
        ...,
        [2.3299e-03, 2.5292e-03, 9.8533e-06,  ..., 2.3202e-03, 2.2898e-03,
         2.6377e-03],
        [1.9757e-03, 1.9550e-03, 6.5726e-04,  ..., 1.9685e-03, 1.7199e-03,
         2.0146e-03],
        [2.0531e-03, 1.9972e-03, 2.4833e-05,  ..., 2.0854e-03, 1.9519e-03,
         2.4856e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3,
        3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3,
        3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 2, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3]
original label
tensor([2, 2, 2, 2, 2, 2, 6, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        6, 2, 3, 5, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 1, 3, 5, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 1,
        3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3,
        0, 3, 2, 2, 2, 3, 2, 2, 3, 8, 3, 7, 2, 2, 3, 3, 2, 3, 8, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 3, 3])
soft_pseudo_label
tensor([[3.5586e-03, 3.4915e-03, 2.4530e-03,  ..., 3.7992e-03, 3.8076e-03,
         3.6877e-03],
        [2.3501e-03, 2.7361e-03, 9.7913e-01,  ..., 2.4961e-03, 2.7596e-03,
         2.3923e-03],
        [1.9822e-03, 1.9151e-03, 9.8434e-01,  ..., 1.9184e-03, 1.8937e-03,
         1.8239e-03],
        ...,
        [9.8263e-06, 1.0001e-05, 9.9983e-01,  ..., 9.5426e-06, 1.0501e-05,
         8.6802e-06],
        [2.1141e-03, 2.3060e-03, 9.7212e-01,  ..., 2.3378e-03, 2.0390e-03,
         2.3207e-03],
        [1.5448e-03, 1.6373e-03, 9.8552e-01,  ..., 1.6445e-03, 1.9058e-03,
         1.9567e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3,
        3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2,
        3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2,
        2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2]
original label
tensor([3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 0, 2, 3, 2, 3, 3, 2,
        3, 2, 5, 3, 2, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3,
        3, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 5, 3, 2, 3, 2,
        3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 3, 2, 0, 8, 3, 8, 2, 3, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 0, 3, 3, 2, 2, 3, 3, 3, 3,
        7, 2, 2, 2, 2, 2, 9, 2])
soft_pseudo_label
tensor([[2.1010e-03, 2.0664e-03, 9.8228e-01,  ..., 1.9498e-03, 2.1725e-03,
         2.3137e-03],
        [7.7600e-04, 6.9441e-04, 9.9393e-01,  ..., 7.8861e-04, 6.9679e-04,
         7.7657e-04],
        [1.2728e-04, 1.3326e-04, 1.9417e-01,  ..., 1.1829e-04, 1.2691e-04,
         1.3496e-04],
        ...,
        [5.2855e-03, 5.5771e-03, 8.1489e-01,  ..., 5.6594e-03, 5.4666e-03,
         5.8476e-03],
        [1.4013e-03, 1.3870e-03, 9.8855e-01,  ..., 1.4935e-03, 1.3569e-03,
         1.3819e-03],
        [4.9841e-03, 4.6330e-03, 1.9677e-05,  ..., 4.6956e-03, 4.6956e-03,
         5.1111e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2,
        2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3]
original label
tensor([2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 4, 7, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3,
        2, 5, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 4, 3, 3, 2, 2, 3,
        2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 1, 3, 3, 2, 3, 2, 3, 9,
        2, 2, 6, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 6, 2, 3, 3, 2, 2, 3, 2, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3])
soft_pseudo_label
tensor([[0.0044, 0.0044, 0.8028,  ..., 0.0044, 0.0044, 0.0044],
        [0.0071, 0.0072, 0.8186,  ..., 0.0070, 0.0073, 0.0069],
        [0.0015, 0.0014, 0.0036,  ..., 0.0015, 0.0014, 0.0013],
        ...,
        [0.0014, 0.0015, 0.9828,  ..., 0.0015, 0.0015, 0.0014],
        [0.0017, 0.0015, 0.9873,  ..., 0.0015, 0.0017, 0.0016],
        [0.0021, 0.0021, 0.9812,  ..., 0.0023, 0.0022, 0.0022]],
       device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2,
        3, 2, 3, 3, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2]
original label
tensor([2, 2, 2, 3, 2, 6, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 8, 2, 2, 2, 7, 0, 3, 3,
        3, 1, 5, 2, 1, 5, 2, 2, 3, 3, 3, 3, 5, 2, 3, 3, 3, 2, 1, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 2, 8, 2, 3, 9, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2,
        2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 4, 2, 3, 2, 3, 8, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[2.4303e-03, 2.3884e-03, 1.8635e-05,  ..., 2.1738e-03, 2.5592e-03,
         2.6058e-03],
        [7.2342e-03, 7.1779e-03, 1.3613e-02,  ..., 7.7764e-03, 7.2661e-03,
         8.0586e-03],
        [2.1721e-03, 2.1669e-03, 9.8094e-01,  ..., 2.2993e-03, 2.4536e-03,
         2.4055e-03],
        ...,
        [2.0984e-03, 2.0872e-03, 9.7962e-01,  ..., 1.8985e-03, 2.0092e-03,
         2.1231e-03],
        [1.7361e-03, 2.1653e-03, 9.8384e-01,  ..., 1.8960e-03, 2.2587e-03,
         1.8512e-03],
        [6.3466e-03, 6.5353e-03, 5.4090e-01,  ..., 6.4371e-03, 6.5481e-03,
         6.7395e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2,
        3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3,
        3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2,
        2, 2, 3, 3, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2]
original label
tensor([3, 3, 2, 3, 5, 2, 3, 9, 3, 2, 3, 3, 2, 7, 2, 2, 2, 2, 0, 2, 3, 3, 3, 9,
        2, 3, 2, 2, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 8, 2, 2, 0, 2, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 3, 2, 9, 2, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 2, 7, 7,
        3, 2, 5, 3, 3, 2, 3, 3, 3, 3, 3, 2, 6, 9, 3, 3, 2, 3, 3, 2, 0, 2, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 9, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2,
        2, 2, 3, 3, 9, 2, 2, 3])
soft_pseudo_label
tensor([[2.4844e-03, 2.3453e-03, 2.8349e-02,  ..., 2.6215e-03, 2.4506e-03,
         2.4807e-03],
        [2.4684e-03, 2.6664e-03, 9.7087e-01,  ..., 2.4254e-03, 2.6547e-03,
         2.4207e-03],
        [1.1908e-02, 1.0986e-02, 1.3508e-01,  ..., 1.0731e-02, 1.1479e-02,
         1.1575e-02],
        ...,
        [1.0906e-03, 1.1285e-03, 9.9007e-01,  ..., 1.2190e-03, 1.2504e-03,
         1.2434e-03],
        [2.1002e-03, 1.9608e-03, 9.8399e-01,  ..., 2.0515e-03, 1.9606e-03,
         2.1318e-03],
        [1.6352e-04, 1.8340e-04, 3.9505e-04,  ..., 1.7407e-04, 1.8296e-04,
         1.7950e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3,
        3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3,
        3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3]
original label
tensor([3, 2, 2, 2, 2, 2, 2, 3, 3, 0, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3,
        3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 8, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 2, 3, 8, 1, 3, 3, 2, 3, 3, 3, 2, 2, 9, 2, 3, 2, 2, 2, 2, 3, 6, 6,
        3, 2, 3, 2, 2, 9, 3, 3, 3, 3, 2, 2, 3, 8, 2, 2, 2, 3, 3, 3, 4, 3, 2, 3,
        3, 3, 2, 2, 2, 3, 3, 5, 2, 2, 2, 3, 3, 4, 3, 7, 3, 3, 4, 2, 3, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3])
[INFO] main.py:340 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:348 > [2-3] Start to train under online
[INFO] main.py:363 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42
[INFO] rainbow_memory.py:119 > Streamed samples: 800
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 9984
[INFO] rainbow_memory.py:127 > Train samples: 11284
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 1/1 | train_loss 1.8667 | train_acc 0.5096 | test_loss 0.9076 | test_acc 0.9265 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
bird          118
automobile    108
dog            73
deer           69
airplane       26
cat            26
ship           25
frog           22
horse          17
truck          16
Name: klass, dtype: int64
[INFO] main.py:379 > Train over memory
batch_size : 64 stream_batch_size : 22 memory_batch_size : 21
[INFO] rainbow_memory.py:119 > Streamed samples: 0
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 0
[INFO] rainbow_memory.py:127 > Train samples: 500
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 1/256 | train_loss 1.8314 | train_acc 0.3740 | test_loss 0.9818 | test_acc 0.8595 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 2/256 | train_loss 1.5218 | train_acc 0.5160 | test_loss 1.1913 | test_acc 0.6050 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 3/256 | train_loss 1.3346 | train_acc 0.5660 | test_loss 0.5324 | test_acc 0.8035 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 4/256 | train_loss 1.1190 | train_acc 0.6360 | test_loss 0.9111 | test_acc 0.6825 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 5/256 | train_loss 1.3428 | train_acc 0.5680 | test_loss 0.6990 | test_acc 0.7585 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 6/256 | train_loss 1.2325 | train_acc 0.5840 | test_loss 1.0670 | test_acc 0.6890 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 7/256 | train_loss 1.1080 | train_acc 0.6200 | test_loss 0.7196 | test_acc 0.7360 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 8/256 | train_loss 1.0551 | train_acc 0.6440 | test_loss 0.6814 | test_acc 0.7430 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 9/256 | train_loss 1.2271 | train_acc 0.5940 | test_loss 1.0666 | test_acc 0.6405 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 10/256 | train_loss 1.3003 | train_acc 0.5580 | test_loss 0.8099 | test_acc 0.6885 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 11/256 | train_loss 1.2883 | train_acc 0.6100 | test_loss 0.8007 | test_acc 0.7085 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 12/256 | train_loss 1.1810 | train_acc 0.6180 | test_loss 0.5644 | test_acc 0.7895 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 13/256 | train_loss 1.1513 | train_acc 0.6620 | test_loss 0.6802 | test_acc 0.7555 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 14/256 | train_loss 1.1761 | train_acc 0.6600 | test_loss 0.6841 | test_acc 0.7600 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 15/256 | train_loss 1.0082 | train_acc 0.6480 | test_loss 0.5704 | test_acc 0.8035 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 16/256 | train_loss 0.9002 | train_acc 0.7180 | test_loss 0.5466 | test_acc 0.8125 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 17/256 | train_loss 1.2089 | train_acc 0.6080 | test_loss 0.9453 | test_acc 0.6795 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 18/256 | train_loss 1.3074 | train_acc 0.6040 | test_loss 0.7251 | test_acc 0.7285 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 19/256 | train_loss 1.1157 | train_acc 0.6560 | test_loss 0.8838 | test_acc 0.6985 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 20/256 | train_loss 1.1991 | train_acc 0.6020 | test_loss 1.1702 | test_acc 0.5810 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 21/256 | train_loss 1.1082 | train_acc 0.6420 | test_loss 0.7895 | test_acc 0.7060 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 22/256 | train_loss 1.1179 | train_acc 0.6560 | test_loss 0.5181 | test_acc 0.8255 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 23/256 | train_loss 1.0470 | train_acc 0.7100 | test_loss 0.7968 | test_acc 0.6880 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 24/256 | train_loss 1.0237 | train_acc 0.6900 | test_loss 1.1535 | test_acc 0.5990 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 25/256 | train_loss 0.8653 | train_acc 0.6880 | test_loss 0.9399 | test_acc 0.6710 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 26/256 | train_loss 1.1272 | train_acc 0.6120 | test_loss 0.6984 | test_acc 0.7540 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 27/256 | train_loss 1.2123 | train_acc 0.6080 | test_loss 0.5966 | test_acc 0.8005 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 28/256 | train_loss 0.9304 | train_acc 0.7540 | test_loss 0.6669 | test_acc 0.7695 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 29/256 | train_loss 0.8749 | train_acc 0.7320 | test_loss 0.6449 | test_acc 0.7660 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 30/256 | train_loss 0.9300 | train_acc 0.7240 | test_loss 0.6685 | test_acc 0.7570 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 31/256 | train_loss 0.9250 | train_acc 0.7200 | test_loss 0.6599 | test_acc 0.7615 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 32/256 | train_loss 0.8326 | train_acc 0.7520 | test_loss 0.6216 | test_acc 0.7700 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 33/256 | train_loss 0.9364 | train_acc 0.7200 | test_loss 1.1815 | test_acc 0.5845 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 34/256 | train_loss 1.3010 | train_acc 0.6000 | test_loss 0.6664 | test_acc 0.7760 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 35/256 | train_loss 1.0582 | train_acc 0.6640 | test_loss 0.6642 | test_acc 0.7485 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 36/256 | train_loss 1.1971 | train_acc 0.6380 | test_loss 1.8197 | test_acc 0.4750 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 37/256 | train_loss 1.2075 | train_acc 0.6120 | test_loss 0.9794 | test_acc 0.6375 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 38/256 | train_loss 1.0474 | train_acc 0.6800 | test_loss 0.7916 | test_acc 0.7120 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 39/256 | train_loss 1.1244 | train_acc 0.6560 | test_loss 0.8555 | test_acc 0.7205 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 40/256 | train_loss 1.1813 | train_acc 0.6220 | test_loss 0.7441 | test_acc 0.7360 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 41/256 | train_loss 0.9965 | train_acc 0.7100 | test_loss 0.8820 | test_acc 0.6775 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 42/256 | train_loss 0.9577 | train_acc 0.7260 | test_loss 0.6031 | test_acc 0.7820 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 43/256 | train_loss 0.9408 | train_acc 0.7320 | test_loss 0.7807 | test_acc 0.7365 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 44/256 | train_loss 0.9259 | train_acc 0.6980 | test_loss 0.6816 | test_acc 0.7440 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 45/256 | train_loss 0.9256 | train_acc 0.7220 | test_loss 0.6772 | test_acc 0.7720 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 46/256 | train_loss 0.9549 | train_acc 0.7160 | test_loss 0.7040 | test_acc 0.7465 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 47/256 | train_loss 1.1149 | train_acc 0.6020 | test_loss 0.7283 | test_acc 0.7530 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 48/256 | train_loss 0.9748 | train_acc 0.7100 | test_loss 0.6146 | test_acc 0.7750 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 49/256 | train_loss 1.1116 | train_acc 0.6440 | test_loss 0.7185 | test_acc 0.7570 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 50/256 | train_loss 0.9264 | train_acc 0.7420 | test_loss 0.6934 | test_acc 0.7575 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 51/256 | train_loss 0.9124 | train_acc 0.7100 | test_loss 0.7410 | test_acc 0.7565 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 52/256 | train_loss 0.8051 | train_acc 0.7800 | test_loss 0.4496 | test_acc 0.8355 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 53/256 | train_loss 0.8903 | train_acc 0.7340 | test_loss 0.4724 | test_acc 0.8400 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 54/256 | train_loss 1.0223 | train_acc 0.6740 | test_loss 0.5682 | test_acc 0.8115 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 55/256 | train_loss 0.7502 | train_acc 0.7780 | test_loss 0.6576 | test_acc 0.7740 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 56/256 | train_loss 0.7255 | train_acc 0.7880 | test_loss 0.5844 | test_acc 0.7990 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 57/256 | train_loss 0.8898 | train_acc 0.7620 | test_loss 0.6161 | test_acc 0.7785 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 58/256 | train_loss 0.8821 | train_acc 0.7960 | test_loss 0.5159 | test_acc 0.8170 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 59/256 | train_loss 0.7180 | train_acc 0.8000 | test_loss 0.5021 | test_acc 0.8165 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 60/256 | train_loss 0.7580 | train_acc 0.7960 | test_loss 0.5763 | test_acc 0.7885 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 61/256 | train_loss 0.6947 | train_acc 0.8000 | test_loss 0.5474 | test_acc 0.8000 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 62/256 | train_loss 0.8259 | train_acc 0.7760 | test_loss 0.5686 | test_acc 0.7930 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 63/256 | train_loss 0.9659 | train_acc 0.6760 | test_loss 0.5149 | test_acc 0.8140 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 64/256 | train_loss 0.7220 | train_acc 0.8100 | test_loss 0.5522 | test_acc 0.7965 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 65/256 | train_loss 0.9909 | train_acc 0.6960 | test_loss 0.5775 | test_acc 0.8100 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 66/256 | train_loss 1.1060 | train_acc 0.6720 | test_loss 0.8523 | test_acc 0.6935 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 67/256 | train_loss 1.0086 | train_acc 0.6740 | test_loss 0.9405 | test_acc 0.6860 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 68/256 | train_loss 1.1479 | train_acc 0.6060 | test_loss 0.6096 | test_acc 0.8050 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 69/256 | train_loss 1.0074 | train_acc 0.7060 | test_loss 1.0770 | test_acc 0.6490 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 70/256 | train_loss 0.9198 | train_acc 0.7100 | test_loss 0.5932 | test_acc 0.7990 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 71/256 | train_loss 0.9738 | train_acc 0.7040 | test_loss 1.2396 | test_acc 0.5600 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 72/256 | train_loss 1.1906 | train_acc 0.6540 | test_loss 1.6746 | test_acc 0.4555 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 73/256 | train_loss 1.2042 | train_acc 0.6300 | test_loss 0.8921 | test_acc 0.7060 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 74/256 | train_loss 1.1327 | train_acc 0.6720 | test_loss 0.6274 | test_acc 0.7690 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 75/256 | train_loss 1.0924 | train_acc 0.6420 | test_loss 0.6922 | test_acc 0.7670 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 76/256 | train_loss 1.0213 | train_acc 0.6560 | test_loss 0.5664 | test_acc 0.8080 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 77/256 | train_loss 0.8579 | train_acc 0.7300 | test_loss 1.2642 | test_acc 0.6020 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 78/256 | train_loss 1.0953 | train_acc 0.6200 | test_loss 0.6078 | test_acc 0.7840 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 79/256 | train_loss 0.9825 | train_acc 0.6940 | test_loss 0.8014 | test_acc 0.7190 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 80/256 | train_loss 1.0595 | train_acc 0.6560 | test_loss 0.4164 | test_acc 0.8850 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 81/256 | train_loss 0.7666 | train_acc 0.7940 | test_loss 0.6528 | test_acc 0.7870 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 82/256 | train_loss 0.9219 | train_acc 0.7220 | test_loss 1.0717 | test_acc 0.6505 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 83/256 | train_loss 0.8321 | train_acc 0.7860 | test_loss 0.6923 | test_acc 0.7610 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 84/256 | train_loss 0.9602 | train_acc 0.6720 | test_loss 0.7290 | test_acc 0.7400 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 85/256 | train_loss 1.0366 | train_acc 0.6840 | test_loss 0.4770 | test_acc 0.8400 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 86/256 | train_loss 0.8823 | train_acc 0.7580 | test_loss 0.7025 | test_acc 0.7485 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 87/256 | train_loss 0.9362 | train_acc 0.6600 | test_loss 0.8468 | test_acc 0.6920 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 88/256 | train_loss 0.6889 | train_acc 0.8060 | test_loss 0.5887 | test_acc 0.7925 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 89/256 | train_loss 0.7527 | train_acc 0.7820 | test_loss 0.4085 | test_acc 0.8680 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 90/256 | train_loss 0.7655 | train_acc 0.8020 | test_loss 0.7236 | test_acc 0.7540 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 91/256 | train_loss 0.7847 | train_acc 0.7140 | test_loss 0.7698 | test_acc 0.7320 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 92/256 | train_loss 0.8856 | train_acc 0.7320 | test_loss 0.6657 | test_acc 0.7720 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 93/256 | train_loss 0.9377 | train_acc 0.6940 | test_loss 0.8708 | test_acc 0.6980 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 94/256 | train_loss 0.7864 | train_acc 0.7780 | test_loss 0.6492 | test_acc 0.7715 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 95/256 | train_loss 0.9326 | train_acc 0.7360 | test_loss 0.6830 | test_acc 0.7615 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 96/256 | train_loss 0.6974 | train_acc 0.8300 | test_loss 0.4778 | test_acc 0.8350 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 97/256 | train_loss 0.8051 | train_acc 0.7400 | test_loss 0.5013 | test_acc 0.8290 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 98/256 | train_loss 0.8859 | train_acc 0.7680 | test_loss 0.8712 | test_acc 0.7040 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 99/256 | train_loss 0.8375 | train_acc 0.7500 | test_loss 0.5664 | test_acc 0.8030 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 100/256 | train_loss 1.0488 | train_acc 0.6680 | test_loss 0.5116 | test_acc 0.8235 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 101/256 | train_loss 0.9737 | train_acc 0.7220 | test_loss 0.4906 | test_acc 0.8400 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 102/256 | train_loss 0.8301 | train_acc 0.6960 | test_loss 0.5202 | test_acc 0.8195 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 103/256 | train_loss 0.6094 | train_acc 0.8560 | test_loss 0.4147 | test_acc 0.8600 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 104/256 | train_loss 0.6732 | train_acc 0.8500 | test_loss 0.6859 | test_acc 0.7780 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 105/256 | train_loss 0.5850 | train_acc 0.8160 | test_loss 0.5288 | test_acc 0.8200 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 106/256 | train_loss 0.6535 | train_acc 0.8220 | test_loss 0.6409 | test_acc 0.7840 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 107/256 | train_loss 0.6011 | train_acc 0.8420 | test_loss 0.5021 | test_acc 0.8340 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 108/256 | train_loss 0.7464 | train_acc 0.7880 | test_loss 0.5489 | test_acc 0.8205 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 109/256 | train_loss 0.6292 | train_acc 0.8500 | test_loss 0.4831 | test_acc 0.8360 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 110/256 | train_loss 0.9108 | train_acc 0.7360 | test_loss 0.4781 | test_acc 0.8520 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 111/256 | train_loss 0.6592 | train_acc 0.8000 | test_loss 0.4991 | test_acc 0.8365 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 112/256 | train_loss 0.7199 | train_acc 0.7820 | test_loss 0.5048 | test_acc 0.8350 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 113/256 | train_loss 0.7135 | train_acc 0.7820 | test_loss 0.5405 | test_acc 0.8160 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 114/256 | train_loss 0.8498 | train_acc 0.7480 | test_loss 0.6004 | test_acc 0.8000 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 115/256 | train_loss 0.7867 | train_acc 0.7360 | test_loss 0.4850 | test_acc 0.8395 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 116/256 | train_loss 0.6526 | train_acc 0.8160 | test_loss 0.4950 | test_acc 0.8320 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 117/256 | train_loss 0.6004 | train_acc 0.8380 | test_loss 0.5047 | test_acc 0.8305 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 118/256 | train_loss 0.6311 | train_acc 0.7860 | test_loss 0.4855 | test_acc 0.8305 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 119/256 | train_loss 0.6383 | train_acc 0.7900 | test_loss 0.4910 | test_acc 0.8320 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 120/256 | train_loss 0.5383 | train_acc 0.8280 | test_loss 0.5075 | test_acc 0.8225 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 121/256 | train_loss 0.8002 | train_acc 0.7500 | test_loss 0.4968 | test_acc 0.8310 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 122/256 | train_loss 0.8068 | train_acc 0.7760 | test_loss 0.5038 | test_acc 0.8340 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 123/256 | train_loss 0.7177 | train_acc 0.7960 | test_loss 0.4808 | test_acc 0.8410 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 124/256 | train_loss 0.5424 | train_acc 0.8460 | test_loss 0.4832 | test_acc 0.8350 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 125/256 | train_loss 0.6283 | train_acc 0.8600 | test_loss 0.4984 | test_acc 0.8310 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 126/256 | train_loss 0.5951 | train_acc 0.8520 | test_loss 0.4702 | test_acc 0.8415 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 127/256 | train_loss 0.5645 | train_acc 0.8420 | test_loss 0.4578 | test_acc 0.8440 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 128/256 | train_loss 0.6471 | train_acc 0.7980 | test_loss 0.4730 | test_acc 0.8390 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 129/256 | train_loss 0.8739 | train_acc 0.7180 | test_loss 0.6555 | test_acc 0.7635 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 130/256 | train_loss 0.8161 | train_acc 0.7120 | test_loss 0.6857 | test_acc 0.7805 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 131/256 | train_loss 0.9799 | train_acc 0.7420 | test_loss 0.5039 | test_acc 0.8305 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 132/256 | train_loss 0.7445 | train_acc 0.7840 | test_loss 0.8357 | test_acc 0.7180 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 133/256 | train_loss 1.1591 | train_acc 0.6140 | test_loss 0.4754 | test_acc 0.8555 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 134/256 | train_loss 0.9100 | train_acc 0.7780 | test_loss 1.0894 | test_acc 0.6555 | lr 0.0498
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 135/256 | train_loss 0.7274 | train_acc 0.7880 | test_loss 1.2899 | test_acc 0.6015 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 136/256 | train_loss 0.9709 | train_acc 0.7560 | test_loss 0.5042 | test_acc 0.8205 | lr 0.0496
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 137/256 | train_loss 0.8553 | train_acc 0.7860 | test_loss 0.5361 | test_acc 0.8155 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 138/256 | train_loss 0.8463 | train_acc 0.7460 | test_loss 0.4606 | test_acc 0.8500 | lr 0.0494
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 139/256 | train_loss 0.6679 | train_acc 0.7960 | test_loss 0.6983 | test_acc 0.7630 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 140/256 | train_loss 1.0288 | train_acc 0.6940 | test_loss 0.6869 | test_acc 0.7640 | lr 0.0491
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 141/256 | train_loss 0.9740 | train_acc 0.7380 | test_loss 0.7698 | test_acc 0.7390 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 142/256 | train_loss 0.9027 | train_acc 0.7000 | test_loss 1.0233 | test_acc 0.6510 | lr 0.0488
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 143/256 | train_loss 0.9234 | train_acc 0.7520 | test_loss 0.6004 | test_acc 0.8035 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 144/256 | train_loss 0.7638 | train_acc 0.7600 | test_loss 1.0074 | test_acc 0.6730 | lr 0.0483
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 145/256 | train_loss 0.9908 | train_acc 0.6820 | test_loss 0.4057 | test_acc 0.8680 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 146/256 | train_loss 0.8692 | train_acc 0.7660 | test_loss 0.7939 | test_acc 0.7485 | lr 0.0479
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 147/256 | train_loss 0.9159 | train_acc 0.7160 | test_loss 1.0748 | test_acc 0.6380 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 148/256 | train_loss 0.9123 | train_acc 0.7440 | test_loss 0.6911 | test_acc 0.7505 | lr 0.0474
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 149/256 | train_loss 0.8755 | train_acc 0.7800 | test_loss 0.6562 | test_acc 0.7700 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 150/256 | train_loss 0.6012 | train_acc 0.8160 | test_loss 0.6988 | test_acc 0.7885 | lr 0.0468
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 151/256 | train_loss 0.8742 | train_acc 0.7740 | test_loss 0.5906 | test_acc 0.8015 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 152/256 | train_loss 0.8557 | train_acc 0.7360 | test_loss 0.6755 | test_acc 0.7620 | lr 0.0462
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 153/256 | train_loss 0.7233 | train_acc 0.8220 | test_loss 0.7444 | test_acc 0.7415 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 154/256 | train_loss 0.7662 | train_acc 0.7760 | test_loss 0.7221 | test_acc 0.7605 | lr 0.0455
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 155/256 | train_loss 0.7239 | train_acc 0.7920 | test_loss 0.7021 | test_acc 0.7555 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 156/256 | train_loss 0.7381 | train_acc 0.8040 | test_loss 1.0592 | test_acc 0.6450 | lr 0.0448
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 157/256 | train_loss 0.7580 | train_acc 0.7940 | test_loss 0.6571 | test_acc 0.7895 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 158/256 | train_loss 0.8276 | train_acc 0.7300 | test_loss 0.6710 | test_acc 0.7815 | lr 0.0440
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 159/256 | train_loss 0.8808 | train_acc 0.6840 | test_loss 0.7108 | test_acc 0.7715 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 160/256 | train_loss 0.8778 | train_acc 0.7560 | test_loss 0.6012 | test_acc 0.8005 | lr 0.0432
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 161/256 | train_loss 0.8756 | train_acc 0.7740 | test_loss 0.5100 | test_acc 0.8275 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 162/256 | train_loss 0.6895 | train_acc 0.8000 | test_loss 0.7347 | test_acc 0.7550 | lr 0.0423
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 163/256 | train_loss 0.7555 | train_acc 0.8100 | test_loss 0.6269 | test_acc 0.7930 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 164/256 | train_loss 0.9635 | train_acc 0.6800 | test_loss 0.4728 | test_acc 0.8415 | lr 0.0414
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 165/256 | train_loss 0.6556 | train_acc 0.8220 | test_loss 0.7997 | test_acc 0.7240 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 166/256 | train_loss 0.9197 | train_acc 0.7120 | test_loss 0.7549 | test_acc 0.7420 | lr 0.0405
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 167/256 | train_loss 0.7897 | train_acc 0.7720 | test_loss 1.2155 | test_acc 0.6245 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 168/256 | train_loss 0.9342 | train_acc 0.7500 | test_loss 0.6056 | test_acc 0.7920 | lr 0.0395
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 169/256 | train_loss 0.7290 | train_acc 0.7800 | test_loss 0.3247 | test_acc 0.8880 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 170/256 | train_loss 0.9715 | train_acc 0.6900 | test_loss 0.5985 | test_acc 0.8065 | lr 0.0385
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 171/256 | train_loss 0.8910 | train_acc 0.7800 | test_loss 0.6037 | test_acc 0.8135 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 172/256 | train_loss 0.5613 | train_acc 0.8500 | test_loss 0.8010 | test_acc 0.7415 | lr 0.0374
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 173/256 | train_loss 0.7194 | train_acc 0.7980 | test_loss 0.5722 | test_acc 0.8070 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 174/256 | train_loss 0.6978 | train_acc 0.7820 | test_loss 0.6017 | test_acc 0.8065 | lr 0.0364
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 175/256 | train_loss 0.9783 | train_acc 0.7240 | test_loss 0.7298 | test_acc 0.7615 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 176/256 | train_loss 0.9817 | train_acc 0.7340 | test_loss 0.5999 | test_acc 0.8015 | lr 0.0353
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 177/256 | train_loss 0.6751 | train_acc 0.8480 | test_loss 0.6710 | test_acc 0.7845 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 178/256 | train_loss 0.7730 | train_acc 0.8020 | test_loss 0.4427 | test_acc 0.8585 | lr 0.0342
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 179/256 | train_loss 0.6184 | train_acc 0.8500 | test_loss 0.4291 | test_acc 0.8590 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 180/256 | train_loss 0.5762 | train_acc 0.8640 | test_loss 0.2989 | test_acc 0.9095 | lr 0.0330
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 181/256 | train_loss 0.6807 | train_acc 0.8180 | test_loss 0.5629 | test_acc 0.8190 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 182/256 | train_loss 0.6159 | train_acc 0.8460 | test_loss 0.4856 | test_acc 0.8295 | lr 0.0319
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 183/256 | train_loss 0.7890 | train_acc 0.7900 | test_loss 0.5642 | test_acc 0.8165 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 184/256 | train_loss 0.7002 | train_acc 0.8480 | test_loss 0.4210 | test_acc 0.8645 | lr 0.0307
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 185/256 | train_loss 0.5530 | train_acc 0.8820 | test_loss 0.5960 | test_acc 0.8075 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 186/256 | train_loss 0.5485 | train_acc 0.8280 | test_loss 0.6130 | test_acc 0.8090 | lr 0.0295
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 187/256 | train_loss 0.9898 | train_acc 0.6960 | test_loss 0.4233 | test_acc 0.8750 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 188/256 | train_loss 0.7447 | train_acc 0.8200 | test_loss 0.4655 | test_acc 0.8540 | lr 0.0283
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 189/256 | train_loss 0.6288 | train_acc 0.7880 | test_loss 0.6597 | test_acc 0.7825 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 190/256 | train_loss 0.7375 | train_acc 0.8140 | test_loss 0.4620 | test_acc 0.8685 | lr 0.0271
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 191/256 | train_loss 0.7340 | train_acc 0.7620 | test_loss 0.4969 | test_acc 0.8480 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 192/256 | train_loss 0.5666 | train_acc 0.8400 | test_loss 0.7695 | test_acc 0.7480 | lr 0.0259
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 193/256 | train_loss 0.6479 | train_acc 0.8340 | test_loss 0.4699 | test_acc 0.8510 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 194/256 | train_loss 0.7407 | train_acc 0.7420 | test_loss 0.7610 | test_acc 0.7525 | lr 0.0246
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 195/256 | train_loss 0.6253 | train_acc 0.8260 | test_loss 0.5489 | test_acc 0.8245 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 196/256 | train_loss 0.7033 | train_acc 0.8140 | test_loss 0.5001 | test_acc 0.8320 | lr 0.0234
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 197/256 | train_loss 0.5956 | train_acc 0.8340 | test_loss 0.3219 | test_acc 0.9065 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 198/256 | train_loss 0.7434 | train_acc 0.7460 | test_loss 0.5153 | test_acc 0.8355 | lr 0.0222
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 199/256 | train_loss 0.6137 | train_acc 0.8300 | test_loss 0.2916 | test_acc 0.9065 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 200/256 | train_loss 0.8030 | train_acc 0.7380 | test_loss 0.7815 | test_acc 0.7485 | lr 0.0210
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 201/256 | train_loss 0.5893 | train_acc 0.8440 | test_loss 0.6402 | test_acc 0.7925 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 202/256 | train_loss 0.6360 | train_acc 0.8260 | test_loss 0.5170 | test_acc 0.8410 | lr 0.0198
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 203/256 | train_loss 0.7170 | train_acc 0.7400 | test_loss 0.5840 | test_acc 0.8040 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 204/256 | train_loss 0.5518 | train_acc 0.8480 | test_loss 0.5781 | test_acc 0.8155 | lr 0.0186
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 205/256 | train_loss 0.6371 | train_acc 0.8320 | test_loss 0.4550 | test_acc 0.8505 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 206/256 | train_loss 0.7630 | train_acc 0.7900 | test_loss 0.5943 | test_acc 0.8090 | lr 0.0175
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 207/256 | train_loss 0.6767 | train_acc 0.7980 | test_loss 0.3829 | test_acc 0.8915 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 208/256 | train_loss 0.5115 | train_acc 0.8640 | test_loss 0.5010 | test_acc 0.8390 | lr 0.0163
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 209/256 | train_loss 0.5074 | train_acc 0.8620 | test_loss 0.4187 | test_acc 0.8610 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 210/256 | train_loss 0.6046 | train_acc 0.8100 | test_loss 0.4827 | test_acc 0.8400 | lr 0.0152
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 211/256 | train_loss 0.7264 | train_acc 0.7540 | test_loss 0.5786 | test_acc 0.8200 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 212/256 | train_loss 0.6416 | train_acc 0.7980 | test_loss 0.6500 | test_acc 0.7890 | lr 0.0141
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 213/256 | train_loss 0.6590 | train_acc 0.8040 | test_loss 0.6360 | test_acc 0.7925 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 214/256 | train_loss 0.6527 | train_acc 0.8340 | test_loss 0.4756 | test_acc 0.8525 | lr 0.0131
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 215/256 | train_loss 0.5930 | train_acc 0.8420 | test_loss 0.5798 | test_acc 0.8160 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 216/256 | train_loss 0.5999 | train_acc 0.8500 | test_loss 0.4847 | test_acc 0.8555 | lr 0.0120
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 217/256 | train_loss 0.5989 | train_acc 0.8600 | test_loss 0.5034 | test_acc 0.8385 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 218/256 | train_loss 0.6601 | train_acc 0.7660 | test_loss 0.4252 | test_acc 0.8655 | lr 0.0110
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 219/256 | train_loss 0.5662 | train_acc 0.8280 | test_loss 0.5243 | test_acc 0.8340 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 220/256 | train_loss 0.8469 | train_acc 0.7060 | test_loss 0.5451 | test_acc 0.8315 | lr 0.0100
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 221/256 | train_loss 0.5233 | train_acc 0.8580 | test_loss 0.5060 | test_acc 0.8390 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 222/256 | train_loss 0.5978 | train_acc 0.8500 | test_loss 0.4442 | test_acc 0.8665 | lr 0.0091
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 223/256 | train_loss 0.5580 | train_acc 0.8340 | test_loss 0.4744 | test_acc 0.8480 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 224/256 | train_loss 0.6983 | train_acc 0.8340 | test_loss 0.4992 | test_acc 0.8445 | lr 0.0082
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 225/256 | train_loss 0.5847 | train_acc 0.8300 | test_loss 0.5019 | test_acc 0.8415 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 226/256 | train_loss 0.8355 | train_acc 0.7640 | test_loss 0.4639 | test_acc 0.8690 | lr 0.0073
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 227/256 | train_loss 0.5376 | train_acc 0.8960 | test_loss 0.5167 | test_acc 0.8435 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 228/256 | train_loss 0.6401 | train_acc 0.8520 | test_loss 0.4538 | test_acc 0.8620 | lr 0.0065
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 229/256 | train_loss 0.5549 | train_acc 0.8520 | test_loss 0.5156 | test_acc 0.8360 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 230/256 | train_loss 0.6911 | train_acc 0.7960 | test_loss 0.4948 | test_acc 0.8455 | lr 0.0057
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 231/256 | train_loss 0.4482 | train_acc 0.8920 | test_loss 0.3991 | test_acc 0.8730 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 232/256 | train_loss 0.5808 | train_acc 0.8280 | test_loss 0.4769 | test_acc 0.8455 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 233/256 | train_loss 0.7582 | train_acc 0.7220 | test_loss 0.4716 | test_acc 0.8505 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 234/256 | train_loss 0.8965 | train_acc 0.7220 | test_loss 0.5001 | test_acc 0.8550 | lr 0.0043
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 235/256 | train_loss 0.5970 | train_acc 0.8420 | test_loss 0.4684 | test_acc 0.8600 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 236/256 | train_loss 0.6912 | train_acc 0.7920 | test_loss 0.4202 | test_acc 0.8705 | lr 0.0037
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 237/256 | train_loss 0.4900 | train_acc 0.8480 | test_loss 0.4506 | test_acc 0.8590 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 238/256 | train_loss 0.5410 | train_acc 0.8380 | test_loss 0.5014 | test_acc 0.8390 | lr 0.0031
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 239/256 | train_loss 0.5808 | train_acc 0.8100 | test_loss 0.4828 | test_acc 0.8500 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 240/256 | train_loss 0.2231 | train_acc 0.9320 | test_loss 0.4941 | test_acc 0.8435 | lr 0.0026
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 241/256 | train_loss 0.6014 | train_acc 0.8140 | test_loss 0.4371 | test_acc 0.8660 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 242/256 | train_loss 0.4694 | train_acc 0.8440 | test_loss 0.4678 | test_acc 0.8460 | lr 0.0022
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 243/256 | train_loss 0.6485 | train_acc 0.7960 | test_loss 0.4507 | test_acc 0.8525 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 244/256 | train_loss 0.5222 | train_acc 0.8460 | test_loss 0.4751 | test_acc 0.8445 | lr 0.0017
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 245/256 | train_loss 0.5904 | train_acc 0.8220 | test_loss 0.4756 | test_acc 0.8455 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 246/256 | train_loss 0.4760 | train_acc 0.8380 | test_loss 0.4554 | test_acc 0.8515 | lr 0.0014
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 247/256 | train_loss 0.6639 | train_acc 0.7760 | test_loss 0.4701 | test_acc 0.8545 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 248/256 | train_loss 0.6375 | train_acc 0.8360 | test_loss 0.4408 | test_acc 0.8615 | lr 0.0011
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 249/256 | train_loss 0.6517 | train_acc 0.8480 | test_loss 0.4580 | test_acc 0.8635 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 250/256 | train_loss 0.6284 | train_acc 0.7580 | test_loss 0.4648 | test_acc 0.8535 | lr 0.0009
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 251/256 | train_loss 0.6000 | train_acc 0.8760 | test_loss 0.5010 | test_acc 0.8385 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 252/256 | train_loss 0.4160 | train_acc 0.9100 | test_loss 0.4455 | test_acc 0.8575 | lr 0.0007
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 253/256 | train_loss 0.4364 | train_acc 0.8700 | test_loss 0.4492 | test_acc 0.8600 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 254/256 | train_loss 0.6656 | train_acc 0.8040 | test_loss 0.4411 | test_acc 0.8660 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 255/256 | train_loss 0.4966 | train_acc 0.8560 | test_loss 0.4335 | test_acc 0.8635 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 2 | Epoch 256/256 | train_loss 0.6613 | train_acc 0.8020 | test_loss 0.4204 | test_acc 0.8705 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (2)
[INFO] main.py:389 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (2)
[INFO] main.py:396 > [2-5] Report task result

##################################################
# Task 3 iteration
##################################################

[INFO] main.py:308 > [2-1] Prepare a datalist for the current task
meta_pseudo_init
total : 5000  current step :  0
total : 5000  current step :  1
total : 5000  current step :  2
total : 5000  current step :  3
total : 5000  current step :  4
total : 5000  current step :  5
total : 5000  current step :  6
total : 5000  current step :  7
total : 5000  current step :  8
total : 5000  current step :  9
total : 5000  current step :  10
total : 5000  current step :  11
total : 5000  current step :  12
total : 5000  current step :  13
total : 5000  current step :  14
total : 5000  current step :  15
total : 5000  current step :  16
total : 5000  current step :  17
total : 5000  current step :  18
total : 5000  current step :  19
total : 5000  current step :  20
total : 5000  current step :  21
total : 5000  current step :  22
total : 5000  current step :  23
total : 5000  current step :  24
total : 5000  current step :  25
total : 5000  current step :  26
total : 5000  current step :  27
total : 5000  current step :  28
total : 5000  current step :  29
total : 5000  current step :  30
total : 5000  current step :  31
total : 5000  current step :  32
total : 5000  current step :  33
total : 5000  current step :  34
total : 5000  current step :  35
total : 5000  current step :  36
total : 5000  current step :  37
total : 5000  current step :  38
total : 5000  current step :  39
total : 5000  current step :  40
total : 5000  current step :  41
total : 5000  current step :  42
total : 5000  current step :  43
total : 5000  current step :  44
total : 5000  current step :  45
total : 5000  current step :  46
total : 5000  current step :  47
total : 5000  current step :  48
total : 5000  current step :  49
total : 5000  current step :  50
total : 5000  current step :  51
total : 5000  current step :  52
total : 5000  current step :  53
total : 5000  current step :  54
total : 5000  current step :  55
total : 5000  current step :  56
total : 5000  current step :  57
total : 5000  current step :  58
total : 5000  current step :  59
total : 5000  current step :  60
total : 5000  current step :  61
total : 5000  current step :  62
total : 5000  current step :  63
total : 5000  current step :  64
total : 5000  current step :  65
total : 5000  current step :  66
total : 5000  current step :  67
total : 5000  current step :  68
total : 5000  current step :  69
total : 5000  current step :  70
total : 5000  current step :  71
total : 5000  current step :  72
total : 5000  current step :  73
total : 5000  current step :  74
total : 5000  current step :  75
total : 5000  current step :  76
total : 5000  current step :  77
total : 5000  current step :  78
total : 5000  current step :  79
total : 5000  current step :  80
total : 5000  current step :  81
total : 5000  current step :  82
total : 5000  current step :  83
total : 5000  current step :  84
total : 5000  current step :  85
total : 5000  current step :  86
total : 5000  current step :  87
total : 5000  current step :  88
total : 5000  current step :  89
total : 5000  current step :  90
total : 5000  current step :  91
total : 5000  current step :  92
total : 5000  current step :  93
total : 5000  current step :  94
total : 5000  current step :  95
total : 5000  current step :  96
total : 5000  current step :  97
total : 5000  current step :  98
total : 5000  current step :  99
total : 5000  current step :  100
total : 5000  current step :  101
total : 5000  current step :  102
total : 5000  current step :  103
total : 5000  current step :  104
total : 5000  current step :  105
total : 5000  current step :  106
total : 5000  current step :  107
total : 5000  current step :  108
total : 5000  current step :  109
total : 5000  current step :  110
total : 5000  current step :  111
total : 5000  current step :  112
total : 5000  current step :  113
total : 5000  current step :  114
total : 5000  current step :  115
total : 5000  current step :  116
total : 5000  current step :  117
total : 5000  current step :  118
total : 5000  current step :  119
total : 5000  current step :  120
total : 5000  current step :  121
total : 5000  current step :  122
total : 5000  current step :  123
total : 5000  current step :  124
total : 5000  current step :  125
total : 5000  current step :  126
total : 5000  current step :  127
total : 5000  current step :  128
total : 5000  current step :  129
total : 5000  current step :  130
total : 5000  current step :  131
total : 5000  current step :  132
total : 5000  current step :  133
total : 5000  current step :  134
total : 5000  current step :  135
total : 5000  current step :  136
total : 5000  current step :  137
total : 5000  current step :  138
total : 5000  current step :  139
total : 5000  current step :  140
total : 5000  current step :  141
total : 5000  current step :  142
total : 5000  current step :  143
total : 5000  current step :  144
total : 5000  current step :  145
total : 5000  current step :  146
total : 5000  current step :  147
total : 5000  current step :  148
total : 5000  current step :  149
total : 5000  current step :  150
total : 5000  current step :  151
total : 5000  current step :  152
total : 5000  current step :  153
total : 5000  current step :  154
total : 5000  current step :  155
total : 5000  current step :  156
total : 5000  current step :  157
total : 5000  current step :  158
total : 5000  current step :  159
total : 5000  current step :  160
total : 5000  current step :  161
total : 5000  current step :  162
total : 5000  current step :  163
total : 5000  current step :  164
total : 5000  current step :  165
total : 5000  current step :  166
total : 5000  current step :  167
total : 5000  current step :  168
total : 5000  current step :  169
total : 5000  current step :  170
total : 5000  current step :  171
total : 5000  current step :  172
total : 5000  current step :  173
total : 5000  current step :  174
total : 5000  current step :  175
total : 5000  current step :  176
total : 5000  current step :  177
total : 5000  current step :  178
total : 5000  current step :  179
total : 5000  current step :  180
total : 5000  current step :  181
total : 5000  current step :  182
total : 5000  current step :  183
total : 5000  current step :  184
total : 5000  current step :  185
total : 5000  current step :  186
total : 5000  current step :  187
total : 5000  current step :  188
total : 5000  current step :  189
total : 5000  current step :  190
total : 5000  current step :  191
total : 5000  current step :  192
total : 5000  current step :  193
total : 5000  current step :  194
total : 5000  current step :  195
total : 5000  current step :  196
total : 5000  current step :  197
total : 5000  current step :  198
total : 5000  current step :  199
total : 5000  current step :  200
total : 5000  current step :  201
total : 5000  current step :  202
total : 5000  current step :  203
total : 5000  current step :  204
total : 5000  current step :  205
total : 5000  current step :  206
total : 5000  current step :  207
total : 5000  current step :  208
total : 5000  current step :  209
total : 5000  current step :  210
total : 5000  current step :  211
total : 5000  current step :  212
total : 5000  current step :  213
total : 5000  current step :  214
total : 5000  current step :  215
total : 5000  current step :  216
total : 5000  current step :  217
total : 5000  current step :  218
total : 5000  current step :  219
total : 5000  current step :  220
total : 5000  current step :  221
total : 5000  current step :  222
total : 5000  current step :  223
total : 5000  current step :  224
total : 5000  current step :  225
total : 5000  current step :  226
total : 5000  current step :  227
total : 5000  current step :  228
total : 5000  current step :  229
total : 5000  current step :  230
total : 5000  current step :  231
total : 5000  current step :  232
total : 5000  current step :  233
total : 5000  current step :  234
total : 5000  current step :  235
total : 5000  current step :  236
total : 5000  current step :  237
total : 5000  current step :  238
total : 5000  current step :  239
total : 5000  current step :  240
total : 5000  current step :  241
total : 5000  current step :  242
total : 5000  current step :  243
total : 5000  current step :  244
total : 5000  current step :  245
total : 5000  current step :  246
total : 5000  current step :  247
total : 5000  current step :  248
total : 5000  current step :  249
total : 5000  current step :  250
total : 5000  current step :  251
total : 5000  current step :  252
total : 5000  current step :  253
total : 5000  current step :  254
total : 5000  current step :  255
total : 5000  current step :  256
total : 5000  current step :  257
total : 5000  current step :  258
total : 5000  current step :  259
total : 5000  current step :  260
total : 5000  current step :  261
total : 5000  current step :  262
total : 5000  current step :  263
total : 5000  current step :  264
total : 5000  current step :  265
total : 5000  current step :  266
total : 5000  current step :  267
total : 5000  current step :  268
total : 5000  current step :  269
total : 5000  current step :  270
total : 5000  current step :  271
total : 5000  current step :  272
total : 5000  current step :  273
total : 5000  current step :  274
total : 5000  current step :  275
total : 5000  current step :  276
total : 5000  current step :  277
total : 5000  current step :  278
total : 5000  current step :  279
total : 5000  current step :  280
total : 5000  current step :  281
total : 5000  current step :  282
total : 5000  current step :  283
total : 5000  current step :  284
total : 5000  current step :  285
total : 5000  current step :  286
total : 5000  current step :  287
total : 5000  current step :  288
total : 5000  current step :  289
total : 5000  current step :  290
total : 5000  current step :  291
total : 5000  current step :  292
total : 5000  current step :  293
total : 5000  current step :  294
total : 5000  current step :  295
total : 5000  current step :  296
total : 5000  current step :  297
total : 5000  current step :  298
total : 5000  current step :  299
total : 5000  current step :  300
total : 5000  current step :  301
total : 5000  current step :  302
total : 5000  current step :  303
total : 5000  current step :  304
total : 5000  current step :  305
total : 5000  current step :  306
total : 5000  current step :  307
total : 5000  current step :  308
total : 5000  current step :  309
total : 5000  current step :  310
total : 5000  current step :  311
total : 5000  current step :  312
total : 5000  current step :  313
total : 5000  current step :  314
total : 5000  current step :  315
total : 5000  current step :  316
total : 5000  current step :  317
total : 5000  current step :  318
total : 5000  current step :  319
total : 5000  current step :  320
total : 5000  current step :  321
total : 5000  current step :  322
total : 5000  current step :  323
total : 5000  current step :  324
total : 5000  current step :  325
total : 5000  current step :  326
total : 5000  current step :  327
total : 5000  current step :  328
total : 5000  current step :  329
total : 5000  current step :  330
total : 5000  current step :  331
total : 5000  current step :  332
total : 5000  current step :  333
total : 5000  current step :  334
total : 5000  current step :  335
total : 5000  current step :  336
total : 5000  current step :  337
total : 5000  current step :  338
total : 5000  current step :  339
total : 5000  current step :  340
total : 5000  current step :  341
total : 5000  current step :  342
total : 5000  current step :  343
total : 5000  current step :  344
total : 5000  current step :  345
total : 5000  current step :  346
total : 5000  current step :  347
total : 5000  current step :  348
total : 5000  current step :  349
total : 5000  current step :  350
total : 5000  current step :  351
total : 5000  current step :  352
total : 5000  current step :  353
total : 5000  current step :  354
total : 5000  current step :  355
total : 5000  current step :  356
total : 5000  current step :  357
total : 5000  current step :  358
total : 5000  current step :  359
total : 5000  current step :  360
total : 5000  current step :  361
total : 5000  current step :  362
total : 5000  current step :  363
total : 5000  current step :  364
total : 5000  current step :  365
total : 5000  current step :  366
total : 5000  current step :  367
total : 5000  current step :  368
total : 5000  current step :  369
total : 5000  current step :  370
total : 5000  current step :  371
total : 5000  current step :  372
total : 5000  current step :  373
total : 5000  current step :  374
total : 5000  current step :  375
total : 5000  current step :  376
total : 5000  current step :  377
total : 5000  current step :  378
total : 5000  current step :  379
total : 5000  current step :  380
total : 5000  current step :  381
total : 5000  current step :  382
total : 5000  current step :  383
total : 5000  current step :  384
total : 5000  current step :  385
total : 5000  current step :  386
total : 5000  current step :  387
total : 5000  current step :  388
total : 5000  current step :  389
total : 5000  current step :  390
total : 5000  current step :  391
total : 5000  current step :  392
total : 5000  current step :  393
total : 5000  current step :  394
total : 5000  current step :  395
total : 5000  current step :  396
total : 5000  current step :  397
total : 5000  current step :  398
total : 5000  current step :  399
total : 5000  current step :  400
total : 5000  current step :  401
total : 5000  current step :  402
total : 5000  current step :  403
total : 5000  current step :  404
total : 5000  current step :  405
total : 5000  current step :  406
total : 5000  current step :  407
total : 5000  current step :  408
total : 5000  current step :  409
total : 5000  current step :  410
total : 5000  current step :  411
total : 5000  current step :  412
total : 5000  current step :  413
total : 5000  current step :  414
total : 5000  current step :  415
total : 5000  current step :  416
total : 5000  current step :  417
total : 5000  current step :  418
total : 5000  current step :  419
total : 5000  current step :  420
total : 5000  current step :  421
total : 5000  current step :  422
total : 5000  current step :  423
total : 5000  current step :  424
total : 5000  current step :  425
total : 5000  current step :  426
total : 5000  current step :  427
total : 5000  current step :  428
total : 5000  current step :  429
total : 5000  current step :  430
total : 5000  current step :  431
total : 5000  current step :  432
total : 5000  current step :  433
total : 5000  current step :  434
total : 5000  current step :  435
total : 5000  current step :  436
total : 5000  current step :  437
total : 5000  current step :  438
total : 5000  current step :  439
total : 5000  current step :  440
total : 5000  current step :  441
total : 5000  current step :  442
total : 5000  current step :  443
total : 5000  current step :  444
total : 5000  current step :  445
total : 5000  current step :  446
total : 5000  current step :  447
total : 5000  current step :  448
total : 5000  current step :  449
total : 5000  current step :  450
total : 5000  current step :  451
total : 5000  current step :  452
total : 5000  current step :  453
total : 5000  current step :  454
total : 5000  current step :  455
total : 5000  current step :  456
total : 5000  current step :  457
total : 5000  current step :  458
total : 5000  current step :  459
total : 5000  current step :  460
total : 5000  current step :  461
total : 5000  current step :  462
total : 5000  current step :  463
total : 5000  current step :  464
total : 5000  current step :  465
total : 5000  current step :  466
total : 5000  current step :  467
total : 5000  current step :  468
total : 5000  current step :  469
total : 5000  current step :  470
total : 5000  current step :  471
total : 5000  current step :  472
total : 5000  current step :  473
total : 5000  current step :  474
total : 5000  current step :  475
total : 5000  current step :  476
total : 5000  current step :  477
total : 5000  current step :  478
total : 5000  current step :  479
total : 5000  current step :  480
total : 5000  current step :  481
total : 5000  current step :  482
total : 5000  current step :  483
total : 5000  current step :  484
total : 5000  current step :  485
total : 5000  current step :  486
total : 5000  current step :  487
total : 5000  current step :  488
total : 5000  current step :  489
total : 5000  current step :  490
total : 5000  current step :  491
total : 5000  current step :  492
total : 5000  current step :  493
total : 5000  current step :  494
total : 5000  current step :  495
total : 5000  current step :  496
total : 5000  current step :  497
total : 5000  current step :  498
total : 5000  current step :  499
total : 5000  current step :  500
total : 5000  current step :  501
total : 5000  current step :  502
total : 5000  current step :  503
total : 5000  current step :  504
total : 5000  current step :  505
total : 5000  current step :  506
total : 5000  current step :  507
total : 5000  current step :  508
total : 5000  current step :  509
total : 5000  current step :  510
total : 5000  current step :  511
total : 5000  current step :  512
total : 5000  current step :  513
total : 5000  current step :  514
total : 5000  current step :  515
total : 5000  current step :  516
total : 5000  current step :  517
total : 5000  current step :  518
total : 5000  current step :  519
total : 5000  current step :  520
total : 5000  current step :  521
total : 5000  current step :  522
total : 5000  current step :  523
total : 5000  current step :  524
total : 5000  current step :  525
total : 5000  current step :  526
total : 5000  current step :  527
total : 5000  current step :  528
total : 5000  current step :  529
total : 5000  current step :  530
total : 5000  current step :  531
total : 5000  current step :  532
total : 5000  current step :  533
total : 5000  current step :  534
total : 5000  current step :  535
total : 5000  current step :  536
total : 5000  current step :  537
total : 5000  current step :  538
total : 5000  current step :  539
total : 5000  current step :  540
total : 5000  current step :  541
total : 5000  current step :  542
total : 5000  current step :  543
total : 5000  current step :  544
total : 5000  current step :  545
total : 5000  current step :  546
total : 5000  current step :  547
total : 5000  current step :  548
total : 5000  current step :  549
total : 5000  current step :  550
total : 5000  current step :  551
total : 5000  current step :  552
total : 5000  current step :  553
total : 5000  current step :  554
total : 5000  current step :  555
total : 5000  current step :  556
total : 5000  current step :  557
total : 5000  current step :  558
total : 5000  current step :  559
total : 5000  current step :  560
total : 5000  current step :  561
total : 5000  current step :  562
total : 5000  current step :  563
total : 5000  current step :  564
total : 5000  current step :  565
total : 5000  current step :  566
total : 5000  current step :  567
total : 5000  current step :  568
total : 5000  current step :  569
total : 5000  current step :  570
total : 5000  current step :  571
total : 5000  current step :  572
total : 5000  current step :  573
total : 5000  current step :  574
total : 5000  current step :  575
total : 5000  current step :  576
total : 5000  current step :  577
total : 5000  current step :  578
total : 5000  current step :  579
total : 5000  current step :  580
total : 5000  current step :  581
total : 5000  current step :  582
total : 5000  current step :  583
total : 5000  current step :  584
total : 5000  current step :  585
total : 5000  current step :  586
total : 5000  current step :  587
total : 5000  current step :  588
total : 5000  current step :  589
total : 5000  current step :  590
total : 5000  current step :  591
total : 5000  current step :  592
total : 5000  current step :  593
total : 5000  current step :  594
total : 5000  current step :  595
total : 5000  current step :  596
total : 5000  current step :  597
total : 5000  current step :  598
total : 5000  current step :  599
total : 5000  current step :  600
total : 5000  current step :  601
total : 5000  current step :  602
total : 5000  current step :  603
total : 5000  current step :  604
total : 5000  current step :  605
total : 5000  current step :  606
total : 5000  current step :  607
total : 5000  current step :  608
total : 5000  current step :  609
total : 5000  current step :  610
total : 5000  current step :  611
total : 5000  current step :  612
total : 5000  current step :  613
total : 5000  current step :  614
total : 5000  current step :  615
total : 5000  current step :  616
total : 5000  current step :  617
total : 5000  current step :  618
total : 5000  current step :  619
total : 5000  current step :  620
total : 5000  current step :  621
total : 5000  current step :  622
total : 5000  current step :  623
total : 5000  current step :  624
total : 5000  current step :  625
total : 5000  current step :  626
total : 5000  current step :  627
total : 5000  current step :  628
total : 5000  current step :  629
total : 5000  current step :  630
total : 5000  current step :  631
total : 5000  current step :  632
total : 5000  current step :  633
total : 5000  current step :  634
total : 5000  current step :  635
total : 5000  current step :  636
total : 5000  current step :  637
total : 5000  current step :  638
total : 5000  current step :  639
total : 5000  current step :  640
total : 5000  current step :  641
total : 5000  current step :  642
total : 5000  current step :  643
total : 5000  current step :  644
total : 5000  current step :  645
total : 5000  current step :  646
total : 5000  current step :  647
total : 5000  current step :  648
total : 5000  current step :  649
total : 5000  current step :  650
total : 5000  current step :  651
total : 5000  current step :  652
total : 5000  current step :  653
total : 5000  current step :  654
total : 5000  current step :  655
total : 5000  current step :  656
total : 5000  current step :  657
total : 5000  current step :  658
total : 5000  current step :  659
total : 5000  current step :  660
total : 5000  current step :  661
total : 5000  current step :  662
total : 5000  current step :  663
total : 5000  current step :  664
total : 5000  current step :  665
total : 5000  current step :  666
total : 5000  current step :  667
total : 5000  current step :  668
total : 5000  current step :  669
total : 5000  current step :  670
total : 5000  current step :  671
total : 5000  current step :  672
total : 5000  current step :  673
total : 5000  current step :  674
total : 5000  current step :  675
total : 5000  current step :  676
total : 5000  current step :  677
total : 5000  current step :  678
total : 5000  current step :  679
total : 5000  current step :  680
total : 5000  current step :  681
total : 5000  current step :  682
total : 5000  current step :  683
total : 5000  current step :  684
total : 5000  current step :  685
total : 5000  current step :  686
total : 5000  current step :  687
total : 5000  current step :  688
total : 5000  current step :  689
total : 5000  current step :  690
total : 5000  current step :  691
total : 5000  current step :  692
total : 5000  current step :  693
total : 5000  current step :  694
total : 5000  current step :  695
total : 5000  current step :  696
total : 5000  current step :  697
total : 5000  current step :  698
total : 5000  current step :  699
total : 5000  current step :  700
total : 5000  current step :  701
total : 5000  current step :  702
total : 5000  current step :  703
total : 5000  current step :  704
total : 5000  current step :  705
total : 5000  current step :  706
total : 5000  current step :  707
total : 5000  current step :  708
total : 5000  current step :  709
total : 5000  current step :  710
total : 5000  current step :  711
total : 5000  current step :  712
total : 5000  current step :  713
total : 5000  current step :  714
total : 5000  current step :  715
total : 5000  current step :  716
total : 5000  current step :  717
total : 5000  current step :  718
total : 5000  current step :  719
total : 5000  current step :  720
total : 5000  current step :  721
total : 5000  current step :  722
total : 5000  current step :  723
total : 5000  current step :  724
total : 5000  current step :  725
total : 5000  current step :  726
total : 5000  current step :  727
total : 5000  current step :  728
total : 5000  current step :  729
total : 5000  current step :  730
total : 5000  current step :  731
total : 5000  current step :  732
total : 5000  current step :  733
total : 5000  current step :  734
total : 5000  current step :  735
total : 5000  current step :  736
total : 5000  current step :  737
total : 5000  current step :  738
total : 5000  current step :  739
total : 5000  current step :  740
total : 5000  current step :  741
total : 5000  current step :  742
total : 5000  current step :  743
total : 5000  current step :  744
total : 5000  current step :  745
total : 5000  current step :  746
total : 5000  current step :  747
total : 5000  current step :  748
total : 5000  current step :  749
total : 5000  current step :  750
total : 5000  current step :  751
total : 5000  current step :  752
total : 5000  current step :  753
total : 5000  current step :  754
total : 5000  current step :  755
total : 5000  current step :  756
total : 5000  current step :  757
total : 5000  current step :  758
total : 5000  current step :  759
total : 5000  current step :  760
total : 5000  current step :  761
total : 5000  current step :  762
total : 5000  current step :  763
total : 5000  current step :  764
total : 5000  current step :  765
total : 5000  current step :  766
total : 5000  current step :  767
total : 5000  current step :  768
total : 5000  current step :  769
total : 5000  current step :  770
total : 5000  current step :  771
total : 5000  current step :  772
total : 5000  current step :  773
total : 5000  current step :  774
total : 5000  current step :  775
total : 5000  current step :  776
total : 5000  current step :  777
total : 5000  current step :  778
total : 5000  current step :  779
total : 5000  current step :  780
total : 5000  current step :  781
total : 5000  current step :  782
total : 5000  current step :  783
total : 5000  current step :  784
total : 5000  current step :  785
total : 5000  current step :  786
total : 5000  current step :  787
total : 5000  current step :  788
total : 5000  current step :  789
total : 5000  current step :  790
total : 5000  current step :  791
total : 5000  current step :  792
total : 5000  current step :  793
total : 5000  current step :  794
total : 5000  current step :  795
total : 5000  current step :  796
total : 5000  current step :  797
total : 5000  current step :  798
total : 5000  current step :  799
total : 5000  current step :  800
total : 5000  current step :  801
total : 5000  current step :  802
total : 5000  current step :  803
total : 5000  current step :  804
total : 5000  current step :  805
total : 5000  current step :  806
total : 5000  current step :  807
total : 5000  current step :  808
total : 5000  current step :  809
total : 5000  current step :  810
total : 5000  current step :  811
total : 5000  current step :  812
total : 5000  current step :  813
total : 5000  current step :  814
total : 5000  current step :  815
total : 5000  current step :  816
total : 5000  current step :  817
total : 5000  current step :  818
total : 5000  current step :  819
total : 5000  current step :  820
total : 5000  current step :  821
total : 5000  current step :  822
total : 5000  current step :  823
total : 5000  current step :  824
total : 5000  current step :  825
total : 5000  current step :  826
total : 5000  current step :  827
total : 5000  current step :  828
total : 5000  current step :  829
total : 5000  current step :  830
total : 5000  current step :  831
total : 5000  current step :  832
total : 5000  current step :  833
total : 5000  current step :  834
total : 5000  current step :  835
total : 5000  current step :  836
total : 5000  current step :  837
total : 5000  current step :  838
total : 5000  current step :  839
total : 5000  current step :  840
total : 5000  current step :  841
total : 5000  current step :  842
total : 5000  current step :  843
total : 5000  current step :  844
total : 5000  current step :  845
total : 5000  current step :  846
total : 5000  current step :  847
total : 5000  current step :  848
total : 5000  current step :  849
total : 5000  current step :  850
total : 5000  current step :  851
total : 5000  current step :  852
total : 5000  current step :  853
total : 5000  current step :  854
total : 5000  current step :  855
total : 5000  current step :  856
total : 5000  current step :  857
total : 5000  current step :  858
total : 5000  current step :  859
total : 5000  current step :  860
total : 5000  current step :  861
total : 5000  current step :  862
total : 5000  current step :  863
total : 5000  current step :  864
total : 5000  current step :  865
total : 5000  current step :  866
total : 5000  current step :  867
total : 5000  current step :  868
total : 5000  current step :  869
total : 5000  current step :  870
total : 5000  current step :  871
total : 5000  current step :  872
total : 5000  current step :  873
total : 5000  current step :  874
total : 5000  current step :  875
total : 5000  current step :  876
total : 5000  current step :  877
total : 5000  current step :  878
total : 5000  current step :  879
total : 5000  current step :  880
total : 5000  current step :  881
total : 5000  current step :  882
total : 5000  current step :  883
total : 5000  current step :  884
total : 5000  current step :  885
total : 5000  current step :  886
total : 5000  current step :  887
total : 5000  current step :  888
total : 5000  current step :  889
total : 5000  current step :  890
total : 5000  current step :  891
total : 5000  current step :  892
total : 5000  current step :  893
total : 5000  current step :  894
total : 5000  current step :  895
total : 5000  current step :  896
total : 5000  current step :  897
total : 5000  current step :  898
total : 5000  current step :  899
total : 5000  current step :  900
total : 5000  current step :  901
total : 5000  current step :  902
total : 5000  current step :  903
total : 5000  current step :  904
total : 5000  current step :  905
total : 5000  current step :  906
total : 5000  current step :  907
total : 5000  current step :  908
total : 5000  current step :  909
total : 5000  current step :  910
total : 5000  current step :  911
total : 5000  current step :  912
total : 5000  current step :  913
total : 5000  current step :  914
total : 5000  current step :  915
total : 5000  current step :  916
total : 5000  current step :  917
total : 5000  current step :  918
total : 5000  current step :  919
total : 5000  current step :  920
total : 5000  current step :  921
total : 5000  current step :  922
total : 5000  current step :  923
total : 5000  current step :  924
total : 5000  current step :  925
total : 5000  current step :  926
total : 5000  current step :  927
total : 5000  current step :  928
total : 5000  current step :  929
total : 5000  current step :  930
total : 5000  current step :  931
total : 5000  current step :  932
total : 5000  current step :  933
total : 5000  current step :  934
total : 5000  current step :  935
total : 5000  current step :  936
total : 5000  current step :  937
total : 5000  current step :  938
total : 5000  current step :  939
total : 5000  current step :  940
total : 5000  current step :  941
total : 5000  current step :  942
total : 5000  current step :  943
total : 5000  current step :  944
total : 5000  current step :  945
total : 5000  current step :  946
total : 5000  current step :  947
total : 5000  current step :  948
total : 5000  current step :  949
total : 5000  current step :  950
total : 5000  current step :  951
total : 5000  current step :  952
total : 5000  current step :  953
total : 5000  current step :  954
total : 5000  current step :  955
total : 5000  current step :  956
total : 5000  current step :  957
total : 5000  current step :  958
total : 5000  current step :  959
total : 5000  current step :  960
total : 5000  current step :  961
total : 5000  current step :  962
total : 5000  current step :  963
total : 5000  current step :  964
total : 5000  current step :  965
total : 5000  current step :  966
total : 5000  current step :  967
total : 5000  current step :  968
total : 5000  current step :  969
total : 5000  current step :  970
total : 5000  current step :  971
total : 5000  current step :  972
total : 5000  current step :  973
total : 5000  current step :  974
total : 5000  current step :  975
total : 5000  current step :  976
total : 5000  current step :  977
total : 5000  current step :  978
total : 5000  current step :  979
total : 5000  current step :  980
total : 5000  current step :  981
total : 5000  current step :  982
total : 5000  current step :  983
total : 5000  current step :  984
total : 5000  current step :  985
total : 5000  current step :  986
total : 5000  current step :  987
total : 5000  current step :  988
total : 5000  current step :  989
total : 5000  current step :  990
total : 5000  current step :  991
total : 5000  current step :  992
total : 5000  current step :  993
total : 5000  current step :  994
total : 5000  current step :  995
total : 5000  current step :  996
total : 5000  current step :  997
total : 5000  current step :  998
total : 5000  current step :  999
total : 5000  current step :  1000
total : 5000  current step :  1001
total : 5000  current step :  1002
total : 5000  current step :  1003
total : 5000  current step :  1004
total : 5000  current step :  1005
total : 5000  current step :  1006
total : 5000  current step :  1007
total : 5000  current step :  1008
total : 5000  current step :  1009
total : 5000  current step :  1010
total : 5000  current step :  1011
total : 5000  current step :  1012
total : 5000  current step :  1013
total : 5000  current step :  1014
total : 5000  current step :  1015
total : 5000  current step :  1016
total : 5000  current step :  1017
total : 5000  current step :  1018
total : 5000  current step :  1019
total : 5000  current step :  1020
total : 5000  current step :  1021
total : 5000  current step :  1022
total : 5000  current step :  1023
total : 5000  current step :  1024
total : 5000  current step :  1025
total : 5000  current step :  1026
total : 5000  current step :  1027
total : 5000  current step :  1028
total : 5000  current step :  1029
total : 5000  current step :  1030
total : 5000  current step :  1031
total : 5000  current step :  1032
total : 5000  current step :  1033
total : 5000  current step :  1034
total : 5000  current step :  1035
total : 5000  current step :  1036
total : 5000  current step :  1037
total : 5000  current step :  1038
total : 5000  current step :  1039
total : 5000  current step :  1040
total : 5000  current step :  1041
total : 5000  current step :  1042
total : 5000  current step :  1043
total : 5000  current step :  1044
total : 5000  current step :  1045
total : 5000  current step :  1046
total : 5000  current step :  1047
total : 5000  current step :  1048
total : 5000  current step :  1049
total : 5000  current step :  1050
total : 5000  current step :  1051
total : 5000  current step :  1052
total : 5000  current step :  1053
total : 5000  current step :  1054
total : 5000  current step :  1055
total : 5000  current step :  1056
total : 5000  current step :  1057
total : 5000  current step :  1058
total : 5000  current step :  1059
total : 5000  current step :  1060
total : 5000  current step :  1061
total : 5000  current step :  1062
total : 5000  current step :  1063
total : 5000  current step :  1064
total : 5000  current step :  1065
total : 5000  current step :  1066
total : 5000  current step :  1067
total : 5000  current step :  1068
total : 5000  current step :  1069
total : 5000  current step :  1070
total : 5000  current step :  1071
total : 5000  current step :  1072
total : 5000  current step :  1073
total : 5000  current step :  1074
total : 5000  current step :  1075
total : 5000  current step :  1076
total : 5000  current step :  1077
total : 5000  current step :  1078
total : 5000  current step :  1079
total : 5000  current step :  1080
total : 5000  current step :  1081
total : 5000  current step :  1082
total : 5000  current step :  1083
total : 5000  current step :  1084
total : 5000  current step :  1085
total : 5000  current step :  1086
total : 5000  current step :  1087
total : 5000  current step :  1088
total : 5000  current step :  1089
total : 5000  current step :  1090
total : 5000  current step :  1091
total : 5000  current step :  1092
total : 5000  current step :  1093
total : 5000  current step :  1094
total : 5000  current step :  1095
total : 5000  current step :  1096
total : 5000  current step :  1097
total : 5000  current step :  1098
total : 5000  current step :  1099
total : 5000  current step :  1100
total : 5000  current step :  1101
total : 5000  current step :  1102
total : 5000  current step :  1103
total : 5000  current step :  1104
total : 5000  current step :  1105
total : 5000  current step :  1106
total : 5000  current step :  1107
total : 5000  current step :  1108
total : 5000  current step :  1109
total : 5000  current step :  1110
total : 5000  current step :  1111
total : 5000  current step :  1112
total : 5000  current step :  1113
total : 5000  current step :  1114
total : 5000  current step :  1115
total : 5000  current step :  1116
total : 5000  current step :  1117
total : 5000  current step :  1118
total : 5000  current step :  1119
total : 5000  current step :  1120
total : 5000  current step :  1121
total : 5000  current step :  1122
total : 5000  current step :  1123
total : 5000  current step :  1124
total : 5000  current step :  1125
total : 5000  current step :  1126
total : 5000  current step :  1127
total : 5000  current step :  1128
total : 5000  current step :  1129
total : 5000  current step :  1130
total : 5000  current step :  1131
total : 5000  current step :  1132
total : 5000  current step :  1133
total : 5000  current step :  1134
total : 5000  current step :  1135
total : 5000  current step :  1136
total : 5000  current step :  1137
total : 5000  current step :  1138
total : 5000  current step :  1139
total : 5000  current step :  1140
total : 5000  current step :  1141
total : 5000  current step :  1142
total : 5000  current step :  1143
total : 5000  current step :  1144
total : 5000  current step :  1145
total : 5000  current step :  1146
total : 5000  current step :  1147
total : 5000  current step :  1148
total : 5000  current step :  1149
total : 5000  current step :  1150
total : 5000  current step :  1151
total : 5000  current step :  1152
total : 5000  current step :  1153
total : 5000  current step :  1154
total : 5000  current step :  1155
total : 5000  current step :  1156
total : 5000  current step :  1157
total : 5000  current step :  1158
total : 5000  current step :  1159
total : 5000  current step :  1160
total : 5000  current step :  1161
total : 5000  current step :  1162
total : 5000  current step :  1163
total : 5000  current step :  1164
total : 5000  current step :  1165
total : 5000  current step :  1166
total : 5000  current step :  1167
total : 5000  current step :  1168
total : 5000  current step :  1169
total : 5000  current step :  1170
total : 5000  current step :  1171
total : 5000  current step :  1172
total : 5000  current step :  1173
total : 5000  current step :  1174
total : 5000  current step :  1175
total : 5000  current step :  1176
total : 5000  current step :  1177
total : 5000  current step :  1178
total : 5000  current step :  1179
total : 5000  current step :  1180
total : 5000  current step :  1181
total : 5000  current step :  1182
total : 5000  current step :  1183
total : 5000  current step :  1184
total : 5000  current step :  1185
total : 5000  current step :  1186
total : 5000  current step :  1187
total : 5000  current step :  1188
total : 5000  current step :  1189
total : 5000  current step :  1190
total : 5000  current step :  1191
total : 5000  current step :  1192
total : 5000  current step :  1193
total : 5000  current step :  1194
total : 5000  current step :  1195
total : 5000  current step :  1196
total : 5000  current step :  1197
total : 5000  current step :  1198
total : 5000  current step :  1199
total : 5000  current step :  1200
total : 5000  current step :  1201
total : 5000  current step :  1202
total : 5000  current step :  1203
total : 5000  current step :  1204
total : 5000  current step :  1205
total : 5000  current step :  1206
total : 5000  current step :  1207
total : 5000  current step :  1208
total : 5000  current step :  1209
total : 5000  current step :  1210
total : 5000  current step :  1211
total : 5000  current step :  1212
total : 5000  current step :  1213
total : 5000  current step :  1214
total : 5000  current step :  1215
total : 5000  current step :  1216
total : 5000  current step :  1217
total : 5000  current step :  1218
total : 5000  current step :  1219
total : 5000  current step :  1220
total : 5000  current step :  1221
total : 5000  current step :  1222
total : 5000  current step :  1223
total : 5000  current step :  1224
total : 5000  current step :  1225
total : 5000  current step :  1226
total : 5000  current step :  1227
total : 5000  current step :  1228
total : 5000  current step :  1229
total : 5000  current step :  1230
total : 5000  current step :  1231
total : 5000  current step :  1232
total : 5000  current step :  1233
total : 5000  current step :  1234
total : 5000  current step :  1235
total : 5000  current step :  1236
total : 5000  current step :  1237
total : 5000  current step :  1238
total : 5000  current step :  1239
total : 5000  current step :  1240
total : 5000  current step :  1241
total : 5000  current step :  1242
total : 5000  current step :  1243
total : 5000  current step :  1244
total : 5000  current step :  1245
total : 5000  current step :  1246
total : 5000  current step :  1247
total : 5000  current step :  1248
total : 5000  current step :  1249
total : 5000  current step :  1250
total : 5000  current step :  1251
total : 5000  current step :  1252
total : 5000  current step :  1253
total : 5000  current step :  1254
total : 5000  current step :  1255
total : 5000  current step :  1256
total : 5000  current step :  1257
total : 5000  current step :  1258
total : 5000  current step :  1259
total : 5000  current step :  1260
total : 5000  current step :  1261
total : 5000  current step :  1262
total : 5000  current step :  1263
total : 5000  current step :  1264
total : 5000  current step :  1265
total : 5000  current step :  1266
total : 5000  current step :  1267
total : 5000  current step :  1268
total : 5000  current step :  1269
total : 5000  current step :  1270
total : 5000  current step :  1271
total : 5000  current step :  1272
total : 5000  current step :  1273
total : 5000  current step :  1274
total : 5000  current step :  1275
total : 5000  current step :  1276
total : 5000  current step :  1277
total : 5000  current step :  1278
total : 5000  current step :  1279
total : 5000  current step :  1280
total : 5000  current step :  1281
total : 5000  current step :  1282
total : 5000  current step :  1283
total : 5000  current step :  1284
total : 5000  current step :  1285
total : 5000  current step :  1286
total : 5000  current step :  1287
total : 5000  current step :  1288
total : 5000  current step :  1289
total : 5000  current step :  1290
total : 5000  current step :  1291
total : 5000  current step :  1292
total : 5000  current step :  1293
total : 5000  current step :  1294
total : 5000  current step :  1295
total : 5000  current step :  1296
total : 5000  current step :  1297
total : 5000  current step :  1298
total : 5000  current step :  1299
total : 5000  current step :  1300
total : 5000  current step :  1301
total : 5000  current step :  1302
total : 5000  current step :  1303
total : 5000  current step :  1304
total : 5000  current step :  1305
total : 5000  current step :  1306
total : 5000  current step :  1307
total : 5000  current step :  1308
total : 5000  current step :  1309
total : 5000  current step :  1310
total : 5000  current step :  1311
total : 5000  current step :  1312
total : 5000  current step :  1313
total : 5000  current step :  1314
total : 5000  current step :  1315
total : 5000  current step :  1316
total : 5000  current step :  1317
total : 5000  current step :  1318
total : 5000  current step :  1319
total : 5000  current step :  1320
total : 5000  current step :  1321
total : 5000  current step :  1322
total : 5000  current step :  1323
total : 5000  current step :  1324
total : 5000  current step :  1325
total : 5000  current step :  1326
total : 5000  current step :  1327
total : 5000  current step :  1328
total : 5000  current step :  1329
total : 5000  current step :  1330
total : 5000  current step :  1331
total : 5000  current step :  1332
total : 5000  current step :  1333
total : 5000  current step :  1334
total : 5000  current step :  1335
total : 5000  current step :  1336
total : 5000  current step :  1337
total : 5000  current step :  1338
total : 5000  current step :  1339
total : 5000  current step :  1340
total : 5000  current step :  1341
total : 5000  current step :  1342
total : 5000  current step :  1343
total : 5000  current step :  1344
total : 5000  current step :  1345
total : 5000  current step :  1346
total : 5000  current step :  1347
total : 5000  current step :  1348
total : 5000  current step :  1349
total : 5000  current step :  1350
total : 5000  current step :  1351
total : 5000  current step :  1352
total : 5000  current step :  1353
total : 5000  current step :  1354
total : 5000  current step :  1355
total : 5000  current step :  1356
total : 5000  current step :  1357
total : 5000  current step :  1358
total : 5000  current step :  1359
total : 5000  current step :  1360
total : 5000  current step :  1361
total : 5000  current step :  1362
total : 5000  current step :  1363
total : 5000  current step :  1364
total : 5000  current step :  1365
total : 5000  current step :  1366
total : 5000  current step :  1367
total : 5000  current step :  1368
total : 5000  current step :  1369
total : 5000  current step :  1370
total : 5000  current step :  1371
total : 5000  current step :  1372
total : 5000  current step :  1373
total : 5000  current step :  1374
total : 5000  current step :  1375
total : 5000  current step :  1376
total : 5000  current step :  1377
total : 5000  current step :  1378
total : 5000  current step :  1379
total : 5000  current step :  1380
total : 5000  current step :  1381
total : 5000  current step :  1382
total : 5000  current step :  1383
total : 5000  current step :  1384
total : 5000  current step :  1385
total : 5000  current step :  1386
total : 5000  current step :  1387
total : 5000  current step :  1388
total : 5000  current step :  1389
total : 5000  current step :  1390
total : 5000  current step :  1391
total : 5000  current step :  1392
total : 5000  current step :  1393
total : 5000  current step :  1394
total : 5000  current step :  1395
total : 5000  current step :  1396
total : 5000  current step :  1397
total : 5000  current step :  1398
total : 5000  current step :  1399
total : 5000  current step :  1400
total : 5000  current step :  1401
total : 5000  current step :  1402
total : 5000  current step :  1403
total : 5000  current step :  1404
total : 5000  current step :  1405
total : 5000  current step :  1406
total : 5000  current step :  1407
total : 5000  current step :  1408
total : 5000  current step :  1409
total : 5000  current step :  1410
total : 5000  current step :  1411
total : 5000  current step :  1412
total : 5000  current step :  1413
total : 5000  current step :  1414
total : 5000  current step :  1415
total : 5000  current step :  1416
total : 5000  current step :  1417
total : 5000  current step :  1418
total : 5000  current step :  1419
total : 5000  current step :  1420
total : 5000  current step :  1421
total : 5000  current step :  1422
total : 5000  current step :  1423
total : 5000  current step :  1424
total : 5000  current step :  1425
total : 5000  current step :  1426
total : 5000  current step :  1427
total : 5000  current step :  1428
total : 5000  current step :  1429
total : 5000  current step :  1430
total : 5000  current step :  1431
total : 5000  current step :  1432
total : 5000  current step :  1433
total : 5000  current step :  1434
total : 5000  current step :  1435
total : 5000  current step :  1436
total : 5000  current step :  1437
total : 5000  current step :  1438
total : 5000  current step :  1439
total : 5000  current step :  1440
total : 5000  current step :  1441
total : 5000  current step :  1442
total : 5000  current step :  1443
total : 5000  current step :  1444
total : 5000  current step :  1445
total : 5000  current step :  1446
total : 5000  current step :  1447
total : 5000  current step :  1448
total : 5000  current step :  1449
total : 5000  current step :  1450
total : 5000  current step :  1451
total : 5000  current step :  1452
total : 5000  current step :  1453
total : 5000  current step :  1454
total : 5000  current step :  1455
total : 5000  current step :  1456
total : 5000  current step :  1457
total : 5000  current step :  1458
total : 5000  current step :  1459
total : 5000  current step :  1460
total : 5000  current step :  1461
total : 5000  current step :  1462
total : 5000  current step :  1463
total : 5000  current step :  1464
total : 5000  current step :  1465
total : 5000  current step :  1466
total : 5000  current step :  1467
total : 5000  current step :  1468
total : 5000  current step :  1469
total : 5000  current step :  1470
total : 5000  current step :  1471
total : 5000  current step :  1472
total : 5000  current step :  1473
total : 5000  current step :  1474
total : 5000  current step :  1475
total : 5000  current step :  1476
total : 5000  current step :  1477
total : 5000  current step :  1478
total : 5000  current step :  1479
total : 5000  current step :  1480
total : 5000  current step :  1481
total : 5000  current step :  1482
total : 5000  current step :  1483
total : 5000  current step :  1484
total : 5000  current step :  1485
total : 5000  current step :  1486
total : 5000  current step :  1487
total : 5000  current step :  1488
total : 5000  current step :  1489
total : 5000  current step :  1490
total : 5000  current step :  1491
total : 5000  current step :  1492
total : 5000  current step :  1493
total : 5000  current step :  1494
total : 5000  current step :  1495
total : 5000  current step :  1496
total : 5000  current step :  1497
total : 5000  current step :  1498
total : 5000  current step :  1499
total : 5000  current step :  1500
total : 5000  current step :  1501
total : 5000  current step :  1502
total : 5000  current step :  1503
total : 5000  current step :  1504
total : 5000  current step :  1505
total : 5000  current step :  1506
total : 5000  current step :  1507
total : 5000  current step :  1508
total : 5000  current step :  1509
total : 5000  current step :  1510
total : 5000  current step :  1511
total : 5000  current step :  1512
total : 5000  current step :  1513
total : 5000  current step :  1514
total : 5000  current step :  1515
total : 5000  current step :  1516
total : 5000  current step :  1517
total : 5000  current step :  1518
total : 5000  current step :  1519
total : 5000  current step :  1520
total : 5000  current step :  1521
total : 5000  current step :  1522
total : 5000  current step :  1523
total : 5000  current step :  1524
total : 5000  current step :  1525
total : 5000  current step :  1526
total : 5000  current step :  1527
total : 5000  current step :  1528
total : 5000  current step :  1529
total : 5000  current step :  1530
total : 5000  current step :  1531
total : 5000  current step :  1532
total : 5000  current step :  1533
total : 5000  current step :  1534
total : 5000  current step :  1535
total : 5000  current step :  1536
total : 5000  current step :  1537
total : 5000  current step :  1538
total : 5000  current step :  1539
total : 5000  current step :  1540
total : 5000  current step :  1541
total : 5000  current step :  1542
total : 5000  current step :  1543
total : 5000  current step :  1544
total : 5000  current step :  1545
total : 5000  current step :  1546
total : 5000  current step :  1547
total : 5000  current step :  1548
total : 5000  current step :  1549
total : 5000  current step :  1550
total : 5000  current step :  1551
total : 5000  current step :  1552
total : 5000  current step :  1553
total : 5000  current step :  1554
total : 5000  current step :  1555
total : 5000  current step :  1556
total : 5000  current step :  1557
total : 5000  current step :  1558
total : 5000  current step :  1559
total : 5000  current step :  1560
total : 5000  current step :  1561
total : 5000  current step :  1562
total : 5000  current step :  1563
total : 5000  current step :  1564
total : 5000  current step :  1565
total : 5000  current step :  1566
total : 5000  current step :  1567
total : 5000  current step :  1568
total : 5000  current step :  1569
total : 5000  current step :  1570
total : 5000  current step :  1571
total : 5000  current step :  1572
total : 5000  current step :  1573
total : 5000  current step :  1574
total : 5000  current step :  1575
total : 5000  current step :  1576
total : 5000  current step :  1577
total : 5000  current step :  1578
total : 5000  current step :  1579
total : 5000  current step :  1580
total : 5000  current step :  1581
total : 5000  current step :  1582
total : 5000  current step :  1583
total : 5000  current step :  1584
total : 5000  current step :  1585
total : 5000  current step :  1586
total : 5000  current step :  1587
total : 5000  current step :  1588
total : 5000  current step :  1589
total : 5000  current step :  1590
total : 5000  current step :  1591
total : 5000  current step :  1592
total : 5000  current step :  1593
total : 5000  current step :  1594
total : 5000  current step :  1595
total : 5000  current step :  1596
total : 5000  current step :  1597
total : 5000  current step :  1598
total : 5000  current step :  1599
total : 5000  current step :  1600
total : 5000  current step :  1601
total : 5000  current step :  1602
total : 5000  current step :  1603
total : 5000  current step :  1604
total : 5000  current step :  1605
total : 5000  current step :  1606
total : 5000  current step :  1607
total : 5000  current step :  1608
total : 5000  current step :  1609
total : 5000  current step :  1610
total : 5000  current step :  1611
total : 5000  current step :  1612
total : 5000  current step :  1613
total : 5000  current step :  1614
total : 5000  current step :  1615
total : 5000  current step :  1616
total : 5000  current step :  1617
total : 5000  current step :  1618
total : 5000  current step :  1619
total : 5000  current step :  1620
total : 5000  current step :  1621
total : 5000  current step :  1622
total : 5000  current step :  1623
total : 5000  current step :  1624
total : 5000  current step :  1625
total : 5000  current step :  1626
total : 5000  current step :  1627
total : 5000  current step :  1628
total : 5000  current step :  1629
total : 5000  current step :  1630
total : 5000  current step :  1631
total : 5000  current step :  1632
total : 5000  current step :  1633
total : 5000  current step :  1634
total : 5000  current step :  1635
total : 5000  current step :  1636
total : 5000  current step :  1637
total : 5000  current step :  1638
total : 5000  current step :  1639
total : 5000  current step :  1640
total : 5000  current step :  1641
total : 5000  current step :  1642
total : 5000  current step :  1643
total : 5000  current step :  1644
total : 5000  current step :  1645
total : 5000  current step :  1646
total : 5000  current step :  1647
total : 5000  current step :  1648
total : 5000  current step :  1649
total : 5000  current step :  1650
total : 5000  current step :  1651
total : 5000  current step :  1652
total : 5000  current step :  1653
total : 5000  current step :  1654
total : 5000  current step :  1655
total : 5000  current step :  1656
total : 5000  current step :  1657
total : 5000  current step :  1658
total : 5000  current step :  1659
total : 5000  current step :  1660
total : 5000  current step :  1661
total : 5000  current step :  1662
total : 5000  current step :  1663
total : 5000  current step :  1664
total : 5000  current step :  1665
total : 5000  current step :  1666
total : 5000  current step :  1667
total : 5000  current step :  1668
total : 5000  current step :  1669
total : 5000  current step :  1670
total : 5000  current step :  1671
total : 5000  current step :  1672
total : 5000  current step :  1673
total : 5000  current step :  1674
total : 5000  current step :  1675
total : 5000  current step :  1676
total : 5000  current step :  1677
total : 5000  current step :  1678
total : 5000  current step :  1679
total : 5000  current step :  1680
total : 5000  current step :  1681
total : 5000  current step :  1682
total : 5000  current step :  1683
total : 5000  current step :  1684
total : 5000  current step :  1685
total : 5000  current step :  1686
total : 5000  current step :  1687
total : 5000  current step :  1688
total : 5000  current step :  1689
total : 5000  current step :  1690
total : 5000  current step :  1691
total : 5000  current step :  1692
total : 5000  current step :  1693
total : 5000  current step :  1694
total : 5000  current step :  1695
total : 5000  current step :  1696
total : 5000  current step :  1697
total : 5000  current step :  1698
total : 5000  current step :  1699
total : 5000  current step :  1700
total : 5000  current step :  1701
total : 5000  current step :  1702
total : 5000  current step :  1703
total : 5000  current step :  1704
total : 5000  current step :  1705
total : 5000  current step :  1706
total : 5000  current step :  1707
total : 5000  current step :  1708
total : 5000  current step :  1709
total : 5000  current step :  1710
total : 5000  current step :  1711
total : 5000  current step :  1712
total : 5000  current step :  1713
total : 5000  current step :  1714
total : 5000  current step :  1715
total : 5000  current step :  1716
total : 5000  current step :  1717
total : 5000  current step :  1718
total : 5000  current step :  1719
total : 5000  current step :  1720
total : 5000  current step :  1721
total : 5000  current step :  1722
total : 5000  current step :  1723
total : 5000  current step :  1724
total : 5000  current step :  1725
total : 5000  current step :  1726
total : 5000  current step :  1727
total : 5000  current step :  1728
total : 5000  current step :  1729
total : 5000  current step :  1730
total : 5000  current step :  1731
total : 5000  current step :  1732
total : 5000  current step :  1733
total : 5000  current step :  1734
total : 5000  current step :  1735
total : 5000  current step :  1736
total : 5000  current step :  1737
total : 5000  current step :  1738
total : 5000  current step :  1739
total : 5000  current step :  1740
total : 5000  current step :  1741
total : 5000  current step :  1742
total : 5000  current step :  1743
total : 5000  current step :  1744
total : 5000  current step :  1745
total : 5000  current step :  1746
total : 5000  current step :  1747
total : 5000  current step :  1748
total : 5000  current step :  1749
total : 5000  current step :  1750
total : 5000  current step :  1751
total : 5000  current step :  1752
total : 5000  current step :  1753
total : 5000  current step :  1754
total : 5000  current step :  1755
total : 5000  current step :  1756
total : 5000  current step :  1757
total : 5000  current step :  1758
total : 5000  current step :  1759
total : 5000  current step :  1760
total : 5000  current step :  1761
total : 5000  current step :  1762
total : 5000  current step :  1763
total : 5000  current step :  1764
total : 5000  current step :  1765
total : 5000  current step :  1766
total : 5000  current step :  1767
total : 5000  current step :  1768
total : 5000  current step :  1769
total : 5000  current step :  1770
total : 5000  current step :  1771
total : 5000  current step :  1772
total : 5000  current step :  1773
total : 5000  current step :  1774
total : 5000  current step :  1775
total : 5000  current step :  1776
total : 5000  current step :  1777
total : 5000  current step :  1778
total : 5000  current step :  1779
total : 5000  current step :  1780
total : 5000  current step :  1781
total : 5000  current step :  1782
total : 5000  current step :  1783
total : 5000  current step :  1784
total : 5000  current step :  1785
total : 5000  current step :  1786
total : 5000  current step :  1787
total : 5000  current step :  1788
total : 5000  current step :  1789
total : 5000  current step :  1790
total : 5000  current step :  1791
total : 5000  current step :  1792
total : 5000  current step :  1793
total : 5000  current step :  1794
total : 5000  current step :  1795
total : 5000  current step :  1796
total : 5000  current step :  1797
total : 5000  current step :  1798
total : 5000  current step :  1799
total : 5000  current step :  1800
total : 5000  current step :  1801
total : 5000  current step :  1802
total : 5000  current step :  1803
total : 5000  current step :  1804
total : 5000  current step :  1805
total : 5000  current step :  1806
total : 5000  current step :  1807
total : 5000  current step :  1808
total : 5000  current step :  1809
total : 5000  current step :  1810
total : 5000  current step :  1811
total : 5000  current step :  1812
total : 5000  current step :  1813
total : 5000  current step :  1814
total : 5000  current step :  1815
total : 5000  current step :  1816
total : 5000  current step :  1817
total : 5000  current step :  1818
total : 5000  current step :  1819
total : 5000  current step :  1820
total : 5000  current step :  1821
total : 5000  current step :  1822
total : 5000  current step :  1823
total : 5000  current step :  1824
total : 5000  current step :  1825
total : 5000  current step :  1826
total : 5000  current step :  1827
total : 5000  current step :  1828
total : 5000  current step :  1829
total : 5000  current step :  1830
total : 5000  current step :  1831
total : 5000  current step :  1832
total : 5000  current step :  1833
total : 5000  current step :  1834
total : 5000  current step :  1835
total : 5000  current step :  1836
total : 5000  current step :  1837
total : 5000  current step :  1838
total : 5000  current step :  1839
total : 5000  current step :  1840
total : 5000  current step :  1841
total : 5000  current step :  1842
total : 5000  current step :  1843
total : 5000  current step :  1844
total : 5000  current step :  1845
total : 5000  current step :  1846
total : 5000  current step :  1847
total : 5000  current step :  1848
total : 5000  current step :  1849
total : 5000  current step :  1850
total : 5000  current step :  1851
total : 5000  current step :  1852
total : 5000  current step :  1853
total : 5000  current step :  1854
total : 5000  current step :  1855
total : 5000  current step :  1856
total : 5000  current step :  1857
total : 5000  current step :  1858
total : 5000  current step :  1859
total : 5000  current step :  1860
total : 5000  current step :  1861
total : 5000  current step :  1862
total : 5000  current step :  1863
total : 5000  current step :  1864
total : 5000  current step :  1865
total : 5000  current step :  1866
total : 5000  current step :  1867
total : 5000  current step :  1868
total : 5000  current step :  1869
total : 5000  current step :  1870
total : 5000  current step :  1871
total : 5000  current step :  1872
total : 5000  current step :  1873
total : 5000  current step :  1874
total : 5000  current step :  1875
total : 5000  current step :  1876
total : 5000  current step :  1877
total : 5000  current step :  1878
total : 5000  current step :  1879
total : 5000  current step :  1880
total : 5000  current step :  1881
total : 5000  current step :  1882
total : 5000  current step :  1883
total : 5000  current step :  1884
total : 5000  current step :  1885
total : 5000  current step :  1886
total : 5000  current step :  1887
total : 5000  current step :  1888
total : 5000  current step :  1889
total : 5000  current step :  1890
total : 5000  current step :  1891
total : 5000  current step :  1892
total : 5000  current step :  1893
total : 5000  current step :  1894
total : 5000  current step :  1895
total : 5000  current step :  1896
total : 5000  current step :  1897
total : 5000  current step :  1898
total : 5000  current step :  1899
total : 5000  current step :  1900
total : 5000  current step :  1901
total : 5000  current step :  1902
total : 5000  current step :  1903
total : 5000  current step :  1904
total : 5000  current step :  1905
total : 5000  current step :  1906
total : 5000  current step :  1907
total : 5000  current step :  1908
total : 5000  current step :  1909
total : 5000  current step :  1910
total : 5000  current step :  1911
total : 5000  current step :  1912
total : 5000  current step :  1913
total : 5000  current step :  1914
total : 5000  current step :  1915
total : 5000  current step :  1916
total : 5000  current step :  1917
total : 5000  current step :  1918
total : 5000  current step :  1919
total : 5000  current step :  1920
total : 5000  current step :  1921
total : 5000  current step :  1922
total : 5000  current step :  1923
total : 5000  current step :  1924
total : 5000  current step :  1925
total : 5000  current step :  1926
total : 5000  current step :  1927
total : 5000  current step :  1928
total : 5000  current step :  1929
total : 5000  current step :  1930
total : 5000  current step :  1931
total : 5000  current step :  1932
total : 5000  current step :  1933
total : 5000  current step :  1934
total : 5000  current step :  1935
total : 5000  current step :  1936
total : 5000  current step :  1937
total : 5000  current step :  1938
total : 5000  current step :  1939
total : 5000  current step :  1940
total : 5000  current step :  1941
total : 5000  current step :  1942
total : 5000  current step :  1943
total : 5000  current step :  1944
total : 5000  current step :  1945
total : 5000  current step :  1946
total : 5000  current step :  1947
total : 5000  current step :  1948
total : 5000  current step :  1949
total : 5000  current step :  1950
total : 5000  current step :  1951
total : 5000  current step :  1952
total : 5000  current step :  1953
total : 5000  current step :  1954
total : 5000  current step :  1955
total : 5000  current step :  1956
total : 5000  current step :  1957
total : 5000  current step :  1958
total : 5000  current step :  1959
total : 5000  current step :  1960
total : 5000  current step :  1961
total : 5000  current step :  1962
total : 5000  current step :  1963
total : 5000  current step :  1964
total : 5000  current step :  1965
total : 5000  current step :  1966
total : 5000  current step :  1967
total : 5000  current step :  1968
total : 5000  current step :  1969
total : 5000  current step :  1970
total : 5000  current step :  1971
total : 5000  current step :  1972
total : 5000  current step :  1973
total : 5000  current step :  1974
total : 5000  current step :  1975
total : 5000  current step :  1976
total : 5000  current step :  1977
total : 5000  current step :  1978
total : 5000  current step :  1979
total : 5000  current step :  1980
total : 5000  current step :  1981
total : 5000  current step :  1982
total : 5000  current step :  1983
total : 5000  current step :  1984
total : 5000  current step :  1985
total : 5000  current step :  1986
total : 5000  current step :  1987
total : 5000  current step :  1988
total : 5000  current step :  1989
total : 5000  current step :  1990
total : 5000  current step :  1991
total : 5000  current step :  1992
total : 5000  current step :  1993
total : 5000  current step :  1994
total : 5000  current step :  1995
total : 5000  current step :  1996
total : 5000  current step :  1997
total : 5000  current step :  1998
total : 5000  current step :  1999
total : 5000  current step :  2000
total : 5000  current step :  2001
total : 5000  current step :  2002
total : 5000  current step :  2003
total : 5000  current step :  2004
total : 5000  current step :  2005
total : 5000  current step :  2006
total : 5000  current step :  2007
total : 5000  current step :  2008
total : 5000  current step :  2009
total : 5000  current step :  2010
total : 5000  current step :  2011
total : 5000  current step :  2012
total : 5000  current step :  2013
total : 5000  current step :  2014
total : 5000  current step :  2015
total : 5000  current step :  2016
total : 5000  current step :  2017
total : 5000  current step :  2018
total : 5000  current step :  2019
total : 5000  current step :  2020
total : 5000  current step :  2021
total : 5000  current step :  2022
total : 5000  current step :  2023
total : 5000  current step :  2024
total : 5000  current step :  2025
total : 5000  current step :  2026
total : 5000  current step :  2027
total : 5000  current step :  2028
total : 5000  current step :  2029
total : 5000  current step :  2030
total : 5000  current step :  2031
total : 5000  current step :  2032
total : 5000  current step :  2033
total : 5000  current step :  2034
total : 5000  current step :  2035
total : 5000  current step :  2036
total : 5000  current step :  2037
total : 5000  current step :  2038
total : 5000  current step :  2039
total : 5000  current step :  2040
total : 5000  current step :  2041
total : 5000  current step :  2042
total : 5000  current step :  2043
total : 5000  current step :  2044
total : 5000  current step :  2045
total : 5000  current step :  2046
total : 5000  current step :  2047
total : 5000  current step :  2048
total : 5000  current step :  2049
total : 5000  current step :  2050
total : 5000  current step :  2051
total : 5000  current step :  2052
total : 5000  current step :  2053
total : 5000  current step :  2054
total : 5000  current step :  2055
total : 5000  current step :  2056
total : 5000  current step :  2057
total : 5000  current step :  2058
total : 5000  current step :  2059
total : 5000  current step :  2060
total : 5000  current step :  2061
total : 5000  current step :  2062
total : 5000  current step :  2063
total : 5000  current step :  2064
total : 5000  current step :  2065
total : 5000  current step :  2066
total : 5000  current step :  2067
total : 5000  current step :  2068
total : 5000  current step :  2069
total : 5000  current step :  2070
total : 5000  current step :  2071
total : 5000  current step :  2072
total : 5000  current step :  2073
total : 5000  current step :  2074
total : 5000  current step :  2075
total : 5000  current step :  2076
total : 5000  current step :  2077
total : 5000  current step :  2078
total : 5000  current step :  2079
total : 5000  current step :  2080
total : 5000  current step :  2081
total : 5000  current step :  2082
total : 5000  current step :  2083
total : 5000  current step :  2084
total : 5000  current step :  2085
total : 5000  current step :  2086
total : 5000  current step :  2087
total : 5000  current step :  2088
total : 5000  current step :  2089
total : 5000  current step :  2090
total : 5000  current step :  2091
total : 5000  current step :  2092
total : 5000  current step :  2093
total : 5000  current step :  2094
total : 5000  current step :  2095
total : 5000  current step :  2096
total : 5000  current step :  2097
total : 5000  current step :  2098
total : 5000  current step :  2099
total : 5000  current step :  2100
total : 5000  current step :  2101
total : 5000  current step :  2102
total : 5000  current step :  2103
total : 5000  current step :  2104
total : 5000  current step :  2105
total : 5000  current step :  2106
total : 5000  current step :  2107
total : 5000  current step :  2108
total : 5000  current step :  2109
total : 5000  current step :  2110
total : 5000  current step :  2111
total : 5000  current step :  2112
total : 5000  current step :  2113
total : 5000  current step :  2114
total : 5000  current step :  2115
total : 5000  current step :  2116
total : 5000  current step :  2117
total : 5000  current step :  2118
total : 5000  current step :  2119
total : 5000  current step :  2120
total : 5000  current step :  2121
total : 5000  current step :  2122
total : 5000  current step :  2123
total : 5000  current step :  2124
total : 5000  current step :  2125
total : 5000  current step :  2126
total : 5000  current step :  2127
total : 5000  current step :  2128
total : 5000  current step :  2129
total : 5000  current step :  2130
total : 5000  current step :  2131
total : 5000  current step :  2132
total : 5000  current step :  2133
total : 5000  current step :  2134
total : 5000  current step :  2135
total : 5000  current step :  2136
total : 5000  current step :  2137
total : 5000  current step :  2138
total : 5000  current step :  2139
total : 5000  current step :  2140
total : 5000  current step :  2141
total : 5000  current step :  2142
total : 5000  current step :  2143
total : 5000  current step :  2144
total : 5000  current step :  2145
total : 5000  current step :  2146
total : 5000  current step :  2147
total : 5000  current step :  2148
total : 5000  current step :  2149
total : 5000  current step :  2150
total : 5000  current step :  2151
total : 5000  current step :  2152
total : 5000  current step :  2153
total : 5000  current step :  2154
total : 5000  current step :  2155
total : 5000  current step :  2156
total : 5000  current step :  2157
total : 5000  current step :  2158
total : 5000  current step :  2159
total : 5000  current step :  2160
total : 5000  current step :  2161
total : 5000  current step :  2162
total : 5000  current step :  2163
total : 5000  current step :  2164
total : 5000  current step :  2165
total : 5000  current step :  2166
total : 5000  current step :  2167
total : 5000  current step :  2168
total : 5000  current step :  2169
total : 5000  current step :  2170
total : 5000  current step :  2171
total : 5000  current step :  2172
total : 5000  current step :  2173
total : 5000  current step :  2174
total : 5000  current step :  2175
total : 5000  current step :  2176
total : 5000  current step :  2177
total : 5000  current step :  2178
total : 5000  current step :  2179
total : 5000  current step :  2180
total : 5000  current step :  2181
total : 5000  current step :  2182
total : 5000  current step :  2183
total : 5000  current step :  2184
total : 5000  current step :  2185
total : 5000  current step :  2186
total : 5000  current step :  2187
total : 5000  current step :  2188
total : 5000  current step :  2189
total : 5000  current step :  2190
total : 5000  current step :  2191
total : 5000  current step :  2192
total : 5000  current step :  2193
total : 5000  current step :  2194
total : 5000  current step :  2195
total : 5000  current step :  2196
total : 5000  current step :  2197
total : 5000  current step :  2198
total : 5000  current step :  2199
total : 5000  current step :  2200
total : 5000  current step :  2201
total : 5000  current step :  2202
total : 5000  current step :  2203
total : 5000  current step :  2204
total : 5000  current step :  2205
total : 5000  current step :  2206
total : 5000  current step :  2207
total : 5000  current step :  2208
total : 5000  current step :  2209
total : 5000  current step :  2210
total : 5000  current step :  2211
total : 5000  current step :  2212
total : 5000  current step :  2213
total : 5000  current step :  2214
total : 5000  current step :  2215
total : 5000  current step :  2216
total : 5000  current step :  2217
total : 5000  current step :  2218
total : 5000  current step :  2219
total : 5000  current step :  2220
total : 5000  current step :  2221
total : 5000  current step :  2222
total : 5000  current step :  2223
total : 5000  current step :  2224
total : 5000  current step :  2225
total : 5000  current step :  2226
total : 5000  current step :  2227
total : 5000  current step :  2228
total : 5000  current step :  2229
total : 5000  current step :  2230
total : 5000  current step :  2231
total : 5000  current step :  2232
total : 5000  current step :  2233
total : 5000  current step :  2234
total : 5000  current step :  2235
total : 5000  current step :  2236
total : 5000  current step :  2237
total : 5000  current step :  2238
total : 5000  current step :  2239
total : 5000  current step :  2240
total : 5000  current step :  2241
total : 5000  current step :  2242
total : 5000  current step :  2243
total : 5000  current step :  2244
total : 5000  current step :  2245
total : 5000  current step :  2246
total : 5000  current step :  2247
total : 5000  current step :  2248
total : 5000  current step :  2249
total : 5000  current step :  2250
total : 5000  current step :  2251
total : 5000  current step :  2252
total : 5000  current step :  2253
total : 5000  current step :  2254
total : 5000  current step :  2255
total : 5000  current step :  2256
total : 5000  current step :  2257
total : 5000  current step :  2258
total : 5000  current step :  2259
total : 5000  current step :  2260
total : 5000  current step :  2261
total : 5000  current step :  2262
total : 5000  current step :  2263
total : 5000  current step :  2264
total : 5000  current step :  2265
total : 5000  current step :  2266
total : 5000  current step :  2267
total : 5000  current step :  2268
total : 5000  current step :  2269
total : 5000  current step :  2270
total : 5000  current step :  2271
total : 5000  current step :  2272
total : 5000  current step :  2273
total : 5000  current step :  2274
total : 5000  current step :  2275
total : 5000  current step :  2276
total : 5000  current step :  2277
total : 5000  current step :  2278
total : 5000  current step :  2279
total : 5000  current step :  2280
total : 5000  current step :  2281
total : 5000  current step :  2282
total : 5000  current step :  2283
total : 5000  current step :  2284
total : 5000  current step :  2285
total : 5000  current step :  2286
total : 5000  current step :  2287
total : 5000  current step :  2288
total : 5000  current step :  2289
total : 5000  current step :  2290
total : 5000  current step :  2291
total : 5000  current step :  2292
total : 5000  current step :  2293
total : 5000  current step :  2294
total : 5000  current step :  2295
total : 5000  current step :  2296
total : 5000  current step :  2297
total : 5000  current step :  2298
total : 5000  current step :  2299
total : 5000  current step :  2300
total : 5000  current step :  2301
total : 5000  current step :  2302
total : 5000  current step :  2303
total : 5000  current step :  2304
total : 5000  current step :  2305
total : 5000  current step :  2306
total : 5000  current step :  2307
total : 5000  current step :  2308
total : 5000  current step :  2309
total : 5000  current step :  2310
total : 5000  current step :  2311
total : 5000  current step :  2312
total : 5000  current step :  2313
total : 5000  current step :  2314
total : 5000  current step :  2315
total : 5000  current step :  2316
total : 5000  current step :  2317
total : 5000  current step :  2318
total : 5000  current step :  2319
total : 5000  current step :  2320
total : 5000  current step :  2321
total : 5000  current step :  2322
total : 5000  current step :  2323
total : 5000  current step :  2324
total : 5000  current step :  2325
total : 5000  current step :  2326
total : 5000  current step :  2327
total : 5000  current step :  2328
total : 5000  current step :  2329
total : 5000  current step :  2330
total : 5000  current step :  2331
total : 5000  current step :  2332
total : 5000  current step :  2333
total : 5000  current step :  2334
total : 5000  current step :  2335
total : 5000  current step :  2336
total : 5000  current step :  2337
total : 5000  current step :  2338
total : 5000  current step :  2339
total : 5000  current step :  2340
total : 5000  current step :  2341
total : 5000  current step :  2342
total : 5000  current step :  2343
total : 5000  current step :  2344
total : 5000  current step :  2345
total : 5000  current step :  2346
total : 5000  current step :  2347
total : 5000  current step :  2348
total : 5000  current step :  2349
total : 5000  current step :  2350
total : 5000  current step :  2351
total : 5000  current step :  2352
total : 5000  current step :  2353
total : 5000  current step :  2354
total : 5000  current step :  2355
total : 5000  current step :  2356
total : 5000  current step :  2357
total : 5000  current step :  2358
total : 5000  current step :  2359
total : 5000  current step :  2360
total : 5000  current step :  2361
total : 5000  current step :  2362
total : 5000  current step :  2363
total : 5000  current step :  2364
total : 5000  current step :  2365
total : 5000  current step :  2366
total : 5000  current step :  2367
total : 5000  current step :  2368
total : 5000  current step :  2369
total : 5000  current step :  2370
total : 5000  current step :  2371
total : 5000  current step :  2372
total : 5000  current step :  2373
total : 5000  current step :  2374
total : 5000  current step :  2375
total : 5000  current step :  2376
total : 5000  current step :  2377
total : 5000  current step :  2378
total : 5000  current step :  2379
total : 5000  current step :  2380
total : 5000  current step :  2381
total : 5000  current step :  2382
total : 5000  current step :  2383
total : 5000  current step :  2384
total : 5000  current step :  2385
total : 5000  current step :  2386
total : 5000  current step :  2387
total : 5000  current step :  2388
total : 5000  current step :  2389
total : 5000  current step :  2390
total : 5000  current step :  2391
total : 5000  current step :  2392
total : 5000  current step :  2393
total : 5000  current step :  2394
total : 5000  current step :  2395
total : 5000  current step :  2396
total : 5000  current step :  2397
total : 5000  current step :  2398
total : 5000  current step :  2399
total : 5000  current step :  2400
total : 5000  current step :  2401
total : 5000  current step :  2402
total : 5000  current step :  2403
total : 5000  current step :  2404
total : 5000  current step :  2405
total : 5000  current step :  2406
total : 5000  current step :  2407
total : 5000  current step :  2408
total : 5000  current step :  2409
total : 5000  current step :  2410
total : 5000  current step :  2411
total : 5000  current step :  2412
total : 5000  current step :  2413
total : 5000  current step :  2414
total : 5000  current step :  2415
total : 5000  current step :  2416
total : 5000  current step :  2417
total : 5000  current step :  2418
total : 5000  current step :  2419
total : 5000  current step :  2420
total : 5000  current step :  2421
total : 5000  current step :  2422
total : 5000  current step :  2423
total : 5000  current step :  2424
total : 5000  current step :  2425
total : 5000  current step :  2426
total : 5000  current step :  2427
total : 5000  current step :  2428
total : 5000  current step :  2429
total : 5000  current step :  2430
total : 5000  current step :  2431
total : 5000  current step :  2432
total : 5000  current step :  2433
total : 5000  current step :  2434
total : 5000  current step :  2435
total : 5000  current step :  2436
total : 5000  current step :  2437
total : 5000  current step :  2438
total : 5000  current step :  2439
total : 5000  current step :  2440
total : 5000  current step :  2441
total : 5000  current step :  2442
total : 5000  current step :  2443
total : 5000  current step :  2444
total : 5000  current step :  2445
total : 5000  current step :  2446
total : 5000  current step :  2447
total : 5000  current step :  2448
total : 5000  current step :  2449
total : 5000  current step :  2450
total : 5000  current step :  2451
total : 5000  current step :  2452
total : 5000  current step :  2453
total : 5000  current step :  2454
total : 5000  current step :  2455
total : 5000  current step :  2456
total : 5000  current step :  2457
total : 5000  current step :  2458
total : 5000  current step :  2459
total : 5000  current step :  2460
total : 5000  current step :  2461
total : 5000  current step :  2462
total : 5000  current step :  2463
total : 5000  current step :  2464
total : 5000  current step :  2465
total : 5000  current step :  2466
total : 5000  current step :  2467
total : 5000  current step :  2468
total : 5000  current step :  2469
total : 5000  current step :  2470
total : 5000  current step :  2471
total : 5000  current step :  2472
total : 5000  current step :  2473
total : 5000  current step :  2474
total : 5000  current step :  2475
total : 5000  current step :  2476
total : 5000  current step :  2477
total : 5000  current step :  2478
total : 5000  current step :  2479
total : 5000  current step :  2480
total : 5000  current step :  2481
total : 5000  current step :  2482
total : 5000  current step :  2483
total : 5000  current step :  2484
total : 5000  current step :  2485
total : 5000  current step :  2486
total : 5000  current step :  2487
total : 5000  current step :  2488
total : 5000  current step :  2489
total : 5000  current step :  2490
total : 5000  current step :  2491
total : 5000  current step :  2492
total : 5000  current step :  2493
total : 5000  current step :  2494
total : 5000  current step :  2495
total : 5000  current step :  2496
total : 5000  current step :  2497
total : 5000  current step :  2498
total : 5000  current step :  2499
total : 5000  current step :  2500
total : 5000  current step :  2501
total : 5000  current step :  2502
total : 5000  current step :  2503
total : 5000  current step :  2504
total : 5000  current step :  2505
total : 5000  current step :  2506
total : 5000  current step :  2507
total : 5000  current step :  2508
total : 5000  current step :  2509
total : 5000  current step :  2510
total : 5000  current step :  2511
total : 5000  current step :  2512
total : 5000  current step :  2513
total : 5000  current step :  2514
total : 5000  current step :  2515
total : 5000  current step :  2516
total : 5000  current step :  2517
total : 5000  current step :  2518
total : 5000  current step :  2519
total : 5000  current step :  2520
total : 5000  current step :  2521
total : 5000  current step :  2522
total : 5000  current step :  2523
total : 5000  current step :  2524
total : 5000  current step :  2525
total : 5000  current step :  2526
total : 5000  current step :  2527
total : 5000  current step :  2528
total : 5000  current step :  2529
total : 5000  current step :  2530
total : 5000  current step :  2531
total : 5000  current step :  2532
total : 5000  current step :  2533
total : 5000  current step :  2534
total : 5000  current step :  2535
total : 5000  current step :  2536
total : 5000  current step :  2537
total : 5000  current step :  2538
total : 5000  current step :  2539
total : 5000  current step :  2540
total : 5000  current step :  2541
total : 5000  current step :  2542
total : 5000  current step :  2543
total : 5000  current step :  2544
total : 5000  current step :  2545
total : 5000  current step :  2546
total : 5000  current step :  2547
total : 5000  current step :  2548
total : 5000  current step :  2549
total : 5000  current step :  2550
total : 5000  current step :  2551
total : 5000  current step :  2552
total : 5000  current step :  2553
total : 5000  current step :  2554
total : 5000  current step :  2555
total : 5000  current step :  2556
total : 5000  current step :  2557
total : 5000  current step :  2558
total : 5000  current step :  2559
total : 5000  current step :  2560
total : 5000  current step :  2561
total : 5000  current step :  2562
total : 5000  current step :  2563
total : 5000  current step :  2564
total : 5000  current step :  2565
total : 5000  current step :  2566
total : 5000  current step :  2567
total : 5000  current step :  2568
total : 5000  current step :  2569
total : 5000  current step :  2570
total : 5000  current step :  2571
total : 5000  current step :  2572
total : 5000  current step :  2573
total : 5000  current step :  2574
total : 5000  current step :  2575
total : 5000  current step :  2576
total : 5000  current step :  2577
total : 5000  current step :  2578
total : 5000  current step :  2579
total : 5000  current step :  2580
total : 5000  current step :  2581
total : 5000  current step :  2582
total : 5000  current step :  2583
total : 5000  current step :  2584
total : 5000  current step :  2585
total : 5000  current step :  2586
total : 5000  current step :  2587
total : 5000  current step :  2588
total : 5000  current step :  2589
total : 5000  current step :  2590
total : 5000  current step :  2591
total : 5000  current step :  2592
total : 5000  current step :  2593
total : 5000  current step :  2594
total : 5000  current step :  2595
total : 5000  current step :  2596
total : 5000  current step :  2597
total : 5000  current step :  2598
total : 5000  current step :  2599
total : 5000  current step :  2600
total : 5000  current step :  2601
total : 5000  current step :  2602
total : 5000  current step :  2603
total : 5000  current step :  2604
total : 5000  current step :  2605
total : 5000  current step :  2606
total : 5000  current step :  2607
total : 5000  current step :  2608
total : 5000  current step :  2609
total : 5000  current step :  2610
total : 5000  current step :  2611
total : 5000  current step :  2612
total : 5000  current step :  2613
total : 5000  current step :  2614
total : 5000  current step :  2615
total : 5000  current step :  2616
total : 5000  current step :  2617
total : 5000  current step :  2618
total : 5000  current step :  2619
total : 5000  current step :  2620
total : 5000  current step :  2621
total : 5000  current step :  2622
total : 5000  current step :  2623
total : 5000  current step :  2624
total : 5000  current step :  2625
total : 5000  current step :  2626
total : 5000  current step :  2627
total : 5000  current step :  2628
total : 5000  current step :  2629
total : 5000  current step :  2630
total : 5000  current step :  2631
total : 5000  current step :  2632
total : 5000  current step :  2633
total : 5000  current step :  2634
total : 5000  current step :  2635
total : 5000  current step :  2636
total : 5000  current step :  2637
total : 5000  current step :  2638
total : 5000  current step :  2639
total : 5000  current step :  2640
total : 5000  current step :  2641
total : 5000  current step :  2642
total : 5000  current step :  2643
total : 5000  current step :  2644
total : 5000  current step :  2645
total : 5000  current step :  2646
total : 5000  current step :  2647
total : 5000  current step :  2648
total : 5000  current step :  2649
total : 5000  current step :  2650
total : 5000  current step :  2651
total : 5000  current step :  2652
total : 5000  current step :  2653
total : 5000  current step :  2654
total : 5000  current step :  2655
total : 5000  current step :  2656
total : 5000  current step :  2657
total : 5000  current step :  2658
total : 5000  current step :  2659
total : 5000  current step :  2660
total : 5000  current step :  2661
total : 5000  current step :  2662
total : 5000  current step :  2663
total : 5000  current step :  2664
total : 5000  current step :  2665
total : 5000  current step :  2666
total : 5000  current step :  2667
total : 5000  current step :  2668
total : 5000  current step :  2669
total : 5000  current step :  2670
total : 5000  current step :  2671
total : 5000  current step :  2672
total : 5000  current step :  2673
total : 5000  current step :  2674
total : 5000  current step :  2675
total : 5000  current step :  2676
total : 5000  current step :  2677
total : 5000  current step :  2678
total : 5000  current step :  2679
total : 5000  current step :  2680
total : 5000  current step :  2681
total : 5000  current step :  2682
total : 5000  current step :  2683
total : 5000  current step :  2684
total : 5000  current step :  2685
total : 5000  current step :  2686
total : 5000  current step :  2687
total : 5000  current step :  2688
total : 5000  current step :  2689
total : 5000  current step :  2690
total : 5000  current step :  2691
total : 5000  current step :  2692
total : 5000  current step :  2693
total : 5000  current step :  2694
total : 5000  current step :  2695
total : 5000  current step :  2696
total : 5000  current step :  2697
total : 5000  current step :  2698
total : 5000  current step :  2699
total : 5000  current step :  2700
total : 5000  current step :  2701
total : 5000  current step :  2702
total : 5000  current step :  2703
total : 5000  current step :  2704
total : 5000  current step :  2705
total : 5000  current step :  2706
total : 5000  current step :  2707
total : 5000  current step :  2708
total : 5000  current step :  2709
total : 5000  current step :  2710
total : 5000  current step :  2711
total : 5000  current step :  2712
total : 5000  current step :  2713
total : 5000  current step :  2714
total : 5000  current step :  2715
total : 5000  current step :  2716
total : 5000  current step :  2717
total : 5000  current step :  2718
total : 5000  current step :  2719
total : 5000  current step :  2720
total : 5000  current step :  2721
total : 5000  current step :  2722
total : 5000  current step :  2723
total : 5000  current step :  2724
total : 5000  current step :  2725
total : 5000  current step :  2726
total : 5000  current step :  2727
total : 5000  current step :  2728
total : 5000  current step :  2729
total : 5000  current step :  2730
total : 5000  current step :  2731
total : 5000  current step :  2732
total : 5000  current step :  2733
total : 5000  current step :  2734
total : 5000  current step :  2735
total : 5000  current step :  2736
total : 5000  current step :  2737
total : 5000  current step :  2738
total : 5000  current step :  2739
total : 5000  current step :  2740
total : 5000  current step :  2741
total : 5000  current step :  2742
total : 5000  current step :  2743
total : 5000  current step :  2744
total : 5000  current step :  2745
total : 5000  current step :  2746
total : 5000  current step :  2747
total : 5000  current step :  2748
total : 5000  current step :  2749
total : 5000  current step :  2750
total : 5000  current step :  2751
total : 5000  current step :  2752
total : 5000  current step :  2753
total : 5000  current step :  2754
total : 5000  current step :  2755
total : 5000  current step :  2756
total : 5000  current step :  2757
total : 5000  current step :  2758
total : 5000  current step :  2759
total : 5000  current step :  2760
total : 5000  current step :  2761
total : 5000  current step :  2762
total : 5000  current step :  2763
total : 5000  current step :  2764
total : 5000  current step :  2765
total : 5000  current step :  2766
total : 5000  current step :  2767
total : 5000  current step :  2768
total : 5000  current step :  2769
total : 5000  current step :  2770
total : 5000  current step :  2771
total : 5000  current step :  2772
total : 5000  current step :  2773
total : 5000  current step :  2774
total : 5000  current step :  2775
total : 5000  current step :  2776
total : 5000  current step :  2777
total : 5000  current step :  2778
total : 5000  current step :  2779
total : 5000  current step :  2780
total : 5000  current step :  2781
total : 5000  current step :  2782
total : 5000  current step :  2783
total : 5000  current step :  2784
total : 5000  current step :  2785
total : 5000  current step :  2786
total : 5000  current step :  2787
total : 5000  current step :  2788
total : 5000  current step :  2789
total : 5000  current step :  2790
total : 5000  current step :  2791
total : 5000  current step :  2792
total : 5000  current step :  2793
total : 5000  current step :  2794
total : 5000  current step :  2795
total : 5000  current step :  2796
total : 5000  current step :  2797
total : 5000  current step :  2798
total : 5000  current step :  2799
total : 5000  current step :  2800
total : 5000  current step :  2801
total : 5000  current step :  2802
total : 5000  current step :  2803
total : 5000  current step :  2804
total : 5000  current step :  2805
total : 5000  current step :  2806
total : 5000  current step :  2807
total : 5000  current step :  2808
total : 5000  current step :  2809
total : 5000  current step :  2810
total : 5000  current step :  2811
total : 5000  current step :  2812
total : 5000  current step :  2813
total : 5000  current step :  2814
total : 5000  current step :  2815
total : 5000  current step :  2816
total : 5000  current step :  2817
total : 5000  current step :  2818
total : 5000  current step :  2819
total : 5000  current step :  2820
total : 5000  current step :  2821
total : 5000  current step :  2822
total : 5000  current step :  2823
total : 5000  current step :  2824
total : 5000  current step :  2825
total : 5000  current step :  2826
total : 5000  current step :  2827
total : 5000  current step :  2828
total : 5000  current step :  2829
total : 5000  current step :  2830
total : 5000  current step :  2831
total : 5000  current step :  2832
total : 5000  current step :  2833
total : 5000  current step :  2834
total : 5000  current step :  2835
total : 5000  current step :  2836
total : 5000  current step :  2837
total : 5000  current step :  2838
total : 5000  current step :  2839
total : 5000  current step :  2840
total : 5000  current step :  2841
total : 5000  current step :  2842
total : 5000  current step :  2843
total : 5000  current step :  2844
total : 5000  current step :  2845
total : 5000  current step :  2846
total : 5000  current step :  2847
total : 5000  current step :  2848
total : 5000  current step :  2849
total : 5000  current step :  2850
total : 5000  current step :  2851
total : 5000  current step :  2852
total : 5000  current step :  2853
total : 5000  current step :  2854
total : 5000  current step :  2855
total : 5000  current step :  2856
total : 5000  current step :  2857
total : 5000  current step :  2858
total : 5000  current step :  2859
total : 5000  current step :  2860
total : 5000  current step :  2861
total : 5000  current step :  2862
total : 5000  current step :  2863
total : 5000  current step :  2864
total : 5000  current step :  2865
total : 5000  current step :  2866
total : 5000  current step :  2867
total : 5000  current step :  2868
total : 5000  current step :  2869
total : 5000  current step :  2870
total : 5000  current step :  2871
total : 5000  current step :  2872
total : 5000  current step :  2873
total : 5000  current step :  2874
total : 5000  current step :  2875
total : 5000  current step :  2876
total : 5000  current step :  2877
total : 5000  current step :  2878
total : 5000  current step :  2879
total : 5000  current step :  2880
total : 5000  current step :  2881
total : 5000  current step :  2882
total : 5000  current step :  2883
total : 5000  current step :  2884
total : 5000  current step :  2885
total : 5000  current step :  2886
total : 5000  current step :  2887
total : 5000  current step :  2888
total : 5000  current step :  2889
total : 5000  current step :  2890
total : 5000  current step :  2891
total : 5000  current step :  2892
total : 5000  current step :  2893
total : 5000  current step :  2894
total : 5000  current step :  2895
total : 5000  current step :  2896
total : 5000  current step :  2897
total : 5000  current step :  2898
total : 5000  current step :  2899
total : 5000  current step :  2900
total : 5000  current step :  2901
total : 5000  current step :  2902
total : 5000  current step :  2903
total : 5000  current step :  2904
total : 5000  current step :  2905
total : 5000  current step :  2906
total : 5000  current step :  2907
total : 5000  current step :  2908
total : 5000  current step :  2909
total : 5000  current step :  2910
total : 5000  current step :  2911
total : 5000  current step :  2912
total : 5000  current step :  2913
total : 5000  current step :  2914
total : 5000  current step :  2915
total : 5000  current step :  2916
total : 5000  current step :  2917
total : 5000  current step :  2918
total : 5000  current step :  2919
total : 5000  current step :  2920
total : 5000  current step :  2921
total : 5000  current step :  2922
total : 5000  current step :  2923
total : 5000  current step :  2924
total : 5000  current step :  2925
total : 5000  current step :  2926
total : 5000  current step :  2927
total : 5000  current step :  2928
total : 5000  current step :  2929
total : 5000  current step :  2930
total : 5000  current step :  2931
total : 5000  current step :  2932
total : 5000  current step :  2933
total : 5000  current step :  2934
total : 5000  current step :  2935
total : 5000  current step :  2936
total : 5000  current step :  2937
total : 5000  current step :  2938
total : 5000  current step :  2939
total : 5000  current step :  2940
total : 5000  current step :  2941
total : 5000  current step :  2942
total : 5000  current step :  2943
total : 5000  current step :  2944
total : 5000  current step :  2945
total : 5000  current step :  2946
total : 5000  current step :  2947
total : 5000  current step :  2948
total : 5000  current step :  2949
total : 5000  current step :  2950
total : 5000  current step :  2951
total : 5000  current step :  2952
total : 5000  current step :  2953
total : 5000  current step :  2954
total : 5000  current step :  2955
total : 5000  current step :  2956
total : 5000  current step :  2957
total : 5000  current step :  2958
total : 5000  current step :  2959
total : 5000  current step :  2960
total : 5000  current step :  2961
total : 5000  current step :  2962
total : 5000  current step :  2963
total : 5000  current step :  2964
total : 5000  current step :  2965
total : 5000  current step :  2966
total : 5000  current step :  2967
total : 5000  current step :  2968
total : 5000  current step :  2969
total : 5000  current step :  2970
total : 5000  current step :  2971
total : 5000  current step :  2972
total : 5000  current step :  2973
total : 5000  current step :  2974
total : 5000  current step :  2975
total : 5000  current step :  2976
total : 5000  current step :  2977
total : 5000  current step :  2978
total : 5000  current step :  2979
total : 5000  current step :  2980
total : 5000  current step :  2981
total : 5000  current step :  2982
total : 5000  current step :  2983
total : 5000  current step :  2984
total : 5000  current step :  2985
total : 5000  current step :  2986
total : 5000  current step :  2987
total : 5000  current step :  2988
total : 5000  current step :  2989
total : 5000  current step :  2990
total : 5000  current step :  2991
total : 5000  current step :  2992
total : 5000  current step :  2993
total : 5000  current step :  2994
total : 5000  current step :  2995
total : 5000  current step :  2996
total : 5000  current step :  2997
total : 5000  current step :  2998
total : 5000  current step :  2999
total : 5000  current step :  3000
total : 5000  current step :  3001
total : 5000  current step :  3002
total : 5000  current step :  3003
total : 5000  current step :  3004
total : 5000  current step :  3005
total : 5000  current step :  3006
total : 5000  current step :  3007
total : 5000  current step :  3008
total : 5000  current step :  3009
total : 5000  current step :  3010
total : 5000  current step :  3011
total : 5000  current step :  3012
total : 5000  current step :  3013
total : 5000  current step :  3014
total : 5000  current step :  3015
total : 5000  current step :  3016
total : 5000  current step :  3017
total : 5000  current step :  3018
total : 5000  current step :  3019
total : 5000  current step :  3020
total : 5000  current step :  3021
total : 5000  current step :  3022
total : 5000  current step :  3023
total : 5000  current step :  3024
total : 5000  current step :  3025
total : 5000  current step :  3026
total : 5000  current step :  3027
total : 5000  current step :  3028
total : 5000  current step :  3029
total : 5000  current step :  3030
total : 5000  current step :  3031
total : 5000  current step :  3032
total : 5000  current step :  3033
total : 5000  current step :  3034
total : 5000  current step :  3035
total : 5000  current step :  3036
total : 5000  current step :  3037
total : 5000  current step :  3038
total : 5000  current step :  3039
total : 5000  current step :  3040
total : 5000  current step :  3041
total : 5000  current step :  3042
total : 5000  current step :  3043
total : 5000  current step :  3044
total : 5000  current step :  3045
total : 5000  current step :  3046
total : 5000  current step :  3047
total : 5000  current step :  3048
total : 5000  current step :  3049
total : 5000  current step :  3050
total : 5000  current step :  3051
total : 5000  current step :  3052
total : 5000  current step :  3053
total : 5000  current step :  3054
total : 5000  current step :  3055
total : 5000  current step :  3056
total : 5000  current step :  3057
total : 5000  current step :  3058
total : 5000  current step :  3059
total : 5000  current step :  3060
total : 5000  current step :  3061
total : 5000  current step :  3062
total : 5000  current step :  3063
total : 5000  current step :  3064
total : 5000  current step :  3065
total : 5000  current step :  3066
total : 5000  current step :  3067
total : 5000  current step :  3068
total : 5000  current step :  3069
total : 5000  current step :  3070
total : 5000  current step :  3071
total : 5000  current step :  3072
total : 5000  current step :  3073
total : 5000  current step :  3074
total : 5000  current step :  3075
total : 5000  current step :  3076
total : 5000  current step :  3077
total : 5000  current step :  3078
total : 5000  current step :  3079
total : 5000  current step :  3080
total : 5000  current step :  3081
total : 5000  current step :  3082
total : 5000  current step :  3083
total : 5000  current step :  3084
total : 5000  current step :  3085
total : 5000  current step :  3086
total : 5000  current step :  3087
total : 5000  current step :  3088
total : 5000  current step :  3089
total : 5000  current step :  3090
total : 5000  current step :  3091
total : 5000  current step :  3092
total : 5000  current step :  3093
total : 5000  current step :  3094
total : 5000  current step :  3095
total : 5000  current step :  3096
total : 5000  current step :  3097
total : 5000  current step :  3098
total : 5000  current step :  3099
total : 5000  current step :  3100
total : 5000  current step :  3101
total : 5000  current step :  3102
total : 5000  current step :  3103
total : 5000  current step :  3104
total : 5000  current step :  3105
total : 5000  current step :  3106
total : 5000  current step :  3107
total : 5000  current step :  3108
total : 5000  current step :  3109
total : 5000  current step :  3110
total : 5000  current step :  3111
total : 5000  current step :  3112
total : 5000  current step :  3113
total : 5000  current step :  3114
total : 5000  current step :  3115
total : 5000  current step :  3116
total : 5000  current step :  3117
total : 5000  current step :  3118
total : 5000  current step :  3119
total : 5000  current step :  3120
total : 5000  current step :  3121
total : 5000  current step :  3122
total : 5000  current step :  3123
total : 5000  current step :  3124
total : 5000  current step :  3125
total : 5000  current step :  3126
total : 5000  current step :  3127
total : 5000  current step :  3128
total : 5000  current step :  3129
total : 5000  current step :  3130
total : 5000  current step :  3131
total : 5000  current step :  3132
total : 5000  current step :  3133
total : 5000  current step :  3134
total : 5000  current step :  3135
total : 5000  current step :  3136
total : 5000  current step :  3137
total : 5000  current step :  3138
total : 5000  current step :  3139
total : 5000  current step :  3140
total : 5000  current step :  3141
total : 5000  current step :  3142
total : 5000  current step :  3143
total : 5000  current step :  3144
total : 5000  current step :  3145
total : 5000  current step :  3146
total : 5000  current step :  3147
total : 5000  current step :  3148
total : 5000  current step :  3149
total : 5000  current step :  3150
total : 5000  current step :  3151
total : 5000  current step :  3152
total : 5000  current step :  3153
total : 5000  current step :  3154
total : 5000  current step :  3155
total : 5000  current step :  3156
total : 5000  current step :  3157
total : 5000  current step :  3158
total : 5000  current step :  3159
total : 5000  current step :  3160
total : 5000  current step :  3161
total : 5000  current step :  3162
total : 5000  current step :  3163
total : 5000  current step :  3164
total : 5000  current step :  3165
total : 5000  current step :  3166
total : 5000  current step :  3167
total : 5000  current step :  3168
total : 5000  current step :  3169
total : 5000  current step :  3170
total : 5000  current step :  3171
total : 5000  current step :  3172
total : 5000  current step :  3173
total : 5000  current step :  3174
total : 5000  current step :  3175
total : 5000  current step :  3176
total : 5000  current step :  3177
total : 5000  current step :  3178
total : 5000  current step :  3179
total : 5000  current step :  3180
total : 5000  current step :  3181
total : 5000  current step :  3182
total : 5000  current step :  3183
total : 5000  current step :  3184
total : 5000  current step :  3185
total : 5000  current step :  3186
total : 5000  current step :  3187
total : 5000  current step :  3188
total : 5000  current step :  3189
total : 5000  current step :  3190
total : 5000  current step :  3191
total : 5000  current step :  3192
total : 5000  current step :  3193
total : 5000  current step :  3194
total : 5000  current step :  3195
total : 5000  current step :  3196
total : 5000  current step :  3197
total : 5000  current step :  3198
total : 5000  current step :  3199
total : 5000  current step :  3200
total : 5000  current step :  3201
total : 5000  current step :  3202
total : 5000  current step :  3203
total : 5000  current step :  3204
total : 5000  current step :  3205
total : 5000  current step :  3206
total : 5000  current step :  3207
total : 5000  current step :  3208
total : 5000  current step :  3209
total : 5000  current step :  3210
total : 5000  current step :  3211
total : 5000  current step :  3212
total : 5000  current step :  3213
total : 5000  current step :  3214
total : 5000  current step :  3215
total : 5000  current step :  3216
total : 5000  current step :  3217
total : 5000  current step :  3218
total : 5000  current step :  3219
total : 5000  current step :  3220
total : 5000  current step :  3221
total : 5000  current step :  3222
total : 5000  current step :  3223
total : 5000  current step :  3224
total : 5000  current step :  3225
total : 5000  current step :  3226
total : 5000  current step :  3227
total : 5000  current step :  3228
total : 5000  current step :  3229
total : 5000  current step :  3230
total : 5000  current step :  3231
total : 5000  current step :  3232
total : 5000  current step :  3233
total : 5000  current step :  3234
total : 5000  current step :  3235
total : 5000  current step :  3236
total : 5000  current step :  3237
total : 5000  current step :  3238
total : 5000  current step :  3239
total : 5000  current step :  3240
total : 5000  current step :  3241
total : 5000  current step :  3242
total : 5000  current step :  3243
total : 5000  current step :  3244
total : 5000  current step :  3245
total : 5000  current step :  3246
total : 5000  current step :  3247
total : 5000  current step :  3248
total : 5000  current step :  3249
total : 5000  current step :  3250
total : 5000  current step :  3251
total : 5000  current step :  3252
total : 5000  current step :  3253
total : 5000  current step :  3254
total : 5000  current step :  3255
total : 5000  current step :  3256
total : 5000  current step :  3257
total : 5000  current step :  3258
total : 5000  current step :  3259
total : 5000  current step :  3260
total : 5000  current step :  3261
total : 5000  current step :  3262
total : 5000  current step :  3263
total : 5000  current step :  3264
total : 5000  current step :  3265
total : 5000  current step :  3266
total : 5000  current step :  3267
total : 5000  current step :  3268
total : 5000  current step :  3269
total : 5000  current step :  3270
total : 5000  current step :  3271
total : 5000  current step :  3272
total : 5000  current step :  3273
total : 5000  current step :  3274
total : 5000  current step :  3275
total : 5000  current step :  3276
total : 5000  current step :  3277
total : 5000  current step :  3278
total : 5000  current step :  3279
total : 5000  current step :  3280
total : 5000  current step :  3281
total : 5000  current step :  3282
total : 5000  current step :  3283
total : 5000  current step :  3284
total : 5000  current step :  3285
total : 5000  current step :  3286
total : 5000  current step :  3287
total : 5000  current step :  3288
total : 5000  current step :  3289
total : 5000  current step :  3290
total : 5000  current step :  3291
total : 5000  current step :  3292
total : 5000  current step :  3293
total : 5000  current step :  3294
total : 5000  current step :  3295
total : 5000  current step :  3296
total : 5000  current step :  3297
total : 5000  current step :  3298
total : 5000  current step :  3299
total : 5000  current step :  3300
total : 5000  current step :  3301
total : 5000  current step :  3302
total : 5000  current step :  3303
total : 5000  current step :  3304
total : 5000  current step :  3305
total : 5000  current step :  3306
total : 5000  current step :  3307
total : 5000  current step :  3308
total : 5000  current step :  3309
total : 5000  current step :  3310
total : 5000  current step :  3311
total : 5000  current step :  3312
total : 5000  current step :  3313
total : 5000  current step :  3314
total : 5000  current step :  3315
total : 5000  current step :  3316
total : 5000  current step :  3317
total : 5000  current step :  3318
total : 5000  current step :  3319
total : 5000  current step :  3320
total : 5000  current step :  3321
total : 5000  current step :  3322
total : 5000  current step :  3323
total : 5000  current step :  3324
total : 5000  current step :  3325
total : 5000  current step :  3326
total : 5000  current step :  3327
total : 5000  current step :  3328
total : 5000  current step :  3329
total : 5000  current step :  3330
total : 5000  current step :  3331
total : 5000  current step :  3332
total : 5000  current step :  3333
total : 5000  current step :  3334
total : 5000  current step :  3335
total : 5000  current step :  3336
total : 5000  current step :  3337
total : 5000  current step :  3338
total : 5000  current step :  3339
total : 5000  current step :  3340
total : 5000  current step :  3341
total : 5000  current step :  3342
total : 5000  current step :  3343
total : 5000  current step :  3344
total : 5000  current step :  3345
total : 5000  current step :  3346
total : 5000  current step :  3347
total : 5000  current step :  3348
total : 5000  current step :  3349
total : 5000  current step :  3350
total : 5000  current step :  3351
total : 5000  current step :  3352
total : 5000  current step :  3353
total : 5000  current step :  3354
total : 5000  current step :  3355
total : 5000  current step :  3356
total : 5000  current step :  3357
total : 5000  current step :  3358
total : 5000  current step :  3359
total : 5000  current step :  3360
total : 5000  current step :  3361
total : 5000  current step :  3362
total : 5000  current step :  3363
total : 5000  current step :  3364
total : 5000  current step :  3365
total : 5000  current step :  3366
total : 5000  current step :  3367
total : 5000  current step :  3368
total : 5000  current step :  3369
total : 5000  current step :  3370
total : 5000  current step :  3371
total : 5000  current step :  3372
total : 5000  current step :  3373
total : 5000  current step :  3374
total : 5000  current step :  3375
total : 5000  current step :  3376
total : 5000  current step :  3377
total : 5000  current step :  3378
total : 5000  current step :  3379
total : 5000  current step :  3380
total : 5000  current step :  3381
total : 5000  current step :  3382
total : 5000  current step :  3383
total : 5000  current step :  3384
total : 5000  current step :  3385
total : 5000  current step :  3386
total : 5000  current step :  3387
total : 5000  current step :  3388
total : 5000  current step :  3389
total : 5000  current step :  3390
total : 5000  current step :  3391
total : 5000  current step :  3392
total : 5000  current step :  3393
total : 5000  current step :  3394
total : 5000  current step :  3395
total : 5000  current step :  3396
total : 5000  current step :  3397
total : 5000  current step :  3398
total : 5000  current step :  3399
total : 5000  current step :  3400
total : 5000  current step :  3401
total : 5000  current step :  3402
total : 5000  current step :  3403
total : 5000  current step :  3404
total : 5000  current step :  3405
total : 5000  current step :  3406
total : 5000  current step :  3407
total : 5000  current step :  3408
total : 5000  current step :  3409
total : 5000  current step :  3410
total : 5000  current step :  3411
total : 5000  current step :  3412
total : 5000  current step :  3413
total : 5000  current step :  3414
total : 5000  current step :  3415
total : 5000  current step :  3416
total : 5000  current step :  3417
total : 5000  current step :  3418
total : 5000  current step :  3419
total : 5000  current step :  3420
total : 5000  current step :  3421
total : 5000  current step :  3422
total : 5000  current step :  3423
total : 5000  current step :  3424
total : 5000  current step :  3425
total : 5000  current step :  3426
total : 5000  current step :  3427
total : 5000  current step :  3428
total : 5000  current step :  3429
total : 5000  current step :  3430
total : 5000  current step :  3431
total : 5000  current step :  3432
total : 5000  current step :  3433
total : 5000  current step :  3434
total : 5000  current step :  3435
total : 5000  current step :  3436
total : 5000  current step :  3437
total : 5000  current step :  3438
total : 5000  current step :  3439
total : 5000  current step :  3440
total : 5000  current step :  3441
total : 5000  current step :  3442
total : 5000  current step :  3443
total : 5000  current step :  3444
total : 5000  current step :  3445
total : 5000  current step :  3446
total : 5000  current step :  3447
total : 5000  current step :  3448
total : 5000  current step :  3449
total : 5000  current step :  3450
total : 5000  current step :  3451
total : 5000  current step :  3452
total : 5000  current step :  3453
total : 5000  current step :  3454
total : 5000  current step :  3455
total : 5000  current step :  3456
total : 5000  current step :  3457
total : 5000  current step :  3458
total : 5000  current step :  3459
total : 5000  current step :  3460
total : 5000  current step :  3461
total : 5000  current step :  3462
total : 5000  current step :  3463
total : 5000  current step :  3464
total : 5000  current step :  3465
total : 5000  current step :  3466
total : 5000  current step :  3467
total : 5000  current step :  3468
total : 5000  current step :  3469
total : 5000  current step :  3470
total : 5000  current step :  3471
total : 5000  current step :  3472
total : 5000  current step :  3473
total : 5000  current step :  3474
total : 5000  current step :  3475
total : 5000  current step :  3476
total : 5000  current step :  3477
total : 5000  current step :  3478
total : 5000  current step :  3479
total : 5000  current step :  3480
total : 5000  current step :  3481
total : 5000  current step :  3482
total : 5000  current step :  3483
total : 5000  current step :  3484
total : 5000  current step :  3485
total : 5000  current step :  3486
total : 5000  current step :  3487
total : 5000  current step :  3488
total : 5000  current step :  3489
total : 5000  current step :  3490
total : 5000  current step :  3491
total : 5000  current step :  3492
total : 5000  current step :  3493
total : 5000  current step :  3494
total : 5000  current step :  3495
total : 5000  current step :  3496
total : 5000  current step :  3497
total : 5000  current step :  3498
total : 5000  current step :  3499
total : 5000  current step :  3500
total : 5000  current step :  3501
total : 5000  current step :  3502
total : 5000  current step :  3503
total : 5000  current step :  3504
total : 5000  current step :  3505
total : 5000  current step :  3506
total : 5000  current step :  3507
total : 5000  current step :  3508
total : 5000  current step :  3509
total : 5000  current step :  3510
total : 5000  current step :  3511
total : 5000  current step :  3512
total : 5000  current step :  3513
total : 5000  current step :  3514
total : 5000  current step :  3515
total : 5000  current step :  3516
total : 5000  current step :  3517
total : 5000  current step :  3518
total : 5000  current step :  3519
total : 5000  current step :  3520
total : 5000  current step :  3521
total : 5000  current step :  3522
total : 5000  current step :  3523
total : 5000  current step :  3524
total : 5000  current step :  3525
total : 5000  current step :  3526
total : 5000  current step :  3527
total : 5000  current step :  3528
total : 5000  current step :  3529
total : 5000  current step :  3530
total : 5000  current step :  3531
total : 5000  current step :  3532
total : 5000  current step :  3533
total : 5000  current step :  3534
total : 5000  current step :  3535
total : 5000  current step :  3536
total : 5000  current step :  3537
total : 5000  current step :  3538
total : 5000  current step :  3539
total : 5000  current step :  3540
total : 5000  current step :  3541
total : 5000  current step :  3542
total : 5000  current step :  3543
total : 5000  current step :  3544
total : 5000  current step :  3545
total : 5000  current step :  3546
total : 5000  current step :  3547
total : 5000  current step :  3548
total : 5000  current step :  3549
total : 5000  current step :  3550
total : 5000  current step :  3551
total : 5000  current step :  3552
total : 5000  current step :  3553
total : 5000  current step :  3554
total : 5000  current step :  3555
total : 5000  current step :  3556
total : 5000  current step :  3557
total : 5000  current step :  3558
total : 5000  current step :  3559
total : 5000  current step :  3560
total : 5000  current step :  3561
total : 5000  current step :  3562
total : 5000  current step :  3563
total : 5000  current step :  3564
total : 5000  current step :  3565
total : 5000  current step :  3566
total : 5000  current step :  3567
total : 5000  current step :  3568
total : 5000  current step :  3569
total : 5000  current step :  3570
total : 5000  current step :  3571
total : 5000  current step :  3572
total : 5000  current step :  3573
total : 5000  current step :  3574
total : 5000  current step :  3575
total : 5000  current step :  3576
total : 5000  current step :  3577
total : 5000  current step :  3578
total : 5000  current step :  3579
total : 5000  current step :  3580
total : 5000  current step :  3581
total : 5000  current step :  3582
total : 5000  current step :  3583
total : 5000  current step :  3584
total : 5000  current step :  3585
total : 5000  current step :  3586
total : 5000  current step :  3587
total : 5000  current step :  3588
total : 5000  current step :  3589
total : 5000  current step :  3590
total : 5000  current step :  3591
total : 5000  current step :  3592
total : 5000  current step :  3593
total : 5000  current step :  3594
total : 5000  current step :  3595
total : 5000  current step :  3596
total : 5000  current step :  3597
total : 5000  current step :  3598
total : 5000  current step :  3599
total : 5000  current step :  3600
total : 5000  current step :  3601
total : 5000  current step :  3602
total : 5000  current step :  3603
total : 5000  current step :  3604
total : 5000  current step :  3605
total : 5000  current step :  3606
total : 5000  current step :  3607
total : 5000  current step :  3608
total : 5000  current step :  3609
total : 5000  current step :  3610
total : 5000  current step :  3611
total : 5000  current step :  3612
total : 5000  current step :  3613
total : 5000  current step :  3614
total : 5000  current step :  3615
total : 5000  current step :  3616
total : 5000  current step :  3617
total : 5000  current step :  3618
total : 5000  current step :  3619
total : 5000  current step :  3620
total : 5000  current step :  3621
total : 5000  current step :  3622
total : 5000  current step :  3623
total : 5000  current step :  3624
total : 5000  current step :  3625
total : 5000  current step :  3626
total : 5000  current step :  3627
total : 5000  current step :  3628
total : 5000  current step :  3629
total : 5000  current step :  3630
total : 5000  current step :  3631
total : 5000  current step :  3632
total : 5000  current step :  3633
total : 5000  current step :  3634
total : 5000  current step :  3635
total : 5000  current step :  3636
total : 5000  current step :  3637
total : 5000  current step :  3638
total : 5000  current step :  3639
total : 5000  current step :  3640
total : 5000  current step :  3641
total : 5000  current step :  3642
total : 5000  current step :  3643
total : 5000  current step :  3644
total : 5000  current step :  3645
total : 5000  current step :  3646
total : 5000  current step :  3647
total : 5000  current step :  3648
total : 5000  current step :  3649
total : 5000  current step :  3650
total : 5000  current step :  3651
total : 5000  current step :  3652
total : 5000  current step :  3653
total : 5000  current step :  3654
total : 5000  current step :  3655
total : 5000  current step :  3656
total : 5000  current step :  3657
total : 5000  current step :  3658
total : 5000  current step :  3659
total : 5000  current step :  3660
total : 5000  current step :  3661
total : 5000  current step :  3662
total : 5000  current step :  3663
total : 5000  current step :  3664
total : 5000  current step :  3665
total : 5000  current step :  3666
total : 5000  current step :  3667
total : 5000  current step :  3668
total : 5000  current step :  3669
total : 5000  current step :  3670
total : 5000  current step :  3671
total : 5000  current step :  3672
total : 5000  current step :  3673
total : 5000  current step :  3674
total : 5000  current step :  3675
total : 5000  current step :  3676
total : 5000  current step :  3677
total : 5000  current step :  3678
total : 5000  current step :  3679
total : 5000  current step :  3680
total : 5000  current step :  3681
total : 5000  current step :  3682
total : 5000  current step :  3683
total : 5000  current step :  3684
total : 5000  current step :  3685
total : 5000  current step :  3686
total : 5000  current step :  3687
total : 5000  current step :  3688
total : 5000  current step :  3689
total : 5000  current step :  3690
total : 5000  current step :  3691
total : 5000  current step :  3692
total : 5000  current step :  3693
total : 5000  current step :  3694
total : 5000  current step :  3695
total : 5000  current step :  3696
total : 5000  current step :  3697
total : 5000  current step :  3698
total : 5000  current step :  3699
total : 5000  current step :  3700
total : 5000  current step :  3701
total : 5000  current step :  3702
total : 5000  current step :  3703
total : 5000  current step :  3704
total : 5000  current step :  3705
total : 5000  current step :  3706
total : 5000  current step :  3707
total : 5000  current step :  3708
total : 5000  current step :  3709
total : 5000  current step :  3710
total : 5000  current step :  3711
total : 5000  current step :  3712
total : 5000  current step :  3713
total : 5000  current step :  3714
total : 5000  current step :  3715
total : 5000  current step :  3716
total : 5000  current step :  3717
total : 5000  current step :  3718
total : 5000  current step :  3719
total : 5000  current step :  3720
total : 5000  current step :  3721
total : 5000  current step :  3722
total : 5000  current step :  3723
total : 5000  current step :  3724
total : 5000  current step :  3725
total : 5000  current step :  3726
total : 5000  current step :  3727
total : 5000  current step :  3728
total : 5000  current step :  3729
total : 5000  current step :  3730
total : 5000  current step :  3731
total : 5000  current step :  3732
total : 5000  current step :  3733
total : 5000  current step :  3734
total : 5000  current step :  3735
total : 5000  current step :  3736
total : 5000  current step :  3737
total : 5000  current step :  3738
total : 5000  current step :  3739
total : 5000  current step :  3740
total : 5000  current step :  3741
total : 5000  current step :  3742
total : 5000  current step :  3743
total : 5000  current step :  3744
total : 5000  current step :  3745
total : 5000  current step :  3746
total : 5000  current step :  3747
total : 5000  current step :  3748
total : 5000  current step :  3749
total : 5000  current step :  3750
total : 5000  current step :  3751
total : 5000  current step :  3752
total : 5000  current step :  3753
total : 5000  current step :  3754
total : 5000  current step :  3755
total : 5000  current step :  3756
total : 5000  current step :  3757
total : 5000  current step :  3758
total : 5000  current step :  3759
total : 5000  current step :  3760
total : 5000  current step :  3761
total : 5000  current step :  3762
total : 5000  current step :  3763
total : 5000  current step :  3764
total : 5000  current step :  3765
total : 5000  current step :  3766
total : 5000  current step :  3767
total : 5000  current step :  3768
total : 5000  current step :  3769
total : 5000  current step :  3770
total : 5000  current step :  3771
total : 5000  current step :  3772
total : 5000  current step :  3773
total : 5000  current step :  3774
total : 5000  current step :  3775
total : 5000  current step :  3776
total : 5000  current step :  3777
total : 5000  current step :  3778
total : 5000  current step :  3779
total : 5000  current step :  3780
total : 5000  current step :  3781
total : 5000  current step :  3782
total : 5000  current step :  3783
total : 5000  current step :  3784
total : 5000  current step :  3785
total : 5000  current step :  3786
total : 5000  current step :  3787
total : 5000  current step :  3788
total : 5000  current step :  3789
total : 5000  current step :  3790
total : 5000  current step :  3791
total : 5000  current step :  3792
total : 5000  current step :  3793
total : 5000  current step :  3794
total : 5000  current step :  3795
total : 5000  current step :  3796
total : 5000  current step :  3797
total : 5000  current step :  3798
total : 5000  current step :  3799
total : 5000  current step :  3800
total : 5000  current step :  3801
total : 5000  current step :  3802
total : 5000  current step :  3803
total : 5000  current step :  3804
total : 5000  current step :  3805
total : 5000  current step :  3806
total : 5000  current step :  3807
total : 5000  current step :  3808
total : 5000  current step :  3809
total : 5000  current step :  3810
total : 5000  current step :  3811
total : 5000  current step :  3812
total : 5000  current step :  3813
total : 5000  current step :  3814
total : 5000  current step :  3815
total : 5000  current step :  3816
total : 5000  current step :  3817
total : 5000  current step :  3818
total : 5000  current step :  3819
total : 5000  current step :  3820
total : 5000  current step :  3821
total : 5000  current step :  3822
total : 5000  current step :  3823
total : 5000  current step :  3824
total : 5000  current step :  3825
total : 5000  current step :  3826
total : 5000  current step :  3827
total : 5000  current step :  3828
total : 5000  current step :  3829
total : 5000  current step :  3830
total : 5000  current step :  3831
total : 5000  current step :  3832
total : 5000  current step :  3833
total : 5000  current step :  3834
total : 5000  current step :  3835
total : 5000  current step :  3836
total : 5000  current step :  3837
total : 5000  current step :  3838
total : 5000  current step :  3839
total : 5000  current step :  3840
total : 5000  current step :  3841
total : 5000  current step :  3842
total : 5000  current step :  3843
total : 5000  current step :  3844
total : 5000  current step :  3845
total : 5000  current step :  3846
total : 5000  current step :  3847
total : 5000  current step :  3848
total : 5000  current step :  3849
total : 5000  current step :  3850
total : 5000  current step :  3851
total : 5000  current step :  3852
total : 5000  current step :  3853
total : 5000  current step :  3854
total : 5000  current step :  3855
total : 5000  current step :  3856
total : 5000  current step :  3857
total : 5000  current step :  3858
total : 5000  current step :  3859
total : 5000  current step :  3860
total : 5000  current step :  3861
total : 5000  current step :  3862
total : 5000  current step :  3863
total : 5000  current step :  3864
total : 5000  current step :  3865
total : 5000  current step :  3866
total : 5000  current step :  3867
total : 5000  current step :  3868
total : 5000  current step :  3869
total : 5000  current step :  3870
total : 5000  current step :  3871
total : 5000  current step :  3872
total : 5000  current step :  3873
total : 5000  current step :  3874
total : 5000  current step :  3875
total : 5000  current step :  3876
total : 5000  current step :  3877
total : 5000  current step :  3878
total : 5000  current step :  3879
total : 5000  current step :  3880
total : 5000  current step :  3881
total : 5000  current step :  3882
total : 5000  current step :  3883
total : 5000  current step :  3884
total : 5000  current step :  3885
total : 5000  current step :  3886
total : 5000  current step :  3887
total : 5000  current step :  3888
total : 5000  current step :  3889
total : 5000  current step :  3890
total : 5000  current step :  3891
total : 5000  current step :  3892
total : 5000  current step :  3893
total : 5000  current step :  3894
total : 5000  current step :  3895
total : 5000  current step :  3896
total : 5000  current step :  3897
total : 5000  current step :  3898
total : 5000  current step :  3899
total : 5000  current step :  3900
total : 5000  current step :  3901
total : 5000  current step :  3902
total : 5000  current step :  3903
total : 5000  current step :  3904
total : 5000  current step :  3905
total : 5000  current step :  3906
total : 5000  current step :  3907
total : 5000  current step :  3908
total : 5000  current step :  3909
total : 5000  current step :  3910
total : 5000  current step :  3911
total : 5000  current step :  3912
total : 5000  current step :  3913
total : 5000  current step :  3914
total : 5000  current step :  3915
total : 5000  current step :  3916
total : 5000  current step :  3917
total : 5000  current step :  3918
total : 5000  current step :  3919
total : 5000  current step :  3920
total : 5000  current step :  3921
total : 5000  current step :  3922
total : 5000  current step :  3923
total : 5000  current step :  3924
total : 5000  current step :  3925
total : 5000  current step :  3926
total : 5000  current step :  3927
total : 5000  current step :  3928
total : 5000  current step :  3929
total : 5000  current step :  3930
total : 5000  current step :  3931
total : 5000  current step :  3932
total : 5000  current step :  3933
total : 5000  current step :  3934
total : 5000  current step :  3935
total : 5000  current step :  3936
total : 5000  current step :  3937
total : 5000  current step :  3938
total : 5000  current step :  3939
total : 5000  current step :  3940
total : 5000  current step :  3941
total : 5000  current step :  3942
total : 5000  current step :  3943
total : 5000  current step :  3944
total : 5000  current step :  3945
total : 5000  current step :  3946
total : 5000  current step :  3947
total : 5000  current step :  3948
total : 5000  current step :  3949
total : 5000  current step :  3950
total : 5000  current step :  3951
total : 5000  current step :  3952
total : 5000  current step :  3953
total : 5000  current step :  3954
total : 5000  current step :  3955
total : 5000  current step :  3956
total : 5000  current step :  3957
total : 5000  current step :  3958
total : 5000  current step :  3959
total : 5000  current step :  3960
total : 5000  current step :  3961
total : 5000  current step :  3962
total : 5000  current step :  3963
total : 5000  current step :  3964
total : 5000  current step :  3965
total : 5000  current step :  3966
total : 5000  current step :  3967
total : 5000  current step :  3968
total : 5000  current step :  3969
total : 5000  current step :  3970
total : 5000  current step :  3971
total : 5000  current step :  3972
total : 5000  current step :  3973
total : 5000  current step :  3974
total : 5000  current step :  3975
total : 5000  current step :  3976
total : 5000  current step :  3977
total : 5000  current step :  3978
total : 5000  current step :  3979
total : 5000  current step :  3980
total : 5000  current step :  3981
total : 5000  current step :  3982
total : 5000  current step :  3983
total : 5000  current step :  3984
total : 5000  current step :  3985
total : 5000  current step :  3986
total : 5000  current step :  3987
total : 5000  current step :  3988
total : 5000  current step :  3989
total : 5000  current step :  3990
total : 5000  current step :  3991
total : 5000  current step :  3992
total : 5000  current step :  3993
total : 5000  current step :  3994
total : 5000  current step :  3995
total : 5000  current step :  3996
total : 5000  current step :  3997
total : 5000  current step :  3998
total : 5000  current step :  3999
total : 5000  current step :  4000
total : 5000  current step :  4001
total : 5000  current step :  4002
total : 5000  current step :  4003
total : 5000  current step :  4004
total : 5000  current step :  4005
total : 5000  current step :  4006
total : 5000  current step :  4007
total : 5000  current step :  4008
total : 5000  current step :  4009
total : 5000  current step :  4010
total : 5000  current step :  4011
total : 5000  current step :  4012
total : 5000  current step :  4013
total : 5000  current step :  4014
total : 5000  current step :  4015
total : 5000  current step :  4016
total : 5000  current step :  4017
total : 5000  current step :  4018
total : 5000  current step :  4019
total : 5000  current step :  4020
total : 5000  current step :  4021
total : 5000  current step :  4022
total : 5000  current step :  4023
total : 5000  current step :  4024
total : 5000  current step :  4025
total : 5000  current step :  4026
total : 5000  current step :  4027
total : 5000  current step :  4028
total : 5000  current step :  4029
total : 5000  current step :  4030
total : 5000  current step :  4031
total : 5000  current step :  4032
total : 5000  current step :  4033
total : 5000  current step :  4034
total : 5000  current step :  4035
total : 5000  current step :  4036
total : 5000  current step :  4037
total : 5000  current step :  4038
total : 5000  current step :  4039
total : 5000  current step :  4040
total : 5000  current step :  4041
total : 5000  current step :  4042
total : 5000  current step :  4043
total : 5000  current step :  4044
total : 5000  current step :  4045
total : 5000  current step :  4046
total : 5000  current step :  4047
total : 5000  current step :  4048
total : 5000  current step :  4049
total : 5000  current step :  4050
total : 5000  current step :  4051
total : 5000  current step :  4052
total : 5000  current step :  4053
total : 5000  current step :  4054
total : 5000  current step :  4055
total : 5000  current step :  4056
total : 5000  current step :  4057
total : 5000  current step :  4058
total : 5000  current step :  4059
total : 5000  current step :  4060
total : 5000  current step :  4061
total : 5000  current step :  4062
total : 5000  current step :  4063
total : 5000  current step :  4064
total : 5000  current step :  4065
total : 5000  current step :  4066
total : 5000  current step :  4067
total : 5000  current step :  4068
total : 5000  current step :  4069
total : 5000  current step :  4070
total : 5000  current step :  4071
total : 5000  current step :  4072
total : 5000  current step :  4073
total : 5000  current step :  4074
total : 5000  current step :  4075
total : 5000  current step :  4076
total : 5000  current step :  4077
total : 5000  current step :  4078
total : 5000  current step :  4079
total : 5000  current step :  4080
total : 5000  current step :  4081
total : 5000  current step :  4082
total : 5000  current step :  4083
total : 5000  current step :  4084
total : 5000  current step :  4085
total : 5000  current step :  4086
total : 5000  current step :  4087
total : 5000  current step :  4088
total : 5000  current step :  4089
total : 5000  current step :  4090
total : 5000  current step :  4091
total : 5000  current step :  4092
total : 5000  current step :  4093
total : 5000  current step :  4094
total : 5000  current step :  4095
total : 5000  current step :  4096
total : 5000  current step :  4097
total : 5000  current step :  4098
total : 5000  current step :  4099
total : 5000  current step :  4100
total : 5000  current step :  4101
total : 5000  current step :  4102
total : 5000  current step :  4103
total : 5000  current step :  4104
total : 5000  current step :  4105
total : 5000  current step :  4106
total : 5000  current step :  4107
total : 5000  current step :  4108
total : 5000  current step :  4109
total : 5000  current step :  4110
total : 5000  current step :  4111
total : 5000  current step :  4112
total : 5000  current step :  4113
total : 5000  current step :  4114
total : 5000  current step :  4115
total : 5000  current step :  4116
total : 5000  current step :  4117
total : 5000  current step :  4118
total : 5000  current step :  4119
total : 5000  current step :  4120
total : 5000  current step :  4121
total : 5000  current step :  4122
total : 5000  current step :  4123
total : 5000  current step :  4124
total : 5000  current step :  4125
total : 5000  current step :  4126
total : 5000  current step :  4127
total : 5000  current step :  4128
total : 5000  current step :  4129
total : 5000  current step :  4130
total : 5000  current step :  4131
total : 5000  current step :  4132
total : 5000  current step :  4133
total : 5000  current step :  4134
total : 5000  current step :  4135
total : 5000  current step :  4136
total : 5000  current step :  4137
total : 5000  current step :  4138
total : 5000  current step :  4139
total : 5000  current step :  4140
total : 5000  current step :  4141
total : 5000  current step :  4142
total : 5000  current step :  4143
total : 5000  current step :  4144
total : 5000  current step :  4145
total : 5000  current step :  4146
total : 5000  current step :  4147
total : 5000  current step :  4148
total : 5000  current step :  4149
total : 5000  current step :  4150
total : 5000  current step :  4151
total : 5000  current step :  4152
total : 5000  current step :  4153
total : 5000  current step :  4154
total : 5000  current step :  4155
total : 5000  current step :  4156
total : 5000  current step :  4157
total : 5000  current step :  4158
total : 5000  current step :  4159
total : 5000  current step :  4160
total : 5000  current step :  4161
total : 5000  current step :  4162
total : 5000  current step :  4163
total : 5000  current step :  4164
total : 5000  current step :  4165
total : 5000  current step :  4166
total : 5000  current step :  4167
total : 5000  current step :  4168
total : 5000  current step :  4169
total : 5000  current step :  4170
total : 5000  current step :  4171
total : 5000  current step :  4172
total : 5000  current step :  4173
total : 5000  current step :  4174
total : 5000  current step :  4175
total : 5000  current step :  4176
total : 5000  current step :  4177
total : 5000  current step :  4178
total : 5000  current step :  4179
total : 5000  current step :  4180
total : 5000  current step :  4181
total : 5000  current step :  4182
total : 5000  current step :  4183
total : 5000  current step :  4184
total : 5000  current step :  4185
total : 5000  current step :  4186
total : 5000  current step :  4187
total : 5000  current step :  4188
total : 5000  current step :  4189
total : 5000  current step :  4190
total : 5000  current step :  4191
total : 5000  current step :  4192
total : 5000  current step :  4193
total : 5000  current step :  4194
total : 5000  current step :  4195
total : 5000  current step :  4196
total : 5000  current step :  4197
total : 5000  current step :  4198
total : 5000  current step :  4199
total : 5000  current step :  4200
total : 5000  current step :  4201
total : 5000  current step :  4202
total : 5000  current step :  4203
total : 5000  current step :  4204
total : 5000  current step :  4205
total : 5000  current step :  4206
total : 5000  current step :  4207
total : 5000  current step :  4208
total : 5000  current step :  4209
total : 5000  current step :  4210
total : 5000  current step :  4211
total : 5000  current step :  4212
total : 5000  current step :  4213
total : 5000  current step :  4214
total : 5000  current step :  4215
total : 5000  current step :  4216
total : 5000  current step :  4217
total : 5000  current step :  4218
total : 5000  current step :  4219
total : 5000  current step :  4220
total : 5000  current step :  4221
total : 5000  current step :  4222
total : 5000  current step :  4223
total : 5000  current step :  4224
total : 5000  current step :  4225
total : 5000  current step :  4226
total : 5000  current step :  4227
total : 5000  current step :  4228
total : 5000  current step :  4229
total : 5000  current step :  4230
total : 5000  current step :  4231
total : 5000  current step :  4232
total : 5000  current step :  4233
total : 5000  current step :  4234
total : 5000  current step :  4235
total : 5000  current step :  4236
total : 5000  current step :  4237
total : 5000  current step :  4238
total : 5000  current step :  4239
total : 5000  current step :  4240
total : 5000  current step :  4241
total : 5000  current step :  4242
total : 5000  current step :  4243
total : 5000  current step :  4244
total : 5000  current step :  4245
total : 5000  current step :  4246
total : 5000  current step :  4247
total : 5000  current step :  4248
total : 5000  current step :  4249
total : 5000  current step :  4250
total : 5000  current step :  4251
total : 5000  current step :  4252
total : 5000  current step :  4253
total : 5000  current step :  4254
total : 5000  current step :  4255
total : 5000  current step :  4256
total : 5000  current step :  4257
total : 5000  current step :  4258
total : 5000  current step :  4259
total : 5000  current step :  4260
total : 5000  current step :  4261
total : 5000  current step :  4262
total : 5000  current step :  4263
total : 5000  current step :  4264
total : 5000  current step :  4265
total : 5000  current step :  4266
total : 5000  current step :  4267
total : 5000  current step :  4268
total : 5000  current step :  4269
total : 5000  current step :  4270
total : 5000  current step :  4271
total : 5000  current step :  4272
total : 5000  current step :  4273
total : 5000  current step :  4274
total : 5000  current step :  4275
total : 5000  current step :  4276
total : 5000  current step :  4277
total : 5000  current step :  4278
total : 5000  current step :  4279
total : 5000  current step :  4280
total : 5000  current step :  4281
total : 5000  current step :  4282
total : 5000  current step :  4283
total : 5000  current step :  4284
total : 5000  current step :  4285
total : 5000  current step :  4286
total : 5000  current step :  4287
total : 5000  current step :  4288
total : 5000  current step :  4289
total : 5000  current step :  4290
total : 5000  current step :  4291
total : 5000  current step :  4292
total : 5000  current step :  4293
total : 5000  current step :  4294
total : 5000  current step :  4295
total : 5000  current step :  4296
total : 5000  current step :  4297
total : 5000  current step :  4298
total : 5000  current step :  4299
total : 5000  current step :  4300
total : 5000  current step :  4301
total : 5000  current step :  4302
total : 5000  current step :  4303
total : 5000  current step :  4304
total : 5000  current step :  4305
total : 5000  current step :  4306
total : 5000  current step :  4307
total : 5000  current step :  4308
total : 5000  current step :  4309
total : 5000  current step :  4310
total : 5000  current step :  4311
total : 5000  current step :  4312
total : 5000  current step :  4313
total : 5000  current step :  4314
total : 5000  current step :  4315
total : 5000  current step :  4316
total : 5000  current step :  4317
total : 5000  current step :  4318
total : 5000  current step :  4319
total : 5000  current step :  4320
total : 5000  current step :  4321
total : 5000  current step :  4322
total : 5000  current step :  4323
total : 5000  current step :  4324
total : 5000  current step :  4325
total : 5000  current step :  4326
total : 5000  current step :  4327
total : 5000  current step :  4328
total : 5000  current step :  4329
total : 5000  current step :  4330
total : 5000  current step :  4331
total : 5000  current step :  4332
total : 5000  current step :  4333
total : 5000  current step :  4334
total : 5000  current step :  4335
total : 5000  current step :  4336
total : 5000  current step :  4337
total : 5000  current step :  4338
total : 5000  current step :  4339
total : 5000  current step :  4340
total : 5000  current step :  4341
total : 5000  current step :  4342
total : 5000  current step :  4343
total : 5000  current step :  4344
total : 5000  current step :  4345
total : 5000  current step :  4346
total : 5000  current step :  4347
total : 5000  current step :  4348
total : 5000  current step :  4349
total : 5000  current step :  4350
total : 5000  current step :  4351
total : 5000  current step :  4352
total : 5000  current step :  4353
total : 5000  current step :  4354
total : 5000  current step :  4355
total : 5000  current step :  4356
total : 5000  current step :  4357
total : 5000  current step :  4358
total : 5000  current step :  4359
total : 5000  current step :  4360
total : 5000  current step :  4361
total : 5000  current step :  4362
total : 5000  current step :  4363
total : 5000  current step :  4364
total : 5000  current step :  4365
total : 5000  current step :  4366
total : 5000  current step :  4367
total : 5000  current step :  4368
total : 5000  current step :  4369
total : 5000  current step :  4370
total : 5000  current step :  4371
total : 5000  current step :  4372
total : 5000  current step :  4373
total : 5000  current step :  4374
total : 5000  current step :  4375
total : 5000  current step :  4376
total : 5000  current step :  4377
total : 5000  current step :  4378
total : 5000  current step :  4379
total : 5000  current step :  4380
total : 5000  current step :  4381
total : 5000  current step :  4382
total : 5000  current step :  4383
total : 5000  current step :  4384
total : 5000  current step :  4385
total : 5000  current step :  4386
total : 5000  current step :  4387
total : 5000  current step :  4388
total : 5000  current step :  4389
total : 5000  current step :  4390
total : 5000  current step :  4391
total : 5000  current step :  4392
total : 5000  current step :  4393
total : 5000  current step :  4394
total : 5000  current step :  4395
total : 5000  current step :  4396
total : 5000  current step :  4397
total : 5000  current step :  4398
total : 5000  current step :  4399
total : 5000  current step :  4400
total : 5000  current step :  4401
total : 5000  current step :  4402
total : 5000  current step :  4403
total : 5000  current step :  4404
total : 5000  current step :  4405
total : 5000  current step :  4406
total : 5000  current step :  4407
total : 5000  current step :  4408
total : 5000  current step :  4409
total : 5000  current step :  4410
total : 5000  current step :  4411
total : 5000  current step :  4412
total : 5000  current step :  4413
total : 5000  current step :  4414
total : 5000  current step :  4415
total : 5000  current step :  4416
total : 5000  current step :  4417
total : 5000  current step :  4418
total : 5000  current step :  4419
total : 5000  current step :  4420
total : 5000  current step :  4421
total : 5000  current step :  4422
total : 5000  current step :  4423
total : 5000  current step :  4424
total : 5000  current step :  4425
total : 5000  current step :  4426
total : 5000  current step :  4427
total : 5000  current step :  4428
total : 5000  current step :  4429
total : 5000  current step :  4430
total : 5000  current step :  4431
total : 5000  current step :  4432
total : 5000  current step :  4433
total : 5000  current step :  4434
total : 5000  current step :  4435
total : 5000  current step :  4436
total : 5000  current step :  4437
total : 5000  current step :  4438
total : 5000  current step :  4439
total : 5000  current step :  4440
total : 5000  current step :  4441
total : 5000  current step :  4442
total : 5000  current step :  4443
total : 5000  current step :  4444
total : 5000  current step :  4445
total : 5000  current step :  4446
total : 5000  current step :  4447
total : 5000  current step :  4448
total : 5000  current step :  4449
total : 5000  current step :  4450
total : 5000  current step :  4451
total : 5000  current step :  4452
total : 5000  current step :  4453
total : 5000  current step :  4454
total : 5000  current step :  4455
total : 5000  current step :  4456
total : 5000  current step :  4457
total : 5000  current step :  4458
total : 5000  current step :  4459
total : 5000  current step :  4460
total : 5000  current step :  4461
total : 5000  current step :  4462
total : 5000  current step :  4463
total : 5000  current step :  4464
total : 5000  current step :  4465
total : 5000  current step :  4466
total : 5000  current step :  4467
total : 5000  current step :  4468
total : 5000  current step :  4469
total : 5000  current step :  4470
total : 5000  current step :  4471
total : 5000  current step :  4472
total : 5000  current step :  4473
total : 5000  current step :  4474
total : 5000  current step :  4475
total : 5000  current step :  4476
total : 5000  current step :  4477
total : 5000  current step :  4478
total : 5000  current step :  4479
total : 5000  current step :  4480
total : 5000  current step :  4481
total : 5000  current step :  4482
total : 5000  current step :  4483
total : 5000  current step :  4484
total : 5000  current step :  4485
total : 5000  current step :  4486
total : 5000  current step :  4487
total : 5000  current step :  4488
total : 5000  current step :  4489
total : 5000  current step :  4490
total : 5000  current step :  4491
total : 5000  current step :  4492
total : 5000  current step :  4493
total : 5000  current step :  4494
total : 5000  current step :  4495
total : 5000  current step :  4496
total : 5000  current step :  4497
total : 5000  current step :  4498
total : 5000  current step :  4499
total : 5000  current step :  4500
total : 5000  current step :  4501
total : 5000  current step :  4502
total : 5000  current step :  4503
total : 5000  current step :  4504
total : 5000  current step :  4505
total : 5000  current step :  4506
total : 5000  current step :  4507
total : 5000  current step :  4508
total : 5000  current step :  4509
total : 5000  current step :  4510
total : 5000  current step :  4511
total : 5000  current step :  4512
total : 5000  current step :  4513
total : 5000  current step :  4514
total : 5000  current step :  4515
total : 5000  current step :  4516
total : 5000  current step :  4517
total : 5000  current step :  4518
total : 5000  current step :  4519
total : 5000  current step :  4520
total : 5000  current step :  4521
total : 5000  current step :  4522
total : 5000  current step :  4523
total : 5000  current step :  4524
total : 5000  current step :  4525
total : 5000  current step :  4526
total : 5000  current step :  4527
total : 5000  current step :  4528
total : 5000  current step :  4529
total : 5000  current step :  4530
total : 5000  current step :  4531
total : 5000  current step :  4532
total : 5000  current step :  4533
total : 5000  current step :  4534
total : 5000  current step :  4535
total : 5000  current step :  4536
total : 5000  current step :  4537
total : 5000  current step :  4538
total : 5000  current step :  4539
total : 5000  current step :  4540
total : 5000  current step :  4541
total : 5000  current step :  4542
total : 5000  current step :  4543
total : 5000  current step :  4544
total : 5000  current step :  4545
total : 5000  current step :  4546
total : 5000  current step :  4547
total : 5000  current step :  4548
total : 5000  current step :  4549
total : 5000  current step :  4550
total : 5000  current step :  4551
total : 5000  current step :  4552
total : 5000  current step :  4553
total : 5000  current step :  4554
total : 5000  current step :  4555
total : 5000  current step :  4556
total : 5000  current step :  4557
total : 5000  current step :  4558
total : 5000  current step :  4559
total : 5000  current step :  4560
total : 5000  current step :  4561
total : 5000  current step :  4562
total : 5000  current step :  4563
total : 5000  current step :  4564
total : 5000  current step :  4565
total : 5000  current step :  4566
total : 5000  current step :  4567
total : 5000  current step :  4568
total : 5000  current step :  4569
total : 5000  current step :  4570
total : 5000  current step :  4571
total : 5000  current step :  4572
total : 5000  current step :  4573
total : 5000  current step :  4574
total : 5000  current step :  4575
total : 5000  current step :  4576
total : 5000  current step :  4577
total : 5000  current step :  4578
total : 5000  current step :  4579
total : 5000  current step :  4580
total : 5000  current step :  4581
total : 5000  current step :  4582
total : 5000  current step :  4583
total : 5000  current step :  4584
total : 5000  current step :  4585
total : 5000  current step :  4586
total : 5000  current step :  4587
total : 5000  current step :  4588
total : 5000  current step :  4589
total : 5000  current step :  4590
total : 5000  current step :  4591
total : 5000  current step :  4592
total : 5000  current step :  4593
total : 5000  current step :  4594
total : 5000  current step :  4595
total : 5000  current step :  4596
total : 5000  current step :  4597
total : 5000  current step :  4598
total : 5000  current step :  4599
total : 5000  current step :  4600
total : 5000  current step :  4601
total : 5000  current step :  4602
total : 5000  current step :  4603
total : 5000  current step :  4604
total : 5000  current step :  4605
total : 5000  current step :  4606
total : 5000  current step :  4607
total : 5000  current step :  4608
total : 5000  current step :  4609
total : 5000  current step :  4610
total : 5000  current step :  4611
total : 5000  current step :  4612
total : 5000  current step :  4613
total : 5000  current step :  4614
total : 5000  current step :  4615
total : 5000  current step :  4616
total : 5000  current step :  4617
total : 5000  current step :  4618
total : 5000  current step :  4619
total : 5000  current step :  4620
total : 5000  current step :  4621
total : 5000  current step :  4622
total : 5000  current step :  4623
total : 5000  current step :  4624
total : 5000  current step :  4625
total : 5000  current step :  4626
total : 5000  current step :  4627
total : 5000  current step :  4628
total : 5000  current step :  4629
total : 5000  current step :  4630
total : 5000  current step :  4631
total : 5000  current step :  4632
total : 5000  current step :  4633
total : 5000  current step :  4634
total : 5000  current step :  4635
total : 5000  current step :  4636
total : 5000  current step :  4637
total : 5000  current step :  4638
total : 5000  current step :  4639
total : 5000  current step :  4640
total : 5000  current step :  4641
total : 5000  current step :  4642
total : 5000  current step :  4643
total : 5000  current step :  4644
total : 5000  current step :  4645
total : 5000  current step :  4646
total : 5000  current step :  4647
total : 5000  current step :  4648
total : 5000  current step :  4649
total : 5000  current step :  4650
total : 5000  current step :  4651
total : 5000  current step :  4652
total : 5000  current step :  4653
total : 5000  current step :  4654
total : 5000  current step :  4655
total : 5000  current step :  4656
total : 5000  current step :  4657
total : 5000  current step :  4658
total : 5000  current step :  4659
total : 5000  current step :  4660
total : 5000  current step :  4661
total : 5000  current step :  4662
total : 5000  current step :  4663
total : 5000  current step :  4664
total : 5000  current step :  4665
total : 5000  current step :  4666
total : 5000  current step :  4667
total : 5000  current step :  4668
total : 5000  current step :  4669
total : 5000  current step :  4670
total : 5000  current step :  4671
total : 5000  current step :  4672
total : 5000  current step :  4673
total : 5000  current step :  4674
total : 5000  current step :  4675
total : 5000  current step :  4676
total : 5000  current step :  4677
total : 5000  current step :  4678
total : 5000  current step :  4679
total : 5000  current step :  4680
total : 5000  current step :  4681
total : 5000  current step :  4682
total : 5000  current step :  4683
total : 5000  current step :  4684
total : 5000  current step :  4685
total : 5000  current step :  4686
total : 5000  current step :  4687
total : 5000  current step :  4688
total : 5000  current step :  4689
total : 5000  current step :  4690
total : 5000  current step :  4691
total : 5000  current step :  4692
total : 5000  current step :  4693
total : 5000  current step :  4694
total : 5000  current step :  4695
total : 5000  current step :  4696
total : 5000  current step :  4697
total : 5000  current step :  4698
total : 5000  current step :  4699
total : 5000  current step :  4700
total : 5000  current step :  4701
total : 5000  current step :  4702
total : 5000  current step :  4703
total : 5000  current step :  4704
total : 5000  current step :  4705
total : 5000  current step :  4706
total : 5000  current step :  4707
total : 5000  current step :  4708
total : 5000  current step :  4709
total : 5000  current step :  4710
total : 5000  current step :  4711
total : 5000  current step :  4712
total : 5000  current step :  4713
total : 5000  current step :  4714
total : 5000  current step :  4715
total : 5000  current step :  4716
total : 5000  current step :  4717
total : 5000  current step :  4718
total : 5000  current step :  4719
total : 5000  current step :  4720
total : 5000  current step :  4721
total : 5000  current step :  4722
total : 5000  current step :  4723
total : 5000  current step :  4724
total : 5000  current step :  4725
total : 5000  current step :  4726
total : 5000  current step :  4727
total : 5000  current step :  4728
total : 5000  current step :  4729
total : 5000  current step :  4730
total : 5000  current step :  4731
total : 5000  current step :  4732
total : 5000  current step :  4733
total : 5000  current step :  4734
total : 5000  current step :  4735
total : 5000  current step :  4736
total : 5000  current step :  4737
total : 5000  current step :  4738
total : 5000  current step :  4739
total : 5000  current step :  4740
total : 5000  current step :  4741
total : 5000  current step :  4742
total : 5000  current step :  4743
total : 5000  current step :  4744
total : 5000  current step :  4745
total : 5000  current step :  4746
total : 5000  current step :  4747
total : 5000  current step :  4748
total : 5000  current step :  4749
total : 5000  current step :  4750
total : 5000  current step :  4751
total : 5000  current step :  4752
total : 5000  current step :  4753
total : 5000  current step :  4754
total : 5000  current step :  4755
total : 5000  current step :  4756
total : 5000  current step :  4757
total : 5000  current step :  4758
total : 5000  current step :  4759
total : 5000  current step :  4760
total : 5000  current step :  4761
total : 5000  current step :  4762
total : 5000  current step :  4763
total : 5000  current step :  4764
total : 5000  current step :  4765
total : 5000  current step :  4766
total : 5000  current step :  4767
total : 5000  current step :  4768
total : 5000  current step :  4769
total : 5000  current step :  4770
total : 5000  current step :  4771
total : 5000  current step :  4772
total : 5000  current step :  4773
total : 5000  current step :  4774
total : 5000  current step :  4775
total : 5000  current step :  4776
total : 5000  current step :  4777
total : 5000  current step :  4778
total : 5000  current step :  4779
total : 5000  current step :  4780
total : 5000  current step :  4781
total : 5000  current step :  4782
total : 5000  current step :  4783
total : 5000  current step :  4784
total : 5000  current step :  4785
total : 5000  current step :  4786
total : 5000  current step :  4787
total : 5000  current step :  4788
total : 5000  current step :  4789
total : 5000  current step :  4790
total : 5000  current step :  4791
total : 5000  current step :  4792
total : 5000  current step :  4793
total : 5000  current step :  4794
total : 5000  current step :  4795
total : 5000  current step :  4796
total : 5000  current step :  4797
total : 5000  current step :  4798
total : 5000  current step :  4799
total : 5000  current step :  4800
total : 5000  current step :  4801
total : 5000  current step :  4802
total : 5000  current step :  4803
total : 5000  current step :  4804
total : 5000  current step :  4805
total : 5000  current step :  4806
total : 5000  current step :  4807
total : 5000  current step :  4808
total : 5000  current step :  4809
total : 5000  current step :  4810
total : 5000  current step :  4811
total : 5000  current step :  4812
total : 5000  current step :  4813
total : 5000  current step :  4814
total : 5000  current step :  4815
total : 5000  current step :  4816
total : 5000  current step :  4817
total : 5000  current step :  4818
total : 5000  current step :  4819
total : 5000  current step :  4820
total : 5000  current step :  4821
total : 5000  current step :  4822
total : 5000  current step :  4823
total : 5000  current step :  4824
total : 5000  current step :  4825
total : 5000  current step :  4826
total : 5000  current step :  4827
total : 5000  current step :  4828
total : 5000  current step :  4829
total : 5000  current step :  4830
total : 5000  current step :  4831
total : 5000  current step :  4832
total : 5000  current step :  4833
total : 5000  current step :  4834
total : 5000  current step :  4835
total : 5000  current step :  4836
total : 5000  current step :  4837
total : 5000  current step :  4838
total : 5000  current step :  4839
total : 5000  current step :  4840
total : 5000  current step :  4841
total : 5000  current step :  4842
total : 5000  current step :  4843
total : 5000  current step :  4844
total : 5000  current step :  4845
total : 5000  current step :  4846
total : 5000  current step :  4847
total : 5000  current step :  4848
total : 5000  current step :  4849
total : 5000  current step :  4850
total : 5000  current step :  4851
total : 5000  current step :  4852
total : 5000  current step :  4853
total : 5000  current step :  4854
total : 5000  current step :  4855
total : 5000  current step :  4856
total : 5000  current step :  4857
total : 5000  current step :  4858
total : 5000  current step :  4859
total : 5000  current step :  4860
total : 5000  current step :  4861
total : 5000  current step :  4862
total : 5000  current step :  4863
total : 5000  current step :  4864
total : 5000  current step :  4865
total : 5000  current step :  4866
total : 5000  current step :  4867
total : 5000  current step :  4868
total : 5000  current step :  4869
total : 5000  current step :  4870
total : 5000  current step :  4871
total : 5000  current step :  4872
total : 5000  current step :  4873
total : 5000  current step :  4874
total : 5000  current step :  4875
total : 5000  current step :  4876
total : 5000  current step :  4877
total : 5000  current step :  4878
total : 5000  current step :  4879
total : 5000  current step :  4880
total : 5000  current step :  4881
total : 5000  current step :  4882
total : 5000  current step :  4883
total : 5000  current step :  4884
total : 5000  current step :  4885
total : 5000  current step :  4886
total : 5000  current step :  4887
total : 5000  current step :  4888
total : 5000  current step :  4889
total : 5000  current step :  4890
total : 5000  current step :  4891
total : 5000  current step :  4892
total : 5000  current step :  4893
total : 5000  current step :  4894
total : 5000  current step :  4895
total : 5000  current step :  4896
total : 5000  current step :  4897
total : 5000  current step :  4898
total : 5000  current step :  4899
total : 5000  current step :  4900
total : 5000  current step :  4901
total : 5000  current step :  4902
total : 5000  current step :  4903
total : 5000  current step :  4904
total : 5000  current step :  4905
total : 5000  current step :  4906
total : 5000  current step :  4907
total : 5000  current step :  4908
total : 5000  current step :  4909
total : 5000  current step :  4910
total : 5000  current step :  4911
total : 5000  current step :  4912
total : 5000  current step :  4913
total : 5000  current step :  4914
total : 5000  current step :  4915
total : 5000  current step :  4916
total : 5000  current step :  4917
total : 5000  current step :  4918
total : 5000  current step :  4919
total : 5000  current step :  4920
total : 5000  current step :  4921
total : 5000  current step :  4922
total : 5000  current step :  4923
total : 5000  current step :  4924
total : 5000  current step :  4925
total : 5000  current step :  4926
total : 5000  current step :  4927
total : 5000  current step :  4928
total : 5000  current step :  4929
total : 5000  current step :  4930
total : 5000  current step :  4931
total : 5000  current step :  4932
total : 5000  current step :  4933
total : 5000  current step :  4934
total : 5000  current step :  4935
total : 5000  current step :  4936
total : 5000  current step :  4937
total : 5000  current step :  4938
total : 5000  current step :  4939
total : 5000  current step :  4940
total : 5000  current step :  4941
total : 5000  current step :  4942
total : 5000  current step :  4943
total : 5000  current step :  4944
total : 5000  current step :  4945
total : 5000  current step :  4946
total : 5000  current step :  4947
total : 5000  current step :  4948
total : 5000  current step :  4949
total : 5000  current step :  4950
total : 5000  current step :  4951
total : 5000  current step :  4952
total : 5000  current step :  4953
total : 5000  current step :  4954
total : 5000  current step :  4955
total : 5000  current step :  4956
total : 5000  current step :  4957
total : 5000  current step :  4958
total : 5000  current step :  4959
total : 5000  current step :  4960
total : 5000  current step :  4961
total : 5000  current step :  4962
total : 5000  current step :  4963
total : 5000  current step :  4964
total : 5000  current step :  4965
total : 5000  current step :  4966
total : 5000  current step :  4967
total : 5000  current step :  4968
total : 5000  current step :  4969
total : 5000  current step :  4970
total : 5000  current step :  4971
total : 5000  current step :  4972
total : 5000  current step :  4973
total : 5000  current step :  4974
total : 5000  current step :  4975
total : 5000  current step :  4976
total : 5000  current step :  4977
total : 5000  current step :  4978
total : 5000  current step :  4979
total : 5000  current step :  4980
total : 5000  current step :  4981
total : 5000  current step :  4982
total : 5000  current step :  4983
total : 5000  current step :  4984
total : 5000  current step :  4985
total : 5000  current step :  4986
total : 5000  current step :  4987
total : 5000  current step :  4988
total : 5000  current step :  4989
total : 5000  current step :  4990
total : 5000  current step :  4991
total : 5000  current step :  4992
total : 5000  current step :  4993
total : 5000  current step :  4994
total : 5000  current step :  4995
total : 5000  current step :  4996
total : 5000  current step :  4997
total : 5000  current step :  4998
total : 5000  current step :  4999
soft_pseudo_label
tensor([[3.0190e-03, 1.8491e-03, 8.6789e-01,  ..., 2.0421e-02, 3.7978e-03,
         4.4077e-03],
        [2.6613e-04, 7.4892e-04, 9.9455e-01,  ..., 7.8005e-06, 5.7604e-04,
         1.1362e-04],
        [3.6040e-03, 1.2840e-02, 4.9857e-01,  ..., 6.1360e-02, 9.9607e-03,
         9.2858e-02],
        ...,
        [1.3874e-02, 5.2956e-03, 5.3387e-01,  ..., 7.7730e-03, 8.5370e-03,
         7.1364e-03],
        [1.2020e-02, 2.0476e-03, 1.0129e-04,  ..., 9.4379e-04, 6.5225e-03,
         1.0884e-03],
        [1.0818e-03, 1.2790e-03, 9.8954e-01,  ..., 2.4144e-04, 1.1666e-03,
         5.7778e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 2, 3, 0, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3,
        2, 2, 2, 3, 3, 0, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3,
        2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2,
        3, 2, 3, 2, 1, 2, 0, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        2, 3, 2, 7, 2, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 2, 3, 0, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 0, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 1, 2, 0, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 7, 2, 2, 3, 2]
original label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 6, 2, 3, 3, 3, 2, 2, 3, 2, 3,
        7, 2, 2, 3, 3, 3, 2, 3, 8, 3, 3, 2, 2, 3, 3, 2, 6, 2, 2, 2, 3, 2, 2, 3,
        2, 8, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3, 3, 3, 3, 5, 3, 2, 2,
        3, 2, 3, 2, 5, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        9, 3, 4, 3, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.0726e-03, 5.7701e-03, 1.0493e-03,  ..., 4.3634e-04, 2.7954e-03,
         9.7090e-04],
        [5.0524e-03, 5.3947e-03, 9.4558e-01,  ..., 7.5354e-04, 5.9634e-03,
         1.5598e-03],
        [1.7179e-03, 3.1116e-03, 9.7025e-01,  ..., 5.8194e-04, 2.4801e-03,
         2.1297e-03],
        ...,
        [1.6503e-03, 5.4358e-04, 9.9068e-01,  ..., 3.9153e-05, 9.8301e-04,
         4.4977e-05],
        [4.6461e-03, 1.3279e-03, 9.7622e-01,  ..., 1.5289e-04, 2.3350e-03,
         4.4718e-04],
        [8.2200e-04, 2.2674e-03, 8.7200e-01,  ..., 4.8338e-02, 1.8651e-03,
         2.6461e-02]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 4, 2, 3, 2, 2, 4, 3,
        7, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 0, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4,
        3, 3, 2, 3, 3, 2, 2, 2, 2, 5, 2, 3, 3, 2, 2, 2, 0, 2, 2, 4, 5, 7, 3, 2,
        3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3,
        2, 2, 9, 3, 7, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 4, 2, 3, 2, 2, 4, 3, 7, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 0, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 2, 2, 2, 2, 5, 2, 3, 3, 2, 2, 2, 0, 2, 2, 4, 5, 7, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 9, 3, 7, 2, 2, 2]
original label
tensor([3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 6, 3,
        7, 2, 2, 3, 3, 2, 3, 3, 1, 3, 3, 2, 2, 0, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 7, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4,
        7, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 4, 2, 2, 2, 4, 3, 2,
        5, 3, 2, 3, 2, 3, 5, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3,
        2, 2, 2, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[1.3780e-04, 3.7640e-04, 9.9407e-01,  ..., 1.2137e-04, 2.4637e-04,
         1.4771e-03],
        [6.5115e-04, 1.6555e-03, 9.7507e-01,  ..., 1.1906e-03, 1.1872e-03,
         9.2651e-03],
        [4.8540e-04, 4.7787e-04, 9.9059e-01,  ..., 8.3910e-04, 4.6726e-04,
         2.2199e-03],
        ...,
        [5.6406e-04, 1.1785e-03, 9.8557e-01,  ..., 2.6749e-04, 9.3956e-04,
         1.0647e-03],
        [1.1889e-03, 5.8749e-03, 3.3225e-04,  ..., 1.1574e-03, 7.8786e-03,
         3.2838e-04],
        [8.4873e-02, 2.3260e-02, 3.5960e-01,  ..., 4.8946e-03, 4.4845e-02,
         1.5677e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3,
        0, 3, 3, 2, 3, 3, 9, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 7, 2, 3, 2, 3, 4, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 6, 0, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 0, 3, 3, 2, 3, 3, 9, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 7, 2, 3, 2, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 6, 0, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2]
original label
tensor([2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 0, 9, 2, 2, 3, 3,
        1, 3, 3, 2, 3, 3, 9, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 0, 0, 2, 3,
        2, 3, 3, 2, 2, 1, 3, 1, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 0, 0, 2, 3, 2, 2, 3, 9, 2, 3, 3, 2,
        3, 3, 3, 6, 3, 2, 3, 3])
soft_pseudo_label
tensor([[1.2776e-04, 1.3272e-04, 9.9805e-01,  ..., 5.5571e-05, 1.3747e-04,
         5.7316e-04],
        [2.1138e-02, 3.3217e-03, 5.0890e-04,  ..., 2.5119e-04, 1.0608e-02,
         5.5025e-04],
        [1.3076e-04, 8.1718e-04, 9.8566e-01,  ..., 2.0813e-04, 3.6707e-04,
         3.1184e-04],
        ...,
        [1.5980e-03, 4.6482e-04, 9.4213e-01,  ..., 5.0014e-04, 8.7094e-04,
         7.3484e-04],
        [4.5316e-04, 8.4858e-04, 9.9314e-01,  ..., 2.1999e-04, 6.9658e-04,
         3.8123e-04],
        [1.2580e-03, 1.4537e-03, 8.8901e-01,  ..., 3.6904e-03, 1.4007e-03,
         8.5985e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2,
        3, 3, 3, 0, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 5, 2, 2, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2, 3, 9, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3,
        2, 2, 2, 2, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 0, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 5, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 9, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2]
original label
tensor([2, 6, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 0, 3,
        3, 3, 2, 3, 9, 3, 6, 9, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 5, 7, 2, 2, 3, 2,
        3, 2, 2, 8, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 8, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 7, 3, 3, 3, 3, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 1, 2, 3, 0, 3, 8, 3, 2, 3, 8, 3,
        3, 2, 2, 2, 3, 2, 2, 2])
soft_pseudo_label
tensor([[8.1784e-04, 5.2701e-04, 9.8406e-01,  ..., 2.7690e-04, 6.5651e-04,
         4.9010e-03],
        [2.1880e-04, 5.7420e-04, 4.3271e-02,  ..., 2.7451e-02, 6.8589e-04,
         9.9631e-02],
        [5.1330e-04, 1.0213e-03, 4.7241e-04,  ..., 8.2198e-03, 1.2829e-03,
         3.0439e-03],
        ...,
        [4.2379e-03, 2.3831e-03, 2.5269e-03,  ..., 3.3986e-03, 5.1624e-03,
         5.3275e-03],
        [2.8452e-03, 6.5445e-03, 7.5423e-01,  ..., 1.6548e-02, 4.9449e-03,
         7.6899e-02],
        [5.4005e-03, 1.9113e-03, 6.6569e-05,  ..., 6.2372e-03, 6.9039e-03,
         1.2610e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 0, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 4, 3, 2, 3,
        2, 2, 2, 2, 1, 2, 3, 0, 2, 2, 7, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 4, 3,
        3, 3, 3, 2, 3, 3, 7, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3,
        3, 2, 1, 3, 3, 2, 2, 2, 3, 7, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3,
        3, 5, 2, 3, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 0, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 4, 3, 2, 3, 2, 2, 2, 2, 1, 2, 3, 0, 2, 2, 7, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 4, 3, 3, 3, 3, 2, 3, 3, 7, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 2, 3, 7, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 5, 2, 3, 3, 3, 2, 3]
original label
tensor([2, 3, 3, 2, 3, 1, 3, 2, 4, 3, 2, 3, 3, 3, 1, 3, 2, 2, 3, 5, 3, 8, 3, 6,
        3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 3, 2, 3,
        4, 2, 2, 2, 1, 2, 3, 0, 2, 2, 3, 3, 3, 3, 3, 3, 5, 2, 2, 3, 2, 3, 2, 3,
        3, 3, 3, 0, 3, 3, 7, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3,
        3, 5, 6, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 8, 3,
        7, 3, 3, 3, 3, 3, 2, 3])
soft_pseudo_label
tensor([[1.5357e-03, 1.7589e-03, 9.8230e-01,  ..., 1.7165e-03, 2.0354e-03,
         1.4203e-03],
        [1.0348e-03, 3.1196e-03, 4.5243e-05,  ..., 1.7379e-03, 3.2645e-03,
         9.8932e-04],
        [4.9343e-04, 4.5910e-03, 1.2685e-04,  ..., 1.9474e-03, 4.6112e-03,
         4.7918e-04],
        ...,
        [4.9577e-04, 1.6295e-03, 4.4804e-05,  ..., 4.6778e-04, 1.7914e-03,
         8.6898e-04],
        [3.5664e-04, 6.4422e-04, 9.9434e-01,  ..., 7.3561e-05, 6.4030e-04,
         1.0363e-03],
        [3.5691e-03, 1.3159e-01, 2.7095e-04,  ..., 1.4847e-04, 2.7882e-02,
         6.1779e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 4, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3,
        9, 2, 2, 3, 3, 9, 3, 2, 3, 3, 0, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 4, 3, 3, 3, 2, 2, 2, 2, 3,
        0, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        7, 3, 3, 3, 5, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 2, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 4, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 9, 2, 2, 3, 3, 9, 3, 2, 3, 3, 0, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 4, 3, 3, 3, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 7, 3, 3, 3, 5, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3]
original label
tensor([2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3,
        3, 4, 2, 2, 3, 9, 3, 2, 3, 3, 0, 3, 2, 1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 2, 3, 9, 3, 9, 3, 2, 4, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 8,
        2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 7, 2, 3, 3,
        2, 2, 3, 2, 2, 3, 2, 3])
soft_pseudo_label
tensor([[5.1740e-04, 4.7358e-04, 9.9630e-01,  ..., 1.2575e-04, 4.7642e-04,
         2.1833e-04],
        [5.0491e-04, 9.1383e-04, 4.6085e-04,  ..., 7.4803e-04, 1.1784e-03,
         2.0284e-03],
        [1.4239e-03, 8.7894e-04, 9.6860e-01,  ..., 2.1074e-03, 1.1408e-03,
         4.2251e-03],
        ...,
        [1.1470e-03, 1.1283e-02, 7.6739e-01,  ..., 6.3849e-03, 5.5202e-03,
         5.6080e-02],
        [3.6175e-04, 2.0957e-04, 9.8475e-01,  ..., 9.5986e-04, 3.2364e-04,
         2.0271e-03],
        [4.3763e-05, 4.5585e-04, 9.9454e-01,  ..., 5.4357e-05, 1.6260e-04,
         3.4616e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 2, 3, 3, 3, 4, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 5, 3, 3, 3, 2, 0, 2, 2, 3, 0, 3, 2, 2, 2, 3,
        3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2,
        7, 2, 3, 3, 3, 9, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3,
        2, 0, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 3, 0, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 2, 3, 3, 3, 4, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 5, 3, 3, 3, 2, 0, 2, 2, 3, 0, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 7, 2, 3, 3, 3, 9, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 0, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 0, 2, 2, 2, 2, 2]
original label
tensor([2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 8, 3, 2, 2, 3, 3, 6, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 1, 2, 2, 3, 3, 3, 3, 2, 2, 3,
        3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 9, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3,
        2, 6, 1, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 2, 2, 7, 2])
soft_pseudo_label
tensor([[1.2891e-02, 2.7523e-04, 2.4914e-04,  ..., 1.6726e-04, 1.9558e-03,
         3.2462e-04],
        [1.2262e-02, 2.1429e-03, 8.8297e-05,  ..., 6.0105e-04, 9.3288e-03,
         4.8674e-04],
        [3.9999e-04, 8.9165e-04, 1.5887e-04,  ..., 8.9133e-04, 1.3305e-03,
         7.7478e-04],
        ...,
        [7.9812e-03, 5.7599e-03, 6.5332e-03,  ..., 4.3676e-02, 1.6736e-02,
         3.2162e-01],
        [2.2692e-03, 2.9199e-02, 1.0750e-03,  ..., 2.1569e-03, 1.3457e-02,
         4.3188e-03],
        [3.2423e-04, 1.8924e-03, 9.8173e-01,  ..., 1.6237e-05, 7.0513e-04,
         4.7538e-05]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 2, 3, 0, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3,
        7, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2,
        3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 7, 2, 2, 2, 2,
        3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 7, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 7, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2]
original label
tensor([3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 5, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3,
        3, 3, 2, 5, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 2, 3, 3, 5, 3, 3, 2, 2, 2, 2, 2, 3, 2,
        3, 3, 2, 2, 9, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 8, 2, 7, 2, 2, 2, 2,
        3, 2, 9, 3, 2, 2, 3, 3, 3, 3, 0, 2, 0, 7, 3, 3, 6, 3, 3, 3, 3, 2, 2, 2,
        3, 2, 2, 3, 3, 3, 2, 2])
soft_pseudo_label
tensor([[4.7700e-03, 2.4339e-03, 8.6062e-01,  ..., 2.4339e-03, 4.0050e-03,
         3.2944e-03],
        [1.2264e-03, 2.7620e-04, 9.9557e-01,  ..., 3.1926e-05, 5.6431e-04,
         1.1816e-04],
        [5.0967e-04, 3.5240e-03, 2.3391e-01,  ..., 6.2352e-01, 2.3044e-03,
         6.2282e-02],
        ...,
        [6.6838e-05, 2.8167e-04, 9.8839e-01,  ..., 1.5505e-03, 2.1324e-04,
         1.9977e-03],
        [7.0492e-04, 2.7966e-02, 1.7806e-04,  ..., 6.5131e-04, 1.1108e-02,
         9.8925e-04],
        [6.9942e-04, 3.3038e-04, 6.5223e-01,  ..., 6.4941e-03, 5.9013e-04,
         2.8661e-01]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 7, 2, 2, 3, 3, 2, 2, 0, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 4, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 4, 3, 2,
        6, 3, 2, 2, 3, 2, 2, 2, 3, 3, 4, 3, 2, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 2, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 7, 2, 2, 3, 3, 2, 2, 0, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 4, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 4, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 3, 4, 3, 2, 3, 3, 3, 0, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2]
original label
tensor([2, 2, 2, 9, 2, 2, 3, 2, 2, 6, 7, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 5, 1, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 7, 4, 1, 3, 2, 2, 2, 4, 3, 2, 2, 4, 3,
        2, 2, 3, 3, 2, 3, 2, 1, 3, 3, 2, 8, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2,
        6, 3, 2, 2, 3, 6, 9, 2, 3, 1, 4, 3, 2, 8, 3, 3, 0, 2, 3, 2, 3, 3, 2, 2,
        9, 8, 2, 3, 3, 2, 3, 7, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 2, 2, 3, 2])
soft_pseudo_label
tensor([[1.5567e-04, 9.1527e-04, 9.8816e-01,  ..., 1.1866e-04, 5.4760e-04,
         8.2466e-04],
        [6.1966e-04, 1.0190e-03, 3.7165e-06,  ..., 3.2920e-04, 2.3245e-03,
         5.7033e-04],
        [5.5596e-06, 6.7456e-06, 9.9768e-01,  ..., 4.3142e-04, 3.2177e-06,
         6.9009e-04],
        ...,
        [3.8555e-04, 2.5015e-04, 9.8882e-01,  ..., 3.1994e-04, 3.6789e-04,
         6.6237e-03],
        [3.5804e-01, 2.4378e-02, 7.5331e-03,  ..., 5.5275e-03, 1.9202e-01,
         1.0302e-02],
        [1.0420e-03, 6.0574e-04, 9.8653e-01,  ..., 2.9149e-04, 9.4693e-04,
         1.8007e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 3,
        3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 9, 3, 2, 3, 2, 2, 2, 3, 2, 3,
        3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 2, 2, 0, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 9, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 0, 2]
original label
tensor([2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 7, 3, 8, 3, 8, 3, 2, 8, 2, 3, 3, 3, 3, 3,
        3, 2, 2, 5, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 5, 0, 2, 2, 3, 2, 2, 3,
        2, 2, 1, 3, 3, 3, 2, 2, 2, 5, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 2, 6,
        3, 3, 2, 2, 2, 3, 2, 2, 1, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 7, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 0, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 2])
soft_pseudo_label
tensor([[8.9126e-03, 5.2465e-03, 5.6590e-03,  ..., 3.6359e-03, 9.4332e-03,
         2.1917e-02],
        [3.8245e-04, 4.6132e-04, 9.8953e-01,  ..., 4.4844e-04, 4.6177e-04,
         1.5019e-03],
        [2.3102e-02, 8.0083e-03, 1.0447e-03,  ..., 3.9775e-03, 2.8375e-02,
         6.3778e-03],
        ...,
        [6.2798e-05, 1.7578e-04, 9.5813e-01,  ..., 1.7176e-02, 1.2176e-04,
         5.5382e-03],
        [1.2687e-03, 1.7417e-03, 9.7841e-01,  ..., 3.8186e-04, 1.4153e-03,
         2.0089e-03],
        [2.1675e-02, 2.3794e-02, 2.0575e-01,  ..., 6.9244e-03, 3.5294e-02,
         7.7701e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 9, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 4,
        3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2,
        3, 7, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3,
        3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 2, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 9, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 4, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 7, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3]
original label
tensor([3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 7, 2, 3, 2, 2, 8, 3, 2, 3, 3, 3, 3,
        3, 2, 0, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 0, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 8, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3,
        8, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 6, 2, 3, 6, 3, 3, 3, 3, 3, 4,
        3, 2, 2, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[8.9449e-03, 9.2153e-03, 4.1361e-02,  ..., 9.3376e-03, 1.4269e-02,
         3.0535e-02],
        [5.1567e-01, 3.2768e-03, 1.8658e-01,  ..., 1.0383e-02, 6.1168e-02,
         8.5577e-03],
        [2.7415e-03, 2.8361e-03, 2.6339e-03,  ..., 1.6235e-03, 4.6327e-03,
         2.0744e-03],
        ...,
        [5.3471e-04, 1.4076e-02, 1.0106e-04,  ..., 3.9620e-04, 4.2741e-03,
         3.5551e-03],
        [5.9524e-02, 2.3983e-01, 1.4040e-03,  ..., 6.7760e-04, 1.6133e-01,
         7.6632e-04],
        [4.5020e-04, 7.9291e-05, 9.9805e-01,  ..., 1.2957e-05, 1.5246e-04,
         1.0082e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 0, 2, 3, 3, 3, 2,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 9, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 1, 2], device='cuda:0')
hard_pseudo_label
[3, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 0, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 9, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 1, 2]
original label
tensor([0, 8, 3, 2, 3, 2, 2, 4, 3, 2, 0, 3, 2, 8, 2, 2, 3, 2, 0, 2, 3, 3, 3, 2,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 4, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 8, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 9, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 2, 2, 3, 5, 7, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2])
soft_pseudo_label
tensor([[1.0562e-03, 2.9799e-02, 8.5363e-04,  ..., 9.2842e-04, 2.1887e-02,
         2.9405e-03],
        [1.8654e-03, 2.4306e-03, 9.4118e-01,  ..., 5.3965e-03, 3.0547e-03,
         5.4682e-03],
        [4.2074e-03, 3.0801e-02, 8.0037e-03,  ..., 2.6692e-03, 1.9921e-02,
         1.0056e-02],
        ...,
        [7.5374e-02, 9.7527e-03, 4.9032e-04,  ..., 8.2919e-04, 3.2704e-02,
         1.7761e-03],
        [2.9991e-03, 1.4563e-03, 9.8198e-01,  ..., 2.4720e-04, 2.5280e-03,
         1.6922e-03],
        [3.0079e-03, 3.8270e-04, 9.6253e-01,  ..., 6.2483e-04, 8.4905e-04,
         1.0548e-02]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 4, 4, 3, 3, 2, 0, 3, 3, 2, 2, 3, 7, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 9, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 4, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 4, 4, 3, 3, 2, 0, 3, 3, 2, 2, 3, 7, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 9, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 4, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 6, 3, 3, 3, 3, 3, 3, 2, 2, 3, 7, 2,
        4, 2, 3, 3, 2, 3, 2, 6, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 9, 3, 3,
        8, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 6, 2, 2, 3, 1, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 2, 1, 2, 3, 2, 2, 2, 2, 3, 3, 5, 2, 2, 3, 2, 2, 2, 2, 2, 3,
        6, 2, 2, 3, 2, 3, 3, 2, 8, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 2, 8, 2, 4])
soft_pseudo_label
tensor([[4.2980e-02, 3.4228e-03, 1.2037e-02,  ..., 8.8544e-03, 1.9068e-02,
         9.7175e-03],
        [1.4100e-02, 6.2263e-03, 1.2169e-01,  ..., 4.3037e-02, 1.3297e-02,
         1.7043e-01],
        [4.6893e-04, 1.2371e-03, 9.8834e-01,  ..., 5.5578e-04, 1.0188e-03,
         1.5245e-03],
        ...,
        [1.7171e-04, 1.1323e-03, 6.4314e-04,  ..., 2.8476e-04, 7.5614e-04,
         6.7433e-04],
        [6.5783e-03, 6.0898e-03, 8.4203e-01,  ..., 3.8994e-03, 7.6983e-03,
         3.7354e-03],
        [1.1537e-02, 2.0301e-02, 1.1099e-02,  ..., 7.8446e-04, 2.3343e-02,
         1.1459e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 0, 3, 2, 3, 3, 0, 2, 3,
        3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 0, 3, 2, 3, 3,
        2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 4, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 0, 3, 2, 3, 3, 0, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 0, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 4, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3]
original label
tensor([3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2,
        3, 3, 2, 1, 3, 2, 3, 3, 2, 2, 2, 3, 2, 6, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 2, 2, 2, 1, 9, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 3, 2, 3, 3, 2, 3])
soft_pseudo_label
tensor([[4.9920e-01, 1.8028e-04, 2.8037e-06,  ..., 2.2174e-04, 1.9546e-02,
         1.0127e-04],
        [6.2559e-04, 1.7710e-03, 9.8367e-01,  ..., 1.6821e-04, 1.6933e-03,
         7.5534e-04],
        [8.2115e-04, 5.2451e-04, 9.5577e-01,  ..., 9.4898e-03, 7.1343e-04,
         9.5223e-03],
        ...,
        [1.0955e-03, 4.2098e-03, 1.5716e-03,  ..., 2.3417e-03, 2.3710e-03,
         9.0733e-04],
        [2.8070e-02, 4.5440e-02, 4.2681e-03,  ..., 2.2294e-03, 6.9695e-02,
         7.0094e-03],
        [7.1228e-02, 1.9951e-03, 5.9437e-04,  ..., 3.5250e-04, 1.8027e-02,
         7.6767e-04]], device='cuda:0')
hard_pseudo_label
tensor([0, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 0, 3, 3, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3,
        7, 3, 2, 3, 3, 2, 3, 2, 3, 0, 2, 2, 4, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 4, 3, 2, 3, 3, 3, 2, 4, 2, 2, 3,
        2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 5, 3, 3, 3, 2, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[0, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 0, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 7, 3, 2, 3, 3, 2, 3, 2, 3, 0, 2, 2, 4, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 4, 3, 2, 3, 3, 3, 2, 4, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 5, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 2, 2, 3, 3, 7, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2,
        3, 3, 1, 8, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 8, 3, 2, 2, 3,
        7, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 7, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 2, 5, 2, 3, 3, 2, 3, 2, 9, 3, 2, 2, 2, 4, 2, 3,
        2, 2, 3, 2, 2, 2, 3, 3, 6, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 7, 2, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[9.2978e-03, 1.1036e-02, 7.5909e-01,  ..., 1.1058e-02, 1.7101e-02,
         2.0892e-02],
        [9.3867e-03, 2.5605e-03, 7.2093e-03,  ..., 2.0695e-03, 7.7213e-03,
         3.4238e-03],
        [1.5423e-04, 1.3464e-03, 8.2309e-05,  ..., 9.1420e-04, 1.0436e-03,
         3.2175e-04],
        ...,
        [7.0418e-03, 2.8396e-03, 4.9601e-02,  ..., 6.6378e-03, 8.0715e-03,
         1.0363e-02],
        [3.6192e-03, 5.8126e-02, 5.7514e-01,  ..., 6.8939e-04, 2.1165e-02,
         2.5168e-03],
        [1.9535e-04, 1.1338e-03, 9.8961e-01,  ..., 9.4742e-05, 5.8179e-04,
         8.2972e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 4, 2, 3, 2, 3, 3, 2, 3, 2, 2,
        3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2,
        3, 3, 2, 2, 3, 5, 3, 2, 3, 0, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 9, 2, 3, 3,
        3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 2, 7, 2, 3, 3, 2, 3, 2, 2, 3, 0,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 4, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 5, 3, 2, 3, 0, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 9, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 2, 7, 2, 3, 3, 2, 3, 2, 2, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2]
original label
tensor([2, 3, 3, 2, 3, 3, 2, 1, 2, 3, 3, 3, 3, 3, 3, 2, 7, 2, 3, 3, 3, 3, 9, 2,
        0, 3, 6, 2, 3, 2, 3, 3, 2, 3, 5, 2, 3, 2, 3, 6, 2, 3, 2, 2, 3, 2, 3, 2,
        0, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 3,
        3, 4, 3, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 2, 9, 2, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 2, 2, 4, 2, 2, 2, 2, 3, 2, 2, 6, 6, 3, 3, 2, 8, 3, 2, 3, 3, 6, 3, 3,
        3, 3, 3, 1, 3, 3, 6, 2])
soft_pseudo_label
tensor([[4.3617e-06, 1.5046e-05, 8.4301e-01,  ..., 1.4764e-01, 7.5954e-06,
         6.6537e-03],
        [4.8288e-04, 1.3218e-02, 3.0208e-03,  ..., 1.9195e-02, 5.7198e-03,
         3.3339e-03],
        [5.6968e-02, 3.0893e-02, 1.4538e-01,  ..., 5.7904e-03, 7.8657e-02,
         2.1191e-02],
        ...,
        [1.9655e-03, 1.0485e-03, 2.1269e-01,  ..., 1.0744e-03, 9.8689e-04,
         8.2216e-04],
        [5.0061e-02, 1.2583e-02, 1.2090e-02,  ..., 9.5636e-03, 4.3467e-02,
         3.7532e-02],
        [9.2130e-04, 2.6663e-02, 5.1731e-04,  ..., 1.1455e-03, 7.5150e-03,
         7.2243e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 9,
        2, 2, 3, 2, 2, 0, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 0, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3,
        2, 3, 3, 2, 3, 4, 2, 3, 3, 2, 2, 2, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 3, 0, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        2, 3, 2, 2, 2, 3, 4, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 9, 2, 2, 3, 2, 2, 0, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 4, 2, 3, 3, 2, 2, 2, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 0, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 4, 3]
original label
tensor([2, 2, 3, 2, 3, 4, 2, 0, 2, 3, 0, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 9,
        2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2,
        1, 3, 2, 2, 2, 3, 2, 7, 3, 2, 3, 3, 3, 2, 2, 3, 2, 4, 2, 3, 2, 3, 3, 3,
        2, 3, 3, 2, 2, 4, 2, 3, 4, 2, 2, 2, 3, 2, 3, 3, 3, 3, 7, 5, 2, 3, 2, 3,
        2, 0, 3, 3, 3, 2, 3, 3, 3, 2, 5, 2, 2, 2, 2, 3, 2, 5, 2, 3, 3, 7, 3, 3,
        2, 3, 2, 2, 2, 3, 3, 8])
soft_pseudo_label
tensor([[5.8086e-04, 2.3757e-04, 9.9190e-01,  ..., 1.4297e-04, 3.4114e-04,
         7.1303e-04],
        [5.0051e-04, 9.6147e-04, 9.9195e-01,  ..., 2.3072e-04, 7.7068e-04,
         1.2278e-03],
        [6.7030e-04, 7.6178e-04, 9.8626e-01,  ..., 6.7589e-04, 9.7386e-04,
         2.2316e-03],
        ...,
        [6.2018e-02, 1.1598e-01, 9.0992e-05,  ..., 7.6038e-04, 1.5186e-01,
         1.1504e-03],
        [5.0946e-04, 2.7636e-03, 9.5223e-01,  ..., 8.9851e-04, 1.8275e-03,
         9.9471e-03],
        [1.4936e-03, 1.3416e-03, 9.8342e-01,  ..., 1.3942e-04, 1.6500e-03,
         7.8869e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 7, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2,
        2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3,
        3, 2, 3, 9, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2,
        3, 3, 2, 3, 1, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 7, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 9, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 1, 3, 2, 2]
original label
tensor([2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 0, 3, 3, 1, 3, 2, 3, 2,
        2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 1,
        3, 2, 3, 9, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 7, 8, 8, 4,
        3, 3, 2, 3, 3, 1, 2, 2])
soft_pseudo_label
tensor([[1.2444e-01, 1.2661e-03, 5.9924e-04,  ..., 1.6353e-03, 2.0878e-02,
         2.3005e-03],
        [1.3441e-03, 1.5188e-02, 2.1489e-03,  ..., 3.8963e-04, 8.7624e-03,
         1.5899e-03],
        [9.5190e-05, 4.7592e-04, 9.8271e-01,  ..., 6.6301e-04, 3.3518e-04,
         9.2185e-04],
        ...,
        [3.3740e-03, 2.7493e-04, 9.8906e-01,  ..., 2.9162e-05, 8.9161e-04,
         7.1755e-05],
        [1.4870e-04, 4.5804e-04, 8.2927e-05,  ..., 2.4813e-02, 1.0162e-03,
         9.4734e-03],
        [5.3431e-04, 2.6057e-03, 9.5107e-01,  ..., 7.9093e-03, 1.2981e-03,
         1.8399e-02]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 0, 3, 3, 2, 0, 2, 2, 4, 3, 2, 3, 2, 3, 3, 3, 3, 7, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3,
        3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3,
        3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 0, 3, 3, 2, 0, 2, 2, 4, 3, 2, 3, 2, 3, 3, 3, 3, 7, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2]
original label
tensor([3, 3, 2, 3, 0, 3, 8, 2, 0, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 7, 3, 2, 2,
        3, 3, 3, 3, 3, 3, 5, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 6, 2, 3, 3, 2, 1,
        2, 3, 7, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 8, 2, 3, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 0, 2, 2, 2, 3,
        3, 2, 3, 2, 3, 3, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 8, 3, 5, 3, 3,
        3, 3, 2, 2, 3, 2, 3, 2])
soft_pseudo_label
tensor([[2.0273e-04, 6.5378e-04, 4.3099e-02,  ..., 3.5292e-03, 4.0279e-04,
         1.8372e-03],
        [2.0828e-04, 1.0093e-03, 9.6539e-01,  ..., 3.6054e-03, 4.9478e-04,
         1.1813e-02],
        [1.8739e-04, 3.4450e-04, 9.9485e-01,  ..., 3.0740e-05, 2.6738e-04,
         1.8969e-04],
        ...,
        [6.2790e-04, 7.2377e-04, 9.9432e-01,  ..., 4.3243e-05, 6.7942e-04,
         6.2857e-05],
        [2.9863e-04, 3.0875e-03, 6.8789e-01,  ..., 2.1645e-01, 1.4108e-03,
         6.0460e-02],
        [3.5480e-05, 5.0153e-04, 2.2509e-05,  ..., 7.9578e-04, 2.7032e-04,
         3.4795e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 6, 2, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 0, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2,
        3, 2, 3, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 6, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 0, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3]
original label
tensor([3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 9, 3,
        3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 9, 2, 3, 6, 3, 2, 3, 3, 2, 3, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 9, 2, 8, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 5, 2, 2, 3, 2, 3, 3,
        3, 3, 2, 4, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 8, 8, 2, 5, 3, 6, 3, 2, 2,
        3, 2, 3, 3, 2, 2, 2, 2])
soft_pseudo_label
tensor([[7.3413e-04, 1.3229e-03, 9.4675e-01,  ..., 2.6788e-03, 1.0723e-03,
         6.3640e-03],
        [3.3340e-04, 6.1873e-03, 1.1007e-03,  ..., 2.1672e-03, 2.1430e-03,
         7.9512e-04],
        [5.2696e-04, 1.8229e-03, 7.5790e-03,  ..., 7.8643e-04, 1.2935e-03,
         1.1945e-03],
        ...,
        [1.5799e-01, 1.8784e-03, 4.6597e-04,  ..., 5.1780e-04, 1.7571e-02,
         6.8397e-04],
        [2.4607e-03, 7.7440e-03, 9.1542e-01,  ..., 6.0813e-03, 5.0861e-03,
         2.3888e-02],
        [4.7628e-01, 7.5458e-03, 2.9796e-03,  ..., 3.0473e-03, 1.3136e-01,
         7.6571e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 7, 3, 3, 3, 2, 2, 2, 4,
        2, 3, 4, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 4, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 0, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 4, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 7, 2, 2,
        3, 3, 2, 3, 3, 3, 2, 0], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 7, 3, 3, 3, 2, 2, 2, 4, 2, 3, 4, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 4, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 0, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 4, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 7, 2, 2, 3, 3, 2, 3, 3, 3, 2, 0]
original label
tensor([3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 6, 3, 2, 3, 2, 4, 5, 3, 3, 2, 2, 2, 3,
        2, 3, 4, 2, 8, 2, 3, 2, 2, 2, 3, 2, 3, 6, 3, 6, 2, 2, 3, 3, 2, 2, 2, 2,
        2, 3, 3, 7, 3, 2, 3, 3, 5, 3, 0, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 8, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 0, 3, 1, 2, 3, 3, 2, 1, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2,
        4, 7, 2, 3, 3, 8, 2, 3])
soft_pseudo_label
tensor([[1.3157e-03, 2.1823e-02, 1.6958e-04,  ..., 9.7491e-04, 1.2520e-02,
         2.2196e-03],
        [1.3557e-03, 1.7681e-03, 9.4406e-01,  ..., 2.4548e-03, 1.9552e-03,
         1.5282e-02],
        [3.0956e-04, 2.2387e-03, 8.7709e-01,  ..., 2.4210e-02, 1.3020e-03,
         2.4591e-02],
        ...,
        [1.2119e-02, 3.1721e-02, 3.6164e-02,  ..., 7.8054e-03, 3.2681e-02,
         1.1203e-02],
        [1.1038e-01, 4.2800e-03, 7.9362e-01,  ..., 1.7599e-03, 1.9853e-02,
         2.6601e-03],
        [8.8342e-04, 6.0925e-04, 9.8693e-01,  ..., 2.6785e-04, 5.8935e-04,
         6.6759e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 3, 7, 3, 3, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        2, 2, 1, 2, 3, 3, 3, 2, 3, 2, 4, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 0, 3, 6, 3, 2, 2, 2,
        2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 3, 7, 3, 3, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 1, 2, 3, 3, 3, 2, 3, 2, 4, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 0, 3, 6, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]
original label
tensor([3, 2, 2, 6, 3, 3, 7, 3, 3, 3, 2, 3, 5, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 3, 3, 1, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 4,
        2, 2, 3, 2, 2, 9, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 0, 2, 2, 3, 8, 3, 3, 1, 3, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 3, 5, 3, 2, 2])
soft_pseudo_label
tensor([[2.7327e-03, 3.1040e-03, 8.0405e-04,  ..., 1.8414e-03, 4.8008e-03,
         3.2734e-03],
        [1.1598e-03, 4.0210e-02, 3.8276e-05,  ..., 7.1241e-04, 1.1645e-02,
         2.0624e-03],
        [1.1448e-03, 2.0072e-03, 4.1238e-01,  ..., 2.1266e-02, 1.6287e-03,
         1.2262e-01],
        ...,
        [2.5445e-02, 8.1804e-03, 1.9351e-04,  ..., 6.0015e-04, 1.8471e-02,
         9.5051e-05],
        [4.0405e-04, 2.8913e-02, 1.6563e-02,  ..., 8.5705e-04, 7.3605e-03,
         5.6051e-03],
        [5.9468e-04, 7.2083e-04, 9.9174e-01,  ..., 7.5846e-05, 5.6207e-04,
         6.6715e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 4, 2, 2, 2, 2, 3, 2, 3, 3, 0, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3,
        3, 5, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        4, 3, 3, 3, 3, 2, 9, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 7, 7, 2, 2, 2, 3, 3, 4, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 4, 2, 2, 2, 2, 3, 2, 3, 3, 0, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 5, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 4, 3, 3, 3, 3, 2, 9, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 7, 7, 2, 2, 2, 3, 3, 4, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2]
original label
tensor([3, 3, 2, 5, 2, 2, 2, 2, 2, 3, 3, 8, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2,
        2, 1, 2, 9, 3, 3, 2, 3, 0, 2, 2, 3, 3, 3, 3, 2, 3, 2, 4, 3, 2, 7, 3, 3,
        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 9, 2, 3, 2, 8, 3,
        4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 9, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2,
        2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4, 7, 7, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 8, 1, 2])
soft_pseudo_label
tensor([[1.4109e-03, 2.0013e-03, 9.5135e-01,  ..., 1.7320e-03, 2.1640e-03,
         3.0846e-03],
        [4.4117e-04, 9.1276e-04, 9.8592e-01,  ..., 1.9719e-03, 8.1222e-04,
         4.6809e-03],
        [4.8123e-03, 3.6309e-02, 3.2442e-04,  ..., 5.1138e-04, 1.4143e-02,
         6.4645e-04],
        ...,
        [7.8469e-04, 3.7003e-03, 8.0864e-06,  ..., 4.4237e-04, 2.2553e-03,
         7.6640e-04],
        [1.5612e-03, 5.8624e-04, 9.5572e-01,  ..., 7.0577e-04, 9.5997e-04,
         1.0952e-03],
        [2.8069e-04, 1.4393e-04, 9.9754e-01,  ..., 3.6390e-05, 2.2776e-04,
         1.5892e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 5, 3, 3, 3, 3,
        3, 2, 3, 3, 0, 2, 3, 2, 3, 0, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 7, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 3, 9, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 0, 3, 2, 3,
        3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 9, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 2, 2, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 5, 3, 3, 3, 3, 3, 2, 3, 3, 0, 2, 3, 2, 3, 0, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 7, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 9, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 0, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 9, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2]
original label
tensor([2, 2, 3, 3, 2, 2, 3, 2, 3, 9, 2, 4, 1, 3, 2, 2, 2, 2, 3, 5, 3, 9, 3, 3,
        2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 5, 2, 2, 3, 2, 3, 3, 2, 3,
        9, 2, 3, 2, 2, 3, 4, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 8, 2, 2, 2, 3, 2, 3,
        2, 3, 3, 2, 4, 2, 5, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 8, 9, 2, 3,
        3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 2, 2, 3, 3, 2, 2])
soft_pseudo_label
tensor([[6.9379e-03, 4.0614e-03, 9.0930e-01,  ..., 1.4898e-03, 7.6010e-03,
         2.0724e-03],
        [3.8192e-01, 1.5895e-02, 2.8600e-02,  ..., 3.5658e-03, 1.4355e-01,
         4.9121e-03],
        [3.9263e-04, 6.1769e-04, 9.8366e-01,  ..., 2.5774e-03, 4.7545e-04,
         2.4829e-03],
        ...,
        [1.3592e-02, 6.5782e-04, 4.9380e-02,  ..., 7.2272e-03, 2.7954e-03,
         6.6539e-03],
        [1.5595e-03, 6.9730e-04, 1.1486e-04,  ..., 5.1504e-04, 1.6585e-03,
         3.9708e-03],
        [8.3130e-04, 1.9942e-03, 9.5166e-01,  ..., 4.8181e-03, 1.4127e-03,
         1.8089e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 0, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3,
        2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 2, 4, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2,
        2, 3, 3, 3, 2, 2, 7, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 9, 3, 3, 2,
        3, 3, 3, 2, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 0, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 2, 4, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 7, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 9, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2]
original label
tensor([2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 1,
        2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 8, 3, 5, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        2, 3, 3, 3, 2, 3, 3, 6, 3, 2, 2, 6, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 2, 1, 2, 3, 2, 6, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 5, 9, 2,
        3, 3, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.3272e-03, 1.2742e-03, 5.1109e-05,  ..., 2.4482e-03, 2.6497e-03,
         6.8317e-04],
        [4.3250e-03, 3.2604e-02, 1.0305e-02,  ..., 4.2207e-03, 2.2656e-02,
         7.0065e-03],
        [6.4768e-03, 4.2259e-03, 9.2441e-01,  ..., 3.0649e-04, 7.0752e-03,
         1.4034e-03],
        ...,
        [1.1215e-02, 5.2974e-03, 6.7765e-02,  ..., 8.0697e-03, 9.4207e-03,
         5.7982e-03],
        [9.6387e-03, 1.3648e-01, 1.6717e-03,  ..., 2.5218e-03, 5.2824e-02,
         1.3913e-03],
        [1.5958e-03, 7.3081e-02, 7.7317e-04,  ..., 9.3810e-04, 2.2474e-02,
         3.0759e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 7, 3, 2, 3, 2, 2, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 3, 9, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2,
        3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2,
        2, 3, 2, 3, 4, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 0, 3, 2, 2, 3, 0,
        3, 2, 2, 2, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 7, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 9, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 4, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 0, 3, 2, 2, 3, 0, 3, 2, 2, 2, 2, 3, 3, 3]
original label
tensor([3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 1, 2, 3, 3, 2, 3, 3, 2,
        2, 3, 2, 9, 2, 3, 3, 7, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 9, 3, 3, 2, 2,
        3, 4, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 8, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 3, 2, 3, 8, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 5,
        2, 5, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 0,
        3, 3, 2, 2, 2, 3, 3, 3])
soft_pseudo_label
tensor([[1.0318e-03, 6.3082e-03, 2.8001e-01,  ..., 3.6378e-01, 3.6724e-03,
         6.7753e-02],
        [1.6467e-03, 6.4170e-04, 9.8179e-01,  ..., 6.2439e-04, 1.0570e-03,
         2.5648e-03],
        [4.0176e-05, 1.6830e-05, 9.9886e-01,  ..., 1.2277e-05, 1.8996e-05,
         4.2063e-05],
        ...,
        [9.5920e-01, 1.8916e-04, 5.0221e-05,  ..., 9.9487e-05, 2.7747e-02,
         1.2515e-04],
        [2.4309e-03, 7.4500e-04, 4.5207e-01,  ..., 6.8226e-03, 1.2562e-03,
         1.1679e-02],
        [2.1706e-04, 5.7974e-04, 9.8469e-01,  ..., 1.9686e-04, 5.0026e-04,
         1.1409e-03]], device='cuda:0')
hard_pseudo_label
tensor([7, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 9, 3, 3, 0, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 2, 3, 4, 2, 2, 3, 3, 3, 2, 2, 3, 7, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3,
        3, 2, 3, 3, 3, 3, 3, 2, 2, 0, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        9, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2,
        2, 3, 3, 3, 2, 0, 3, 2], device='cuda:0')
hard_pseudo_label
[7, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 9, 3, 3, 0, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 4, 2, 2, 3, 3, 3, 2, 2, 3, 7, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 0, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 9, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 0, 3, 2]
original label
tensor([2, 2, 2, 2, 3, 2, 2, 2, 3, 9, 6, 2, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 4, 3, 7, 3, 3, 0, 2, 3, 2, 2, 2, 2, 2, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 4, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3,
        3, 2, 3, 2, 7, 3, 3, 2, 2, 6, 3, 3, 2, 3, 5, 7, 2, 3, 3, 3, 3, 3, 2, 2,
        9, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 4, 2, 2, 3, 2, 3, 2, 2,
        2, 2, 3, 3, 2, 0, 2, 2])
soft_pseudo_label
tensor([[6.3789e-04, 1.0003e-02, 1.0745e-03,  ..., 1.5551e-03, 5.2173e-03,
         2.3367e-03],
        [6.2306e-04, 2.0355e-04, 9.9554e-01,  ..., 3.9422e-05, 4.5028e-04,
         3.9553e-04],
        [5.0218e-04, 4.4709e-04, 9.8752e-01,  ..., 7.1829e-04, 4.7084e-04,
         5.8263e-03],
        ...,
        [1.9591e-03, 1.4228e-03, 8.1103e-03,  ..., 2.2084e-02, 2.2949e-03,
         4.6999e-03],
        [9.9544e-04, 2.5953e-03, 9.8381e-01,  ..., 5.2945e-04, 1.4608e-03,
         2.8832e-03],
        [6.2636e-04, 3.5373e-03, 9.6584e-01,  ..., 1.6613e-04, 1.4836e-03,
         2.8606e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 2, 7, 2, 2, 3, 3, 4, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2,
        3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        2, 2, 2, 3, 3, 2, 2, 2, 0, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 4, 3, 2, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3,
        3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 2, 7, 2, 2, 3, 3, 4, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 0, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 4, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 3, 2, 9, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 7, 2, 3, 3, 3, 2,
        6, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 2, 0, 3, 2, 3, 2, 0, 2, 2, 3, 2, 2, 2, 7, 3, 2, 2, 3, 2, 3, 2, 2,
        2, 1, 3, 3, 3, 2, 3, 4, 3, 2, 2, 2, 3, 3, 7, 2, 3, 2, 2, 3, 2, 2, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 7, 3, 2, 3, 1, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2,
        2, 2, 3, 3, 2, 8, 2, 2])
soft_pseudo_label
tensor([[9.3197e-05, 7.4543e-03, 2.5284e-04,  ..., 2.9996e-04, 1.5261e-03,
         2.1882e-04],
        [2.3697e-03, 1.2532e-02, 5.2012e-03,  ..., 2.6359e-03, 6.3976e-03,
         3.0877e-03],
        [2.8337e-03, 1.8548e-03, 3.2638e-05,  ..., 4.0328e-04, 2.8117e-03,
         1.5987e-04],
        ...,
        [4.5109e-03, 5.9527e-03, 2.2139e-02,  ..., 1.5139e-03, 7.9408e-03,
         1.1976e-03],
        [4.5390e-04, 3.3769e-03, 5.9691e-01,  ..., 1.4345e-01, 1.9563e-03,
         4.0759e-02],
        [5.7676e-04, 7.9775e-03, 8.4222e-06,  ..., 1.3619e-04, 7.0815e-03,
         1.8273e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 7, 3, 3, 3, 3, 2, 2, 2, 2, 2,
        3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 4, 2, 2, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3,
        2, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 7, 3, 3, 2, 2, 2, 3, 0, 3, 2, 0, 3, 3, 3, 2, 2, 7, 2, 3,
        3, 3, 2, 3, 2, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 7, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 4, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 7, 3, 3, 2, 2, 2, 3, 0, 3, 2, 0, 3, 3, 3, 2, 2, 7, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3]
original label
tensor([3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 6, 3, 7, 1, 3, 1, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 7,
        2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 9, 3, 3, 6, 2, 2, 2, 3, 2, 3, 3, 3,
        2, 3, 3, 7, 0, 3, 3, 2, 3, 2, 5, 3, 2, 3, 2, 3, 3, 9, 2, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 3, 8, 0, 2, 2, 3, 2, 6,
        3, 3, 2, 3, 2, 7, 3, 3])
soft_pseudo_label
tensor([[6.7874e-04, 8.7242e-04, 9.9356e-01,  ..., 9.9103e-05, 7.2181e-04,
         9.6064e-04],
        [1.3529e-01, 7.8046e-04, 5.3067e-04,  ..., 3.1512e-02, 3.1946e-02,
         3.0109e-03],
        [1.0390e-03, 3.3876e-03, 4.8152e-04,  ..., 8.2551e-04, 3.0988e-03,
         2.0502e-03],
        ...,
        [3.9142e-04, 7.3090e-04, 9.8928e-01,  ..., 3.0276e-04, 6.0064e-04,
         8.1578e-04],
        [7.8505e-04, 2.0927e-03, 7.7929e-01,  ..., 2.2139e-02, 1.4524e-03,
         1.1628e-01],
        [2.5859e-03, 3.3798e-02, 6.2856e-03,  ..., 3.2945e-03, 1.8535e-02,
         1.3039e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 4, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2,
        3, 2, 0, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 0, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 0, 2, 3, 7, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        2, 9, 2, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 4, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 0, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 0, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 0, 2, 3, 7, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 9, 2, 2, 3, 2, 2, 3]
original label
tensor([2, 6, 1, 3, 3, 3, 2, 2, 0, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2,
        3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 0, 2, 2, 0, 3, 3, 2,
        3, 2, 3, 3, 2, 6, 2, 3, 3, 3, 3, 8, 2, 2, 3, 2, 2, 3, 3, 3, 3, 0, 3, 2,
        3, 2, 3, 3, 3, 2, 2, 2, 6, 2, 0, 2, 3, 3, 0, 3, 3, 3, 2, 2, 7, 2, 3, 3,
        2, 2, 2, 2, 3, 2, 2, 2])
soft_pseudo_label
tensor([[1.0320e-02, 3.5182e-03, 9.4966e-01,  ..., 2.2197e-04, 6.4112e-03,
         1.5021e-03],
        [1.4194e-03, 1.3892e-03, 5.2178e-03,  ..., 1.1746e-02, 3.6059e-03,
         3.3533e-03],
        [6.3919e-04, 2.6185e-03, 9.0711e-01,  ..., 3.6657e-03, 1.9046e-03,
         4.7376e-02],
        ...,
        [5.9936e-04, 1.6380e-03, 9.7806e-01,  ..., 1.7175e-03, 1.3233e-03,
         3.0128e-03],
        [1.7572e-02, 4.7423e-02, 1.4071e-03,  ..., 3.2335e-03, 4.7702e-02,
         2.3222e-03],
        [6.2868e-05, 1.5456e-03, 4.9546e-04,  ..., 1.7514e-03, 7.5218e-04,
         1.2962e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2,
        2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 3, 2, 3, 7, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2,
        7, 0, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 2, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 7, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 7, 0, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3]
original label
tensor([2, 2, 2, 2, 6, 2, 3, 5, 3, 2, 2, 2, 4, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 2, 8, 6, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 0, 2, 3, 3, 3, 2, 2,
        2, 6, 7, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 6, 3, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 2, 2, 3, 2, 2, 4, 2, 3, 2, 3, 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 2,
        9, 0, 3, 2, 2, 2, 3, 1, 2, 3, 2, 6, 3, 2, 3, 2, 0, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 3, 3])
soft_pseudo_label
tensor([[1.2625e-03, 1.4597e-02, 1.6747e-04,  ..., 4.3017e-04, 8.3173e-03,
         1.1317e-03],
        [2.2848e-02, 6.3912e-03, 2.5136e-02,  ..., 2.0368e-03, 1.7233e-02,
         4.3612e-02],
        [4.3954e-03, 3.0514e-01, 1.4984e-03,  ..., 2.3207e-03, 8.3015e-02,
         2.2493e-03],
        ...,
        [3.8998e-03, 6.6296e-04, 9.9226e-01,  ..., 7.2117e-06, 1.2974e-03,
         2.6535e-05],
        [1.1853e-03, 1.1264e-03, 9.9046e-01,  ..., 2.1244e-04, 1.1608e-03,
         1.1258e-03],
        [1.1568e-04, 4.4020e-04, 9.8809e-01,  ..., 7.3038e-04, 2.2059e-04,
         2.0625e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2,
        3, 2, 3, 2, 2, 2, 3, 8, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 4, 3,
        2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        2, 2, 2, 3, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 8, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 4, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2]
original label
tensor([2, 3, 2, 2, 6, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 8, 2,
        3, 2, 0, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 9, 3, 3, 2, 2, 2, 3, 2, 2,
        2, 2, 2, 0, 2, 2, 2, 2])
soft_pseudo_label
tensor([[8.8956e-03, 7.9138e-03, 1.2688e-03,  ..., 2.7084e-03, 1.5540e-02,
         1.3116e-03],
        [1.8868e-03, 1.3511e-03, 8.5628e-04,  ..., 3.7698e-03, 3.2134e-03,
         6.8446e-03],
        [3.0904e-05, 1.7015e-03, 1.1221e-03,  ..., 8.6710e-02, 3.9186e-04,
         3.8995e-04],
        ...,
        [3.3326e-04, 2.0010e-03, 9.3141e-01,  ..., 1.7166e-03, 1.2738e-03,
         2.7801e-03],
        [4.4758e-04, 1.8446e-02, 2.4912e-04,  ..., 2.7952e-03, 7.8950e-03,
         1.2651e-03],
        [4.4697e-03, 4.7662e-03, 8.2614e-03,  ..., 5.8516e-04, 4.2526e-03,
         2.2232e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 0, 7, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 9, 2,
        3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 3, 2, 4, 2, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2,
        3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 9, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 2, 2, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 0, 7, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 9, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 4, 2, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 9, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3]
original label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 7, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 4, 2, 9, 2,
        2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 7, 3, 3, 3, 3, 3, 3, 3, 4, 2, 3,
        2, 3, 2, 3, 2, 6, 2, 2, 0, 3, 3, 4, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 5, 2, 2, 2,
        1, 2, 3, 2, 2, 3, 3, 2, 2, 7, 3, 2, 3, 4, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3,
        2, 2, 2, 2, 2, 2, 3, 0])
soft_pseudo_label
tensor([[6.0757e-03, 3.3680e-04, 1.5910e-03,  ..., 5.4828e-04, 2.3938e-03,
         2.5789e-03],
        [5.1960e-03, 4.9921e-03, 7.5798e-02,  ..., 2.5911e-03, 6.3819e-03,
         4.5431e-03],
        [3.7288e-01, 1.8239e-03, 1.1113e-02,  ..., 7.8340e-03, 3.9398e-02,
         1.2069e-02],
        ...,
        [5.3154e-02, 1.5085e-02, 1.0224e-02,  ..., 4.3123e-03, 4.4957e-02,
         3.7539e-03],
        [2.2877e-03, 1.5090e-03, 3.6922e-03,  ..., 9.9060e-04, 2.0488e-03,
         1.2776e-03],
        [1.9916e-03, 1.0770e-02, 1.9334e-02,  ..., 3.0756e-03, 7.5217e-03,
         1.5336e-02]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        2, 2, 4, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 5, 2, 1, 0, 3, 3, 3, 9,
        3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 4, 2, 2, 2, 3, 3, 2, 3, 3, 0, 2, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 4, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 5, 2, 1, 0, 3, 3, 3, 9, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 4, 2, 2, 2, 3, 3, 2, 3, 3, 0, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3]
original label
tensor([3, 3, 0, 2, 3, 2, 3, 0, 2, 3, 2, 2, 3, 7, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 0, 2, 3,
        2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 6, 3, 3, 2, 2, 2, 2, 3, 0, 0, 3, 3, 3, 7,
        1, 3, 2, 3, 3, 2, 7, 3, 2, 3, 6, 2, 2, 7, 3, 3, 2, 3, 3, 3, 2, 2, 3, 5,
        1, 3, 2, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 3, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.9019e-04, 1.7306e-03, 9.8534e-01,  ..., 5.8639e-04, 8.9852e-04,
         1.1150e-03],
        [2.4292e-02, 2.8299e-03, 1.3986e-01,  ..., 1.2704e-02, 1.1030e-02,
         2.2907e-02],
        [3.8162e-04, 3.8485e-03, 2.8442e-04,  ..., 1.4679e-03, 1.9895e-03,
         5.7455e-04],
        ...,
        [2.6054e-04, 9.7561e-04, 9.6845e-01,  ..., 7.7946e-03, 5.1562e-04,
         1.1542e-02],
        [1.2461e-03, 1.0763e-03, 9.8333e-01,  ..., 3.6513e-04, 1.0271e-03,
         3.1958e-03],
        [2.8860e-04, 2.1162e-02, 7.9208e-05,  ..., 6.8292e-04, 5.6211e-03,
         7.5445e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 4, 2, 3, 3, 3, 3, 2,
        7, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 1, 3, 3, 0, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,
        0, 3, 2, 3, 0, 3, 2, 2, 0, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 1, 2, 3, 3, 2, 2,
        3, 3, 3, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 4, 2, 3, 3, 3, 3, 2, 7, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 1, 3, 3, 0, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 0, 3, 2, 3, 0, 3, 2, 2, 0, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 1, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3]
original label
tensor([2, 3, 3, 2, 9, 2, 3, 2, 2, 2, 3, 3, 3, 1, 2, 3, 2, 4, 2, 3, 3, 3, 3, 2,
        7, 3, 2, 3, 6, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 8, 2, 2, 2, 3, 3, 2, 6, 3, 3, 3,
        3, 3, 2, 3, 1, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 6, 3, 3, 3, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 2, 3, 9, 2, 2,
        2, 3, 2, 2, 3, 2, 2, 3])
soft_pseudo_label
tensor([[4.9328e-04, 7.1597e-04, 2.1553e-03,  ..., 8.8908e-04, 1.0293e-03,
         8.6700e-04],
        [8.6387e-04, 8.5979e-03, 5.1634e-04,  ..., 2.1821e-04, 3.5964e-03,
         1.9560e-04],
        [4.3057e-04, 3.2922e-03, 9.0483e-01,  ..., 3.2638e-02, 1.7247e-03,
         2.5895e-02],
        ...,
        [1.4432e-03, 3.5172e-02, 1.5890e-02,  ..., 2.7335e-03, 1.3660e-02,
         1.4489e-02],
        [1.2980e-03, 3.9956e-04, 9.7403e-01,  ..., 6.0393e-04, 6.0986e-04,
         2.1584e-03],
        [1.6344e-03, 1.9628e-03, 2.6612e-05,  ..., 2.0749e-04, 2.2702e-03,
         1.6397e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 3, 2, 2, 2, 2, 7, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2,
        2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 9, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3,
        4, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 0,
        2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2,
        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 3, 2, 0, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 3, 2, 2, 2, 2, 7, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 9, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 4, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 0, 2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 0, 3, 2, 3]
original label
tensor([3, 3, 2, 2, 3, 2, 2, 3, 2, 9, 3, 3, 3, 8, 2, 3, 8, 3, 3, 3, 2, 3, 2, 3,
        2, 3, 2, 5, 2, 3, 2, 3, 2, 3, 2, 3, 7, 0, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3,
        2, 2, 3, 2, 2, 1, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,
        2, 3, 2, 3, 3, 3, 2, 6, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 1, 3, 8, 3, 3, 3, 2, 2, 3, 3, 2, 1, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 2, 3])
soft_pseudo_label
tensor([[5.8606e-04, 2.8525e-03, 2.7842e-05,  ..., 1.0992e-03, 3.9256e-03,
         2.9158e-03],
        [1.7079e-03, 2.5440e-03, 9.5476e-01,  ..., 2.8911e-03, 2.9611e-03,
         5.2659e-03],
        [1.0922e-02, 3.8907e-03, 8.0235e-02,  ..., 2.0224e-03, 9.3608e-03,
         6.1931e-03],
        ...,
        [3.1688e-02, 5.1638e-02, 2.2372e-02,  ..., 2.7693e-02, 7.2094e-02,
         1.9390e-02],
        [6.0120e-05, 5.8727e-05, 9.2085e-01,  ..., 5.1748e-02, 4.8497e-05,
         4.9035e-03],
        [1.4837e-01, 9.3655e-03, 3.1515e-05,  ..., 3.5507e-04, 6.1490e-02,
         2.0671e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 9, 2, 2, 0, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2,
        3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 4, 3, 3, 3, 3, 3, 2, 3, 2, 5, 3, 3, 3, 2, 2, 3,
        3, 0, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 9, 9, 2, 3, 3,
        2, 9, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 7, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2,
        3, 2, 2, 2, 3, 3, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 9, 2, 2, 0, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 4, 3, 3, 3, 3, 3, 2, 3, 2, 5, 3, 3, 3, 2, 2, 3, 3, 0, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 9, 9, 2, 3, 3, 2, 9, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 7, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3]
original label
tensor([3, 2, 3, 4, 2, 2, 3, 2, 3, 2, 3, 2, 2, 6, 4, 3, 2, 2, 2, 2, 3, 2, 2, 2,
        3, 5, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 9, 3, 3, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 5, 3, 3, 2, 2, 3,
        3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 9, 2, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 7, 2, 2, 2, 0, 3, 2, 6, 2, 3, 2,
        3, 2, 2, 2, 5, 3, 2, 3])
soft_pseudo_label
tensor([[2.6952e-03, 6.1875e-03, 6.0048e-01,  ..., 6.6004e-02, 6.5353e-03,
         1.2440e-01],
        [4.6313e-04, 2.1048e-01, 1.7342e-03,  ..., 2.8982e-04, 2.0658e-02,
         9.5213e-04],
        [1.6565e-03, 1.7821e-04, 9.8476e-01,  ..., 6.9696e-04, 4.2088e-04,
         8.6316e-04],
        ...,
        [2.3626e-04, 1.1567e-03, 9.8738e-01,  ..., 2.4222e-03, 6.2063e-04,
         1.7571e-03],
        [4.8263e-03, 1.4958e-03, 9.2291e-04,  ..., 2.4822e-03, 4.7329e-03,
         1.0483e-03],
        [2.5056e-03, 4.2491e-03, 9.6124e-01,  ..., 8.8428e-04, 3.4255e-03,
         9.0317e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 9, 2, 3, 2, 2, 3, 3, 3, 3, 2,
        2, 3, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 0, 2, 2, 2, 2, 3,
        2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 9, 3, 3, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 3, 3, 9, 2, 2, 2, 3, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3,
        3, 0, 2, 4, 3, 3, 3, 3, 9, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 7,
        2, 3, 3, 2, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 9, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 0, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 9, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 9, 2, 2, 2, 3, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 0, 2, 4, 3, 3, 3, 3, 9, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 7, 2, 3, 3, 2, 3, 2, 3, 2]
original label
tensor([2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 7, 2, 2, 3, 2, 2, 3, 2,
        2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 2, 2, 3,
        2, 2, 3, 5, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 0, 9, 3, 2, 2, 3, 3,
        2, 2, 3, 3, 3, 2, 5, 3, 9, 2, 2, 2, 3, 8, 3, 2, 3, 2, 2, 3, 7, 3, 3, 3,
        2, 3, 2, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 2, 3, 3, 9, 3, 2, 2, 2, 2, 2, 4,
        2, 3, 8, 2, 3, 2, 3, 2])
soft_pseudo_label
tensor([[1.7432e-01, 1.4281e-02, 5.9102e-02,  ..., 1.7125e-02, 6.2987e-02,
         2.8470e-02],
        [1.4047e-04, 2.3250e-04, 9.9765e-01,  ..., 2.2466e-05, 1.7119e-04,
         1.5181e-04],
        [2.7402e-02, 1.5289e-01, 7.9219e-04,  ..., 8.4164e-04, 1.3336e-01,
         4.5498e-03],
        ...,
        [1.0172e-04, 3.7487e-03, 1.5330e-04,  ..., 1.6414e-03, 1.9496e-03,
         4.7035e-04],
        [1.0316e-04, 3.6028e-03, 3.6146e-04,  ..., 5.7649e-04, 1.2185e-03,
         8.4289e-04],
        [7.0931e-04, 1.8308e-03, 8.9355e-01,  ..., 4.6065e-04, 1.5984e-03,
         5.2092e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 4, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4,
        0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2,
        3, 7, 2, 3, 4, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3,
        3, 2, 3, 3, 2, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 4, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 7, 2, 3, 4, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2]
original label
tensor([6, 2, 3, 3, 3, 2, 3, 2, 3, 9, 4, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 5, 3, 4,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 4,
        3, 7, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        3, 2, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[1.3988e-03, 5.2642e-04, 9.9431e-01,  ..., 4.5674e-05, 6.4863e-04,
         2.0996e-04],
        [1.0019e-02, 2.0524e-03, 1.2020e-02,  ..., 6.9727e-04, 5.5633e-03,
         1.2406e-03],
        [5.6680e-04, 5.0666e-03, 1.0152e-04,  ..., 4.4925e-04, 2.9517e-03,
         1.0925e-03],
        ...,
        [1.0978e-03, 1.7212e-03, 9.8256e-01,  ..., 7.9924e-04, 1.4486e-03,
         3.0729e-03],
        [7.9052e-04, 6.4395e-04, 3.1057e-02,  ..., 1.4902e-02, 8.1801e-04,
         2.5895e-03],
        [1.1924e-03, 2.5615e-03, 9.7556e-01,  ..., 1.0983e-04, 2.2247e-03,
         9.8085e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 9, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3,
        2, 3, 3, 9, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2,
        2, 0, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 4, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 3, 3, 7, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 9, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 9, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 4, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 7, 2, 3, 2]
original label
tensor([2, 3, 3, 3, 9, 5, 2, 8, 2, 3, 3, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 7, 3, 3,
        2, 3, 3, 3, 3, 1, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3,
        2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 4, 2, 2, 3, 3, 2, 2, 2, 3, 3, 5, 3, 3,
        2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 4, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        2, 2, 9, 2, 9, 2, 3, 2])
soft_pseudo_label
tensor([[1.5545e-04, 7.8714e-04, 9.7148e-01,  ..., 1.0440e-02, 3.6785e-04,
         1.0604e-02],
        [3.8086e-03, 2.2459e-02, 4.4571e-03,  ..., 2.3055e-03, 1.9389e-02,
         1.4210e-02],
        [5.5979e-02, 4.0292e-02, 5.1370e-02,  ..., 1.4866e-02, 5.5781e-02,
         8.2411e-02],
        ...,
        [2.5005e-04, 1.1184e-03, 9.5896e-01,  ..., 2.2994e-03, 8.3850e-04,
         5.1856e-03],
        [7.1635e-05, 4.4054e-04, 9.8548e-01,  ..., 3.5364e-04, 1.9454e-04,
         4.3462e-03],
        [5.1619e-04, 3.7422e-03, 8.6983e-05,  ..., 3.4910e-04, 2.4009e-03,
         7.2723e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 4, 2, 2, 3, 2, 3, 3, 3, 0, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 7, 3, 3, 3, 3, 2, 2, 3, 3, 7, 2,
        3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 7, 3,
        0, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 9, 2, 2, 3, 3, 2, 8, 0, 3, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 0, 3, 2, 2, 3, 3, 3, 2, 3,
        3, 2, 2, 2, 2, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 4, 2, 2, 3, 2, 3, 3, 3, 0, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 7, 3, 3, 3, 3, 2, 2, 3, 3, 7, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 7, 3, 0, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 9, 2, 2, 3, 3, 2, 8, 0, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 0, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3]
original label
tensor([2, 2, 3, 2, 2, 3, 2, 3, 3, 6, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 5, 2, 2, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 2, 7, 2,
        3, 7, 2, 8, 3, 3, 3, 5, 8, 3, 3, 2, 2, 3, 3, 2, 3, 7, 3, 3, 2, 3, 3, 3,
        0, 7, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 7, 2, 2, 3, 3, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 6, 3, 2, 2, 3, 5, 3, 2, 3,
        2, 2, 2, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[2.0798e-04, 4.7748e-04, 6.9915e-04,  ..., 8.6896e-03, 7.8742e-04,
         1.1117e-03],
        [1.0915e-02, 1.4799e-01, 1.3018e-03,  ..., 1.8038e-03, 5.6526e-02,
         7.1202e-03],
        [6.2308e-04, 2.1798e-03, 9.8164e-01,  ..., 8.2384e-04, 1.5687e-03,
         1.6740e-03],
        ...,
        [3.6485e-04, 4.7215e-04, 9.8256e-01,  ..., 1.2462e-04, 4.8287e-04,
         8.9205e-04],
        [2.6379e-03, 1.2220e-03, 6.8109e-01,  ..., 3.2910e-03, 1.3659e-03,
         5.7843e-03],
        [2.8333e-04, 4.3669e-04, 9.9227e-01,  ..., 3.5781e-04, 3.6558e-04,
         9.1326e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 7, 3, 3, 3, 3, 2, 3, 3, 2,
        3, 2, 2, 2, 7, 3, 3, 0, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2,
        3, 2, 2, 2, 3, 3, 2, 0, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 0, 2, 4, 2, 3, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 3, 2, 3, 4, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2,
        3, 3, 3, 3, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 7, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 7, 3, 3, 0, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 0, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 0, 2, 4, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 4, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2]
original label
tensor([3, 5, 2, 0, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 9, 3, 3, 3, 3, 2, 3, 3, 2,
        3, 1, 2, 2, 7, 3, 3, 0, 1, 8, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 8, 2,
        3, 2, 2, 2, 2, 2, 2, 0, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 2, 2, 2, 4, 2, 3, 3, 3, 2, 6, 2, 8, 2, 3, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 3, 2, 3, 5, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 9,
        3, 3, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[1.0613e-03, 6.5941e-03, 2.4243e-04,  ..., 3.5067e-04, 4.6759e-03,
         1.7541e-03],
        [4.4056e-04, 1.6584e-03, 2.4239e-03,  ..., 5.3558e-04, 2.7912e-03,
         2.3467e-04],
        [1.4828e-02, 1.3433e-03, 5.1771e-03,  ..., 1.2207e-03, 6.8185e-03,
         8.6984e-04],
        ...,
        [7.5865e-05, 1.9755e-04, 9.9490e-01,  ..., 6.8205e-05, 1.5132e-04,
         2.9685e-04],
        [2.3658e-04, 1.9008e-03, 1.4248e-03,  ..., 4.8188e-04, 8.0562e-04,
         2.1478e-04],
        [1.5972e-03, 1.9829e-03, 1.1236e-04,  ..., 4.3685e-04, 1.7490e-03,
         1.9893e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 0, 2, 7, 2, 3, 3, 2, 2, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 9, 3, 2, 0,
        2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 9, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3,
        3, 9, 2, 2, 3, 2, 8, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2,
        2, 2, 3, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 0, 2, 7, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 9, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 9, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 9, 2, 2, 3, 2, 8, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3]
original label
tensor([3, 3, 3, 3, 2, 4, 3, 3, 2, 3, 3, 3, 3, 2, 7, 2, 5, 3, 2, 2, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 8, 2, 3,
        2, 2, 3, 2, 3, 2, 3, 3, 2, 8, 3, 3, 3, 7, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 1, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 1, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 8, 2, 0, 2, 2,
        2, 2, 3, 3, 2, 2, 3, 3])
soft_pseudo_label
tensor([[2.7898e-01, 3.5141e-02, 1.8347e-02,  ..., 6.6060e-03, 1.2885e-01,
         1.9218e-02],
        [7.8451e-04, 6.3551e-02, 2.3977e-03,  ..., 3.1883e-04, 1.4035e-02,
         7.4859e-04],
        [8.0212e-05, 3.5469e-04, 9.9593e-01,  ..., 1.0783e-04, 1.7857e-04,
         1.1967e-03],
        ...,
        [6.9310e-04, 1.8043e-05, 9.9521e-01,  ..., 4.1618e-06, 4.4395e-05,
         7.5225e-05],
        [5.0663e-04, 4.9244e-03, 4.5636e-04,  ..., 1.1306e-03, 4.1326e-03,
         2.9312e-03],
        [2.2256e-03, 1.3095e-02, 5.0795e-03,  ..., 1.7198e-03, 1.0769e-02,
         1.2958e-02]], device='cuda:0')
hard_pseudo_label
tensor([0, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3,
        3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 9, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3,
        3, 2, 2, 9, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 5, 2, 2, 3, 3, 3, 2, 3, 2,
        2, 2, 3, 3, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[0, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 9, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 9, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 5, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3]
original label
tensor([3, 3, 2, 3, 3, 2, 5, 2, 3, 2, 2, 3, 8, 2, 3, 2, 2, 3, 6, 2, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 2, 4, 3, 3, 2, 2, 3, 2, 2, 6, 2, 2, 2, 3, 3, 1, 0, 3,
        3, 3, 2, 3, 6, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 6, 3, 3, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 5, 2, 3, 3, 3, 3, 3, 4, 2, 2, 6, 2, 3, 3,
        3, 2, 2, 4, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 5, 2, 2, 3, 3, 3, 4, 3, 2,
        2, 2, 3, 3, 3, 4, 3, 3])
soft_pseudo_label
tensor([[4.1378e-04, 2.0228e-03, 3.4791e-01,  ..., 8.8621e-03, 1.3163e-03,
         5.8126e-03],
        [8.1066e-01, 8.0372e-03, 5.7012e-04,  ..., 1.4247e-04, 1.4648e-01,
         8.1018e-05],
        [7.8314e-04, 4.1674e-04, 7.7135e-01,  ..., 3.6430e-02, 6.4043e-04,
         2.7796e-02],
        ...,
        [1.2174e-03, 6.5364e-03, 2.1777e-03,  ..., 2.2889e-03, 5.1992e-03,
         3.2271e-03],
        [1.6669e-03, 1.1700e-03, 9.8354e-01,  ..., 1.1903e-04, 1.8197e-03,
         5.2622e-04],
        [3.0097e-04, 2.7511e-04, 9.8439e-01,  ..., 1.1529e-03, 2.8607e-04,
         5.7346e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 0, 2, 3, 2, 3, 2, 7, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2,
        2, 3, 3, 2, 2, 4, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 2, 3,
        2, 2, 0, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,
        3, 2, 3, 2, 3, 2, 3, 2, 2, 9, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 6, 3, 3, 2, 7, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        2, 2, 3, 3, 3, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 0, 2, 3, 2, 3, 2, 7, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 4, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 2, 3, 2, 2, 0, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 9, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 6, 3, 3, 2, 7, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2]
original label
tensor([3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 2, 2, 2, 3, 3, 3, 7,
        2, 3, 2, 9, 2, 3, 2, 2, 3, 5, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 2, 3,
        2, 2, 0, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 0, 3, 2, 3, 2, 3, 3, 2, 2, 5,
        3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 6, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 2, 3, 2, 6, 0, 3, 2, 9, 2, 3, 2, 3, 3, 3, 2, 2, 2, 8, 2, 3, 3,
        2, 3, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[2.8517e-02, 1.5331e-02, 3.8138e-04,  ..., 5.1722e-04, 2.9250e-02,
         1.2739e-03],
        [1.9921e-03, 1.5321e-02, 1.6211e-03,  ..., 2.1498e-03, 1.0998e-02,
         3.7454e-03],
        [6.4373e-03, 8.7666e-03, 5.0408e-04,  ..., 6.5617e-04, 1.0794e-02,
         7.4383e-03],
        ...,
        [3.5910e-03, 1.3531e-03, 1.7728e-02,  ..., 4.8780e-02, 3.3423e-03,
         6.3705e-03],
        [5.3076e-02, 2.5662e-03, 1.8683e-03,  ..., 2.7505e-03, 1.6832e-02,
         9.0892e-03],
        [7.1305e-04, 1.3551e-03, 9.8752e-01,  ..., 4.3418e-04, 9.7796e-04,
         9.9094e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3,
        2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 9, 2, 2, 3, 2,
        2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 7, 9, 3, 2, 2, 3, 3, 3,
        3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 0, 2,
        3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 4, 3, 7, 2, 3,
        3, 3, 3, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 9, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 7, 9, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 0, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 4, 3, 7, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2]
original label
tensor([5, 3, 3, 3, 2, 9, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 5, 2, 1,
        2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 7, 2, 2, 3, 2,
        2, 2, 3, 8, 3, 3, 3, 3, 3, 2, 3, 3, 6, 5, 2, 2, 3, 2, 8, 2, 2, 3, 3, 2,
        3, 3, 3, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 6, 2,
        3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 9, 2, 3, 3, 3, 3, 3, 3, 2, 3,
        5, 3, 0, 3, 3, 7, 3, 2])
soft_pseudo_label
tensor([[1.6817e-03, 8.4747e-04, 9.9124e-01,  ..., 4.0981e-04, 1.2979e-03,
         6.0596e-04],
        [6.6403e-04, 5.0009e-03, 1.0846e-02,  ..., 1.8351e-02, 4.6093e-03,
         3.4934e-02],
        [4.1578e-02, 1.9014e-03, 2.3258e-01,  ..., 9.7559e-03, 1.0605e-02,
         5.2176e-02],
        ...,
        [1.1661e-04, 2.2423e-04, 9.9609e-01,  ..., 1.0525e-04, 1.5284e-04,
         8.4954e-04],
        [1.3966e-03, 1.8479e-03, 9.6593e-01,  ..., 3.2467e-04, 1.5656e-03,
         1.6520e-03],
        [1.7358e-03, 1.8753e-02, 5.8251e-03,  ..., 1.8460e-03, 1.1041e-02,
         8.1091e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3,
        3, 3, 3, 2, 2, 7, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3,
        2, 2, 2, 3, 3, 3, 3, 3, 7, 2, 2, 2, 3, 4, 2, 2, 3, 3, 3, 3, 3, 8, 2, 3,
        2, 6, 3, 0, 3, 2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,
        3, 2, 3, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 7, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 7, 2, 2, 2, 3, 4, 2, 2, 3, 3, 3, 3, 3, 8, 2, 3, 2, 6, 3, 0, 3, 2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3]
original label
tensor([2, 3, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,
        3, 3, 2, 3, 3, 2, 3, 1, 2, 3, 2, 2, 6, 3, 5, 2, 3, 2, 2, 3, 3, 3, 2, 3,
        3, 3, 3, 2, 5, 3, 2, 2, 3, 4, 3, 3, 0, 3, 2, 2, 3, 3, 2, 3, 2, 7, 2, 2,
        2, 5, 2, 3, 3, 3, 2, 6, 7, 2, 2, 2, 2, 6, 2, 2, 3, 2, 3, 3, 3, 6, 4, 3,
        2, 5, 3, 3, 3, 2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 7, 2, 3, 3, 2, 3, 3,
        3, 2, 3, 2, 4, 2, 2, 2])
soft_pseudo_label
tensor([[6.9247e-01, 6.5414e-03, 1.8176e-02,  ..., 1.1569e-03, 1.0839e-01,
         3.1974e-03],
        [3.9757e-03, 3.1790e-03, 9.7759e-01,  ..., 1.9465e-05, 3.8761e-03,
         5.5886e-05],
        [5.0438e-02, 2.8265e-02, 2.4383e-04,  ..., 8.5772e-04, 7.5127e-02,
         1.3800e-03],
        ...,
        [2.5973e-04, 6.7765e-04, 9.8931e-01,  ..., 6.7293e-05, 4.9457e-04,
         2.9826e-03],
        [3.7466e-05, 1.2824e-04, 1.4507e-02,  ..., 6.2335e-03, 1.6595e-04,
         9.4429e-01],
        [8.7870e-04, 9.8988e-04, 9.9032e-01,  ..., 4.8243e-04, 1.1949e-03,
         1.1934e-03]], device='cuda:0')
hard_pseudo_label
tensor([0, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 0, 3, 2,
        4, 3, 7, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3,
        3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2,
        3, 2, 2, 3, 3, 2, 3, 0, 3, 2, 7, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2,
        2, 2, 3, 3, 2, 2, 9, 2], device='cuda:0')
hard_pseudo_label
[0, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 0, 3, 2, 4, 3, 7, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 0, 3, 2, 7, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 9, 2]
original label
tensor([3, 2, 3, 3, 2, 2, 2, 8, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2,
        3, 3, 9, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 7, 2, 9, 2, 2, 3, 3, 3, 0,
        2, 3, 2, 3, 2, 2, 4, 9, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3,
        3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 5, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2,
        0, 2, 2, 3, 2, 5, 0, 8, 3, 2, 7, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2,
        2, 2, 3, 4, 2, 2, 2, 2])
soft_pseudo_label
tensor([[2.0221e-03, 2.6389e-04, 9.5073e-01,  ..., 3.4555e-03, 7.3721e-04,
         3.6022e-03],
        [1.7346e-03, 1.9540e-03, 9.6965e-01,  ..., 1.5174e-03, 2.3228e-03,
         5.6463e-03],
        [4.1560e-03, 3.6113e-02, 9.3702e-03,  ..., 1.2692e-02, 2.4257e-02,
         1.6804e-02],
        ...,
        [5.5494e-02, 1.1686e-02, 2.5553e-01,  ..., 5.1583e-02, 2.8447e-02,
         9.2006e-02],
        [4.7018e-04, 4.0975e-04, 9.9084e-01,  ..., 2.1881e-05, 4.3399e-04,
         9.1226e-05],
        [9.2390e-04, 5.0609e-03, 3.7843e-04,  ..., 7.9645e-04, 3.0801e-03,
         6.6937e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 7, 3,
        0, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3,
        3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3,
        3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 9, 2, 2, 2, 2, 3, 3, 3,
        2, 3, 3, 2, 2, 4, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 7, 3, 0, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 9, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 4, 2, 3]
original label
tensor([2, 2, 3, 3, 2, 3, 2, 3, 2, 1, 3, 8, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2,
        5, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 1,
        3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 2, 3, 3, 2, 2, 2, 2, 2, 5, 3, 3,
        2, 2, 3, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[1.5351e-04, 1.9405e-04, 9.5745e-01,  ..., 6.4136e-03, 1.6119e-04,
         1.4397e-02],
        [3.0314e-03, 5.7490e-04, 6.2823e-05,  ..., 1.4144e-03, 3.2003e-03,
         1.2074e-03],
        [2.0512e-05, 2.0665e-04, 2.2283e-06,  ..., 1.1536e-04, 6.9153e-05,
         2.2265e-05],
        ...,
        [1.4493e-02, 5.5031e-02, 2.0540e-03,  ..., 1.6995e-03, 5.0009e-02,
         8.9385e-04],
        [1.4407e-02, 1.2708e-02, 3.6031e-01,  ..., 1.9721e-02, 1.7661e-02,
         5.7822e-02],
        [3.4588e-03, 1.7756e-03, 1.1306e-03,  ..., 6.4263e-04, 4.1457e-03,
         3.5386e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 9, 2, 2, 3, 2, 2, 0, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 0, 3, 0,
        3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 0, 3, 3, 3, 3,
        2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 0, 7, 3, 3, 3, 3, 2, 3, 2,
        3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 4, 2, 3, 3, 2, 3,
        2, 2, 2, 3, 4, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        3, 3, 2, 2, 3, 3, 4, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 9, 2, 2, 3, 2, 2, 0, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 0, 3, 0, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 0, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 0, 7, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 4, 2, 3, 3, 2, 3, 2, 2, 2, 3, 4, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 4, 3]
original label
tensor([2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 2, 6, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 8,
        3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 4, 3, 2, 7, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 3, 3, 8, 0, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 8, 2, 2, 8, 2,
        3, 3, 2, 9, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 1, 4, 3, 3, 2, 3,
        2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 2, 2, 3, 3, 3, 0])
soft_pseudo_label
tensor([[3.5644e-03, 4.1585e-04, 1.5335e-01,  ..., 3.0953e-03, 1.2115e-03,
         4.3141e-03],
        [2.7005e-04, 5.5573e-04, 9.7960e-01,  ..., 1.4729e-03, 4.2319e-04,
         1.3716e-02],
        [9.4011e-04, 2.7950e-03, 8.6615e-05,  ..., 7.4079e-04, 1.8619e-03,
         1.5653e-04],
        ...,
        [1.5930e-02, 3.2875e-02, 5.2978e-02,  ..., 1.1758e-02, 3.3319e-02,
         2.1343e-02],
        [8.3612e-02, 7.9008e-02, 2.7084e-04,  ..., 3.1117e-03, 1.9824e-01,
         1.0283e-02],
        [8.1768e-03, 1.0884e-03, 4.6538e-04,  ..., 8.9923e-04, 3.2288e-03,
         4.6492e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 2, 3, 3, 3, 2, 0, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 0, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 4, 2, 3, 3, 2, 2, 3, 3, 2, 2,
        3, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 9, 3, 3, 3, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 3, 1, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 0, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 0, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 4, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 9, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 1, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 3, 2, 3, 2, 5, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2,
        2, 3, 2, 6, 3, 3, 2, 6, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 9, 2, 2, 3, 3, 4,
        3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 8, 3, 3, 2, 2, 3, 3, 4, 2,
        2, 2, 3, 3, 3, 3, 3, 3, 9, 2, 3, 3, 2, 3, 7, 2, 3, 6, 3, 2, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 4, 2, 3, 3, 2, 4, 3, 2, 3, 2,
        3, 3, 6, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[3.2152e-03, 7.2978e-04, 6.7788e-01,  ..., 2.8236e-03, 1.0849e-03,
         9.6623e-03],
        [7.2432e-03, 5.0745e-03, 8.6422e-03,  ..., 3.9791e-03, 8.4758e-03,
         2.8984e-03],
        [9.1115e-05, 3.5173e-03, 1.7489e-06,  ..., 3.4224e-03, 1.1084e-03,
         4.3320e-04],
        ...,
        [3.9380e-03, 2.5401e-03, 2.3381e-02,  ..., 3.8668e-01, 5.2992e-03,
         2.5859e-01],
        [1.5762e-03, 1.6441e-02, 1.6413e-02,  ..., 2.2800e-03, 8.5932e-03,
         8.1827e-03],
        [1.1783e-03, 2.0135e-03, 9.8461e-01,  ..., 7.1993e-05, 1.5027e-03,
         4.9030e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 9, 2, 3, 9, 3, 2,
        2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2,
        3, 0, 3, 2, 9, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 9, 2,
        3, 2, 3, 2, 3, 7, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 9, 2, 3, 9, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 0, 3, 2, 9, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 9, 2, 3, 2, 3, 2, 3, 7, 3, 2]
original label
tensor([2, 3, 3, 2, 9, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 9, 2, 3, 3, 8, 2,
        2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 5, 2, 3, 9, 3, 2, 3, 2, 2, 3, 2, 2, 2,
        2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2,
        3, 3, 3, 3, 2, 4, 3, 3, 2, 5, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,
        0, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 9, 2,
        3, 2, 2, 2, 3, 9, 2, 2])
soft_pseudo_label
tensor([[8.3831e-03, 2.0990e-02, 6.3659e-03,  ..., 1.2538e-03, 1.5167e-02,
         2.2859e-03],
        [1.5218e-02, 3.1733e-02, 9.8271e-02,  ..., 2.2601e-02, 3.7721e-02,
         2.0220e-02],
        [8.4889e-01, 3.4523e-03, 3.2627e-02,  ..., 3.7726e-04, 7.0917e-02,
         5.3306e-04],
        ...,
        [2.9427e-04, 3.6516e-04, 9.9111e-01,  ..., 5.5843e-04, 3.8795e-04,
         2.0537e-03],
        [7.1691e-04, 2.0869e-02, 6.3515e-04,  ..., 2.5194e-03, 9.7266e-03,
         3.1989e-03],
        [1.2105e-02, 6.3461e-03, 1.5275e-04,  ..., 1.2180e-03, 1.9658e-02,
         2.7758e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 4, 0, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2,
        2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2,
        2, 3, 3, 2, 2, 2, 2, 3, 7, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 9, 2, 2,
        2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2,
        3, 2, 3, 3, 3, 3, 2, 2, 4, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3,
        3, 3, 3, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 4, 0, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 7, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 9, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 4, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3]
original label
tensor([3, 1, 0, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 6, 2, 3, 2, 3, 3, 7, 2,
        2, 2, 1, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 7, 3, 2,
        2, 3, 8, 4, 2, 2, 2, 3, 9, 3, 6, 0, 4, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 6, 2, 2,
        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3,
        2, 8, 3, 3, 2, 2, 3, 0])
soft_pseudo_label
tensor([[1.5478e-01, 8.5089e-03, 4.3311e-03,  ..., 2.3183e-03, 6.1506e-02,
         3.2629e-03],
        [9.4443e-04, 3.2703e-04, 9.7485e-01,  ..., 1.9833e-03, 4.8997e-04,
         2.7232e-03],
        [4.7292e-04, 2.5416e-03, 6.1260e-04,  ..., 4.4008e-02, 2.5177e-03,
         1.4482e-03],
        ...,
        [1.8566e-04, 7.0340e-04, 9.8661e-01,  ..., 4.3718e-04, 4.5460e-04,
         3.1157e-03],
        [1.2865e-03, 1.0884e-03, 3.7783e-05,  ..., 1.3673e-04, 1.7585e-03,
         5.9269e-04],
        [1.7802e-03, 1.2362e-02, 1.6948e-02,  ..., 9.3719e-04, 8.8453e-03,
         2.5700e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 7, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3,
        3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 3, 3, 7, 2, 2, 3, 3, 3, 9, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 7, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 7, 2, 2, 3, 3, 3, 9, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3]
original label
tensor([3, 2, 3, 2, 3, 3, 9, 6, 2, 2, 3, 7, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,
        2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 0, 3, 4, 3, 3, 3, 3, 3,
        2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2,
        2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 5, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3,
        1, 2, 2, 4, 2, 2, 3, 3])
soft_pseudo_label
tensor([[0.0172, 0.0297, 0.0105,  ..., 0.0015, 0.0340, 0.0055],
        [0.0015, 0.0021, 0.0004,  ..., 0.0004, 0.0030, 0.0005],
        [0.0017, 0.2684, 0.0015,  ..., 0.0004, 0.0406, 0.0004],
        ...,
        [0.0025, 0.0338, 0.0006,  ..., 0.0008, 0.0119, 0.0008],
        [0.0033, 0.0013, 0.0589,  ..., 0.0024, 0.0020, 0.0030],
        [0.1249, 0.0256, 0.0042,  ..., 0.0045, 0.0892, 0.0095]],
       device='cuda:0')
hard_pseudo_label
tensor([3, 3, 5, 2, 2, 3, 3, 2, 2, 3, 7, 3, 3, 5, 3, 2, 3, 3, 9, 3, 3, 2, 3, 2,
        2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 7, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 0, 3, 3, 3, 4, 3, 3, 9, 4, 3, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 4,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 5, 2, 2, 3, 3, 2, 2, 3, 7, 3, 3, 5, 3, 2, 3, 3, 9, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 7, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 0, 3, 3, 3, 4, 3, 3, 9, 4, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3]
original label
tensor([1, 3, 5, 2, 2, 3, 3, 2, 2, 3, 9, 3, 3, 2, 3, 2, 5, 3, 9, 3, 2, 2, 2, 2,
        2, 2, 2, 3, 2, 4, 2, 3, 3, 2, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 2, 3, 3, 2,
        2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 4, 5, 2, 2, 2, 2, 3,
        3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 5, 3, 3, 3, 3, 3, 3, 2, 5,
        2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 6, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 6])
soft_pseudo_label
tensor([[3.3606e-04, 1.9360e-03, 9.8242e-01,  ..., 1.8200e-04, 1.0367e-03,
         1.0362e-03],
        [3.0786e-03, 7.0195e-03, 8.8588e-01,  ..., 4.0388e-03, 5.7263e-03,
         2.7426e-02],
        [1.3447e-03, 3.1730e-03, 9.7656e-01,  ..., 3.4466e-04, 1.6849e-03,
         1.5736e-03],
        ...,
        [1.0604e-03, 1.1288e-03, 9.6969e-01,  ..., 1.6288e-03, 1.2397e-03,
         8.8827e-03],
        [8.4402e-01, 2.7017e-04, 2.6808e-03,  ..., 7.9408e-04, 2.9973e-02,
         7.1599e-04],
        [2.6343e-04, 8.5131e-03, 3.9655e-05,  ..., 3.1375e-04, 3.4666e-03,
         2.9879e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3, 2, 3, 2, 2, 2, 3, 9, 1, 3, 2, 2,
        3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 7, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 6, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2,
        2, 2, 2, 2, 3, 2, 0, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3, 2, 3, 2, 2, 2, 3, 9, 1, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 7, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 6, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 0, 3]
original label
tensor([2, 2, 2, 3, 9, 3, 2, 3, 3, 9, 3, 0, 2, 2, 1, 2, 2, 2, 3, 2, 6, 3, 2, 2,
        3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3,
        2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 7, 2, 2, 2, 3, 2, 2, 2, 3, 2,
        2, 2, 2, 2, 6, 2, 3, 8])
soft_pseudo_label
tensor([[1.5285e-04, 3.9275e-04, 9.9751e-01,  ..., 6.6529e-06, 2.0895e-04,
         6.7313e-06],
        [1.9055e-01, 3.0981e-03, 1.2889e-03,  ..., 3.2246e-03, 5.4063e-02,
         6.3724e-03],
        [2.6699e-03, 3.2932e-04, 9.9327e-01,  ..., 2.0750e-04, 6.8569e-04,
         4.8198e-04],
        ...,
        [5.0733e-03, 3.4289e-04, 5.8148e-05,  ..., 3.3255e-03, 2.8182e-03,
         3.0786e-03],
        [1.0161e-01, 2.3412e-03, 1.0858e-02,  ..., 1.6929e-03, 1.6636e-02,
         1.6424e-03],
        [4.9302e-04, 1.4010e-03, 9.8861e-01,  ..., 1.4376e-04, 7.8190e-04,
         7.2067e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 3, 2, 3, 3, 2, 2, 3,
        2, 3, 3, 3, 7, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 0, 9, 2, 3,
        3, 2, 9, 3, 3, 3, 9, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3,
        3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2,
        3, 2, 3, 2, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 7, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 0, 9, 2, 3, 3, 2, 9, 3, 3, 3, 9, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2]
original label
tensor([2, 1, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 9, 2, 5, 3, 3, 2, 2, 3,
        3, 3, 3, 3, 3, 6, 2, 3, 6, 2, 3, 3, 2, 3, 3, 3, 2, 2, 6, 3, 3, 3, 3, 3,
        3, 9, 3, 2, 2, 3, 3, 5, 3, 2, 3, 3, 3, 5, 3, 1, 2, 3, 3, 3, 0, 2, 3, 3,
        3, 2, 3, 3, 3, 6, 9, 0, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 8, 3, 3, 2, 3,
        3, 2, 2, 5, 2, 2, 4, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3,
        3, 2, 2, 2, 3, 3, 3, 2])
soft_pseudo_label
tensor([[2.4113e-04, 4.6584e-02, 6.5281e-05,  ..., 4.2237e-04, 8.9430e-03,
         8.5656e-04],
        [1.5670e-01, 2.2976e-02, 5.1281e-01,  ..., 6.2141e-03, 5.9116e-02,
         2.4936e-03],
        [8.1815e-04, 2.7036e-03, 9.8251e-01,  ..., 5.4759e-05, 1.6378e-03,
         2.3122e-04],
        ...,
        [1.8584e-03, 5.7251e-02, 1.2093e-03,  ..., 1.9899e-03, 1.8523e-02,
         1.0203e-03],
        [8.0648e-04, 5.8553e-03, 7.8661e-01,  ..., 4.9458e-02, 3.3070e-03,
         5.8390e-02],
        [4.9048e-04, 6.8765e-04, 9.8450e-01,  ..., 1.5264e-03, 5.0114e-04,
         3.0653e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 0, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 7, 2,
        2, 3, 3, 2, 2, 7, 3, 2, 3, 3, 3, 4, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2,
        2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3,
        3, 9, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 4, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2,
        3, 3, 2, 0, 2, 3, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 0, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 7, 2, 2, 3, 3, 2, 2, 7, 3, 2, 3, 3, 3, 4, 3, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 9, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 4, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 0, 2, 3, 2, 2]
original label
tensor([3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 0, 3, 3, 8, 3, 3, 3, 2, 3, 2, 3, 7, 2,
        2, 3, 3, 2, 2, 3, 3, 9, 3, 3, 3, 2, 3, 2, 2, 9, 8, 2, 2, 3, 3, 2, 3, 2,
        2, 3, 3, 2, 2, 2, 5, 2, 2, 3, 2, 2, 1, 3, 3, 4, 3, 2, 3, 2, 6, 2, 3, 2,
        2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 4, 2, 3, 2, 2, 2, 3, 3,
        3, 9, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2,
        3, 3, 2, 1, 2, 3, 2, 2])
soft_pseudo_label
tensor([[2.1398e-04, 3.6081e-04, 9.9472e-01,  ..., 1.2616e-04, 2.4749e-04,
         1.9100e-03],
        [2.8042e-03, 1.8064e-02, 2.7030e-04,  ..., 6.5477e-04, 1.1167e-02,
         1.5358e-03],
        [5.7503e-03, 2.2011e-02, 2.6567e-01,  ..., 1.0402e-02, 1.9978e-02,
         1.6033e-02],
        ...,
        [1.6804e-03, 1.3621e-03, 9.8705e-01,  ..., 4.2976e-04, 2.3679e-03,
         8.0093e-04],
        [4.5148e-03, 4.2516e-03, 9.2394e-01,  ..., 1.8011e-03, 5.0885e-03,
         1.1994e-02],
        [5.4836e-01, 1.9338e-03, 3.7861e-03,  ..., 1.9913e-03, 5.5800e-02,
         1.4038e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 4, 2, 2, 3,
        3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 9, 0, 4, 3, 3, 2, 3, 2, 3, 3, 2,
        2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 7, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2,
        2, 3, 3, 9, 0, 2, 2, 0], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 4, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 9, 0, 4, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 7, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 9, 0, 2, 2, 0]
original label
tensor([2, 3, 2, 3, 3, 2, 3, 5, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3,
        3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 3, 2, 7, 2, 3, 3, 2, 2, 9, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3,
        2, 3, 2, 7, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 0, 2, 3, 3, 2, 3, 6, 3, 2, 2,
        2, 3, 3, 3, 2, 2, 2, 3, 2, 0, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 9, 2, 2,
        2, 3, 3, 2, 8, 2, 2, 3])
soft_pseudo_label
tensor([[4.1042e-04, 6.9142e-05, 9.9488e-01,  ..., 5.9488e-05, 1.1511e-04,
         1.0514e-03],
        [1.6495e-02, 1.2530e-02, 6.3581e-01,  ..., 7.2307e-03, 1.8527e-02,
         8.1348e-02],
        [5.9102e-02, 6.6117e-03, 6.0259e-03,  ..., 1.3444e-02, 2.8510e-02,
         9.8963e-03],
        ...,
        [7.0211e-03, 1.7049e-03, 1.6257e-01,  ..., 2.1828e-03, 3.4931e-03,
         2.3647e-03],
        [7.1925e-01, 1.1157e-03, 1.3853e-02,  ..., 3.5799e-04, 2.0969e-02,
         1.5979e-04],
        [1.6579e-03, 3.5974e-03, 9.7270e-01,  ..., 1.6227e-03, 4.1534e-03,
         2.3198e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3,
        3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3,
        2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3,
        3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 0, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3,
        2, 0, 3, 2, 2, 3, 0, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 0, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3, 2, 0, 3, 2, 2, 3, 0, 2]
original label
tensor([3, 2, 3, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 8, 3, 3, 2, 0,
        3, 2, 2, 3, 2, 5, 2, 0, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 7, 2, 3, 2, 2, 3,
        2, 3, 4, 2, 2, 3, 2, 2, 0, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 6, 3, 3, 8,
        3, 2, 3, 2, 3, 3, 2, 3, 2, 4, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3,
        2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3,
        2, 0, 3, 2, 2, 3, 3, 2])
soft_pseudo_label
tensor([[4.0222e-04, 2.4424e-03, 7.1624e-05,  ..., 4.1906e-04, 1.8472e-03,
         4.3279e-04],
        [4.2604e-03, 4.7894e-04, 4.5257e-04,  ..., 1.1739e-03, 1.9922e-03,
         1.0359e-03],
        [4.4799e-04, 2.3711e-04, 9.8998e-01,  ..., 1.4911e-04, 3.4365e-04,
         4.0531e-04],
        ...,
        [4.3981e-04, 3.8927e-04, 9.8963e-01,  ..., 3.9117e-04, 3.8624e-04,
         2.0277e-03],
        [4.1517e-04, 1.8869e-01, 3.9765e-05,  ..., 3.3100e-04, 1.9275e-02,
         7.5620e-04],
        [1.9234e-03, 3.3322e-03, 7.3467e-04,  ..., 2.3485e-03, 6.0501e-03,
         1.3519e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 3, 2, 3, 2, 3, 9, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2,
        3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 3, 3, 2, 3,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2,
        3, 3, 3, 3, 2, 4, 2, 2, 0, 2, 2, 3, 3, 3, 2, 0, 2, 3, 3, 2, 3, 3, 3, 2,
        3, 3, 3, 2, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 9, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 4, 2, 2, 0, 2, 2, 3, 3, 3, 2, 0, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3]
original label
tensor([3, 3, 2, 8, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 3, 3, 2, 3, 2, 2, 9, 2, 3, 8, 2, 2, 9, 2, 3, 3, 3, 3, 4, 2, 5, 2, 2,
        3, 2, 3, 3, 2, 3, 2, 4, 3, 2, 2, 4, 9, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2,
        3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 4, 3, 3, 3, 3, 2, 2, 2,
        3, 3, 7, 3, 2, 2, 2, 2, 0, 2, 2, 3, 2, 3, 0, 8, 2, 3, 3, 2, 3, 3, 3, 2,
        3, 3, 3, 2, 3, 2, 3, 3])
soft_pseudo_label
tensor([[7.8290e-04, 3.3265e-04, 9.9594e-01,  ..., 1.3005e-05, 3.3759e-04,
         2.3734e-05],
        [1.1140e-03, 4.6781e-04, 9.1997e-04,  ..., 1.9482e-04, 7.4563e-04,
         2.6500e-04],
        [1.2933e-04, 3.0633e-04, 9.9027e-01,  ..., 2.1129e-03, 2.4735e-04,
         5.0612e-03],
        ...,
        [7.7926e-04, 2.0951e-03, 9.4236e-05,  ..., 6.5317e-04, 2.6240e-03,
         1.3787e-03],
        [2.1840e-02, 1.9693e-02, 5.0468e-04,  ..., 7.3719e-04, 3.0560e-02,
         2.6913e-03],
        [4.6033e-04, 5.1706e-04, 9.8742e-01,  ..., 2.6435e-04, 5.6512e-04,
         1.8075e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 9, 2, 0, 2, 3, 3, 2, 3, 3, 3,
        3, 2, 2, 3, 0, 2, 2, 3, 2, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2,
        3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2,
        3, 4, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3,
        3, 2, 3, 3, 3, 3, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 9, 2, 0, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 0, 2, 2, 3, 2, 2, 3, 2, 3, 0, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 4, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2]
original label
tensor([2, 3, 2, 3, 2, 8, 2, 2, 2, 2, 6, 2, 3, 2, 9, 5, 3, 2, 3, 3, 2, 3, 5, 2,
        3, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 9, 3, 3, 2,
        7, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2,
        3, 4, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 8, 3, 2,
        8, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 7, 2, 3, 3,
        5, 2, 3, 4, 3, 3, 3, 2])
soft_pseudo_label
tensor([[1.6174e-03, 6.6195e-03, 3.7325e-01,  ..., 3.0168e-01, 4.4354e-03,
         1.2849e-01],
        [3.0159e-03, 1.1670e-03, 2.3344e-05,  ..., 1.6093e-04, 4.6620e-03,
         3.6444e-04],
        [2.3203e-04, 1.6135e-04, 9.8403e-01,  ..., 2.0081e-04, 1.5885e-04,
         1.8529e-03],
        ...,
        [9.1403e-05, 6.2524e-04, 9.8120e-01,  ..., 1.3133e-03, 3.2564e-04,
         2.2051e-03],
        [1.4831e-04, 6.2502e-04, 9.0762e-04,  ..., 3.8636e-01, 6.8244e-04,
         2.1085e-02],
        [3.4208e-02, 1.9049e-02, 5.8517e-02,  ..., 4.3735e-03, 3.5440e-02,
         1.0803e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 0, 3, 3, 3, 3, 2, 2, 2, 2, 4, 3, 0, 2, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3,
        3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        2, 3, 0, 2, 2, 3, 3, 2, 2, 3, 2, 9, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3,
        2, 2, 3, 2, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 0, 3, 3, 3, 3, 2, 2, 2, 2, 4, 3, 0, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 0, 2, 2, 3, 3, 2, 2, 3, 2, 9, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3]
original label
tensor([2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3,
        2, 2, 3, 2, 2, 3, 3, 3, 6, 2, 3, 3, 3, 2, 2, 2, 2, 6, 3, 3, 2, 8, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 7, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3,
        2, 2, 3, 2, 2, 2, 7, 3])
soft_pseudo_label
tensor([[2.4054e-03, 6.6882e-02, 9.1479e-04,  ..., 2.1104e-03, 2.4461e-02,
         6.8591e-03],
        [7.8114e-02, 1.3581e-02, 2.9640e-04,  ..., 1.3180e-03, 5.9194e-02,
         1.0037e-03],
        [3.5129e-04, 2.3646e-03, 1.7143e-02,  ..., 5.4325e-03, 1.7512e-03,
         6.2223e-03],
        ...,
        [7.5104e-04, 2.2819e-03, 5.2352e-01,  ..., 3.4161e-02, 1.5486e-03,
         3.5847e-03],
        [7.3233e-03, 1.4878e-03, 2.6571e-04,  ..., 1.1374e-03, 3.8384e-03,
         7.8709e-04],
        [4.8294e-03, 1.0244e-02, 2.2995e-02,  ..., 4.5769e-03, 1.6400e-02,
         6.0191e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 0, 3, 3, 2, 3,
        0, 3, 3, 3, 2, 2, 5, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3,
        2, 2, 9, 2, 3, 0, 3, 2, 3, 2, 2, 3, 3, 1, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 4,
        2, 3, 3, 9, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2,
        3, 2, 2, 3, 3, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 0, 3, 3, 2, 3, 0, 3, 3, 3, 2, 2, 5, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 9, 2, 3, 0, 3, 2, 3, 2, 2, 3, 3, 1, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 4, 2, 3, 3, 9, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3]
original label
tensor([6, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 8, 6, 3, 3, 2, 2,
        3, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 1, 3, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 7, 2, 6, 0, 3, 2, 3, 2, 2, 3, 3, 6, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 6, 3, 2, 3, 4, 2, 3, 2, 2, 3, 3, 2, 3, 4,
        2, 3, 3, 7, 3, 3, 2, 3, 3, 1, 2, 2, 3, 2, 2, 2, 2, 2, 5, 2, 2, 2, 3, 2,
        3, 2, 3, 2, 3, 3, 3, 3])
soft_pseudo_label
tensor([[3.4605e-01, 2.5534e-03, 5.1746e-01,  ..., 2.1272e-03, 3.6278e-02,
         4.6689e-03],
        [4.9600e-04, 2.3965e-03, 9.4350e-01,  ..., 4.9107e-03, 1.8143e-03,
         1.9147e-02],
        [1.1999e-02, 3.8887e-02, 4.9839e-01,  ..., 4.2038e-03, 4.6471e-02,
         2.3535e-03],
        ...,
        [1.4292e-03, 3.6176e-03, 1.7850e-02,  ..., 6.6743e-02, 4.9932e-03,
         4.6601e-01],
        [2.3693e-02, 2.6354e-02, 5.0266e-04,  ..., 1.9764e-03, 4.9260e-02,
         3.1124e-03],
        [6.5446e-04, 1.7056e-04, 8.2205e-01,  ..., 2.0660e-03, 3.0110e-04,
         5.0828e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 0, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3,
        3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 1, 3, 2,
        3, 3, 2, 3, 2, 5, 3, 3, 3, 4, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 4, 2,
        3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2,
        3, 2, 3, 9, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 9, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 0, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 1, 3, 2, 3, 3, 2, 3, 2, 5, 3, 3, 3, 4, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 4, 2, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 9, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 9, 3, 2]
original label
tensor([8, 9, 2, 2, 3, 2, 6, 3, 2, 8, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3,
        3, 2, 2, 2, 2, 3, 3, 2, 4, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 1, 8, 2,
        3, 3, 2, 3, 2, 3, 9, 3, 3, 2, 2, 2, 7, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2,
        3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 8, 3, 2, 3, 2, 3, 3, 2,
        3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2,
        2, 3, 9, 3, 1, 3, 3, 2])
soft_pseudo_label
tensor([[1.4532e-04, 1.0759e-03, 9.8387e-01,  ..., 1.5302e-03, 4.8969e-04,
         1.3468e-03],
        [2.7927e-05, 3.7801e-05, 6.4321e-03,  ..., 3.6228e-02, 6.7917e-05,
         9.5275e-01],
        [1.2746e-03, 7.8883e-02, 2.7086e-06,  ..., 1.2332e-04, 1.3131e-02,
         1.4963e-04],
        ...,
        [7.7340e-04, 8.7126e-04, 9.6200e-01,  ..., 6.9932e-03, 7.9795e-04,
         1.3470e-02],
        [5.2901e-03, 1.9220e-03, 1.0337e-02,  ..., 5.3734e-03, 5.5413e-03,
         1.5575e-02],
        [2.1828e-04, 8.6739e-05, 9.9896e-01,  ..., 1.4868e-05, 1.3191e-04,
         3.2066e-05]], device='cuda:0')
hard_pseudo_label
tensor([2, 9, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 0, 2, 3, 3, 2, 3, 2, 3,
        3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 7, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2,
        2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 0, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 0, 3, 2, 2, 3,
        3, 2, 3, 3, 2, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[2, 9, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 0, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 7, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 0, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 0, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2]
original label
tensor([2, 7, 3, 3, 3, 2, 2, 2, 3, 2, 3, 5, 3, 0, 2, 2, 0, 9, 3, 9, 2, 3, 2, 3,
        3, 3, 8, 3, 3, 2, 8, 3, 2, 3, 4, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2,
        7, 5, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 5, 2, 2, 4, 2, 4, 2,
        2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 0, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2,
        2, 3, 2, 1, 3, 3, 3, 3, 0, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 7, 4, 1,
        3, 6, 3, 3, 2, 2, 3, 2])
soft_pseudo_label
tensor([[4.2476e-04, 4.7991e-04, 9.8708e-01,  ..., 2.5839e-04, 5.3018e-04,
         6.9249e-04],
        [8.2767e-03, 1.4329e-02, 5.6735e-01,  ..., 3.5217e-03, 1.4079e-02,
         8.7505e-03],
        [7.6538e-04, 2.4407e-03, 9.5575e-01,  ..., 2.1476e-03, 2.1020e-03,
         2.3350e-02],
        ...,
        [5.2966e-03, 2.3682e-03, 2.2623e-02,  ..., 2.8671e-03, 2.9828e-03,
         9.8530e-04],
        [2.6306e-02, 9.2637e-03, 3.7629e-02,  ..., 3.7769e-02, 2.7690e-02,
         4.8498e-01],
        [1.1053e-03, 2.1679e-03, 1.2371e-04,  ..., 7.0576e-03, 3.0355e-03,
         8.1459e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 3, 3, 2, 3, 3, 9, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2,
        2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 9, 2, 2, 3,
        3, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3,
        3, 3, 3, 3, 3, 0, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 4, 3, 0,
        2, 3, 3, 2, 3, 2, 3, 0, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3,
        3, 3, 2, 2, 2, 3, 9, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 3, 3, 2, 3, 3, 9, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 9, 2, 2, 3, 3, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 4, 3, 0, 2, 3, 3, 2, 3, 2, 3, 0, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 9, 3]
original label
tensor([2, 3, 2, 2, 2, 3, 2, 3, 3, 7, 3, 3, 3, 3, 3, 2, 2, 2, 9, 2, 3, 2, 7, 2,
        2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 9, 3, 2, 2, 2,
        3, 0, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 1, 3, 2, 2, 2, 3,
        2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 2, 2, 2, 6, 2, 2, 3, 3, 3, 0,
        2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 0, 2, 3, 2, 2, 7, 5, 3,
        3, 3, 3, 2, 2, 2, 2, 3])
soft_pseudo_label
tensor([[2.4499e-02, 8.9413e-04, 3.5059e-02,  ..., 1.8690e-03, 4.0087e-03,
         1.5769e-03],
        [2.7946e-04, 1.3345e-03, 9.7584e-01,  ..., 2.0644e-03, 6.5806e-04,
         2.9762e-03],
        [3.5856e-04, 3.7544e-05, 9.9751e-01,  ..., 1.8369e-05, 1.1285e-04,
         5.7979e-05],
        ...,
        [1.2508e-02, 6.4074e-03, 1.2494e-03,  ..., 9.0610e-04, 1.5613e-02,
         1.8519e-03],
        [3.4382e-04, 7.7020e-04, 1.4438e-04,  ..., 3.2142e-04, 1.1945e-03,
         1.1769e-03],
        [1.8510e-03, 1.1590e-02, 8.6533e-03,  ..., 8.5325e-04, 6.5928e-03,
         9.2619e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2,
        2, 3, 4, 2, 3, 2, 2, 2, 2, 2, 7, 3, 3, 0, 2, 3, 2, 2, 2, 3, 3, 2, 2, 0,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3, 3, 2, 3, 2, 3, 3, 3, 5, 2, 2, 2, 3,
        2, 3, 2, 2, 2, 3, 7, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 2, 7, 3, 3, 0, 2, 3, 2, 2, 2, 3, 3, 2, 2, 0, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 3, 0, 3, 3, 2, 3, 2, 3, 3, 3, 5, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 7, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3]
original label
tensor([3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 9, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2,
        2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 9, 3, 3, 0, 2, 3, 9, 2, 2, 3, 3, 2, 2, 3,
        2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 3, 2, 5, 2, 2, 3, 3, 3, 3, 3, 3, 2, 1, 2, 3, 8, 3, 6, 2, 2, 2, 3,
        2, 3, 2, 2, 2, 2, 4, 3, 6, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 5, 2, 2, 2,
        2, 3, 3, 3, 2, 3, 3, 3])
soft_pseudo_label
tensor([[7.1149e-04, 1.3061e-03, 9.4854e-01,  ..., 3.1485e-03, 1.3215e-03,
         2.6004e-02],
        [4.6593e-03, 8.7175e-03, 6.6297e-02,  ..., 5.1006e-02, 1.0019e-02,
         2.1756e-02],
        [2.0064e-03, 1.8595e-02, 5.1128e-04,  ..., 4.6190e-04, 1.2393e-02,
         8.2342e-04],
        ...,
        [9.8195e-02, 6.5402e-03, 7.7615e-01,  ..., 1.6488e-03, 2.5330e-02,
         2.3388e-03],
        [3.2801e-04, 6.9190e-02, 1.3075e-03,  ..., 3.1979e-04, 6.1823e-03,
         2.1808e-04],
        [2.4332e-03, 7.1794e-03, 8.1740e-04,  ..., 1.2697e-03, 7.8467e-03,
         4.3526e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 3, 3, 3, 2, 2, 4, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 7, 3,
        3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2,
        2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 2, 0, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 7, 2, 2, 2, 3, 2, 2,
        3, 7, 2, 3, 2, 2, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 3, 3, 3, 2, 2, 4, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 7, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 0, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 7, 2, 2, 2, 3, 2, 2, 3, 7, 2, 3, 2, 2, 3, 3]
original label
tensor([2, 2, 3, 3, 2, 2, 7, 3, 2, 2, 3, 2, 2, 2, 3, 2, 0, 3, 1, 3, 3, 2, 3, 3,
        3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 9, 2, 3, 3, 2,
        2, 5, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 5, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2,
        3, 3, 2, 2, 3, 3, 2, 3, 4, 2, 2, 3, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3,
        9, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 2, 4, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2,
        3, 2, 2, 3, 2, 2, 3, 0])
soft_pseudo_label
tensor([[1.7815e-03, 8.6483e-04, 2.3151e-01,  ..., 3.0861e-02, 1.7351e-03,
         1.1778e-01],
        [1.9206e-02, 1.5284e-03, 1.0058e-04,  ..., 7.7170e-04, 6.9017e-03,
         4.0726e-04],
        [3.3959e-03, 1.0613e-03, 9.8485e-01,  ..., 4.9741e-04, 1.6639e-03,
         1.5277e-03],
        ...,
        [1.0552e-03, 2.4796e-04, 9.9173e-01,  ..., 9.2024e-05, 6.6373e-04,
         3.1010e-04],
        [2.3477e-03, 3.0129e-03, 3.4069e-04,  ..., 8.6278e-04, 6.7302e-03,
         1.7503e-04],
        [2.2683e-04, 4.0452e-04, 9.9640e-01,  ..., 4.7315e-05, 2.4205e-04,
         4.1204e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 4, 4, 3, 7, 2, 0, 2, 3, 3,
        3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 9, 3, 2, 3,
        7, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 0, 3, 2, 3, 2, 3, 3, 0, 2, 2, 2,
        2, 2, 2, 3, 2, 5, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3,
        2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,
        2, 2, 3, 7, 3, 2, 3, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 4, 4, 3, 7, 2, 0, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 9, 3, 2, 3, 7, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 0, 3, 2, 3, 2, 3, 3, 0, 2, 2, 2, 2, 2, 2, 3, 2, 5, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 7, 3, 2, 3, 2]
original label
tensor([3, 3, 2, 2, 2, 3, 3, 3, 6, 2, 3, 3, 3, 3, 2, 2, 6, 3, 3, 2, 0, 2, 3, 5,
        3, 3, 1, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3,
        9, 3, 2, 2, 2, 3, 8, 2, 2, 2, 4, 2, 2, 3, 3, 2, 3, 2, 3, 3, 0, 2, 2, 2,
        2, 2, 2, 3, 2, 5, 9, 3, 3, 2, 6, 6, 2, 3, 2, 3, 2, 2, 3, 7, 3, 3, 3, 3,
        3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,
        2, 2, 2, 7, 3, 2, 3, 2])
soft_pseudo_label
tensor([[1.1012e-02, 3.5752e-03, 1.6068e-03,  ..., 1.1517e-03, 1.1301e-02,
         1.4962e-03],
        [1.2693e-04, 3.4537e-04, 3.5356e-04,  ..., 4.2205e-02, 4.3681e-04,
         2.8303e-03],
        [5.2611e-03, 3.3278e-03, 1.3649e-02,  ..., 6.9902e-03, 6.6603e-03,
         1.0199e-02],
        ...,
        [6.1511e-04, 1.0992e-03, 9.7668e-01,  ..., 1.4734e-03, 9.0553e-04,
         1.0273e-02],
        [4.8608e-04, 6.8281e-04, 9.8716e-01,  ..., 2.7028e-04, 6.2996e-04,
         1.4711e-03],
        [4.6646e-04, 2.1855e-03, 9.8698e-01,  ..., 1.1691e-04, 1.2223e-03,
         9.5852e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 3, 3, 2, 3, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3,
        3, 3, 2, 3, 3, 2, 7, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2,
        5, 9, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3,
        2, 3, 3, 4, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 7, 3, 3,
        3, 2, 3, 2, 2, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[3, 3, 3, 3, 2, 3, 4, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 0, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 7, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 5, 9, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 7, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2]
original label
tensor([3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2,
        2, 3, 2, 3, 2, 3, 0, 7, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 8,
        2, 3, 2, 6, 2, 5, 9, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 4, 3, 3, 3, 2, 3, 2, 0, 2, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 3, 3, 3,
        2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 6, 3, 2,
        2, 2, 3, 2, 2, 2, 2, 2])
soft_pseudo_label
tensor([[2.2800e-04, 1.4766e-03, 8.9687e-01,  ..., 6.7450e-03, 9.2403e-04,
         2.0087e-02],
        [1.7268e-03, 9.4590e-03, 8.3053e-01,  ..., 3.4510e-03, 6.5265e-03,
         2.2536e-02],
        [1.1304e-03, 1.2585e-03, 8.2413e-01,  ..., 2.0672e-02, 2.0190e-03,
         2.4585e-02],
        ...,
        [5.0976e-02, 9.2157e-03, 1.9195e-02,  ..., 8.6616e-03, 4.0921e-02,
         6.9225e-03],
        [9.1426e-02, 1.5396e-03, 6.5714e-03,  ..., 2.1543e-03, 1.8720e-02,
         8.9230e-03],
        [1.6287e-02, 1.6863e-03, 7.6929e-02,  ..., 9.5131e-03, 7.4079e-03,
         7.3988e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 4, 7, 0, 2, 2, 2, 4, 3, 3, 3, 3, 2, 0, 2,
        2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3,
        2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0,
        2, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 4, 7, 0, 2, 2, 2, 4, 3, 3, 3, 3, 2, 0, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 3]
original label
tensor([2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 9, 0, 2, 2, 2, 4, 3, 3, 1, 3, 2, 4, 2,
        2, 9, 2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 1, 2, 2, 9, 3, 2, 3,
        2, 2, 3, 0, 1, 2, 9, 3, 5, 2, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 5, 2, 6, 3, 3, 3, 9, 2, 4, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 8, 2, 3, 3, 3, 3, 3, 2,
        2, 3, 3, 3, 3, 3, 3, 3])
soft_pseudo_label
tensor([[2.5615e-04, 1.3899e-04, 9.7281e-01,  ..., 3.5493e-04, 2.1654e-04,
         5.5684e-03],
        [1.3963e-03, 2.2248e-03, 9.7141e-01,  ..., 5.6306e-04, 2.1892e-03,
         2.9409e-03],
        [2.5804e-03, 1.1104e-03, 9.6325e-01,  ..., 2.2474e-03, 2.0028e-03,
         4.4157e-03],
        ...,
        [4.7853e-04, 1.1171e-02, 2.9621e-05,  ..., 4.2519e-04, 5.1046e-03,
         1.0255e-03],
        [2.8972e-02, 1.2856e-02, 1.1344e-03,  ..., 2.6974e-03, 4.0817e-02,
         1.4368e-03],
        [1.7033e-03, 1.7824e-03, 2.7616e-04,  ..., 1.1708e-02, 3.2916e-03,
         2.0505e-03]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 2, 2, 2, 2, 3, 8, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 0, 3, 2, 3, 2, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 4, 2, 3, 2, 2, 3, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 2, 9, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3,
        3, 3, 2, 2, 2, 3, 3, 2, 3, 0, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 3, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 2, 2, 2, 2, 3, 8, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 0, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 4, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 9, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 3, 0, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3]
original label
tensor([2, 2, 2, 2, 2, 2, 6, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,
        6, 2, 3, 5, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3, 2,
        2, 2, 3, 3, 2, 3, 2, 3, 3, 1, 3, 5, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 1,
        3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3,
        0, 3, 2, 2, 2, 3, 2, 2, 3, 8, 3, 7, 2, 2, 3, 3, 2, 3, 8, 2, 3, 2, 2, 3,
        2, 2, 2, 3, 3, 3, 3, 3])
soft_pseudo_label
tensor([[2.0354e-03, 1.0812e-02, 6.5855e-04,  ..., 1.1832e-03, 1.1414e-02,
         8.3493e-04],
        [6.6249e-03, 6.2898e-04, 9.8611e-01,  ..., 5.9775e-05, 1.8696e-03,
         3.2094e-04],
        [1.2492e-03, 1.0284e-03, 9.9187e-01,  ..., 4.9942e-05, 8.9012e-04,
         6.2133e-04],
        ...,
        [1.0989e-04, 5.7403e-05, 9.9243e-01,  ..., 1.0079e-03, 5.8307e-05,
         4.2380e-03],
        [9.8251e-05, 9.3204e-05, 4.7347e-02,  ..., 2.0009e-02, 1.6518e-04,
         9.0744e-01],
        [1.4839e-04, 1.1105e-03, 9.7756e-01,  ..., 2.1491e-03, 6.5345e-04,
         1.5448e-03]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 7,
        3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 7, 3, 2, 2, 2, 3, 3, 3,
        3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 9, 3, 2,
        3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 0, 3, 3, 3, 2, 5, 2, 2, 2, 2, 2,
        2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3,
        7, 2, 2, 2, 2, 2, 9, 2], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 7, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 7, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 9, 3, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 0, 3, 3, 3, 2, 5, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 7, 2, 2, 2, 2, 2, 9, 2]
original label
tensor([3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 0, 2, 3, 2, 3, 3, 2,
        3, 2, 5, 3, 2, 3, 3, 3, 3, 0, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3,
        3, 2, 3, 3, 3, 3, 7, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 5, 3, 2, 3, 2,
        3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 3, 2, 0, 8, 3, 8, 2, 3, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 3, 0, 3, 3, 2, 2, 3, 3, 3, 3,
        7, 2, 2, 2, 2, 2, 9, 2])
soft_pseudo_label
tensor([[1.3482e-03, 4.2076e-04, 9.8930e-01,  ..., 1.6786e-04, 1.0612e-03,
         3.2662e-03],
        [9.5812e-05, 2.7942e-04, 9.9688e-01,  ..., 5.5887e-05, 1.4581e-04,
         3.7714e-04],
        [3.8068e-03, 1.0659e-03, 3.4520e-01,  ..., 5.5837e-03, 1.7167e-03,
         7.9326e-03],
        ...,
        [6.8411e-04, 3.6161e-03, 6.3919e-01,  ..., 4.9385e-02, 2.6276e-03,
         5.7785e-03],
        [2.7870e-04, 8.0057e-04, 9.8950e-01,  ..., 2.8061e-04, 4.9105e-04,
         2.5035e-03],
        [1.4853e-01, 7.6220e-03, 6.3479e-05,  ..., 2.1224e-04, 3.3728e-02,
         7.6806e-04]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 4, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3,
        2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 4, 3, 3, 2, 2, 0,
        2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3,
        2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 4, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 4, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 4, 3, 3, 2, 2, 0, 2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 4, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3]
original label
tensor([2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 4, 7, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3,
        2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3,
        2, 5, 3, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 4, 3, 3, 2, 2, 3,
        2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 1, 3, 3, 2, 3, 2, 3, 9,
        2, 2, 6, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 6, 2, 3, 3, 2, 2, 3, 2, 2, 2,
        3, 3, 2, 2, 3, 2, 2, 3])
soft_pseudo_label
tensor([[3.2977e-03, 5.5443e-03, 4.7715e-01,  ..., 7.8339e-03, 6.5904e-03,
         4.7106e-02],
        [1.8011e-03, 7.4787e-04, 9.5138e-01,  ..., 3.6820e-03, 1.2080e-03,
         9.0114e-03],
        [4.6515e-04, 1.2607e-03, 3.0272e-03,  ..., 2.3419e-03, 1.1948e-03,
         2.2899e-03],
        ...,
        [4.9009e-04, 2.5072e-03, 9.4880e-01,  ..., 1.7375e-03, 1.4746e-03,
         8.7105e-03],
        [2.0086e-04, 7.6733e-04, 9.8235e-01,  ..., 6.0840e-05, 4.3721e-04,
         5.9382e-04],
        [7.5767e-04, 2.2786e-03, 9.6667e-01,  ..., 3.5643e-03, 1.5993e-03,
         1.0398e-02]], device='cuda:0')
hard_pseudo_label
tensor([2, 2, 3, 7, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 0, 3, 3,
        3, 3, 3, 2, 3, 5, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 0, 3, 2, 2, 3, 2,
        2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2,
        3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2,
        2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 0, 2, 2,
        3, 2, 3, 4, 3, 2, 2, 2], device='cuda:0')
hard_pseudo_label
[2, 2, 3, 7, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 2, 3, 5, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 0, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 0, 2, 2, 3, 2, 3, 4, 3, 2, 2, 2]
original label
tensor([2, 2, 2, 3, 2, 6, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 8, 2, 2, 2, 7, 0, 3, 3,
        3, 1, 5, 2, 1, 5, 2, 2, 3, 3, 3, 3, 5, 2, 3, 3, 3, 2, 1, 3, 2, 2, 3, 3,
        2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2,
        3, 3, 2, 3, 3, 2, 8, 2, 3, 9, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2,
        2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 4, 2, 3, 2, 3, 8, 2, 2,
        2, 2, 3, 3, 3, 2, 2, 2])
soft_pseudo_label
tensor([[7.2040e-03, 3.2513e-01, 1.1012e-05,  ..., 9.1665e-05, 5.4867e-02,
         8.8325e-05],
        [2.4474e-03, 4.5802e-03, 2.3380e-02,  ..., 1.3319e-03, 4.6048e-03,
         2.4391e-03],
        [9.7909e-04, 1.0743e-03, 9.8240e-01,  ..., 1.5628e-04, 1.0853e-03,
         3.0851e-03],
        ...,
        [3.6564e-03, 2.5216e-03, 9.2356e-01,  ..., 3.1321e-03, 3.1659e-03,
         2.3439e-02],
        [2.3596e-04, 1.2532e-03, 9.8805e-01,  ..., 7.3957e-05, 7.1763e-04,
         5.8087e-04],
        [2.3495e-02, 2.4840e-02, 1.6525e-01,  ..., 3.0197e-02, 3.5287e-02,
         4.4699e-02]], device='cuda:0')
hard_pseudo_label
tensor([3, 3, 2, 3, 3, 2, 0, 2, 3, 2, 3, 3, 2, 9, 2, 2, 2, 2, 0, 2, 3, 2, 3, 3,
        2, 3, 2, 2, 4, 2, 4, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 2, 2, 2,
        3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 4, 3, 3, 2, 3, 3, 2, 0, 2, 3, 3,
        3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 7, 3, 3, 3, 2, 3, 2, 3, 2,
        2, 2, 3, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 3, 2, 3, 3, 2, 0, 2, 3, 2, 3, 3, 2, 9, 2, 2, 2, 2, 0, 2, 3, 2, 3, 3, 2, 3, 2, 2, 4, 2, 4, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 4, 3, 3, 2, 3, 3, 2, 0, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 7, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3]
original label
tensor([3, 3, 2, 3, 5, 2, 3, 9, 3, 2, 3, 3, 2, 7, 2, 2, 2, 2, 0, 2, 3, 3, 3, 9,
        2, 3, 2, 2, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 8, 2, 2, 0, 2, 2, 3, 3, 2, 3,
        2, 2, 2, 2, 3, 3, 2, 9, 2, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 2, 7, 7,
        3, 2, 5, 3, 3, 2, 3, 3, 3, 3, 3, 2, 6, 9, 3, 3, 2, 3, 3, 2, 0, 2, 3, 3,
        3, 2, 2, 3, 3, 3, 3, 3, 9, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2,
        2, 2, 3, 3, 9, 2, 2, 3])
soft_pseudo_label
tensor([[7.4296e-04, 1.1122e-02, 2.8953e-04,  ..., 1.0884e-03, 6.1693e-03,
         4.2623e-04],
        [4.0611e-04, 5.8830e-04, 9.9193e-01,  ..., 1.2170e-04, 4.6561e-04,
         3.4371e-03],
        [7.3494e-03, 3.2869e-03, 7.7346e-01,  ..., 1.2354e-03, 6.7442e-03,
         6.7113e-03],
        ...,
        [5.5591e-04, 6.4267e-04, 9.9388e-01,  ..., 4.6927e-04, 7.3664e-04,
         5.7777e-04],
        [3.4749e-04, 4.6651e-04, 9.9544e-01,  ..., 5.5371e-05, 3.4285e-04,
         1.9440e-04],
        [1.0191e-03, 5.9423e-03, 8.7509e-04,  ..., 1.8471e-03, 3.9589e-03,
         8.3828e-04]], device='cuda:0')
hard_pseudo_label
tensor([3, 2, 2, 2, 9, 2, 2, 3, 3, 0, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3,
        2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 6, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 2, 3, 3, 5, 3, 0, 2, 3, 2, 3, 2, 2, 2, 2, 3, 4, 2, 2, 2, 3, 3, 3,
        3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3,
        3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 4, 3, 3, 2, 3, 9, 3, 3, 2, 2, 3, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3], device='cuda:0')
hard_pseudo_label
[3, 2, 2, 2, 9, 2, 2, 3, 3, 0, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 6, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 5, 3, 0, 2, 3, 2, 3, 2, 2, 2, 2, 3, 4, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 4, 3, 3, 2, 3, 9, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 3]
original label
tensor([3, 2, 2, 2, 2, 2, 2, 3, 3, 0, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3,
        3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 8, 3, 2, 3, 2, 2, 3, 2,
        2, 2, 2, 3, 8, 1, 3, 3, 2, 3, 3, 3, 2, 2, 9, 2, 3, 2, 2, 2, 2, 3, 6, 6,
        3, 2, 3, 2, 2, 9, 3, 3, 3, 3, 2, 2, 3, 8, 2, 2, 2, 3, 3, 3, 4, 3, 2, 3,
        3, 3, 2, 2, 2, 3, 3, 5, 2, 2, 2, 3, 3, 4, 3, 7, 3, 3, 4, 2, 3, 3, 2, 2,
        3, 3, 2, 3, 3, 2, 2, 3])
[INFO] main.py:340 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:348 > [2-3] Start to train under online
[INFO] main.py:363 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42
[INFO] rainbow_memory.py:119 > Streamed samples: 800
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 9984
[INFO] rainbow_memory.py:127 > Train samples: 11284
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 1/1 | train_loss 1.8613 | train_acc 0.6100 | test_loss 1.0619 | test_acc 0.9610 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[WARNING] finetune.py:650 > Duplicated samples in memory: 5
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
bird          70
automobile    68
dog           52
deer          51
ship          50
airplane      48
cat           47
truck         40
frog          38
horse         36
Name: klass, dtype: int64
[INFO] main.py:379 > Train over memory
batch_size : 64 stream_batch_size : 22 memory_batch_size : 21
[INFO] rainbow_memory.py:119 > Streamed samples: 0
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 0
[INFO] rainbow_memory.py:127 > Train samples: 500
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 1/256 | train_loss 1.9432 | train_acc 0.2920 | test_loss 1.0432 | test_acc 0.9630 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 2/256 | train_loss 1.4940 | train_acc 0.5820 | test_loss 1.1943 | test_acc 0.6220 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 3/256 | train_loss 1.4119 | train_acc 0.5820 | test_loss 0.4797 | test_acc 0.8605 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 4/256 | train_loss 1.1300 | train_acc 0.6640 | test_loss 0.6482 | test_acc 0.8055 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 5/256 | train_loss 1.0986 | train_acc 0.6420 | test_loss 0.7414 | test_acc 0.7485 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 6/256 | train_loss 1.2095 | train_acc 0.6240 | test_loss 0.9217 | test_acc 0.7020 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 7/256 | train_loss 1.1538 | train_acc 0.6320 | test_loss 0.7077 | test_acc 0.7785 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 8/256 | train_loss 0.9551 | train_acc 0.6960 | test_loss 0.5729 | test_acc 0.8045 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 9/256 | train_loss 0.9606 | train_acc 0.6740 | test_loss 0.9563 | test_acc 0.6770 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 10/256 | train_loss 1.2449 | train_acc 0.6200 | test_loss 1.8765 | test_acc 0.3735 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 11/256 | train_loss 1.2194 | train_acc 0.6560 | test_loss 0.8113 | test_acc 0.7370 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 12/256 | train_loss 1.1664 | train_acc 0.6200 | test_loss 0.8927 | test_acc 0.7155 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 13/256 | train_loss 1.0076 | train_acc 0.7160 | test_loss 0.5490 | test_acc 0.8140 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 14/256 | train_loss 1.0619 | train_acc 0.6980 | test_loss 0.8431 | test_acc 0.7150 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 15/256 | train_loss 1.0803 | train_acc 0.6860 | test_loss 0.6724 | test_acc 0.7830 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 16/256 | train_loss 0.9071 | train_acc 0.7020 | test_loss 0.6832 | test_acc 0.7705 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 17/256 | train_loss 0.9180 | train_acc 0.6840 | test_loss 0.9259 | test_acc 0.6870 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 18/256 | train_loss 1.2053 | train_acc 0.6080 | test_loss 0.7254 | test_acc 0.7535 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 19/256 | train_loss 1.2079 | train_acc 0.6580 | test_loss 0.7743 | test_acc 0.7585 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 20/256 | train_loss 1.0346 | train_acc 0.7040 | test_loss 0.3670 | test_acc 0.8795 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 21/256 | train_loss 1.0248 | train_acc 0.6760 | test_loss 1.1168 | test_acc 0.6355 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 22/256 | train_loss 0.9396 | train_acc 0.7080 | test_loss 0.7678 | test_acc 0.7465 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 23/256 | train_loss 0.8811 | train_acc 0.7360 | test_loss 0.5280 | test_acc 0.8190 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 24/256 | train_loss 0.8234 | train_acc 0.7700 | test_loss 0.7538 | test_acc 0.7485 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 25/256 | train_loss 0.8578 | train_acc 0.7580 | test_loss 0.7872 | test_acc 0.7275 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 26/256 | train_loss 0.8265 | train_acc 0.7460 | test_loss 0.6220 | test_acc 0.7925 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 27/256 | train_loss 1.1344 | train_acc 0.6140 | test_loss 0.6043 | test_acc 0.7935 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 28/256 | train_loss 1.0850 | train_acc 0.7000 | test_loss 0.5633 | test_acc 0.8080 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 29/256 | train_loss 0.7963 | train_acc 0.7800 | test_loss 0.6572 | test_acc 0.7780 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 30/256 | train_loss 0.7695 | train_acc 0.8080 | test_loss 0.5849 | test_acc 0.7955 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 31/256 | train_loss 0.8276 | train_acc 0.7620 | test_loss 0.5735 | test_acc 0.8020 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 32/256 | train_loss 0.8560 | train_acc 0.7400 | test_loss 0.5524 | test_acc 0.8090 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 33/256 | train_loss 0.7924 | train_acc 0.8000 | test_loss 1.1863 | test_acc 0.6395 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 34/256 | train_loss 1.0410 | train_acc 0.7060 | test_loss 0.3115 | test_acc 0.9070 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 35/256 | train_loss 1.1895 | train_acc 0.6380 | test_loss 0.7815 | test_acc 0.7385 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 36/256 | train_loss 0.8954 | train_acc 0.7680 | test_loss 0.5593 | test_acc 0.8155 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 37/256 | train_loss 1.0853 | train_acc 0.6900 | test_loss 0.4680 | test_acc 0.8490 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 38/256 | train_loss 1.0466 | train_acc 0.7040 | test_loss 0.6526 | test_acc 0.8005 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 39/256 | train_loss 1.0110 | train_acc 0.7100 | test_loss 0.6556 | test_acc 0.7890 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 40/256 | train_loss 0.9625 | train_acc 0.7360 | test_loss 0.9349 | test_acc 0.6790 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 41/256 | train_loss 1.1256 | train_acc 0.6640 | test_loss 1.0262 | test_acc 0.6755 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 42/256 | train_loss 0.9343 | train_acc 0.7460 | test_loss 1.0730 | test_acc 0.6430 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 43/256 | train_loss 0.8939 | train_acc 0.7740 | test_loss 0.8523 | test_acc 0.7015 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 44/256 | train_loss 0.8689 | train_acc 0.7900 | test_loss 0.4514 | test_acc 0.8450 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 45/256 | train_loss 0.8284 | train_acc 0.7660 | test_loss 0.6358 | test_acc 0.7970 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 46/256 | train_loss 0.8942 | train_acc 0.7320 | test_loss 0.7852 | test_acc 0.7295 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 47/256 | train_loss 0.8686 | train_acc 0.7460 | test_loss 0.6798 | test_acc 0.7745 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 48/256 | train_loss 0.9514 | train_acc 0.6800 | test_loss 0.7945 | test_acc 0.7385 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 49/256 | train_loss 0.9359 | train_acc 0.7260 | test_loss 0.6876 | test_acc 0.7740 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 50/256 | train_loss 0.9989 | train_acc 0.7160 | test_loss 0.5773 | test_acc 0.8065 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 51/256 | train_loss 0.8222 | train_acc 0.7720 | test_loss 0.7061 | test_acc 0.7705 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 52/256 | train_loss 0.7681 | train_acc 0.7640 | test_loss 0.7760 | test_acc 0.7415 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 53/256 | train_loss 0.7131 | train_acc 0.8440 | test_loss 0.6025 | test_acc 0.7950 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 54/256 | train_loss 0.9103 | train_acc 0.7160 | test_loss 0.6336 | test_acc 0.7850 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 55/256 | train_loss 0.8160 | train_acc 0.7340 | test_loss 0.6212 | test_acc 0.7980 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 56/256 | train_loss 0.6157 | train_acc 0.8720 | test_loss 0.6068 | test_acc 0.7975 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 57/256 | train_loss 0.7713 | train_acc 0.7760 | test_loss 0.5701 | test_acc 0.8050 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 58/256 | train_loss 0.8029 | train_acc 0.7940 | test_loss 0.6557 | test_acc 0.7765 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 59/256 | train_loss 0.7969 | train_acc 0.8320 | test_loss 0.5477 | test_acc 0.8195 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 60/256 | train_loss 0.7434 | train_acc 0.7860 | test_loss 0.5861 | test_acc 0.8050 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 61/256 | train_loss 0.5969 | train_acc 0.8640 | test_loss 0.5573 | test_acc 0.8130 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 62/256 | train_loss 0.6724 | train_acc 0.8000 | test_loss 0.5851 | test_acc 0.8010 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 63/256 | train_loss 0.7636 | train_acc 0.7920 | test_loss 0.5777 | test_acc 0.8000 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 64/256 | train_loss 0.8930 | train_acc 0.7120 | test_loss 0.5619 | test_acc 0.8120 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 65/256 | train_loss 0.9076 | train_acc 0.7420 | test_loss 0.9342 | test_acc 0.6765 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 66/256 | train_loss 0.8231 | train_acc 0.7600 | test_loss 1.1900 | test_acc 0.6220 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 67/256 | train_loss 1.0650 | train_acc 0.7420 | test_loss 1.0250 | test_acc 0.6550 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 68/256 | train_loss 0.7826 | train_acc 0.7860 | test_loss 0.8734 | test_acc 0.7140 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 69/256 | train_loss 1.0306 | train_acc 0.6860 | test_loss 0.8345 | test_acc 0.7285 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 70/256 | train_loss 0.8879 | train_acc 0.7660 | test_loss 0.7735 | test_acc 0.7425 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 71/256 | train_loss 0.7058 | train_acc 0.8260 | test_loss 0.8884 | test_acc 0.7130 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 72/256 | train_loss 0.8921 | train_acc 0.7320 | test_loss 0.9720 | test_acc 0.6775 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 73/256 | train_loss 1.0078 | train_acc 0.7280 | test_loss 0.6696 | test_acc 0.7880 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 74/256 | train_loss 1.0245 | train_acc 0.7500 | test_loss 0.7487 | test_acc 0.7530 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 75/256 | train_loss 1.0260 | train_acc 0.7020 | test_loss 0.9106 | test_acc 0.6990 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 76/256 | train_loss 1.0375 | train_acc 0.7140 | test_loss 0.5822 | test_acc 0.8140 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 77/256 | train_loss 0.9802 | train_acc 0.6920 | test_loss 0.9013 | test_acc 0.6795 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 78/256 | train_loss 0.6447 | train_acc 0.8180 | test_loss 0.8771 | test_acc 0.7105 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 79/256 | train_loss 0.8783 | train_acc 0.7040 | test_loss 0.7942 | test_acc 0.7210 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 80/256 | train_loss 1.0537 | train_acc 0.6600 | test_loss 0.6410 | test_acc 0.8045 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 81/256 | train_loss 0.9999 | train_acc 0.6700 | test_loss 0.6383 | test_acc 0.7970 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 82/256 | train_loss 0.7376 | train_acc 0.7940 | test_loss 0.9173 | test_acc 0.6920 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 83/256 | train_loss 0.9183 | train_acc 0.7080 | test_loss 0.9060 | test_acc 0.7140 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 84/256 | train_loss 0.8079 | train_acc 0.7840 | test_loss 0.6324 | test_acc 0.7895 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 85/256 | train_loss 0.9214 | train_acc 0.7100 | test_loss 0.8748 | test_acc 0.7170 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 86/256 | train_loss 0.9984 | train_acc 0.6780 | test_loss 0.7214 | test_acc 0.7605 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 87/256 | train_loss 0.8555 | train_acc 0.7540 | test_loss 0.7012 | test_acc 0.7675 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 88/256 | train_loss 0.9174 | train_acc 0.6740 | test_loss 0.5477 | test_acc 0.8310 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 89/256 | train_loss 0.5953 | train_acc 0.8240 | test_loss 0.5339 | test_acc 0.8205 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 90/256 | train_loss 0.6295 | train_acc 0.8160 | test_loss 0.6909 | test_acc 0.7780 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 91/256 | train_loss 0.7730 | train_acc 0.7900 | test_loss 0.6494 | test_acc 0.7665 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 92/256 | train_loss 0.6663 | train_acc 0.7760 | test_loss 0.4438 | test_acc 0.8495 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 93/256 | train_loss 0.7421 | train_acc 0.7820 | test_loss 0.7937 | test_acc 0.7370 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 94/256 | train_loss 0.8884 | train_acc 0.7140 | test_loss 0.4881 | test_acc 0.8530 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 95/256 | train_loss 0.7360 | train_acc 0.8240 | test_loss 0.5342 | test_acc 0.8275 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 96/256 | train_loss 0.8077 | train_acc 0.7660 | test_loss 0.5294 | test_acc 0.8240 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 97/256 | train_loss 0.6784 | train_acc 0.8220 | test_loss 0.8911 | test_acc 0.7050 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 98/256 | train_loss 0.7848 | train_acc 0.7880 | test_loss 0.5881 | test_acc 0.8015 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 99/256 | train_loss 0.8220 | train_acc 0.7680 | test_loss 0.5521 | test_acc 0.8215 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 100/256 | train_loss 0.7412 | train_acc 0.7760 | test_loss 0.7173 | test_acc 0.7590 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 101/256 | train_loss 1.0302 | train_acc 0.7140 | test_loss 0.6362 | test_acc 0.7950 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 102/256 | train_loss 0.9706 | train_acc 0.7260 | test_loss 0.5934 | test_acc 0.8165 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 103/256 | train_loss 0.7846 | train_acc 0.7140 | test_loss 0.6525 | test_acc 0.7835 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 104/256 | train_loss 0.6064 | train_acc 0.8580 | test_loss 0.5066 | test_acc 0.8285 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 105/256 | train_loss 0.6645 | train_acc 0.8620 | test_loss 0.5136 | test_acc 0.8285 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 106/256 | train_loss 0.5650 | train_acc 0.8520 | test_loss 0.5584 | test_acc 0.8075 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 107/256 | train_loss 0.6554 | train_acc 0.8200 | test_loss 0.7796 | test_acc 0.7365 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 108/256 | train_loss 0.6236 | train_acc 0.8340 | test_loss 0.7178 | test_acc 0.7530 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 109/256 | train_loss 0.6658 | train_acc 0.8220 | test_loss 0.6483 | test_acc 0.7840 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 110/256 | train_loss 0.5759 | train_acc 0.8840 | test_loss 0.6099 | test_acc 0.7980 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 111/256 | train_loss 0.8720 | train_acc 0.7360 | test_loss 0.5162 | test_acc 0.8320 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 112/256 | train_loss 0.6846 | train_acc 0.7880 | test_loss 0.5930 | test_acc 0.8035 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 113/256 | train_loss 0.6968 | train_acc 0.8160 | test_loss 0.5662 | test_acc 0.8065 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 114/256 | train_loss 0.7028 | train_acc 0.8160 | test_loss 0.5695 | test_acc 0.8110 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 115/256 | train_loss 0.6599 | train_acc 0.8240 | test_loss 0.5086 | test_acc 0.8270 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 116/256 | train_loss 0.7870 | train_acc 0.7320 | test_loss 0.6351 | test_acc 0.7950 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 117/256 | train_loss 0.6309 | train_acc 0.8140 | test_loss 0.5330 | test_acc 0.8215 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 118/256 | train_loss 0.5776 | train_acc 0.8440 | test_loss 0.5703 | test_acc 0.8075 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 119/256 | train_loss 0.5972 | train_acc 0.8200 | test_loss 0.5966 | test_acc 0.7960 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 120/256 | train_loss 0.6205 | train_acc 0.7960 | test_loss 0.5182 | test_acc 0.8215 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 121/256 | train_loss 0.4938 | train_acc 0.8480 | test_loss 0.5197 | test_acc 0.8185 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 122/256 | train_loss 0.6779 | train_acc 0.8060 | test_loss 0.5703 | test_acc 0.8055 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 123/256 | train_loss 0.8138 | train_acc 0.8000 | test_loss 0.5300 | test_acc 0.8150 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 124/256 | train_loss 0.7086 | train_acc 0.8100 | test_loss 0.5693 | test_acc 0.8105 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 125/256 | train_loss 0.4992 | train_acc 0.8420 | test_loss 0.5535 | test_acc 0.8080 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 126/256 | train_loss 0.6057 | train_acc 0.8640 | test_loss 0.5904 | test_acc 0.7970 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 127/256 | train_loss 0.6563 | train_acc 0.8460 | test_loss 0.5505 | test_acc 0.8150 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 128/256 | train_loss 0.5155 | train_acc 0.8600 | test_loss 0.5713 | test_acc 0.8015 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 129/256 | train_loss 0.7493 | train_acc 0.7360 | test_loss 1.2068 | test_acc 0.6135 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 130/256 | train_loss 0.8445 | train_acc 0.7180 | test_loss 0.6184 | test_acc 0.8170 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 131/256 | train_loss 0.7484 | train_acc 0.7540 | test_loss 0.6915 | test_acc 0.7635 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 132/256 | train_loss 0.8608 | train_acc 0.7840 | test_loss 0.6827 | test_acc 0.7600 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 133/256 | train_loss 0.7806 | train_acc 0.8040 | test_loss 0.8349 | test_acc 0.7195 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 134/256 | train_loss 1.1309 | train_acc 0.6600 | test_loss 1.0096 | test_acc 0.6825 | lr 0.0498
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 135/256 | train_loss 0.9528 | train_acc 0.7340 | test_loss 0.5143 | test_acc 0.8265 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 136/256 | train_loss 0.5612 | train_acc 0.8580 | test_loss 1.6496 | test_acc 0.5165 | lr 0.0496
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 137/256 | train_loss 0.9008 | train_acc 0.7720 | test_loss 0.8110 | test_acc 0.7275 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 138/256 | train_loss 0.8934 | train_acc 0.7840 | test_loss 0.5581 | test_acc 0.8180 | lr 0.0494
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 139/256 | train_loss 0.7194 | train_acc 0.7940 | test_loss 0.7685 | test_acc 0.7530 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 140/256 | train_loss 0.5562 | train_acc 0.8360 | test_loss 1.2603 | test_acc 0.6180 | lr 0.0491
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 141/256 | train_loss 0.9098 | train_acc 0.7700 | test_loss 1.0114 | test_acc 0.6640 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 142/256 | train_loss 0.9639 | train_acc 0.7300 | test_loss 1.0419 | test_acc 0.6455 | lr 0.0488
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 143/256 | train_loss 0.8193 | train_acc 0.7680 | test_loss 0.7521 | test_acc 0.7620 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 144/256 | train_loss 0.8544 | train_acc 0.7940 | test_loss 0.7203 | test_acc 0.7690 | lr 0.0483
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 145/256 | train_loss 0.7481 | train_acc 0.7600 | test_loss 0.9520 | test_acc 0.6880 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 146/256 | train_loss 1.0246 | train_acc 0.6920 | test_loss 0.6747 | test_acc 0.7885 | lr 0.0479
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 147/256 | train_loss 0.9099 | train_acc 0.7760 | test_loss 0.8131 | test_acc 0.7290 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 148/256 | train_loss 0.8002 | train_acc 0.7840 | test_loss 0.5272 | test_acc 0.8260 | lr 0.0474
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 149/256 | train_loss 0.7594 | train_acc 0.8340 | test_loss 0.8517 | test_acc 0.7150 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 150/256 | train_loss 0.8439 | train_acc 0.7860 | test_loss 1.1174 | test_acc 0.6255 | lr 0.0468
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 151/256 | train_loss 0.6744 | train_acc 0.8160 | test_loss 1.0437 | test_acc 0.6420 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 152/256 | train_loss 0.7544 | train_acc 0.8200 | test_loss 1.2655 | test_acc 0.5940 | lr 0.0462
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 153/256 | train_loss 0.8666 | train_acc 0.7400 | test_loss 1.1090 | test_acc 0.6700 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 154/256 | train_loss 0.6969 | train_acc 0.8340 | test_loss 0.7840 | test_acc 0.7455 | lr 0.0455
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 155/256 | train_loss 0.7425 | train_acc 0.7820 | test_loss 0.7174 | test_acc 0.7665 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 156/256 | train_loss 0.6282 | train_acc 0.7880 | test_loss 0.6810 | test_acc 0.7785 | lr 0.0448
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 157/256 | train_loss 0.8151 | train_acc 0.7920 | test_loss 0.6364 | test_acc 0.7775 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 158/256 | train_loss 0.6641 | train_acc 0.8320 | test_loss 0.6957 | test_acc 0.7620 | lr 0.0440
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 159/256 | train_loss 0.8223 | train_acc 0.7400 | test_loss 0.7025 | test_acc 0.7755 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 160/256 | train_loss 0.9248 | train_acc 0.6860 | test_loss 1.0338 | test_acc 0.6625 | lr 0.0432
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 161/256 | train_loss 0.8574 | train_acc 0.7680 | test_loss 0.5655 | test_acc 0.8095 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 162/256 | train_loss 0.7567 | train_acc 0.7940 | test_loss 0.4710 | test_acc 0.8455 | lr 0.0423
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 163/256 | train_loss 0.7481 | train_acc 0.7820 | test_loss 0.5565 | test_acc 0.8225 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 164/256 | train_loss 0.6681 | train_acc 0.8200 | test_loss 0.5916 | test_acc 0.8005 | lr 0.0414
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 165/256 | train_loss 0.8904 | train_acc 0.7380 | test_loss 0.7462 | test_acc 0.7595 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 166/256 | train_loss 0.6811 | train_acc 0.8260 | test_loss 0.7681 | test_acc 0.7635 | lr 0.0405
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 167/256 | train_loss 0.7981 | train_acc 0.7440 | test_loss 0.6489 | test_acc 0.7890 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 168/256 | train_loss 0.8397 | train_acc 0.7720 | test_loss 0.7699 | test_acc 0.7475 | lr 0.0395
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 169/256 | train_loss 0.9571 | train_acc 0.7080 | test_loss 0.8842 | test_acc 0.7100 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 170/256 | train_loss 0.8210 | train_acc 0.7720 | test_loss 0.5372 | test_acc 0.8235 | lr 0.0385
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 171/256 | train_loss 0.9235 | train_acc 0.7160 | test_loss 0.7530 | test_acc 0.7515 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 172/256 | train_loss 0.7660 | train_acc 0.7860 | test_loss 0.8766 | test_acc 0.7015 | lr 0.0374
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 173/256 | train_loss 0.5221 | train_acc 0.8520 | test_loss 0.9288 | test_acc 0.7050 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 174/256 | train_loss 0.8109 | train_acc 0.7760 | test_loss 0.6933 | test_acc 0.7745 | lr 0.0364
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 175/256 | train_loss 0.6587 | train_acc 0.8040 | test_loss 0.4821 | test_acc 0.8435 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 176/256 | train_loss 0.8474 | train_acc 0.7960 | test_loss 0.6681 | test_acc 0.7790 | lr 0.0353
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 177/256 | train_loss 0.9101 | train_acc 0.7240 | test_loss 0.8306 | test_acc 0.7185 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 178/256 | train_loss 0.7032 | train_acc 0.8560 | test_loss 0.7566 | test_acc 0.7415 | lr 0.0342
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 179/256 | train_loss 0.7299 | train_acc 0.8100 | test_loss 0.7398 | test_acc 0.7560 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 180/256 | train_loss 0.5561 | train_acc 0.8860 | test_loss 0.6486 | test_acc 0.7835 | lr 0.0330
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 181/256 | train_loss 0.6899 | train_acc 0.8460 | test_loss 0.9055 | test_acc 0.6975 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 182/256 | train_loss 0.5463 | train_acc 0.8540 | test_loss 0.6230 | test_acc 0.7910 | lr 0.0319
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 183/256 | train_loss 0.4707 | train_acc 0.8780 | test_loss 0.6540 | test_acc 0.7865 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 184/256 | train_loss 0.8335 | train_acc 0.8040 | test_loss 0.6600 | test_acc 0.7875 | lr 0.0307
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 185/256 | train_loss 0.6936 | train_acc 0.8600 | test_loss 0.6926 | test_acc 0.7785 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 186/256 | train_loss 0.4717 | train_acc 0.8800 | test_loss 0.7202 | test_acc 0.7685 | lr 0.0295
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 187/256 | train_loss 0.5662 | train_acc 0.8040 | test_loss 0.5093 | test_acc 0.8380 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 188/256 | train_loss 1.0266 | train_acc 0.6960 | test_loss 0.6387 | test_acc 0.7940 | lr 0.0283
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 189/256 | train_loss 0.7934 | train_acc 0.7960 | test_loss 0.8764 | test_acc 0.7110 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 190/256 | train_loss 0.5578 | train_acc 0.8380 | test_loss 0.7786 | test_acc 0.7495 | lr 0.0271
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 191/256 | train_loss 0.7908 | train_acc 0.7740 | test_loss 0.5721 | test_acc 0.8110 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 192/256 | train_loss 0.7124 | train_acc 0.7900 | test_loss 0.9898 | test_acc 0.6755 | lr 0.0259
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 193/256 | train_loss 0.4670 | train_acc 0.8840 | test_loss 0.7900 | test_acc 0.7355 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 194/256 | train_loss 0.6499 | train_acc 0.8420 | test_loss 0.4493 | test_acc 0.8525 | lr 0.0246
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 195/256 | train_loss 0.6362 | train_acc 0.7920 | test_loss 0.7433 | test_acc 0.7475 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 196/256 | train_loss 0.6776 | train_acc 0.8020 | test_loss 0.5040 | test_acc 0.8410 | lr 0.0234
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 197/256 | train_loss 0.7578 | train_acc 0.8280 | test_loss 0.5000 | test_acc 0.8390 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 198/256 | train_loss 0.5416 | train_acc 0.8620 | test_loss 0.8410 | test_acc 0.7350 | lr 0.0222
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 199/256 | train_loss 0.7742 | train_acc 0.7080 | test_loss 0.7110 | test_acc 0.7665 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 200/256 | train_loss 0.6419 | train_acc 0.8100 | test_loss 0.7408 | test_acc 0.7625 | lr 0.0210
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 201/256 | train_loss 0.7637 | train_acc 0.7940 | test_loss 0.7330 | test_acc 0.7595 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 202/256 | train_loss 0.6041 | train_acc 0.8360 | test_loss 0.6805 | test_acc 0.7855 | lr 0.0198
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 203/256 | train_loss 0.5016 | train_acc 0.8980 | test_loss 0.5583 | test_acc 0.8295 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 204/256 | train_loss 0.6716 | train_acc 0.7740 | test_loss 0.5310 | test_acc 0.8245 | lr 0.0186
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 205/256 | train_loss 0.5110 | train_acc 0.8680 | test_loss 0.6089 | test_acc 0.8020 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 206/256 | train_loss 0.6615 | train_acc 0.8300 | test_loss 1.0237 | test_acc 0.6660 | lr 0.0175
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 207/256 | train_loss 0.7821 | train_acc 0.7960 | test_loss 0.6046 | test_acc 0.8045 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 208/256 | train_loss 0.6658 | train_acc 0.7820 | test_loss 0.7157 | test_acc 0.7665 | lr 0.0163
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 209/256 | train_loss 0.4818 | train_acc 0.8760 | test_loss 0.6507 | test_acc 0.7855 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 210/256 | train_loss 0.5229 | train_acc 0.8680 | test_loss 0.7140 | test_acc 0.7725 | lr 0.0152
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 211/256 | train_loss 0.6095 | train_acc 0.8080 | test_loss 0.7107 | test_acc 0.7645 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 212/256 | train_loss 0.6181 | train_acc 0.7760 | test_loss 0.5368 | test_acc 0.8275 | lr 0.0141
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 213/256 | train_loss 0.6203 | train_acc 0.8000 | test_loss 0.5299 | test_acc 0.8345 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 214/256 | train_loss 0.6542 | train_acc 0.8080 | test_loss 0.4996 | test_acc 0.8455 | lr 0.0131
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 215/256 | train_loss 0.5512 | train_acc 0.8680 | test_loss 0.5535 | test_acc 0.8235 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 216/256 | train_loss 0.5610 | train_acc 0.8780 | test_loss 0.7114 | test_acc 0.7660 | lr 0.0120
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 217/256 | train_loss 0.6105 | train_acc 0.8480 | test_loss 0.6524 | test_acc 0.7905 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 218/256 | train_loss 0.6792 | train_acc 0.8460 | test_loss 0.6257 | test_acc 0.7975 | lr 0.0110
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 219/256 | train_loss 0.6084 | train_acc 0.7960 | test_loss 0.7264 | test_acc 0.7595 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 220/256 | train_loss 0.6316 | train_acc 0.8220 | test_loss 0.6731 | test_acc 0.7810 | lr 0.0100
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 221/256 | train_loss 0.8125 | train_acc 0.7200 | test_loss 0.7024 | test_acc 0.7705 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 222/256 | train_loss 0.4517 | train_acc 0.8820 | test_loss 0.6484 | test_acc 0.7865 | lr 0.0091
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 223/256 | train_loss 0.6049 | train_acc 0.8280 | test_loss 0.6901 | test_acc 0.7755 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 224/256 | train_loss 0.4845 | train_acc 0.8760 | test_loss 0.7380 | test_acc 0.7700 | lr 0.0082
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 225/256 | train_loss 0.7190 | train_acc 0.8440 | test_loss 0.6551 | test_acc 0.7825 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 226/256 | train_loss 0.5624 | train_acc 0.8500 | test_loss 0.7089 | test_acc 0.7705 | lr 0.0073
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 227/256 | train_loss 0.8403 | train_acc 0.7800 | test_loss 0.6499 | test_acc 0.7880 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 228/256 | train_loss 0.5640 | train_acc 0.9000 | test_loss 0.5734 | test_acc 0.8125 | lr 0.0065
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 229/256 | train_loss 0.6330 | train_acc 0.8660 | test_loss 0.6593 | test_acc 0.7910 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 230/256 | train_loss 0.5720 | train_acc 0.8580 | test_loss 0.7036 | test_acc 0.7715 | lr 0.0057
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 231/256 | train_loss 0.5482 | train_acc 0.8480 | test_loss 0.6506 | test_acc 0.7915 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 232/256 | train_loss 0.4651 | train_acc 0.8880 | test_loss 0.6637 | test_acc 0.7880 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 233/256 | train_loss 0.6310 | train_acc 0.8020 | test_loss 0.6415 | test_acc 0.7975 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 234/256 | train_loss 0.7841 | train_acc 0.7240 | test_loss 0.6112 | test_acc 0.8075 | lr 0.0043
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 235/256 | train_loss 0.8831 | train_acc 0.7340 | test_loss 0.6334 | test_acc 0.8070 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 236/256 | train_loss 0.5890 | train_acc 0.8460 | test_loss 0.6622 | test_acc 0.7845 | lr 0.0037
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 237/256 | train_loss 0.6854 | train_acc 0.8220 | test_loss 0.6717 | test_acc 0.7875 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 238/256 | train_loss 0.4239 | train_acc 0.8720 | test_loss 0.6370 | test_acc 0.8010 | lr 0.0031
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 239/256 | train_loss 0.5908 | train_acc 0.8280 | test_loss 0.6400 | test_acc 0.7955 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 240/256 | train_loss 0.5368 | train_acc 0.8260 | test_loss 0.5451 | test_acc 0.8305 | lr 0.0026
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 241/256 | train_loss 0.2401 | train_acc 0.9240 | test_loss 0.5879 | test_acc 0.8145 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 242/256 | train_loss 0.6110 | train_acc 0.7840 | test_loss 0.5528 | test_acc 0.8255 | lr 0.0022
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 243/256 | train_loss 0.4413 | train_acc 0.8800 | test_loss 0.6525 | test_acc 0.7905 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 244/256 | train_loss 0.6976 | train_acc 0.7660 | test_loss 0.6015 | test_acc 0.8085 | lr 0.0017
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 245/256 | train_loss 0.4769 | train_acc 0.8600 | test_loss 0.6630 | test_acc 0.7885 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 246/256 | train_loss 0.5458 | train_acc 0.8560 | test_loss 0.5988 | test_acc 0.8030 | lr 0.0014
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 247/256 | train_loss 0.4716 | train_acc 0.8380 | test_loss 0.6656 | test_acc 0.7855 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 248/256 | train_loss 0.6383 | train_acc 0.8140 | test_loss 0.6691 | test_acc 0.7845 | lr 0.0011
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 249/256 | train_loss 0.6549 | train_acc 0.8560 | test_loss 0.6535 | test_acc 0.7855 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 250/256 | train_loss 0.6213 | train_acc 0.8800 | test_loss 0.6057 | test_acc 0.8060 | lr 0.0009
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 251/256 | train_loss 0.5405 | train_acc 0.8100 | test_loss 0.5918 | test_acc 0.8095 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 252/256 | train_loss 0.5669 | train_acc 0.8980 | test_loss 0.6138 | test_acc 0.7995 | lr 0.0007
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 253/256 | train_loss 0.4439 | train_acc 0.9180 | test_loss 0.6172 | test_acc 0.8000 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 254/256 | train_loss 0.4849 | train_acc 0.8600 | test_loss 0.5906 | test_acc 0.8110 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 255/256 | train_loss 0.5736 | train_acc 0.8320 | test_loss 0.6005 | test_acc 0.8080 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 3 | Epoch 256/256 | train_loss 0.5486 | train_acc 0.8480 | test_loss 0.6094 | test_acc 0.8030 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (3)
[INFO] main.py:389 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (3)
[INFO] main.py:396 > [2-5] Report task result

##################################################
# Task 4 iteration
##################################################

[INFO] main.py:308 > [2-1] Prepare a datalist for the current task
meta_pseudo_init
total : 5000  current step :  0
total : 5000  current step :  1
total : 5000  current step :  2
total : 5000  current step :  3
total : 5000  current step :  4
total : 5000  current step :  5
total : 5000  current step :  6
total : 5000  current step :  7
total : 5000  current step :  8
total : 5000  current step :  9
total : 5000  current step :  10
total : 5000  current step :  11
total : 5000  current step :  12
total : 5000  current step :  13
total : 5000  current step :  14
total : 5000  current step :  15
total : 5000  current step :  16
total : 5000  current step :  17
total : 5000  current step :  18
total : 5000  current step :  19
total : 5000  current step :  20
total : 5000  current step :  21
total : 5000  current step :  22
total : 5000  current step :  23
total : 5000  current step :  24
total : 5000  current step :  25
total : 5000  current step :  26
total : 5000  current step :  27
total : 5000  current step :  28
total : 5000  current step :  29
total : 5000  current step :  30
total : 5000  current step :  31
total : 5000  current step :  32
total : 5000  current step :  33
total : 5000  current step :  34
total : 5000  current step :  35
total : 5000  current step :  36
total : 5000  current step :  37
total : 5000  current step :  38
total : 5000  current step :  39
total : 5000  current step :  40
total : 5000  current step :  41
total : 5000  current step :  42
total : 5000  current step :  43
total : 5000  current step :  44
total : 5000  current step :  45
total : 5000  current step :  46
total : 5000  current step :  47
total : 5000  current step :  48
total : 5000  current step :  49
total : 5000  current step :  50
total : 5000  current step :  51
total : 5000  current step :  52
total : 5000  current step :  53
total : 5000  current step :  54
total : 5000  current step :  55
total : 5000  current step :  56
total : 5000  current step :  57
total : 5000  current step :  58
total : 5000  current step :  59
total : 5000  current step :  60
total : 5000  current step :  61
total : 5000  current step :  62
total : 5000  current step :  63
total : 5000  current step :  64
total : 5000  current step :  65
total : 5000  current step :  66
total : 5000  current step :  67
total : 5000  current step :  68
total : 5000  current step :  69
total : 5000  current step :  70
total : 5000  current step :  71
total : 5000  current step :  72
total : 5000  current step :  73
total : 5000  current step :  74
total : 5000  current step :  75
total : 5000  current step :  76
total : 5000  current step :  77
total : 5000  current step :  78
total : 5000  current step :  79
total : 5000  current step :  80
total : 5000  current step :  81
total : 5000  current step :  82
total : 5000  current step :  83
total : 5000  current step :  84
total : 5000  current step :  85
total : 5000  current step :  86
total : 5000  current step :  87
total : 5000  current step :  88
total : 5000  current step :  89
total : 5000  current step :  90
total : 5000  current step :  91
total : 5000  current step :  92
total : 5000  current step :  93
total : 5000  current step :  94
total : 5000  current step :  95
total : 5000  current step :  96
total : 5000  current step :  97
total : 5000  current step :  98
total : 5000  current step :  99
total : 5000  current step :  100
total : 5000  current step :  101
total : 5000  current step :  102
total : 5000  current step :  103
total : 5000  current step :  104
total : 5000  current step :  105
total : 5000  current step :  106
total : 5000  current step :  107
total : 5000  current step :  108
total : 5000  current step :  109
total : 5000  current step :  110
total : 5000  current step :  111
total : 5000  current step :  112
total : 5000  current step :  113
total : 5000  current step :  114
total : 5000  current step :  115
total : 5000  current step :  116
total : 5000  current step :  117
total : 5000  current step :  118
total : 5000  current step :  119
total : 5000  current step :  120
total : 5000  current step :  121
total : 5000  current step :  122
total : 5000  current step :  123
total : 5000  current step :  124
total : 5000  current step :  125
total : 5000  current step :  126
total : 5000  current step :  127
total : 5000  current step :  128
total : 5000  current step :  129
total : 5000  current step :  130
total : 5000  current step :  131
total : 5000  current step :  132
total : 5000  current step :  133
total : 5000  current step :  134
total : 5000  current step :  135
total : 5000  current step :  136
total : 5000  current step :  137
total : 5000  current step :  138
total : 5000  current step :  139
total : 5000  current step :  140
total : 5000  current step :  141
total : 5000  current step :  142
total : 5000  current step :  143
total : 5000  current step :  144
total : 5000  current step :  145
total : 5000  current step :  146
total : 5000  current step :  147
total : 5000  current step :  148
total : 5000  current step :  149
total : 5000  current step :  150
total : 5000  current step :  151
total : 5000  current step :  152
total : 5000  current step :  153
total : 5000  current step :  154
total : 5000  current step :  155
total : 5000  current step :  156
total : 5000  current step :  157
total : 5000  current step :  158
total : 5000  current step :  159
total : 5000  current step :  160
total : 5000  current step :  161
total : 5000  current step :  162
total : 5000  current step :  163
total : 5000  current step :  164
total : 5000  current step :  165
total : 5000  current step :  166
total : 5000  current step :  167
total : 5000  current step :  168
total : 5000  current step :  169
total : 5000  current step :  170
total : 5000  current step :  171
total : 5000  current step :  172
total : 5000  current step :  173
total : 5000  current step :  174
total : 5000  current step :  175
total : 5000  current step :  176
total : 5000  current step :  177
total : 5000  current step :  178
total : 5000  current step :  179
total : 5000  current step :  180
total : 5000  current step :  181
total : 5000  current step :  182
total : 5000  current step :  183
total : 5000  current step :  184
total : 5000  current step :  185
total : 5000  current step :  186
total : 5000  current step :  187
total : 5000  current step :  188
total : 5000  current step :  189
total : 5000  current step :  190
total : 5000  current step :  191
total : 5000  current step :  192
total : 5000  current step :  193
total : 5000  current step :  194
total : 5000  current step :  195
total : 5000  current step :  196
total : 5000  current step :  197
total : 5000  current step :  198
total : 5000  current step :  199
total : 5000  current step :  200
total : 5000  current step :  201
total : 5000  current step :  202
total : 5000  current step :  203
total : 5000  current step :  204
total : 5000  current step :  205
total : 5000  current step :  206
total : 5000  current step :  207
total : 5000  current step :  208
total : 5000  current step :  209
total : 5000  current step :  210
total : 5000  current step :  211
total : 5000  current step :  212
total : 5000  current step :  213
total : 5000  current step :  214
total : 5000  current step :  215
total : 5000  current step :  216
total : 5000  current step :  217
total : 5000  current step :  218
total : 5000  current step :  219
total : 5000  current step :  220
total : 5000  current step :  221
total : 5000  current step :  222
total : 5000  current step :  223
total : 5000  current step :  224
total : 5000  current step :  225
total : 5000  current step :  226
total : 5000  current step :  227
total : 5000  current step :  228
total : 5000  current step :  229
total : 5000  current step :  230
total : 5000  current step :  231
total : 5000  current step :  232
total : 5000  current step :  233
total : 5000  current step :  234
total : 5000  current step :  235
total : 5000  current step :  236
total : 5000  current step :  237
total : 5000  current step :  238
total : 5000  current step :  239
total : 5000  current step :  240
total : 5000  current step :  241
total : 5000  current step :  242
total : 5000  current step :  243
total : 5000  current step :  244
total : 5000  current step :  245
total : 5000  current step :  246
total : 5000  current step :  247
total : 5000  current step :  248
total : 5000  current step :  249
total : 5000  current step :  250
total : 5000  current step :  251
total : 5000  current step :  252
total : 5000  current step :  253
total : 5000  current step :  254
total : 5000  current step :  255
total : 5000  current step :  256
total : 5000  current step :  257
total : 5000  current step :  258
total : 5000  current step :  259
total : 5000  current step :  260
total : 5000  current step :  261
total : 5000  current step :  262
total : 5000  current step :  263
total : 5000  current step :  264
total : 5000  current step :  265
total : 5000  current step :  266
total : 5000  current step :  267
total : 5000  current step :  268
total : 5000  current step :  269
total : 5000  current step :  270
total : 5000  current step :  271
total : 5000  current step :  272
total : 5000  current step :  273
total : 5000  current step :  274
total : 5000  current step :  275
total : 5000  current step :  276
total : 5000  current step :  277
total : 5000  current step :  278
total : 5000  current step :  279
total : 5000  current step :  280
total : 5000  current step :  281
total : 5000  current step :  282
total : 5000  current step :  283
total : 5000  current step :  284
total : 5000  current step :  285
total : 5000  current step :  286
total : 5000  current step :  287
total : 5000  current step :  288
total : 5000  current step :  289
total : 5000  current step :  290
total : 5000  current step :  291
total : 5000  current step :  292
total : 5000  current step :  293
total : 5000  current step :  294
total : 5000  current step :  295
total : 5000  current step :  296
total : 5000  current step :  297
total : 5000  current step :  298
total : 5000  current step :  299
total : 5000  current step :  300
total : 5000  current step :  301
total : 5000  current step :  302
total : 5000  current step :  303
total : 5000  current step :  304
total : 5000  current step :  305
total : 5000  current step :  306
total : 5000  current step :  307
total : 5000  current step :  308
total : 5000  current step :  309
total : 5000  current step :  310
total : 5000  current step :  311
total : 5000  current step :  312
total : 5000  current step :  313
total : 5000  current step :  314
total : 5000  current step :  315
total : 5000  current step :  316
total : 5000  current step :  317
total : 5000  current step :  318
total : 5000  current step :  319
total : 5000  current step :  320
total : 5000  current step :  321
total : 5000  current step :  322
total : 5000  current step :  323
total : 5000  current step :  324
total : 5000  current step :  325
total : 5000  current step :  326
total : 5000  current step :  327
total : 5000  current step :  328
total : 5000  current step :  329
total : 5000  current step :  330
total : 5000  current step :  331
total : 5000  current step :  332
total : 5000  current step :  333
total : 5000  current step :  334
total : 5000  current step :  335
total : 5000  current step :  336
total : 5000  current step :  337
total : 5000  current step :  338
total : 5000  current step :  339
total : 5000  current step :  340
total : 5000  current step :  341
total : 5000  current step :  342
total : 5000  current step :  343
total : 5000  current step :  344
total : 5000  current step :  345
total : 5000  current step :  346
total : 5000  current step :  347
total : 5000  current step :  348
total : 5000  current step :  349
total : 5000  current step :  350
total : 5000  current step :  351
total : 5000  current step :  352
total : 5000  current step :  353
total : 5000  current step :  354
total : 5000  current step :  355
total : 5000  current step :  356
total : 5000  current step :  357
total : 5000  current step :  358
total : 5000  current step :  359
total : 5000  current step :  360
total : 5000  current step :  361
total : 5000  current step :  362
total : 5000  current step :  363
total : 5000  current step :  364
total : 5000  current step :  365
total : 5000  current step :  366
total : 5000  current step :  367
total : 5000  current step :  368
total : 5000  current step :  369
total : 5000  current step :  370
total : 5000  current step :  371
total : 5000  current step :  372
total : 5000  current step :  373
total : 5000  current step :  374
total : 5000  current step :  375
total : 5000  current step :  376
total : 5000  current step :  377
total : 5000  current step :  378
total : 5000  current step :  379
total : 5000  current step :  380
total : 5000  current step :  381
total : 5000  current step :  382
total : 5000  current step :  383
total : 5000  current step :  384
total : 5000  current step :  385
total : 5000  current step :  386
total : 5000  current step :  387
total : 5000  current step :  388
total : 5000  current step :  389
total : 5000  current step :  390
total : 5000  current step :  391
total : 5000  current step :  392
total : 5000  current step :  393
total : 5000  current step :  394
total : 5000  current step :  395
total : 5000  current step :  396
total : 5000  current step :  397
total : 5000  current step :  398
total : 5000  current step :  399
total : 5000  current step :  400
total : 5000  current step :  401
total : 5000  current step :  402
total : 5000  current step :  403
total : 5000  current step :  404
total : 5000  current step :  405
total : 5000  current step :  406
total : 5000  current step :  407
total : 5000  current step :  408
total : 5000  current step :  409
total : 5000  current step :  410
total : 5000  current step :  411
total : 5000  current step :  412
total : 5000  current step :  413
total : 5000  current step :  414
total : 5000  current step :  415
total : 5000  current step :  416
total : 5000  current step :  417
total : 5000  current step :  418
total : 5000  current step :  419
total : 5000  current step :  420
total : 5000  current step :  421
total : 5000  current step :  422
total : 5000  current step :  423
total : 5000  current step :  424
total : 5000  current step :  425
total : 5000  current step :  426
total : 5000  current step :  427
total : 5000  current step :  428
total : 5000  current step :  429
total : 5000  current step :  430
total : 5000  current step :  431
total : 5000  current step :  432
total : 5000  current step :  433
total : 5000  current step :  434
total : 5000  current step :  435
total : 5000  current step :  436
total : 5000  current step :  437
total : 5000  current step :  438
total : 5000  current step :  439
total : 5000  current step :  440
total : 5000  current step :  441
total : 5000  current step :  442
total : 5000  current step :  443
total : 5000  current step :  444
total : 5000  current step :  445
total : 5000  current step :  446
total : 5000  current step :  447
total : 5000  current step :  448
total : 5000  current step :  449
total : 5000  current step :  450
total : 5000  current step :  451
total : 5000  current step :  452
total : 5000  current step :  453
total : 5000  current step :  454
total : 5000  current step :  455
total : 5000  current step :  456
total : 5000  current step :  457
total : 5000  current step :  458
total : 5000  current step :  459
total : 5000  current step :  460
total : 5000  current step :  461
total : 5000  current step :  462
total : 5000  current step :  463
total : 5000  current step :  464
total : 5000  current step :  465
total : 5000  current step :  466
total : 5000  current step :  467
total : 5000  current step :  468
total : 5000  current step :  469
total : 5000  current step :  470
total : 5000  current step :  471
total : 5000  current step :  472
total : 5000  current step :  473
total : 5000  current step :  474
total : 5000  current step :  475
total : 5000  current step :  476
total : 5000  current step :  477
total : 5000  current step :  478
total : 5000  current step :  479
total : 5000  current step :  480
total : 5000  current step :  481
total : 5000  current step :  482
total : 5000  current step :  483
total : 5000  current step :  484
total : 5000  current step :  485
total : 5000  current step :  486
total : 5000  current step :  487
total : 5000  current step :  488
total : 5000  current step :  489
total : 5000  current step :  490
total : 5000  current step :  491
total : 5000  current step :  492
total : 5000  current step :  493
total : 5000  current step :  494
total : 5000  current step :  495
total : 5000  current step :  496
total : 5000  current step :  497
total : 5000  current step :  498
total : 5000  current step :  499
total : 5000  current step :  500
total : 5000  current step :  501
total : 5000  current step :  502
total : 5000  current step :  503
total : 5000  current step :  504
total : 5000  current step :  505
total : 5000  current step :  506
total : 5000  current step :  507
total : 5000  current step :  508
total : 5000  current step :  509
total : 5000  current step :  510
total : 5000  current step :  511
total : 5000  current step :  512
total : 5000  current step :  513
total : 5000  current step :  514
total : 5000  current step :  515
total : 5000  current step :  516
total : 5000  current step :  517
total : 5000  current step :  518
total : 5000  current step :  519
total : 5000  current step :  520
total : 5000  current step :  521
total : 5000  current step :  522
total : 5000  current step :  523
total : 5000  current step :  524
total : 5000  current step :  525
total : 5000  current step :  526
total : 5000  current step :  527
total : 5000  current step :  528
total : 5000  current step :  529
total : 5000  current step :  530
total : 5000  current step :  531
total : 5000  current step :  532
total : 5000  current step :  533
total : 5000  current step :  534
total : 5000  current step :  535
total : 5000  current step :  536
total : 5000  current step :  537
total : 5000  current step :  538
total : 5000  current step :  539
total : 5000  current step :  540
total : 5000  current step :  541
total : 5000  current step :  542
total : 5000  current step :  543
total : 5000  current step :  544
total : 5000  current step :  545
total : 5000  current step :  546
total : 5000  current step :  547
total : 5000  current step :  548
total : 5000  current step :  549
total : 5000  current step :  550
total : 5000  current step :  551
total : 5000  current step :  552
total : 5000  current step :  553
total : 5000  current step :  554
total : 5000  current step :  555
total : 5000  current step :  556
total : 5000  current step :  557
total : 5000  current step :  558
total : 5000  current step :  559
total : 5000  current step :  560
total : 5000  current step :  561
total : 5000  current step :  562
total : 5000  current step :  563
total : 5000  current step :  564
total : 5000  current step :  565
total : 5000  current step :  566
total : 5000  current step :  567
total : 5000  current step :  568
total : 5000  current step :  569
total : 5000  current step :  570
total : 5000  current step :  571
total : 5000  current step :  572
total : 5000  current step :  573
total : 5000  current step :  574
total : 5000  current step :  575
total : 5000  current step :  576
total : 5000  current step :  577
total : 5000  current step :  578
total : 5000  current step :  579
total : 5000  current step :  580
total : 5000  current step :  581
total : 5000  current step :  582
total : 5000  current step :  583
total : 5000  current step :  584
total : 5000  current step :  585
total : 5000  current step :  586
total : 5000  current step :  587
total : 5000  current step :  588
total : 5000  current step :  589
total : 5000  current step :  590
total : 5000  current step :  591
total : 5000  current step :  592
total : 5000  current step :  593
total : 5000  current step :  594
total : 5000  current step :  595
total : 5000  current step :  596
total : 5000  current step :  597
total : 5000  current step :  598
total : 5000  current step :  599
total : 5000  current step :  600
total : 5000  current step :  601
total : 5000  current step :  602
total : 5000  current step :  603
total : 5000  current step :  604
total : 5000  current step :  605
total : 5000  current step :  606
total : 5000  current step :  607
total : 5000  current step :  608
total : 5000  current step :  609
total : 5000  current step :  610
total : 5000  current step :  611
total : 5000  current step :  612
total : 5000  current step :  613
total : 5000  current step :  614
total : 5000  current step :  615
total : 5000  current step :  616
total : 5000  current step :  617
total : 5000  current step :  618
total : 5000  current step :  619
total : 5000  current step :  620
total : 5000  current step :  621
total : 5000  current step :  622
total : 5000  current step :  623
total : 5000  current step :  624
total : 5000  current step :  625
total : 5000  current step :  626
total : 5000  current step :  627
total : 5000  current step :  628
total : 5000  current step :  629
total : 5000  current step :  630
total : 5000  current step :  631
total : 5000  current step :  632
total : 5000  current step :  633
total : 5000  current step :  634
total : 5000  current step :  635
total : 5000  current step :  636
total : 5000  current step :  637
total : 5000  current step :  638
total : 5000  current step :  639
total : 5000  current step :  640
total : 5000  current step :  641
total : 5000  current step :  642
total : 5000  current step :  643
total : 5000  current step :  644
total : 5000  current step :  645
total : 5000  current step :  646
total : 5000  current step :  647
total : 5000  current step :  648
total : 5000  current step :  649
total : 5000  current step :  650
total : 5000  current step :  651
total : 5000  current step :  652
total : 5000  current step :  653
total : 5000  current step :  654
total : 5000  current step :  655
total : 5000  current step :  656
total : 5000  current step :  657
total : 5000  current step :  658
total : 5000  current step :  659
total : 5000  current step :  660
total : 5000  current step :  661
total : 5000  current step :  662
total : 5000  current step :  663
total : 5000  current step :  664
total : 5000  current step :  665
total : 5000  current step :  666
total : 5000  current step :  667
total : 5000  current step :  668
total : 5000  current step :  669
total : 5000  current step :  670
total : 5000  current step :  671
total : 5000  current step :  672
total : 5000  current step :  673
total : 5000  current step :  674
total : 5000  current step :  675
total : 5000  current step :  676
total : 5000  current step :  677
total : 5000  current step :  678
total : 5000  current step :  679
total : 5000  current step :  680
total : 5000  current step :  681
total : 5000  current step :  682
total : 5000  current step :  683
total : 5000  current step :  684
total : 5000  current step :  685
total : 5000  current step :  686
total : 5000  current step :  687
total : 5000  current step :  688
total : 5000  current step :  689
total : 5000  current step :  690
total : 5000  current step :  691
total : 5000  current step :  692
total : 5000  current step :  693
total : 5000  current step :  694
total : 5000  current step :  695
total : 5000  current step :  696
total : 5000  current step :  697
total : 5000  current step :  698
total : 5000  current step :  699
total : 5000  current step :  700
total : 5000  current step :  701
total : 5000  current step :  702
total : 5000  current step :  703
total : 5000  current step :  704
total : 5000  current step :  705
total : 5000  current step :  706
total : 5000  current step :  707
total : 5000  current step :  708
total : 5000  current step :  709
total : 5000  current step :  710
total : 5000  current step :  711
total : 5000  current step :  712
total : 5000  current step :  713
total : 5000  current step :  714
total : 5000  current step :  715
total : 5000  current step :  716
total : 5000  current step :  717
total : 5000  current step :  718
total : 5000  current step :  719
total : 5000  current step :  720
total : 5000  current step :  721
total : 5000  current step :  722
total : 5000  current step :  723
total : 5000  current step :  724
total : 5000  current step :  725
total : 5000  current step :  726
total : 5000  current step :  727
total : 5000  current step :  728
total : 5000  current step :  729
total : 5000  current step :  730
total : 5000  current step :  731
total : 5000  current step :  732
total : 5000  current step :  733
total : 5000  current step :  734
total : 5000  current step :  735
total : 5000  current step :  736
total : 5000  current step :  737
total : 5000  current step :  738
total : 5000  current step :  739
total : 5000  current step :  740
total : 5000  current step :  741
total : 5000  current step :  742
total : 5000  current step :  743
total : 5000  current step :  744
total : 5000  current step :  745
total : 5000  current step :  746
total : 5000  current step :  747
total : 5000  current step :  748
total : 5000  current step :  749
total : 5000  current step :  750
total : 5000  current step :  751
total : 5000  current step :  752
total : 5000  current step :  753
total : 5000  current step :  754
total : 5000  current step :  755
total : 5000  current step :  756
total : 5000  current step :  757
total : 5000  current step :  758
total : 5000  current step :  759
total : 5000  current step :  760
total : 5000  current step :  761
total : 5000  current step :  762
total : 5000  current step :  763
total : 5000  current step :  764
total : 5000  current step :  765
total : 5000  current step :  766
total : 5000  current step :  767
total : 5000  current step :  768
total : 5000  current step :  769
total : 5000  current step :  770
total : 5000  current step :  771
total : 5000  current step :  772
total : 5000  current step :  773
total : 5000  current step :  774
total : 5000  current step :  775
total : 5000  current step :  776
total : 5000  current step :  777
total : 5000  current step :  778
total : 5000  current step :  779
total : 5000  current step :  780
total : 5000  current step :  781
total : 5000  current step :  782
total : 5000  current step :  783
total : 5000  current step :  784
total : 5000  current step :  785
total : 5000  current step :  786
total : 5000  current step :  787
total : 5000  current step :  788
total : 5000  current step :  789
total : 5000  current step :  790
total : 5000  current step :  791
total : 5000  current step :  792
total : 5000  current step :  793
total : 5000  current step :  794
total : 5000  current step :  795
total : 5000  current step :  796
total : 5000  current step :  797
total : 5000  current step :  798
total : 5000  current step :  799
total : 5000  current step :  800
total : 5000  current step :  801
total : 5000  current step :  802
total : 5000  current step :  803
total : 5000  current step :  804
total : 5000  current step :  805
total : 5000  current step :  806
total : 5000  current step :  807
total : 5000  current step :  808
total : 5000  current step :  809
total : 5000  current step :  810
total : 5000  current step :  811
total : 5000  current step :  812
total : 5000  current step :  813
total : 5000  current step :  814
total : 5000  current step :  815
total : 5000  current step :  816
total : 5000  current step :  817
total : 5000  current step :  818
total : 5000  current step :  819
total : 5000  current step :  820
total : 5000  current step :  821
total : 5000  current step :  822
total : 5000  current step :  823
total : 5000  current step :  824
total : 5000  current step :  825
total : 5000  current step :  826
total : 5000  current step :  827
total : 5000  current step :  828
total : 5000  current step :  829
total : 5000  current step :  830
total : 5000  current step :  831
total : 5000  current step :  832
total : 5000  current step :  833
total : 5000  current step :  834
total : 5000  current step :  835
total : 5000  current step :  836
total : 5000  current step :  837
total : 5000  current step :  838
total : 5000  current step :  839
total : 5000  current step :  840
total : 5000  current step :  841
total : 5000  current step :  842
total : 5000  current step :  843
total : 5000  current step :  844
total : 5000  current step :  845
total : 5000  current step :  846
total : 5000  current step :  847
total : 5000  current step :  848
total : 5000  current step :  849
total : 5000  current step :  850
total : 5000  current step :  851
total : 5000  current step :  852
total : 5000  current step :  853
total : 5000  current step :  854
total : 5000  current step :  855
total : 5000  current step :  856
total : 5000  current step :  857
total : 5000  current step :  858
total : 5000  current step :  859
total : 5000  current step :  860
total : 5000  current step :  861
total : 5000  current step :  862
total : 5000  current step :  863
total : 5000  current step :  864
total : 5000  current step :  865
total : 5000  current step :  866
total : 5000  current step :  867
total : 5000  current step :  868
total : 5000  current step :  869
total : 5000  current step :  870
total : 5000  current step :  871
total : 5000  current step :  872
total : 5000  current step :  873
total : 5000  current step :  874
total : 5000  current step :  875
total : 5000  current step :  876
total : 5000  current step :  877
total : 5000  current step :  878
total : 5000  current step :  879
total : 5000  current step :  880
total : 5000  current step :  881
total : 5000  current step :  882
total : 5000  current step :  883
total : 5000  current step :  884
total : 5000  current step :  885
total : 5000  current step :  886
total : 5000  current step :  887
total : 5000  current step :  888
total : 5000  current step :  889
total : 5000  current step :  890
total : 5000  current step :  891
total : 5000  current step :  892
total : 5000  current step :  893
total : 5000  current step :  894
total : 5000  current step :  895
total : 5000  current step :  896
total : 5000  current step :  897
total : 5000  current step :  898
total : 5000  current step :  899
total : 5000  current step :  900
total : 5000  current step :  901
total : 5000  current step :  902
total : 5000  current step :  903
total : 5000  current step :  904
total : 5000  current step :  905
total : 5000  current step :  906
total : 5000  current step :  907
total : 5000  current step :  908
total : 5000  current step :  909
total : 5000  current step :  910
total : 5000  current step :  911
total : 5000  current step :  912
total : 5000  current step :  913
total : 5000  current step :  914
total : 5000  current step :  915
total : 5000  current step :  916
total : 5000  current step :  917
total : 5000  current step :  918
total : 5000  current step :  919
total : 5000  current step :  920
total : 5000  current step :  921
total : 5000  current step :  922
total : 5000  current step :  923
total : 5000  current step :  924
total : 5000  current step :  925
total : 5000  current step :  926
total : 5000  current step :  927
total : 5000  current step :  928
total : 5000  current step :  929
total : 5000  current step :  930
total : 5000  current step :  931
total : 5000  current step :  932
total : 5000  current step :  933
total : 5000  current step :  934
total : 5000  current step :  935
total : 5000  current step :  936
total : 5000  current step :  937
total : 5000  current step :  938
total : 5000  current step :  939
total : 5000  current step :  940
total : 5000  current step :  941
total : 5000  current step :  942
total : 5000  current step :  943
total : 5000  current step :  944
total : 5000  current step :  945
total : 5000  current step :  946
total : 5000  current step :  947
total : 5000  current step :  948
total : 5000  current step :  949
total : 5000  current step :  950
total : 5000  current step :  951
total : 5000  current step :  952
total : 5000  current step :  953
total : 5000  current step :  954
total : 5000  current step :  955
total : 5000  current step :  956
total : 5000  current step :  957
total : 5000  current step :  958
total : 5000  current step :  959
total : 5000  current step :  960
total : 5000  current step :  961
total : 5000  current step :  962
total : 5000  current step :  963
total : 5000  current step :  964
total : 5000  current step :  965
total : 5000  current step :  966
total : 5000  current step :  967
total : 5000  current step :  968
total : 5000  current step :  969
total : 5000  current step :  970
total : 5000  current step :  971
total : 5000  current step :  972
total : 5000  current step :  973
total : 5000  current step :  974
total : 5000  current step :  975
total : 5000  current step :  976
total : 5000  current step :  977
total : 5000  current step :  978
total : 5000  current step :  979
total : 5000  current step :  980
total : 5000  current step :  981
total : 5000  current step :  982
total : 5000  current step :  983
total : 5000  current step :  984
total : 5000  current step :  985
total : 5000  current step :  986
total : 5000  current step :  987
total : 5000  current step :  988
total : 5000  current step :  989
total : 5000  current step :  990
total : 5000  current step :  991
total : 5000  current step :  992
total : 5000  current step :  993
total : 5000  current step :  994
total : 5000  current step :  995
total : 5000  current step :  996
total : 5000  current step :  997
total : 5000  current step :  998
total : 5000  current step :  999
total : 5000  current step :  1000
total : 5000  current step :  1001
total : 5000  current step :  1002
total : 5000  current step :  1003
total : 5000  current step :  1004
total : 5000  current step :  1005
total : 5000  current step :  1006
total : 5000  current step :  1007
total : 5000  current step :  1008
total : 5000  current step :  1009
total : 5000  current step :  1010
total : 5000  current step :  1011
total : 5000  current step :  1012
total : 5000  current step :  1013
total : 5000  current step :  1014
total : 5000  current step :  1015
total : 5000  current step :  1016
total : 5000  current step :  1017
total : 5000  current step :  1018
total : 5000  current step :  1019
total : 5000  current step :  1020
total : 5000  current step :  1021
total : 5000  current step :  1022
total : 5000  current step :  1023
total : 5000  current step :  1024
total : 5000  current step :  1025
total : 5000  current step :  1026
total : 5000  current step :  1027
total : 5000  current step :  1028
total : 5000  current step :  1029
total : 5000  current step :  1030
total : 5000  current step :  1031
total : 5000  current step :  1032
total : 5000  current step :  1033
total : 5000  current step :  1034
total : 5000  current step :  1035
total : 5000  current step :  1036
total : 5000  current step :  1037
total : 5000  current step :  1038
total : 5000  current step :  1039
total : 5000  current step :  1040
total : 5000  current step :  1041
total : 5000  current step :  1042
total : 5000  current step :  1043
total : 5000  current step :  1044
total : 5000  current step :  1045
total : 5000  current step :  1046
total : 5000  current step :  1047
total : 5000  current step :  1048
total : 5000  current step :  1049
total : 5000  current step :  1050
total : 5000  current step :  1051
total : 5000  current step :  1052
total : 5000  current step :  1053
total : 5000  current step :  1054
total : 5000  current step :  1055
total : 5000  current step :  1056
total : 5000  current step :  1057
total : 5000  current step :  1058
total : 5000  current step :  1059
total : 5000  current step :  1060
total : 5000  current step :  1061
total : 5000  current step :  1062
total : 5000  current step :  1063
total : 5000  current step :  1064
total : 5000  current step :  1065
total : 5000  current step :  1066
total : 5000  current step :  1067
total : 5000  current step :  1068
total : 5000  current step :  1069
total : 5000  current step :  1070
total : 5000  current step :  1071
total : 5000  current step :  1072
total : 5000  current step :  1073
total : 5000  current step :  1074
total : 5000  current step :  1075
total : 5000  current step :  1076
total : 5000  current step :  1077
total : 5000  current step :  1078
total : 5000  current step :  1079
total : 5000  current step :  1080
total : 5000  current step :  1081
total : 5000  current step :  1082
total : 5000  current step :  1083
total : 5000  current step :  1084
total : 5000  current step :  1085
total : 5000  current step :  1086
total : 5000  current step :  1087
total : 5000  current step :  1088
total : 5000  current step :  1089
total : 5000  current step :  1090
total : 5000  current step :  1091
total : 5000  current step :  1092
total : 5000  current step :  1093
total : 5000  current step :  1094
total : 5000  current step :  1095
total : 5000  current step :  1096
total : 5000  current step :  1097
total : 5000  current step :  1098
total : 5000  current step :  1099
total : 5000  current step :  1100
total : 5000  current step :  1101
total : 5000  current step :  1102
total : 5000  current step :  1103
total : 5000  current step :  1104
total : 5000  current step :  1105
total : 5000  current step :  1106
total : 5000  current step :  1107
total : 5000  current step :  1108
total : 5000  current step :  1109
total : 5000  current step :  1110
total : 5000  current step :  1111
total : 5000  current step :  1112
total : 5000  current step :  1113
total : 5000  current step :  1114
total : 5000  current step :  1115
total : 5000  current step :  1116
total : 5000  current step :  1117
total : 5000  current step :  1118
total : 5000  current step :  1119
total : 5000  current step :  1120
total : 5000  current step :  1121
total : 5000  current step :  1122
total : 5000  current step :  1123
total : 5000  current step :  1124
total : 5000  current step :  1125
total : 5000  current step :  1126
total : 5000  current step :  1127
total : 5000  current step :  1128
total : 5000  current step :  1129
total : 5000  current step :  1130
total : 5000  current step :  1131
total : 5000  current step :  1132
total : 5000  current step :  1133
total : 5000  current step :  1134
total : 5000  current step :  1135
total : 5000  current step :  1136
total : 5000  current step :  1137
total : 5000  current step :  1138
total : 5000  current step :  1139
total : 5000  current step :  1140
total : 5000  current step :  1141
total : 5000  current step :  1142
total : 5000  current step :  1143
total : 5000  current step :  1144
total : 5000  current step :  1145
total : 5000  current step :  1146
total : 5000  current step :  1147
total : 5000  current step :  1148
total : 5000  current step :  1149
total : 5000  current step :  1150
total : 5000  current step :  1151
total : 5000  current step :  1152
total : 5000  current step :  1153
total : 5000  current step :  1154
total : 5000  current step :  1155
total : 5000  current step :  1156
total : 5000  current step :  1157
total : 5000  current step :  1158
total : 5000  current step :  1159
total : 5000  current step :  1160
total : 5000  current step :  1161
total : 5000  current step :  1162
total : 5000  current step :  1163
total : 5000  current step :  1164
total : 5000  current step :  1165
total : 5000  current step :  1166
total : 5000  current step :  1167
total : 5000  current step :  1168
total : 5000  current step :  1169
total : 5000  current step :  1170
total : 5000  current step :  1171
total : 5000  current step :  1172
total : 5000  current step :  1173
total : 5000  current step :  1174
total : 5000  current step :  1175
total : 5000  current step :  1176
total : 5000  current step :  1177
total : 5000  current step :  1178
total : 5000  current step :  1179
total : 5000  current step :  1180
total : 5000  current step :  1181
total : 5000  current step :  1182
total : 5000  current step :  1183
total : 5000  current step :  1184
total : 5000  current step :  1185
total : 5000  current step :  1186
total : 5000  current step :  1187
total : 5000  current step :  1188
total : 5000  current step :  1189
total : 5000  current step :  1190
total : 5000  current step :  1191
total : 5000  current step :  1192
total : 5000  current step :  1193
total : 5000  current step :  1194
total : 5000  current step :  1195
total : 5000  current step :  1196
total : 5000  current step :  1197
total : 5000  current step :  1198
total : 5000  current step :  1199
total : 5000  current step :  1200
total : 5000  current step :  1201
total : 5000  current step :  1202
total : 5000  current step :  1203
total : 5000  current step :  1204
total : 5000  current step :  1205
total : 5000  current step :  1206
total : 5000  current step :  1207
total : 5000  current step :  1208
total : 5000  current step :  1209
total : 5000  current step :  1210
total : 5000  current step :  1211
total : 5000  current step :  1212
total : 5000  current step :  1213
total : 5000  current step :  1214
total : 5000  current step :  1215
total : 5000  current step :  1216
total : 5000  current step :  1217
total : 5000  current step :  1218
total : 5000  current step :  1219
total : 5000  current step :  1220
total : 5000  current step :  1221
total : 5000  current step :  1222
total : 5000  current step :  1223
total : 5000  current step :  1224
total : 5000  current step :  1225
total : 5000  current step :  1226
total : 5000  current step :  1227
total : 5000  current step :  1228
total : 5000  current step :  1229
total : 5000  current step :  1230
total : 5000  current step :  1231
total : 5000  current step :  1232
total : 5000  current step :  1233
total : 5000  current step :  1234
total : 5000  current step :  1235
total : 5000  current step :  1236
total : 5000  current step :  1237
total : 5000  current step :  1238
total : 5000  current step :  1239
total : 5000  current step :  1240
total : 5000  current step :  1241
total : 5000  current step :  1242
total : 5000  current step :  1243
total : 5000  current step :  1244
total : 5000  current step :  1245
total : 5000  current step :  1246
total : 5000  current step :  1247
total : 5000  current step :  1248
total : 5000  current step :  1249
total : 5000  current step :  1250
total : 5000  current step :  1251
total : 5000  current step :  1252
total : 5000  current step :  1253
total : 5000  current step :  1254
total : 5000  current step :  1255
total : 5000  current step :  1256
total : 5000  current step :  1257
total : 5000  current step :  1258
total : 5000  current step :  1259
total : 5000  current step :  1260
total : 5000  current step :  1261
total : 5000  current step :  1262
total : 5000  current step :  1263
total : 5000  current step :  1264
total : 5000  current step :  1265
total : 5000  current step :  1266
total : 5000  current step :  1267
total : 5000  current step :  1268
total : 5000  current step :  1269
total : 5000  current step :  1270
total : 5000  current step :  1271
total : 5000  current step :  1272
total : 5000  current step :  1273
total : 5000  current step :  1274
total : 5000  current step :  1275
total : 5000  current step :  1276
total : 5000  current step :  1277
total : 5000  current step :  1278
total : 5000  current step :  1279
total : 5000  current step :  1280
total : 5000  current step :  1281
total : 5000  current step :  1282
total : 5000  current step :  1283
total : 5000  current step :  1284
total : 5000  current step :  1285
total : 5000  current step :  1286
total : 5000  current step :  1287
total : 5000  current step :  1288
total : 5000  current step :  1289
total : 5000  current step :  1290
total : 5000  current step :  1291
total : 5000  current step :  1292
total : 5000  current step :  1293
total : 5000  current step :  1294
total : 5000  current step :  1295
total : 5000  current step :  1296
total : 5000  current step :  1297
total : 5000  current step :  1298
total : 5000  current step :  1299
total : 5000  current step :  1300
total : 5000  current step :  1301
total : 5000  current step :  1302
total : 5000  current step :  1303
total : 5000  current step :  1304
total : 5000  current step :  1305
total : 5000  current step :  1306
total : 5000  current step :  1307
total : 5000  current step :  1308
total : 5000  current step :  1309
total : 5000  current step :  1310
total : 5000  current step :  1311
total : 5000  current step :  1312
total : 5000  current step :  1313
total : 5000  current step :  1314
total : 5000  current step :  1315
total : 5000  current step :  1316
total : 5000  current step :  1317
total : 5000  current step :  1318
total : 5000  current step :  1319
total : 5000  current step :  1320
total : 5000  current step :  1321
total : 5000  current step :  1322
total : 5000  current step :  1323
total : 5000  current step :  1324
total : 5000  current step :  1325
total : 5000  current step :  1326
total : 5000  current step :  1327
total : 5000  current step :  1328
total : 5000  current step :  1329
total : 5000  current step :  1330
total : 5000  current step :  1331
total : 5000  current step :  1332
total : 5000  current step :  1333
total : 5000  current step :  1334
total : 5000  current step :  1335
total : 5000  current step :  1336
total : 5000  current step :  1337
total : 5000  current step :  1338
total : 5000  current step :  1339
total : 5000  current step :  1340
total : 5000  current step :  1341
total : 5000  current step :  1342
total : 5000  current step :  1343
total : 5000  current step :  1344
total : 5000  current step :  1345
total : 5000  current step :  1346
total : 5000  current step :  1347
total : 5000  current step :  1348
total : 5000  current step :  1349
total : 5000  current step :  1350
total : 5000  current step :  1351
total : 5000  current step :  1352
total : 5000  current step :  1353
total : 5000  current step :  1354
total : 5000  current step :  1355
total : 5000  current step :  1356
total : 5000  current step :  1357
total : 5000  current step :  1358
total : 5000  current step :  1359
total : 5000  current step :  1360
total : 5000  current step :  1361
total : 5000  current step :  1362
total : 5000  current step :  1363
total : 5000  current step :  1364
total : 5000  current step :  1365
total : 5000  current step :  1366
total : 5000  current step :  1367
total : 5000  current step :  1368
total : 5000  current step :  1369
total : 5000  current step :  1370
total : 5000  current step :  1371
total : 5000  current step :  1372
total : 5000  current step :  1373
total : 5000  current step :  1374
total : 5000  current step :  1375
total : 5000  current step :  1376
total : 5000  current step :  1377
total : 5000  current step :  1378
total : 5000  current step :  1379
total : 5000  current step :  1380
total : 5000  current step :  1381
total : 5000  current step :  1382
total : 5000  current step :  1383
total : 5000  current step :  1384
total : 5000  current step :  1385
total : 5000  current step :  1386
total : 5000  current step :  1387
total : 5000  current step :  1388
total : 5000  current step :  1389
total : 5000  current step :  1390
total : 5000  current step :  1391
total : 5000  current step :  1392
total : 5000  current step :  1393
total : 5000  current step :  1394
total : 5000  current step :  1395
total : 5000  current step :  1396
total : 5000  current step :  1397
total : 5000  current step :  1398
total : 5000  current step :  1399
total : 5000  current step :  1400
total : 5000  current step :  1401
total : 5000  current step :  1402
total : 5000  current step :  1403
total : 5000  current step :  1404
total : 5000  current step :  1405
total : 5000  current step :  1406
total : 5000  current step :  1407
total : 5000  current step :  1408
total : 5000  current step :  1409
total : 5000  current step :  1410
total : 5000  current step :  1411
total : 5000  current step :  1412
total : 5000  current step :  1413
total : 5000  current step :  1414
total : 5000  current step :  1415
total : 5000  current step :  1416
total : 5000  current step :  1417
total : 5000  current step :  1418
total : 5000  current step :  1419
total : 5000  current step :  1420
total : 5000  current step :  1421
total : 5000  current step :  1422
total : 5000  current step :  1423
total : 5000  current step :  1424
total : 5000  current step :  1425
total : 5000  current step :  1426
total : 5000  current step :  1427
total : 5000  current step :  1428
total : 5000  current step :  1429
total : 5000  current step :  1430
total : 5000  current step :  1431
total : 5000  current step :  1432
total : 5000  current step :  1433
total : 5000  current step :  1434
total : 5000  current step :  1435
total : 5000  current step :  1436
total : 5000  current step :  1437
total : 5000  current step :  1438
total : 5000  current step :  1439
total : 5000  current step :  1440
total : 5000  current step :  1441
total : 5000  current step :  1442
total : 5000  current step :  1443
total : 5000  current step :  1444
total : 5000  current step :  1445
total : 5000  current step :  1446
total : 5000  current step :  1447
total : 5000  current step :  1448
total : 5000  current step :  1449
total : 5000  current step :  1450
total : 5000  current step :  1451
total : 5000  current step :  1452
total : 5000  current step :  1453
total : 5000  current step :  1454
total : 5000  current step :  1455
total : 5000  current step :  1456
total : 5000  current step :  1457
total : 5000  current step :  1458
total : 5000  current step :  1459
total : 5000  current step :  1460
total : 5000  current step :  1461
total : 5000  current step :  1462
total : 5000  current step :  1463
total : 5000  current step :  1464
total : 5000  current step :  1465
total : 5000  current step :  1466
total : 5000  current step :  1467
total : 5000  current step :  1468
total : 5000  current step :  1469
total : 5000  current step :  1470
total : 5000  current step :  1471
total : 5000  current step :  1472
total : 5000  current step :  1473
total : 5000  current step :  1474
total : 5000  current step :  1475
total : 5000  current step :  1476
total : 5000  current step :  1477
total : 5000  current step :  1478
total : 5000  current step :  1479
total : 5000  current step :  1480
total : 5000  current step :  1481
total : 5000  current step :  1482
total : 5000  current step :  1483
total : 5000  current step :  1484
total : 5000  current step :  1485
total : 5000  current step :  1486
total : 5000  current step :  1487
total : 5000  current step :  1488
total : 5000  current step :  1489
total : 5000  current step :  1490
total : 5000  current step :  1491
total : 5000  current step :  1492
total : 5000  current step :  1493
total : 5000  current step :  1494
total : 5000  current step :  1495
total : 5000  current step :  1496
total : 5000  current step :  1497
total : 5000  current step :  1498
total : 5000  current step :  1499
total : 5000  current step :  1500
total : 5000  current step :  1501
total : 5000  current step :  1502
total : 5000  current step :  1503
total : 5000  current step :  1504
total : 5000  current step :  1505
total : 5000  current step :  1506
total : 5000  current step :  1507
total : 5000  current step :  1508
total : 5000  current step :  1509
total : 5000  current step :  1510
total : 5000  current step :  1511
total : 5000  current step :  1512
total : 5000  current step :  1513
total : 5000  current step :  1514
total : 5000  current step :  1515
total : 5000  current step :  1516
total : 5000  current step :  1517
total : 5000  current step :  1518
total : 5000  current step :  1519
total : 5000  current step :  1520
total : 5000  current step :  1521
total : 5000  current step :  1522
total : 5000  current step :  1523
total : 5000  current step :  1524
total : 5000  current step :  1525
total : 5000  current step :  1526
total : 5000  current step :  1527
total : 5000  current step :  1528
total : 5000  current step :  1529
total : 5000  current step :  1530
total : 5000  current step :  1531
total : 5000  current step :  1532
total : 5000  current step :  1533
total : 5000  current step :  1534
total : 5000  current step :  1535
total : 5000  current step :  1536
total : 5000  current step :  1537
total : 5000  current step :  1538
total : 5000  current step :  1539
total : 5000  current step :  1540
total : 5000  current step :  1541
total : 5000  current step :  1542
total : 5000  current step :  1543
total : 5000  current step :  1544
total : 5000  current step :  1545
total : 5000  current step :  1546
total : 5000  current step :  1547
total : 5000  current step :  1548
total : 5000  current step :  1549
total : 5000  current step :  1550
total : 5000  current step :  1551
total : 5000  current step :  1552
total : 5000  current step :  1553
total : 5000  current step :  1554
total : 5000  current step :  1555
total : 5000  current step :  1556
total : 5000  current step :  1557
total : 5000  current step :  1558
total : 5000  current step :  1559
total : 5000  current step :  1560
total : 5000  current step :  1561
total : 5000  current step :  1562
total : 5000  current step :  1563
total : 5000  current step :  1564
total : 5000  current step :  1565
total : 5000  current step :  1566
total : 5000  current step :  1567
total : 5000  current step :  1568
total : 5000  current step :  1569
total : 5000  current step :  1570
total : 5000  current step :  1571
total : 5000  current step :  1572
total : 5000  current step :  1573
total : 5000  current step :  1574
total : 5000  current step :  1575
total : 5000  current step :  1576
total : 5000  current step :  1577
total : 5000  current step :  1578
total : 5000  current step :  1579
total : 5000  current step :  1580
total : 5000  current step :  1581
total : 5000  current step :  1582
total : 5000  current step :  1583
total : 5000  current step :  1584
total : 5000  current step :  1585
total : 5000  current step :  1586
total : 5000  current step :  1587
total : 5000  current step :  1588
total : 5000  current step :  1589
total : 5000  current step :  1590
total : 5000  current step :  1591
total : 5000  current step :  1592
total : 5000  current step :  1593
total : 5000  current step :  1594
total : 5000  current step :  1595
total : 5000  current step :  1596
total : 5000  current step :  1597
total : 5000  current step :  1598
total : 5000  current step :  1599
total : 5000  current step :  1600
total : 5000  current step :  1601
total : 5000  current step :  1602
total : 5000  current step :  1603
total : 5000  current step :  1604
total : 5000  current step :  1605
total : 5000  current step :  1606
total : 5000  current step :  1607
total : 5000  current step :  1608
total : 5000  current step :  1609
total : 5000  current step :  1610
total : 5000  current step :  1611
total : 5000  current step :  1612
total : 5000  current step :  1613
total : 5000  current step :  1614
total : 5000  current step :  1615
total : 5000  current step :  1616
total : 5000  current step :  1617
total : 5000  current step :  1618
total : 5000  current step :  1619
total : 5000  current step :  1620
total : 5000  current step :  1621
total : 5000  current step :  1622
total : 5000  current step :  1623
total : 5000  current step :  1624
total : 5000  current step :  1625
total : 5000  current step :  1626
total : 5000  current step :  1627
total : 5000  current step :  1628
total : 5000  current step :  1629
total : 5000  current step :  1630
total : 5000  current step :  1631
total : 5000  current step :  1632
total : 5000  current step :  1633
total : 5000  current step :  1634
total : 5000  current step :  1635
total : 5000  current step :  1636
total : 5000  current step :  1637
total : 5000  current step :  1638
total : 5000  current step :  1639
total : 5000  current step :  1640
total : 5000  current step :  1641
total : 5000  current step :  1642
total : 5000  current step :  1643
total : 5000  current step :  1644
total : 5000  current step :  1645
total : 5000  current step :  1646
total : 5000  current step :  1647
total : 5000  current step :  1648
total : 5000  current step :  1649
total : 5000  current step :  1650
total : 5000  current step :  1651
total : 5000  current step :  1652
total : 5000  current step :  1653
total : 5000  current step :  1654
total : 5000  current step :  1655
total : 5000  current step :  1656
total : 5000  current step :  1657
total : 5000  current step :  1658
total : 5000  current step :  1659
total : 5000  current step :  1660
total : 5000  current step :  1661
total : 5000  current step :  1662
total : 5000  current step :  1663
total : 5000  current step :  1664
total : 5000  current step :  1665
total : 5000  current step :  1666
total : 5000  current step :  1667
total : 5000  current step :  1668
total : 5000  current step :  1669
total : 5000  current step :  1670
total : 5000  current step :  1671
total : 5000  current step :  1672
total : 5000  current step :  1673
total : 5000  current step :  1674
total : 5000  current step :  1675
total : 5000  current step :  1676
total : 5000  current step :  1677
total : 5000  current step :  1678
total : 5000  current step :  1679
total : 5000  current step :  1680
total : 5000  current step :  1681
total : 5000  current step :  1682
total : 5000  current step :  1683
total : 5000  current step :  1684
total : 5000  current step :  1685
total : 5000  current step :  1686
total : 5000  current step :  1687
total : 5000  current step :  1688
total : 5000  current step :  1689
total : 5000  current step :  1690
total : 5000  current step :  1691
total : 5000  current step :  1692
total : 5000  current step :  1693
total : 5000  current step :  1694
total : 5000  current step :  1695
total : 5000  current step :  1696
total : 5000  current step :  1697
total : 5000  current step :  1698
total : 5000  current step :  1699
total : 5000  current step :  1700
total : 5000  current step :  1701
total : 5000  current step :  1702
total : 5000  current step :  1703
total : 5000  current step :  1704
total : 5000  current step :  1705
total : 5000  current step :  1706
total : 5000  current step :  1707
total : 5000  current step :  1708
total : 5000  current step :  1709
total : 5000  current step :  1710
total : 5000  current step :  1711
total : 5000  current step :  1712
total : 5000  current step :  1713
total : 5000  current step :  1714
total : 5000  current step :  1715
total : 5000  current step :  1716
total : 5000  current step :  1717
total : 5000  current step :  1718
total : 5000  current step :  1719
total : 5000  current step :  1720
total : 5000  current step :  1721
total : 5000  current step :  1722
total : 5000  current step :  1723
total : 5000  current step :  1724
total : 5000  current step :  1725
total : 5000  current step :  1726
total : 5000  current step :  1727
total : 5000  current step :  1728
total : 5000  current step :  1729
total : 5000  current step :  1730
total : 5000  current step :  1731
total : 5000  current step :  1732
total : 5000  current step :  1733
total : 5000  current step :  1734
total : 5000  current step :  1735
total : 5000  current step :  1736
total : 5000  current step :  1737
total : 5000  current step :  1738
total : 5000  current step :  1739
total : 5000  current step :  1740
total : 5000  current step :  1741
total : 5000  current step :  1742
total : 5000  current step :  1743
total : 5000  current step :  1744
total : 5000  current step :  1745
total : 5000  current step :  1746
total : 5000  current step :  1747
total : 5000  current step :  1748
total : 5000  current step :  1749
total : 5000  current step :  1750
total : 5000  current step :  1751
total : 5000  current step :  1752
total : 5000  current step :  1753
total : 5000  current step :  1754
total : 5000  current step :  1755
total : 5000  current step :  1756
total : 5000  current step :  1757
total : 5000  current step :  1758
total : 5000  current step :  1759
total : 5000  current step :  1760
total : 5000  current step :  1761
total : 5000  current step :  1762
total : 5000  current step :  1763
total : 5000  current step :  1764
total : 5000  current step :  1765
total : 5000  current step :  1766
total : 5000  current step :  1767
total : 5000  current step :  1768
total : 5000  current step :  1769
total : 5000  current step :  1770
total : 5000  current step :  1771
total : 5000  current step :  1772
total : 5000  current step :  1773
total : 5000  current step :  1774
total : 5000  current step :  1775
total : 5000  current step :  1776
total : 5000  current step :  1777
total : 5000  current step :  1778
total : 5000  current step :  1779
total : 5000  current step :  1780
total : 5000  current step :  1781
total : 5000  current step :  1782
total : 5000  current step :  1783
total : 5000  current step :  1784
total : 5000  current step :  1785
total : 5000  current step :  1786
total : 5000  current step :  1787
total : 5000  current step :  1788
total : 5000  current step :  1789
total : 5000  current step :  1790
total : 5000  current step :  1791
total : 5000  current step :  1792
total : 5000  current step :  1793
total : 5000  current step :  1794
total : 5000  current step :  1795
total : 5000  current step :  1796
total : 5000  current step :  1797
total : 5000  current step :  1798
total : 5000  current step :  1799
total : 5000  current step :  1800
total : 5000  current step :  1801
total : 5000  current step :  1802
total : 5000  current step :  1803
total : 5000  current step :  1804
total : 5000  current step :  1805
total : 5000  current step :  1806
total : 5000  current step :  1807
total : 5000  current step :  1808
total : 5000  current step :  1809
total : 5000  current step :  1810
total : 5000  current step :  1811
total : 5000  current step :  1812
total : 5000  current step :  1813
total : 5000  current step :  1814
total : 5000  current step :  1815
total : 5000  current step :  1816
total : 5000  current step :  1817
total : 5000  current step :  1818
total : 5000  current step :  1819
total : 5000  current step :  1820
total : 5000  current step :  1821
total : 5000  current step :  1822
total : 5000  current step :  1823
total : 5000  current step :  1824
total : 5000  current step :  1825
total : 5000  current step :  1826
total : 5000  current step :  1827
total : 5000  current step :  1828
total : 5000  current step :  1829
total : 5000  current step :  1830
total : 5000  current step :  1831
total : 5000  current step :  1832
total : 5000  current step :  1833
total : 5000  current step :  1834
total : 5000  current step :  1835
total : 5000  current step :  1836
total : 5000  current step :  1837
total : 5000  current step :  1838
total : 5000  current step :  1839
total : 5000  current step :  1840
total : 5000  current step :  1841
total : 5000  current step :  1842
total : 5000  current step :  1843
total : 5000  current step :  1844
total : 5000  current step :  1845
total : 5000  current step :  1846
total : 5000  current step :  1847
total : 5000  current step :  1848
total : 5000  current step :  1849
total : 5000  current step :  1850
total : 5000  current step :  1851
total : 5000  current step :  1852
total : 5000  current step :  1853
total : 5000  current step :  1854
total : 5000  current step :  1855
total : 5000  current step :  1856
total : 5000  current step :  1857
total : 5000  current step :  1858
total : 5000  current step :  1859
total : 5000  current step :  1860
total : 5000  current step :  1861
total : 5000  current step :  1862
total : 5000  current step :  1863
total : 5000  current step :  1864
total : 5000  current step :  1865
total : 5000  current step :  1866
total : 5000  current step :  1867
total : 5000  current step :  1868
total : 5000  current step :  1869
total : 5000  current step :  1870
total : 5000  current step :  1871
total : 5000  current step :  1872
total : 5000  current step :  1873
total : 5000  current step :  1874
total : 5000  current step :  1875
total : 5000  current step :  1876
total : 5000  current step :  1877
total : 5000  current step :  1878
total : 5000  current step :  1879
total : 5000  current step :  1880
total : 5000  current step :  1881
total : 5000  current step :  1882
total : 5000  current step :  1883
total : 5000  current step :  1884
total : 5000  current step :  1885
total : 5000  current step :  1886
total : 5000  current step :  1887
total : 5000  current step :  1888
total : 5000  current step :  1889
total : 5000  current step :  1890
total : 5000  current step :  1891
total : 5000  current step :  1892
total : 5000  current step :  1893
total : 5000  current step :  1894
total : 5000  current step :  1895
total : 5000  current step :  1896
total : 5000  current step :  1897
total : 5000  current step :  1898
total : 5000  current step :  1899
total : 5000  current step :  1900
total : 5000  current step :  1901
total : 5000  current step :  1902
total : 5000  current step :  1903
total : 5000  current step :  1904
total : 5000  current step :  1905
total : 5000  current step :  1906
total : 5000  current step :  1907
total : 5000  current step :  1908
total : 5000  current step :  1909
total : 5000  current step :  1910
total : 5000  current step :  1911
total : 5000  current step :  1912
total : 5000  current step :  1913
total : 5000  current step :  1914
total : 5000  current step :  1915
total : 5000  current step :  1916
total : 5000  current step :  1917
total : 5000  current step :  1918
total : 5000  current step :  1919
total : 5000  current step :  1920
total : 5000  current step :  1921
total : 5000  current step :  1922
total : 5000  current step :  1923
total : 5000  current step :  1924
total : 5000  current step :  1925
total : 5000  current step :  1926
total : 5000  current step :  1927
total : 5000  current step :  1928
total : 5000  current step :  1929
total : 5000  current step :  1930
total : 5000  current step :  1931
total : 5000  current step :  1932
total : 5000  current step :  1933
total : 5000  current step :  1934
total : 5000  current step :  1935
total : 5000  current step :  1936
total : 5000  current step :  1937
total : 5000  current step :  1938
total : 5000  current step :  1939
total : 5000  current step :  1940
total : 5000  current step :  1941
total : 5000  current step :  1942
total : 5000  current step :  1943
total : 5000  current step :  1944
total : 5000  current step :  1945
total : 5000  current step :  1946
total : 5000  current step :  1947
total : 5000  current step :  1948
total : 5000  current step :  1949
total : 5000  current step :  1950
total : 5000  current step :  1951
total : 5000  current step :  1952
total : 5000  current step :  1953
total : 5000  current step :  1954
total : 5000  current step :  1955
total : 5000  current step :  1956
total : 5000  current step :  1957
total : 5000  current step :  1958
total : 5000  current step :  1959
total : 5000  current step :  1960
total : 5000  current step :  1961
total : 5000  current step :  1962
total : 5000  current step :  1963
total : 5000  current step :  1964
total : 5000  current step :  1965
total : 5000  current step :  1966
total : 5000  current step :  1967
total : 5000  current step :  1968
total : 5000  current step :  1969
total : 5000  current step :  1970
total : 5000  current step :  1971
total : 5000  current step :  1972
total : 5000  current step :  1973
total : 5000  current step :  1974
total : 5000  current step :  1975
total : 5000  current step :  1976
total : 5000  current step :  1977
total : 5000  current step :  1978
total : 5000  current step :  1979
total : 5000  current step :  1980
total : 5000  current step :  1981
total : 5000  current step :  1982
total : 5000  current step :  1983
total : 5000  current step :  1984
total : 5000  current step :  1985
total : 5000  current step :  1986
total : 5000  current step :  1987
total : 5000  current step :  1988
total : 5000  current step :  1989
total : 5000  current step :  1990
total : 5000  current step :  1991
total : 5000  current step :  1992
total : 5000  current step :  1993
total : 5000  current step :  1994
total : 5000  current step :  1995
total : 5000  current step :  1996
total : 5000  current step :  1997
total : 5000  current step :  1998
total : 5000  current step :  1999
total : 5000  current step :  2000
total : 5000  current step :  2001
total : 5000  current step :  2002
total : 5000  current step :  2003
total : 5000  current step :  2004
total : 5000  current step :  2005
total : 5000  current step :  2006
total : 5000  current step :  2007
total : 5000  current step :  2008
total : 5000  current step :  2009
total : 5000  current step :  2010
total : 5000  current step :  2011
total : 5000  current step :  2012
total : 5000  current step :  2013
total : 5000  current step :  2014
total : 5000  current step :  2015
total : 5000  current step :  2016
total : 5000  current step :  2017
total : 5000  current step :  2018
total : 5000  current step :  2019
total : 5000  current step :  2020
total : 5000  current step :  2021
total : 5000  current step :  2022
total : 5000  current step :  2023
total : 5000  current step :  2024
total : 5000  current step :  2025
total : 5000  current step :  2026
total : 5000  current step :  2027
total : 5000  current step :  2028
total : 5000  current step :  2029
total : 5000  current step :  2030
total : 5000  current step :  2031
total : 5000  current step :  2032
total : 5000  current step :  2033
total : 5000  current step :  2034
total : 5000  current step :  2035
total : 5000  current step :  2036
total : 5000  current step :  2037
total : 5000  current step :  2038
total : 5000  current step :  2039
total : 5000  current step :  2040
total : 5000  current step :  2041
total : 5000  current step :  2042
total : 5000  current step :  2043
total : 5000  current step :  2044
total : 5000  current step :  2045
total : 5000  current step :  2046
total : 5000  current step :  2047
total : 5000  current step :  2048
total : 5000  current step :  2049
total : 5000  current step :  2050
total : 5000  current step :  2051
total : 5000  current step :  2052
total : 5000  current step :  2053
total : 5000  current step :  2054
total : 5000  current step :  2055
total : 5000  current step :  2056
total : 5000  current step :  2057
total : 5000  current step :  2058
total : 5000  current step :  2059
total : 5000  current step :  2060
total : 5000  current step :  2061
total : 5000  current step :  2062
total : 5000  current step :  2063
total : 5000  current step :  2064
total : 5000  current step :  2065
total : 5000  current step :  2066
total : 5000  current step :  2067
total : 5000  current step :  2068
total : 5000  current step :  2069
total : 5000  current step :  2070
total : 5000  current step :  2071
total : 5000  current step :  2072
total : 5000  current step :  2073
total : 5000  current step :  2074
total : 5000  current step :  2075
total : 5000  current step :  2076
total : 5000  current step :  2077
total : 5000  current step :  2078
total : 5000  current step :  2079
total : 5000  current step :  2080
total : 5000  current step :  2081
total : 5000  current step :  2082
total : 5000  current step :  2083
total : 5000  current step :  2084
total : 5000  current step :  2085
total : 5000  current step :  2086
total : 5000  current step :  2087
total : 5000  current step :  2088
total : 5000  current step :  2089
total : 5000  current step :  2090
total : 5000  current step :  2091
total : 5000  current step :  2092
total : 5000  current step :  2093
total : 5000  current step :  2094
total : 5000  current step :  2095
total : 5000  current step :  2096
total : 5000  current step :  2097
total : 5000  current step :  2098
total : 5000  current step :  2099
total : 5000  current step :  2100
total : 5000  current step :  2101
total : 5000  current step :  2102
total : 5000  current step :  2103
total : 5000  current step :  2104
total : 5000  current step :  2105
total : 5000  current step :  2106
total : 5000  current step :  2107
total : 5000  current step :  2108
total : 5000  current step :  2109
total : 5000  current step :  2110
total : 5000  current step :  2111
total : 5000  current step :  2112
total : 5000  current step :  2113
total : 5000  current step :  2114
total : 5000  current step :  2115
total : 5000  current step :  2116
total : 5000  current step :  2117
total : 5000  current step :  2118
total : 5000  current step :  2119
total : 5000  current step :  2120
total : 5000  current step :  2121
total : 5000  current step :  2122
total : 5000  current step :  2123
total : 5000  current step :  2124
total : 5000  current step :  2125
total : 5000  current step :  2126
total : 5000  current step :  2127
total : 5000  current step :  2128
total : 5000  current step :  2129
total : 5000  current step :  2130
total : 5000  current step :  2131
total : 5000  current step :  2132
total : 5000  current step :  2133
total : 5000  current step :  2134
total : 5000  current step :  2135
total : 5000  current step :  2136
total : 5000  current step :  2137
total : 5000  current step :  2138
total : 5000  current step :  2139
total : 5000  current step :  2140
total : 5000  current step :  2141
total : 5000  current step :  2142
total : 5000  current step :  2143
total : 5000  current step :  2144
total : 5000  current step :  2145
total : 5000  current step :  2146
total : 5000  current step :  2147
total : 5000  current step :  2148
total : 5000  current step :  2149
total : 5000  current step :  2150
total : 5000  current step :  2151
total : 5000  current step :  2152
total : 5000  current step :  2153
total : 5000  current step :  2154
total : 5000  current step :  2155
total : 5000  current step :  2156
total : 5000  current step :  2157
total : 5000  current step :  2158
total : 5000  current step :  2159
total : 5000  current step :  2160
total : 5000  current step :  2161
total : 5000  current step :  2162
total : 5000  current step :  2163
total : 5000  current step :  2164
total : 5000  current step :  2165
total : 5000  current step :  2166
total : 5000  current step :  2167
total : 5000  current step :  2168
total : 5000  current step :  2169
total : 5000  current step :  2170
total : 5000  current step :  2171
total : 5000  current step :  2172
total : 5000  current step :  2173
total : 5000  current step :  2174
total : 5000  current step :  2175
total : 5000  current step :  2176
total : 5000  current step :  2177
total : 5000  current step :  2178
total : 5000  current step :  2179
total : 5000  current step :  2180
total : 5000  current step :  2181
total : 5000  current step :  2182
total : 5000  current step :  2183
total : 5000  current step :  2184
total : 5000  current step :  2185
total : 5000  current step :  2186
total : 5000  current step :  2187
total : 5000  current step :  2188
total : 5000  current step :  2189
total : 5000  current step :  2190
total : 5000  current step :  2191
total : 5000  current step :  2192
total : 5000  current step :  2193
total : 5000  current step :  2194
total : 5000  current step :  2195
total : 5000  current step :  2196
total : 5000  current step :  2197
total : 5000  current step :  2198
total : 5000  current step :  2199
total : 5000  current step :  2200
total : 5000  current step :  2201
total : 5000  current step :  2202
total : 5000  current step :  2203
total : 5000  current step :  2204
total : 5000  current step :  2205
total : 5000  current step :  2206
total : 5000  current step :  2207
total : 5000  current step :  2208
total : 5000  current step :  2209
total : 5000  current step :  2210
total : 5000  current step :  2211
total : 5000  current step :  2212
total : 5000  current step :  2213
total : 5000  current step :  2214
total : 5000  current step :  2215
total : 5000  current step :  2216
total : 5000  current step :  2217
total : 5000  current step :  2218
total : 5000  current step :  2219
total : 5000  current step :  2220
total : 5000  current step :  2221
total : 5000  current step :  2222
total : 5000  current step :  2223
total : 5000  current step :  2224
total : 5000  current step :  2225
total : 5000  current step :  2226
total : 5000  current step :  2227
total : 5000  current step :  2228
total : 5000  current step :  2229
total : 5000  current step :  2230
total : 5000  current step :  2231
total : 5000  current step :  2232
total : 5000  current step :  2233
total : 5000  current step :  2234
total : 5000  current step :  2235
total : 5000  current step :  2236
total : 5000  current step :  2237
total : 5000  current step :  2238
total : 5000  current step :  2239
total : 5000  current step :  2240
total : 5000  current step :  2241
total : 5000  current step :  2242
total : 5000  current step :  2243
total : 5000  current step :  2244
total : 5000  current step :  2245
total : 5000  current step :  2246
total : 5000  current step :  2247
total : 5000  current step :  2248
total : 5000  current step :  2249
total : 5000  current step :  2250
total : 5000  current step :  2251
total : 5000  current step :  2252
total : 5000  current step :  2253
total : 5000  current step :  2254
total : 5000  current step :  2255
total : 5000  current step :  2256
total : 5000  current step :  2257
total : 5000  current step :  2258
total : 5000  current step :  2259
total : 5000  current step :  2260
total : 5000  current step :  2261
total : 5000  current step :  2262
total : 5000  current step :  2263
total : 5000  current step :  2264
total : 5000  current step :  2265
total : 5000  current step :  2266
total : 5000  current step :  2267
total : 5000  current step :  2268
total : 5000  current step :  2269
total : 5000  current step :  2270
total : 5000  current step :  2271
total : 5000  current step :  2272
total : 5000  current step :  2273
total : 5000  current step :  2274
total : 5000  current step :  2275
total : 5000  current step :  2276
total : 5000  current step :  2277
total : 5000  current step :  2278
total : 5000  current step :  2279
total : 5000  current step :  2280
total : 5000  current step :  2281
total : 5000  current step :  2282
total : 5000  current step :  2283
total : 5000  current step :  2284
total : 5000  current step :  2285
total : 5000  current step :  2286
total : 5000  current step :  2287
total : 5000  current step :  2288
total : 5000  current step :  2289
total : 5000  current step :  2290
total : 5000  current step :  2291
total : 5000  current step :  2292
total : 5000  current step :  2293
total : 5000  current step :  2294
total : 5000  current step :  2295
total : 5000  current step :  2296
total : 5000  current step :  2297
total : 5000  current step :  2298
total : 5000  current step :  2299
total : 5000  current step :  2300
total : 5000  current step :  2301
total : 5000  current step :  2302
total : 5000  current step :  2303
total : 5000  current step :  2304
total : 5000  current step :  2305
total : 5000  current step :  2306
total : 5000  current step :  2307
total : 5000  current step :  2308
total : 5000  current step :  2309
total : 5000  current step :  2310
total : 5000  current step :  2311
total : 5000  current step :  2312
total : 5000  current step :  2313
total : 5000  current step :  2314
total : 5000  current step :  2315
total : 5000  current step :  2316
total : 5000  current step :  2317
total : 5000  current step :  2318
total : 5000  current step :  2319
total : 5000  current step :  2320
total : 5000  current step :  2321
total : 5000  current step :  2322
total : 5000  current step :  2323
total : 5000  current step :  2324
total : 5000  current step :  2325
total : 5000  current step :  2326
total : 5000  current step :  2327
total : 5000  current step :  2328
total : 5000  current step :  2329
total : 5000  current step :  2330
total : 5000  current step :  2331
total : 5000  current step :  2332
total : 5000  current step :  2333
total : 5000  current step :  2334
total : 5000  current step :  2335
total : 5000  current step :  2336
total : 5000  current step :  2337
total : 5000  current step :  2338
total : 5000  current step :  2339
total : 5000  current step :  2340
total : 5000  current step :  2341
total : 5000  current step :  2342
total : 5000  current step :  2343
total : 5000  current step :  2344
total : 5000  current step :  2345
total : 5000  current step :  2346
total : 5000  current step :  2347
total : 5000  current step :  2348
total : 5000  current step :  2349
total : 5000  current step :  2350
total : 5000  current step :  2351
total : 5000  current step :  2352
total : 5000  current step :  2353
total : 5000  current step :  2354
total : 5000  current step :  2355
total : 5000  current step :  2356
total : 5000  current step :  2357
total : 5000  current step :  2358
total : 5000  current step :  2359
total : 5000  current step :  2360
total : 5000  current step :  2361
total : 5000  current step :  2362
total : 5000  current step :  2363
total : 5000  current step :  2364
total : 5000  current step :  2365
total : 5000  current step :  2366
total : 5000  current step :  2367
total : 5000  current step :  2368
total : 5000  current step :  2369
total : 5000  current step :  2370
total : 5000  current step :  2371
total : 5000  current step :  2372
total : 5000  current step :  2373
total : 5000  current step :  2374
total : 5000  current step :  2375
total : 5000  current step :  2376
total : 5000  current step :  2377
total : 5000  current step :  2378
total : 5000  current step :  2379
total : 5000  current step :  2380
total : 5000  current step :  2381
total : 5000  current step :  2382
total : 5000  current step :  2383
total : 5000  current step :  2384
total : 5000  current step :  2385
total : 5000  current step :  2386
total : 5000  current step :  2387
total : 5000  current step :  2388
total : 5000  current step :  2389
total : 5000  current step :  2390
total : 5000  current step :  2391
total : 5000  current step :  2392
total : 5000  current step :  2393
total : 5000  current step :  2394
total : 5000  current step :  2395
total : 5000  current step :  2396
total : 5000  current step :  2397
total : 5000  current step :  2398
total : 5000  current step :  2399
total : 5000  current step :  2400
total : 5000  current step :  2401
total : 5000  current step :  2402
total : 5000  current step :  2403
total : 5000  current step :  2404
total : 5000  current step :  2405
total : 5000  current step :  2406
total : 5000  current step :  2407
total : 5000  current step :  2408
total : 5000  current step :  2409
total : 5000  current step :  2410
total : 5000  current step :  2411
total : 5000  current step :  2412
total : 5000  current step :  2413
total : 5000  current step :  2414
total : 5000  current step :  2415
total : 5000  current step :  2416
total : 5000  current step :  2417
total : 5000  current step :  2418
total : 5000  current step :  2419
total : 5000  current step :  2420
total : 5000  current step :  2421
total : 5000  current step :  2422
total : 5000  current step :  2423
total : 5000  current step :  2424
total : 5000  current step :  2425
total : 5000  current step :  2426
total : 5000  current step :  2427
total : 5000  current step :  2428
total : 5000  current step :  2429
total : 5000  current step :  2430
total : 5000  current step :  2431
total : 5000  current step :  2432
total : 5000  current step :  2433
total : 5000  current step :  2434
total : 5000  current step :  2435
total : 5000  current step :  2436
total : 5000  current step :  2437
total : 5000  current step :  2438
total : 5000  current step :  2439
total : 5000  current step :  2440
total : 5000  current step :  2441
total : 5000  current step :  2442
total : 5000  current step :  2443
total : 5000  current step :  2444
total : 5000  current step :  2445
total : 5000  current step :  2446
total : 5000  current step :  2447
total : 5000  current step :  2448
total : 5000  current step :  2449
total : 5000  current step :  2450
total : 5000  current step :  2451
total : 5000  current step :  2452
total : 5000  current step :  2453
total : 5000  current step :  2454
total : 5000  current step :  2455
total : 5000  current step :  2456
total : 5000  current step :  2457
total : 5000  current step :  2458
total : 5000  current step :  2459
total : 5000  current step :  2460
total : 5000  current step :  2461
total : 5000  current step :  2462
total : 5000  current step :  2463
total : 5000  current step :  2464
total : 5000  current step :  2465
total : 5000  current step :  2466
total : 5000  current step :  2467
total : 5000  current step :  2468
total : 5000  current step :  2469
total : 5000  current step :  2470
total : 5000  current step :  2471
total : 5000  current step :  2472
total : 5000  current step :  2473
total : 5000  current step :  2474
total : 5000  current step :  2475
total : 5000  current step :  2476
total : 5000  current step :  2477
total : 5000  current step :  2478
total : 5000  current step :  2479
total : 5000  current step :  2480
total : 5000  current step :  2481
total : 5000  current step :  2482
total : 5000  current step :  2483
total : 5000  current step :  2484
total : 5000  current step :  2485
total : 5000  current step :  2486
total : 5000  current step :  2487
total : 5000  current step :  2488
total : 5000  current step :  2489
total : 5000  current step :  2490
total : 5000  current step :  2491
total : 5000  current step :  2492
total : 5000  current step :  2493
total : 5000  current step :  2494
total : 5000  current step :  2495
total : 5000  current step :  2496
total : 5000  current step :  2497
total : 5000  current step :  2498
total : 5000  current step :  2499
total : 5000  current step :  2500
total : 5000  current step :  2501
total : 5000  current step :  2502
total : 5000  current step :  2503
total : 5000  current step :  2504
total : 5000  current step :  2505
total : 5000  current step :  2506
total : 5000  current step :  2507
total : 5000  current step :  2508
total : 5000  current step :  2509
total : 5000  current step :  2510
total : 5000  current step :  2511
total : 5000  current step :  2512
total : 5000  current step :  2513
total : 5000  current step :  2514
total : 5000  current step :  2515
total : 5000  current step :  2516
total : 5000  current step :  2517
total : 5000  current step :  2518
total : 5000  current step :  2519
total : 5000  current step :  2520
total : 5000  current step :  2521
total : 5000  current step :  2522
total : 5000  current step :  2523
total : 5000  current step :  2524
total : 5000  current step :  2525
total : 5000  current step :  2526
total : 5000  current step :  2527
total : 5000  current step :  2528
total : 5000  current step :  2529
total : 5000  current step :  2530
total : 5000  current step :  2531
total : 5000  current step :  2532
total : 5000  current step :  2533
total : 5000  current step :  2534
total : 5000  current step :  2535
total : 5000  current step :  2536
total : 5000  current step :  2537
total : 5000  current step :  2538
total : 5000  current step :  2539
total : 5000  current step :  2540
total : 5000  current step :  2541
total : 5000  current step :  2542
total : 5000  current step :  2543
total : 5000  current step :  2544
total : 5000  current step :  2545
total : 5000  current step :  2546
total : 5000  current step :  2547
total : 5000  current step :  2548
total : 5000  current step :  2549
total : 5000  current step :  2550
total : 5000  current step :  2551
total : 5000  current step :  2552
total : 5000  current step :  2553
total : 5000  current step :  2554
total : 5000  current step :  2555
total : 5000  current step :  2556
total : 5000  current step :  2557
total : 5000  current step :  2558
total : 5000  current step :  2559
total : 5000  current step :  2560
total : 5000  current step :  2561
total : 5000  current step :  2562
total : 5000  current step :  2563
total : 5000  current step :  2564
total : 5000  current step :  2565
total : 5000  current step :  2566
total : 5000  current step :  2567
total : 5000  current step :  2568
total : 5000  current step :  2569
total : 5000  current step :  2570
total : 5000  current step :  2571
total : 5000  current step :  2572
total : 5000  current step :  2573
total : 5000  current step :  2574
total : 5000  current step :  2575
total : 5000  current step :  2576
total : 5000  current step :  2577
total : 5000  current step :  2578
total : 5000  current step :  2579
total : 5000  current step :  2580
total : 5000  current step :  2581
total : 5000  current step :  2582
total : 5000  current step :  2583
total : 5000  current step :  2584
total : 5000  current step :  2585
total : 5000  current step :  2586
total : 5000  current step :  2587
total : 5000  current step :  2588
total : 5000  current step :  2589
total : 5000  current step :  2590
total : 5000  current step :  2591
total : 5000  current step :  2592
total : 5000  current step :  2593
total : 5000  current step :  2594
total : 5000  current step :  2595
total : 5000  current step :  2596
total : 5000  current step :  2597
total : 5000  current step :  2598
total : 5000  current step :  2599
total : 5000  current step :  2600
total : 5000  current step :  2601
total : 5000  current step :  2602
total : 5000  current step :  2603
total : 5000  current step :  2604
total : 5000  current step :  2605
total : 5000  current step :  2606
total : 5000  current step :  2607
total : 5000  current step :  2608
total : 5000  current step :  2609
total : 5000  current step :  2610
total : 5000  current step :  2611
total : 5000  current step :  2612
total : 5000  current step :  2613
total : 5000  current step :  2614
total : 5000  current step :  2615
total : 5000  current step :  2616
total : 5000  current step :  2617
total : 5000  current step :  2618
total : 5000  current step :  2619
total : 5000  current step :  2620
total : 5000  current step :  2621
total : 5000  current step :  2622
total : 5000  current step :  2623
total : 5000  current step :  2624
total : 5000  current step :  2625
total : 5000  current step :  2626
total : 5000  current step :  2627
total : 5000  current step :  2628
total : 5000  current step :  2629
total : 5000  current step :  2630
total : 5000  current step :  2631
total : 5000  current step :  2632
total : 5000  current step :  2633
total : 5000  current step :  2634
total : 5000  current step :  2635
total : 5000  current step :  2636
total : 5000  current step :  2637
total : 5000  current step :  2638
total : 5000  current step :  2639
total : 5000  current step :  2640
total : 5000  current step :  2641
total : 5000  current step :  2642
total : 5000  current step :  2643
total : 5000  current step :  2644
total : 5000  current step :  2645
total : 5000  current step :  2646
total : 5000  current step :  2647
total : 5000  current step :  2648
total : 5000  current step :  2649
total : 5000  current step :  2650
total : 5000  current step :  2651
total : 5000  current step :  2652
total : 5000  current step :  2653
total : 5000  current step :  2654
total : 5000  current step :  2655
total : 5000  current step :  2656
total : 5000  current step :  2657
total : 5000  current step :  2658
total : 5000  current step :  2659
total : 5000  current step :  2660
total : 5000  current step :  2661
total : 5000  current step :  2662
total : 5000  current step :  2663
total : 5000  current step :  2664
total : 5000  current step :  2665
total : 5000  current step :  2666
total : 5000  current step :  2667
total : 5000  current step :  2668
total : 5000  current step :  2669
total : 5000  current step :  2670
total : 5000  current step :  2671
total : 5000  current step :  2672
total : 5000  current step :  2673
total : 5000  current step :  2674
total : 5000  current step :  2675
total : 5000  current step :  2676
total : 5000  current step :  2677
total : 5000  current step :  2678
total : 5000  current step :  2679
total : 5000  current step :  2680
total : 5000  current step :  2681
total : 5000  current step :  2682
total : 5000  current step :  2683
total : 5000  current step :  2684
total : 5000  current step :  2685
total : 5000  current step :  2686
total : 5000  current step :  2687
total : 5000  current step :  2688
total : 5000  current step :  2689
total : 5000  current step :  2690
total : 5000  current step :  2691
total : 5000  current step :  2692
total : 5000  current step :  2693
total : 5000  current step :  2694
total : 5000  current step :  2695
total : 5000  current step :  2696
total : 5000  current step :  2697
total : 5000  current step :  2698
total : 5000  current step :  2699
total : 5000  current step :  2700
total : 5000  current step :  2701
total : 5000  current step :  2702
total : 5000  current step :  2703
total : 5000  current step :  2704
total : 5000  current step :  2705
total : 5000  current step :  2706
total : 5000  current step :  2707
total : 5000  current step :  2708
total : 5000  current step :  2709
total : 5000  current step :  2710
total : 5000  current step :  2711
total : 5000  current step :  2712
total : 5000  current step :  2713
total : 5000  current step :  2714
total : 5000  current step :  2715
total : 5000  current step :  2716
total : 5000  current step :  2717
total : 5000  current step :  2718
total : 5000  current step :  2719
total : 5000  current step :  2720
total : 5000  current step :  2721
total : 5000  current step :  2722
total : 5000  current step :  2723
total : 5000  current step :  2724
total : 5000  current step :  2725
total : 5000  current step :  2726
total : 5000  current step :  2727
total : 5000  current step :  2728
total : 5000  current step :  2729
total : 5000  current step :  2730
total : 5000  current step :  2731
total : 5000  current step :  2732
total : 5000  current step :  2733
total : 5000  current step :  2734
total : 5000  current step :  2735
total : 5000  current step :  2736
total : 5000  current step :  2737
total : 5000  current step :  2738
total : 5000  current step :  2739
total : 5000  current step :  2740
total : 5000  current step :  2741
total : 5000  current step :  2742
total : 5000  current step :  2743
total : 5000  current step :  2744
total : 5000  current step :  2745
total : 5000  current step :  2746
total : 5000  current step :  2747
total : 5000  current step :  2748
total : 5000  current step :  2749
total : 5000  current step :  2750
total : 5000  current step :  2751
total : 5000  current step :  2752
total : 5000  current step :  2753
total : 5000  current step :  2754
total : 5000  current step :  2755
total : 5000  current step :  2756
total : 5000  current step :  2757
total : 5000  current step :  2758
total : 5000  current step :  2759
total : 5000  current step :  2760
total : 5000  current step :  2761
total : 5000  current step :  2762
total : 5000  current step :  2763
total : 5000  current step :  2764
total : 5000  current step :  2765
total : 5000  current step :  2766
total : 5000  current step :  2767
total : 5000  current step :  2768
total : 5000  current step :  2769
total : 5000  current step :  2770
total : 5000  current step :  2771
total : 5000  current step :  2772
total : 5000  current step :  2773
total : 5000  current step :  2774
total : 5000  current step :  2775
total : 5000  current step :  2776
total : 5000  current step :  2777
total : 5000  current step :  2778
total : 5000  current step :  2779
total : 5000  current step :  2780
total : 5000  current step :  2781
total : 5000  current step :  2782
total : 5000  current step :  2783
total : 5000  current step :  2784
total : 5000  current step :  2785
total : 5000  current step :  2786
total : 5000  current step :  2787
total : 5000  current step :  2788
total : 5000  current step :  2789
total : 5000  current step :  2790
total : 5000  current step :  2791
total : 5000  current step :  2792
total : 5000  current step :  2793
total : 5000  current step :  2794
total : 5000  current step :  2795
total : 5000  current step :  2796
total : 5000  current step :  2797
total : 5000  current step :  2798
total : 5000  current step :  2799
total : 5000  current step :  2800
total : 5000  current step :  2801
total : 5000  current step :  2802
total : 5000  current step :  2803
total : 5000  current step :  2804
total : 5000  current step :  2805
total : 5000  current step :  2806
total : 5000  current step :  2807
total : 5000  current step :  2808
total : 5000  current step :  2809
total : 5000  current step :  2810
total : 5000  current step :  2811
total : 5000  current step :  2812
total : 5000  current step :  2813
total : 5000  current step :  2814
total : 5000  current step :  2815
total : 5000  current step :  2816
total : 5000  current step :  2817
total : 5000  current step :  2818
total : 5000  current step :  2819
total : 5000  current step :  2820
total : 5000  current step :  2821
total : 5000  current step :  2822
total : 5000  current step :  2823
total : 5000  current step :  2824
total : 5000  current step :  2825
total : 5000  current step :  2826
total : 5000  current step :  2827
total : 5000  current step :  2828
total : 5000  current step :  2829
total : 5000  current step :  2830
total : 5000  current step :  2831
total : 5000  current step :  2832
total : 5000  current step :  2833
total : 5000  current step :  2834
total : 5000  current step :  2835
total : 5000  current step :  2836
total : 5000  current step :  2837
total : 5000  current step :  2838
total : 5000  current step :  2839
total : 5000  current step :  2840
total : 5000  current step :  2841
total : 5000  current step :  2842
total : 5000  current step :  2843
total : 5000  current step :  2844
total : 5000  current step :  2845
total : 5000  current step :  2846
total : 5000  current step :  2847
total : 5000  current step :  2848
total : 5000  current step :  2849
total : 5000  current step :  2850
total : 5000  current step :  2851
total : 5000  current step :  2852
total : 5000  current step :  2853
total : 5000  current step :  2854
total : 5000  current step :  2855
total : 5000  current step :  2856
total : 5000  current step :  2857
total : 5000  current step :  2858
total : 5000  current step :  2859
total : 5000  current step :  2860
total : 5000  current step :  2861
total : 5000  current step :  2862
total : 5000  current step :  2863
total : 5000  current step :  2864
total : 5000  current step :  2865
total : 5000  current step :  2866
total : 5000  current step :  2867
total : 5000  current step :  2868
total : 5000  current step :  2869
total : 5000  current step :  2870
total : 5000  current step :  2871
total : 5000  current step :  2872
total : 5000  current step :  2873
total : 5000  current step :  2874
total : 5000  current step :  2875
total : 5000  current step :  2876
total : 5000  current step :  2877
total : 5000  current step :  2878
total : 5000  current step :  2879
total : 5000  current step :  2880
total : 5000  current step :  2881
total : 5000  current step :  2882
total : 5000  current step :  2883
total : 5000  current step :  2884
total : 5000  current step :  2885
total : 5000  current step :  2886
total : 5000  current step :  2887
total : 5000  current step :  2888
total : 5000  current step :  2889
total : 5000  current step :  2890
total : 5000  current step :  2891
total : 5000  current step :  2892
total : 5000  current step :  2893
total : 5000  current step :  2894
total : 5000  current step :  2895
total : 5000  current step :  2896
total : 5000  current step :  2897
total : 5000  current step :  2898
total : 5000  current step :  2899
total : 5000  current step :  2900
total : 5000  current step :  2901
total : 5000  current step :  2902
total : 5000  current step :  2903
total : 5000  current step :  2904
total : 5000  current step :  2905
total : 5000  current step :  2906
total : 5000  current step :  2907
total : 5000  current step :  2908
total : 5000  current step :  2909
total : 5000  current step :  2910
total : 5000  current step :  2911
total : 5000  current step :  2912
total : 5000  current step :  2913
total : 5000  current step :  2914
total : 5000  current step :  2915
total : 5000  current step :  2916
total : 5000  current step :  2917
total : 5000  current step :  2918
total : 5000  current step :  2919
total : 5000  current step :  2920
total : 5000  current step :  2921
total : 5000  current step :  2922
total : 5000  current step :  2923
total : 5000  current step :  2924
total : 5000  current step :  2925
total : 5000  current step :  2926
total : 5000  current step :  2927
total : 5000  current step :  2928
total : 5000  current step :  2929
total : 5000  current step :  2930
total : 5000  current step :  2931
total : 5000  current step :  2932
total : 5000  current step :  2933
total : 5000  current step :  2934
total : 5000  current step :  2935
total : 5000  current step :  2936
total : 5000  current step :  2937
total : 5000  current step :  2938
total : 5000  current step :  2939
total : 5000  current step :  2940
total : 5000  current step :  2941
total : 5000  current step :  2942
total : 5000  current step :  2943
total : 5000  current step :  2944
total : 5000  current step :  2945
total : 5000  current step :  2946
total : 5000  current step :  2947
total : 5000  current step :  2948
total : 5000  current step :  2949
total : 5000  current step :  2950
total : 5000  current step :  2951
total : 5000  current step :  2952
total : 5000  current step :  2953
total : 5000  current step :  2954
total : 5000  current step :  2955
total : 5000  current step :  2956
total : 5000  current step :  2957
total : 5000  current step :  2958
total : 5000  current step :  2959
total : 5000  current step :  2960
total : 5000  current step :  2961
total : 5000  current step :  2962
total : 5000  current step :  2963
total : 5000  current step :  2964
total : 5000  current step :  2965
total : 5000  current step :  2966
total : 5000  current step :  2967
total : 5000  current step :  2968
total : 5000  current step :  2969
total : 5000  current step :  2970
total : 5000  current step :  2971
total : 5000  current step :  2972
total : 5000  current step :  2973
total : 5000  current step :  2974
total : 5000  current step :  2975
total : 5000  current step :  2976
total : 5000  current step :  2977
total : 5000  current step :  2978
total : 5000  current step :  2979
total : 5000  current step :  2980
total : 5000  current step :  2981
total : 5000  current step :  2982
total : 5000  current step :  2983
total : 5000  current step :  2984
total : 5000  current step :  2985
total : 5000  current step :  2986
total : 5000  current step :  2987
total : 5000  current step :  2988
total : 5000  current step :  2989
total : 5000  current step :  2990
total : 5000  current step :  2991
total : 5000  current step :  2992
total : 5000  current step :  2993
total : 5000  current step :  2994
total : 5000  current step :  2995
total : 5000  current step :  2996
total : 5000  current step :  2997
total : 5000  current step :  2998
total : 5000  current step :  2999
total : 5000  current step :  3000
total : 5000  current step :  3001
total : 5000  current step :  3002
total : 5000  current step :  3003
total : 5000  current step :  3004
total : 5000  current step :  3005
total : 5000  current step :  3006
total : 5000  current step :  3007
total : 5000  current step :  3008
total : 5000  current step :  3009
total : 5000  current step :  3010
total : 5000  current step :  3011
total : 5000  current step :  3012
total : 5000  current step :  3013
total : 5000  current step :  3014
total : 5000  current step :  3015
total : 5000  current step :  3016
total : 5000  current step :  3017
total : 5000  current step :  3018
total : 5000  current step :  3019
total : 5000  current step :  3020
total : 5000  current step :  3021
total : 5000  current step :  3022
total : 5000  current step :  3023
total : 5000  current step :  3024
total : 5000  current step :  3025
total : 5000  current step :  3026
total : 5000  current step :  3027
total : 5000  current step :  3028
total : 5000  current step :  3029
total : 5000  current step :  3030
total : 5000  current step :  3031
total : 5000  current step :  3032
total : 5000  current step :  3033
total : 5000  current step :  3034
total : 5000  current step :  3035
total : 5000  current step :  3036
total : 5000  current step :  3037
total : 5000  current step :  3038
total : 5000  current step :  3039
total : 5000  current step :  3040
total : 5000  current step :  3041
total : 5000  current step :  3042
total : 5000  current step :  3043
total : 5000  current step :  3044
total : 5000  current step :  3045
total : 5000  current step :  3046
total : 5000  current step :  3047
total : 5000  current step :  3048
total : 5000  current step :  3049
total : 5000  current step :  3050
total : 5000  current step :  3051
total : 5000  current step :  3052
total : 5000  current step :  3053
total : 5000  current step :  3054
total : 5000  current step :  3055
total : 5000  current step :  3056
total : 5000  current step :  3057
total : 5000  current step :  3058
total : 5000  current step :  3059
total : 5000  current step :  3060
total : 5000  current step :  3061
total : 5000  current step :  3062
total : 5000  current step :  3063
total : 5000  current step :  3064
total : 5000  current step :  3065
total : 5000  current step :  3066
total : 5000  current step :  3067
total : 5000  current step :  3068
total : 5000  current step :  3069
total : 5000  current step :  3070
total : 5000  current step :  3071
total : 5000  current step :  3072
total : 5000  current step :  3073
total : 5000  current step :  3074
total : 5000  current step :  3075
total : 5000  current step :  3076
total : 5000  current step :  3077
total : 5000  current step :  3078
total : 5000  current step :  3079
total : 5000  current step :  3080
total : 5000  current step :  3081
total : 5000  current step :  3082
total : 5000  current step :  3083
total : 5000  current step :  3084
total : 5000  current step :  3085
total : 5000  current step :  3086
total : 5000  current step :  3087
total : 5000  current step :  3088
total : 5000  current step :  3089
total : 5000  current step :  3090
total : 5000  current step :  3091
total : 5000  current step :  3092
total : 5000  current step :  3093
total : 5000  current step :  3094
total : 5000  current step :  3095
total : 5000  current step :  3096
total : 5000  current step :  3097
total : 5000  current step :  3098
total : 5000  current step :  3099
total : 5000  current step :  3100
total : 5000  current step :  3101
total : 5000  current step :  3102
total : 5000  current step :  3103
total : 5000  current step :  3104
total : 5000  current step :  3105
total : 5000  current step :  3106
total : 5000  current step :  3107
total : 5000  current step :  3108
total : 5000  current step :  3109
total : 5000  current step :  3110
total : 5000  current step :  3111
total : 5000  current step :  3112
total : 5000  current step :  3113
total : 5000  current step :  3114
total : 5000  current step :  3115
total : 5000  current step :  3116
total : 5000  current step :  3117
total : 5000  current step :  3118
total : 5000  current step :  3119
total : 5000  current step :  3120
total : 5000  current step :  3121
total : 5000  current step :  3122
total : 5000  current step :  3123
total : 5000  current step :  3124
total : 5000  current step :  3125
total : 5000  current step :  3126
total : 5000  current step :  3127
total : 5000  current step :  3128
total : 5000  current step :  3129
total : 5000  current step :  3130
total : 5000  current step :  3131
total : 5000  current step :  3132
total : 5000  current step :  3133
total : 5000  current step :  3134
total : 5000  current step :  3135
total : 5000  current step :  3136
total : 5000  current step :  3137
total : 5000  current step :  3138
total : 5000  current step :  3139
total : 5000  current step :  3140
total : 5000  current step :  3141
total : 5000  current step :  3142
total : 5000  current step :  3143
total : 5000  current step :  3144
total : 5000  current step :  3145
total : 5000  current step :  3146
total : 5000  current step :  3147
total : 5000  current step :  3148
total : 5000  current step :  3149
total : 5000  current step :  3150
total : 5000  current step :  3151
total : 5000  current step :  3152
total : 5000  current step :  3153
total : 5000  current step :  3154
total : 5000  current step :  3155
total : 5000  current step :  3156
total : 5000  current step :  3157
total : 5000  current step :  3158
total : 5000  current step :  3159
total : 5000  current step :  3160
total : 5000  current step :  3161
total : 5000  current step :  3162
total : 5000  current step :  3163
total : 5000  current step :  3164
total : 5000  current step :  3165
total : 5000  current step :  3166
total : 5000  current step :  3167
total : 5000  current step :  3168
total : 5000  current step :  3169
total : 5000  current step :  3170
total : 5000  current step :  3171
total : 5000  current step :  3172
total : 5000  current step :  3173
total : 5000  current step :  3174
total : 5000  current step :  3175
total : 5000  current step :  3176
total : 5000  current step :  3177
total : 5000  current step :  3178
total : 5000  current step :  3179
total : 5000  current step :  3180
total : 5000  current step :  3181
total : 5000  current step :  3182
total : 5000  current step :  3183
total : 5000  current step :  3184
total : 5000  current step :  3185
total : 5000  current step :  3186
total : 5000  current step :  3187
total : 5000  current step :  3188
total : 5000  current step :  3189
total : 5000  current step :  3190
total : 5000  current step :  3191
total : 5000  current step :  3192
total : 5000  current step :  3193
total : 5000  current step :  3194
total : 5000  current step :  3195
total : 5000  current step :  3196
total : 5000  current step :  3197
total : 5000  current step :  3198
total : 5000  current step :  3199
total : 5000  current step :  3200
total : 5000  current step :  3201
total : 5000  current step :  3202
total : 5000  current step :  3203
total : 5000  current step :  3204
total : 5000  current step :  3205
total : 5000  current step :  3206
total : 5000  current step :  3207
total : 5000  current step :  3208
total : 5000  current step :  3209
total : 5000  current step :  3210
total : 5000  current step :  3211
total : 5000  current step :  3212
total : 5000  current step :  3213
total : 5000  current step :  3214
total : 5000  current step :  3215
total : 5000  current step :  3216
total : 5000  current step :  3217
total : 5000  current step :  3218
total : 5000  current step :  3219
total : 5000  current step :  3220
total : 5000  current step :  3221
total : 5000  current step :  3222
total : 5000  current step :  3223
total : 5000  current step :  3224
total : 5000  current step :  3225
total : 5000  current step :  3226
total : 5000  current step :  3227
total : 5000  current step :  3228
total : 5000  current step :  3229
total : 5000  current step :  3230
total : 5000  current step :  3231
total : 5000  current step :  3232
total : 5000  current step :  3233
total : 5000  current step :  3234
total : 5000  current step :  3235
total : 5000  current step :  3236
total : 5000  current step :  3237
total : 5000  current step :  3238
total : 5000  current step :  3239
total : 5000  current step :  3240
total : 5000  current step :  3241
total : 5000  current step :  3242
total : 5000  current step :  3243
total : 5000  current step :  3244
total : 5000  current step :  3245
total : 5000  current step :  3246
total : 5000  current step :  3247
total : 5000  current step :  3248
total : 5000  current step :  3249
total : 5000  current step :  3250
total : 5000  current step :  3251
total : 5000  current step :  3252
total : 5000  current step :  3253
total : 5000  current step :  3254
total : 5000  current step :  3255
total : 5000  current step :  3256
total : 5000  current step :  3257
total : 5000  current step :  3258
total : 5000  current step :  3259
total : 5000  current step :  3260
total : 5000  current step :  3261
total : 5000  current step :  3262
total : 5000  current step :  3263
total : 5000  current step :  3264
total : 5000  current step :  3265
total : 5000  current step :  3266
total : 5000  current step :  3267
total : 5000  current step :  3268
total : 5000  current step :  3269
total : 5000  current step :  3270
total : 5000  current step :  3271
total : 5000  current step :  3272
total : 5000  current step :  3273
total : 5000  current step :  3274
total : 5000  current step :  3275
total : 5000  current step :  3276
total : 5000  current step :  3277
total : 5000  current step :  3278
total : 5000  current step :  3279
total : 5000  current step :  3280
total : 5000  current step :  3281
total : 5000  current step :  3282
total : 5000  current step :  3283
total : 5000  current step :  3284
total : 5000  current step :  3285
total : 5000  current step :  3286
total : 5000  current step :  3287
total : 5000  current step :  3288
total : 5000  current step :  3289
total : 5000  current step :  3290
total : 5000  current step :  3291
total : 5000  current step :  3292
total : 5000  current step :  3293
total : 5000  current step :  3294
total : 5000  current step :  3295
total : 5000  current step :  3296
total : 5000  current step :  3297
total : 5000  current step :  3298
total : 5000  current step :  3299
total : 5000  current step :  3300
total : 5000  current step :  3301
total : 5000  current step :  3302
total : 5000  current step :  3303
total : 5000  current step :  3304
total : 5000  current step :  3305
total : 5000  current step :  3306
total : 5000  current step :  3307
total : 5000  current step :  3308
total : 5000  current step :  3309
total : 5000  current step :  3310
total : 5000  current step :  3311
total : 5000  current step :  3312
total : 5000  current step :  3313
total : 5000  current step :  3314
total : 5000  current step :  3315
total : 5000  current step :  3316
total : 5000  current step :  3317
total : 5000  current step :  3318
total : 5000  current step :  3319
total : 5000  current step :  3320
total : 5000  current step :  3321
total : 5000  current step :  3322
total : 5000  current step :  3323
total : 5000  current step :  3324
total : 5000  current step :  3325
total : 5000  current step :  3326
total : 5000  current step :  3327
total : 5000  current step :  3328
total : 5000  current step :  3329
total : 5000  current step :  3330
total : 5000  current step :  3331
total : 5000  current step :  3332
total : 5000  current step :  3333
total : 5000  current step :  3334
total : 5000  current step :  3335
total : 5000  current step :  3336
total : 5000  current step :  3337
total : 5000  current step :  3338
total : 5000  current step :  3339
total : 5000  current step :  3340
total : 5000  current step :  3341
total : 5000  current step :  3342
total : 5000  current step :  3343
total : 5000  current step :  3344
total : 5000  current step :  3345
total : 5000  current step :  3346
total : 5000  current step :  3347
total : 5000  current step :  3348
total : 5000  current step :  3349
total : 5000  current step :  3350
total : 5000  current step :  3351
total : 5000  current step :  3352
total : 5000  current step :  3353
total : 5000  current step :  3354
total : 5000  current step :  3355
total : 5000  current step :  3356
total : 5000  current step :  3357
total : 5000  current step :  3358
total : 5000  current step :  3359
total : 5000  current step :  3360
total : 5000  current step :  3361
total : 5000  current step :  3362
total : 5000  current step :  3363
total : 5000  current step :  3364
total : 5000  current step :  3365
total : 5000  current step :  3366
total : 5000  current step :  3367
total : 5000  current step :  3368
total : 5000  current step :  3369
total : 5000  current step :  3370
total : 5000  current step :  3371
total : 5000  current step :  3372
total : 5000  current step :  3373
total : 5000  current step :  3374
total : 5000  current step :  3375
total : 5000  current step :  3376
total : 5000  current step :  3377
total : 5000  current step :  3378
total : 5000  current step :  3379
total : 5000  current step :  3380
total : 5000  current step :  3381
total : 5000  current step :  3382
total : 5000  current step :  3383
total : 5000  current step :  3384
total : 5000  current step :  3385
total : 5000  current step :  3386
total : 5000  current step :  3387
total : 5000  current step :  3388
total : 5000  current step :  3389
total : 5000  current step :  3390
total : 5000  current step :  3391
total : 5000  current step :  3392
total : 5000  current step :  3393
total : 5000  current step :  3394
total : 5000  current step :  3395
total : 5000  current step :  3396
total : 5000  current step :  3397
total : 5000  current step :  3398
total : 5000  current step :  3399
total : 5000  current step :  3400
total : 5000  current step :  3401
total : 5000  current step :  3402
total : 5000  current step :  3403
total : 5000  current step :  3404
total : 5000  current step :  3405
total : 5000  current step :  3406
total : 5000  current step :  3407
total : 5000  current step :  3408
total : 5000  current step :  3409
total : 5000  current step :  3410
total : 5000  current step :  3411
total : 5000  current step :  3412
total : 5000  current step :  3413
total : 5000  current step :  3414
total : 5000  current step :  3415
total : 5000  current step :  3416
total : 5000  current step :  3417
total : 5000  current step :  3418
total : 5000  current step :  3419
total : 5000  current step :  3420
total : 5000  current step :  3421
total : 5000  current step :  3422
total : 5000  current step :  3423
total : 5000  current step :  3424
total : 5000  current step :  3425
total : 5000  current step :  3426
total : 5000  current step :  3427
total : 5000  current step :  3428
total : 5000  current step :  3429
total : 5000  current step :  3430
total : 5000  current step :  3431
total : 5000  current step :  3432
total : 5000  current step :  3433
total : 5000  current step :  3434
total : 5000  current step :  3435
total : 5000  current step :  3436
total : 5000  current step :  3437
total : 5000  current step :  3438
total : 5000  current step :  3439
total : 5000  current step :  3440
total : 5000  current step :  3441
total : 5000  current step :  3442
total : 5000  current step :  3443
total : 5000  current step :  3444
total : 5000  current step :  3445
total : 5000  current step :  3446
total : 5000  current step :  3447
total : 5000  current step :  3448
total : 5000  current step :  3449
total : 5000  current step :  3450
total : 5000  current step :  3451
total : 5000  current step :  3452
total : 5000  current step :  3453
total : 5000  current step :  3454
total : 5000  current step :  3455
total : 5000  current step :  3456
total : 5000  current step :  3457
total : 5000  current step :  3458
total : 5000  current step :  3459
total : 5000  current step :  3460
total : 5000  current step :  3461
total : 5000  current step :  3462
total : 5000  current step :  3463
total : 5000  current step :  3464
total : 5000  current step :  3465
total : 5000  current step :  3466
total : 5000  current step :  3467
total : 5000  current step :  3468
total : 5000  current step :  3469
total : 5000  current step :  3470
total : 5000  current step :  3471
total : 5000  current step :  3472
total : 5000  current step :  3473
total : 5000  current step :  3474
total : 5000  current step :  3475
total : 5000  current step :  3476
total : 5000  current step :  3477
total : 5000  current step :  3478
total : 5000  current step :  3479
total : 5000  current step :  3480
total : 5000  current step :  3481
total : 5000  current step :  3482
total : 5000  current step :  3483
total : 5000  current step :  3484
total : 5000  current step :  3485
total : 5000  current step :  3486
total : 5000  current step :  3487
total : 5000  current step :  3488
total : 5000  current step :  3489
total : 5000  current step :  3490
total : 5000  current step :  3491
total : 5000  current step :  3492
total : 5000  current step :  3493
total : 5000  current step :  3494
total : 5000  current step :  3495
total : 5000  current step :  3496
total : 5000  current step :  3497
total : 5000  current step :  3498
total : 5000  current step :  3499
total : 5000  current step :  3500
total : 5000  current step :  3501
total : 5000  current step :  3502
total : 5000  current step :  3503
total : 5000  current step :  3504
total : 5000  current step :  3505
total : 5000  current step :  3506
total : 5000  current step :  3507
total : 5000  current step :  3508
total : 5000  current step :  3509
total : 5000  current step :  3510
total : 5000  current step :  3511
total : 5000  current step :  3512
total : 5000  current step :  3513
total : 5000  current step :  3514
total : 5000  current step :  3515
total : 5000  current step :  3516
total : 5000  current step :  3517
total : 5000  current step :  3518
total : 5000  current step :  3519
total : 5000  current step :  3520
total : 5000  current step :  3521
total : 5000  current step :  3522
total : 5000  current step :  3523
total : 5000  current step :  3524
total : 5000  current step :  3525
total : 5000  current step :  3526
total : 5000  current step :  3527
total : 5000  current step :  3528
total : 5000  current step :  3529
total : 5000  current step :  3530
total : 5000  current step :  3531
total : 5000  current step :  3532
total : 5000  current step :  3533
total : 5000  current step :  3534
total : 5000  current step :  3535
total : 5000  current step :  3536
total : 5000  current step :  3537
total : 5000  current step :  3538
total : 5000  current step :  3539
total : 5000  current step :  3540
total : 5000  current step :  3541
total : 5000  current step :  3542
total : 5000  current step :  3543
total : 5000  current step :  3544
total : 5000  current step :  3545
total : 5000  current step :  3546
total : 5000  current step :  3547
total : 5000  current step :  3548
total : 5000  current step :  3549
total : 5000  current step :  3550
total : 5000  current step :  3551
total : 5000  current step :  3552
total : 5000  current step :  3553
total : 5000  current step :  3554
total : 5000  current step :  3555
total : 5000  current step :  3556
total : 5000  current step :  3557
total : 5000  current step :  3558
total : 5000  current step :  3559
total : 5000  current step :  3560
total : 5000  current step :  3561
total : 5000  current step :  3562
total : 5000  current step :  3563
total : 5000  current step :  3564
total : 5000  current step :  3565
total : 5000  current step :  3566
total : 5000  current step :  3567
total : 5000  current step :  3568
total : 5000  current step :  3569
total : 5000  current step :  3570
total : 5000  current step :  3571
total : 5000  current step :  3572
total : 5000  current step :  3573
total : 5000  current step :  3574
total : 5000  current step :  3575
total : 5000  current step :  3576
total : 5000  current step :  3577
total : 5000  current step :  3578
total : 5000  current step :  3579
total : 5000  current step :  3580
total : 5000  current step :  3581
total : 5000  current step :  3582
total : 5000  current step :  3583
total : 5000  current step :  3584
total : 5000  current step :  3585
total : 5000  current step :  3586
total : 5000  current step :  3587
total : 5000  current step :  3588
total : 5000  current step :  3589
total : 5000  current step :  3590
total : 5000  current step :  3591
total : 5000  current step :  3592
total : 5000  current step :  3593
total : 5000  current step :  3594
total : 5000  current step :  3595
total : 5000  current step :  3596
total : 5000  current step :  3597
total : 5000  current step :  3598
total : 5000  current step :  3599
total : 5000  current step :  3600
total : 5000  current step :  3601
total : 5000  current step :  3602
total : 5000  current step :  3603
total : 5000  current step :  3604
total : 5000  current step :  3605
total : 5000  current step :  3606
total : 5000  current step :  3607
total : 5000  current step :  3608
total : 5000  current step :  3609
total : 5000  current step :  3610
total : 5000  current step :  3611
total : 5000  current step :  3612
total : 5000  current step :  3613
total : 5000  current step :  3614
total : 5000  current step :  3615
total : 5000  current step :  3616
total : 5000  current step :  3617
total : 5000  current step :  3618
total : 5000  current step :  3619
total : 5000  current step :  3620
total : 5000  current step :  3621
total : 5000  current step :  3622
total : 5000  current step :  3623
total : 5000  current step :  3624
total : 5000  current step :  3625
total : 5000  current step :  3626
total : 5000  current step :  3627
total : 5000  current step :  3628
total : 5000  current step :  3629
total : 5000  current step :  3630
total : 5000  current step :  3631
total : 5000  current step :  3632
total : 5000  current step :  3633
total : 5000  current step :  3634
total : 5000  current step :  3635
total : 5000  current step :  3636
total : 5000  current step :  3637
total : 5000  current step :  3638
total : 5000  current step :  3639
total : 5000  current step :  3640
total : 5000  current step :  3641
total : 5000  current step :  3642
total : 5000  current step :  3643
total : 5000  current step :  3644
total : 5000  current step :  3645
total : 5000  current step :  3646
total : 5000  current step :  3647
total : 5000  current step :  3648
total : 5000  current step :  3649
total : 5000  current step :  3650
total : 5000  current step :  3651
total : 5000  current step :  3652
total : 5000  current step :  3653
total : 5000  current step :  3654
total : 5000  current step :  3655
total : 5000  current step :  3656
total : 5000  current step :  3657
total : 5000  current step :  3658
total : 5000  current step :  3659
total : 5000  current step :  3660
total : 5000  current step :  3661
total : 5000  current step :  3662
total : 5000  current step :  3663
total : 5000  current step :  3664
total : 5000  current step :  3665
total : 5000  current step :  3666
total : 5000  current step :  3667
total : 5000  current step :  3668
total : 5000  current step :  3669
total : 5000  current step :  3670
total : 5000  current step :  3671
total : 5000  current step :  3672
total : 5000  current step :  3673
total : 5000  current step :  3674
total : 5000  current step :  3675
total : 5000  current step :  3676
total : 5000  current step :  3677
total : 5000  current step :  3678
total : 5000  current step :  3679
total : 5000  current step :  3680
total : 5000  current step :  3681
total : 5000  current step :  3682
total : 5000  current step :  3683
total : 5000  current step :  3684
total : 5000  current step :  3685
total : 5000  current step :  3686
total : 5000  current step :  3687
total : 5000  current step :  3688
total : 5000  current step :  3689
total : 5000  current step :  3690
total : 5000  current step :  3691
total : 5000  current step :  3692
total : 5000  current step :  3693
total : 5000  current step :  3694
total : 5000  current step :  3695
total : 5000  current step :  3696
total : 5000  current step :  3697
total : 5000  current step :  3698
total : 5000  current step :  3699
total : 5000  current step :  3700
total : 5000  current step :  3701
total : 5000  current step :  3702
total : 5000  current step :  3703
total : 5000  current step :  3704
total : 5000  current step :  3705
total : 5000  current step :  3706
total : 5000  current step :  3707
total : 5000  current step :  3708
total : 5000  current step :  3709
total : 5000  current step :  3710
total : 5000  current step :  3711
total : 5000  current step :  3712
total : 5000  current step :  3713
total : 5000  current step :  3714
total : 5000  current step :  3715
total : 5000  current step :  3716
total : 5000  current step :  3717
total : 5000  current step :  3718
total : 5000  current step :  3719
total : 5000  current step :  3720
total : 5000  current step :  3721
total : 5000  current step :  3722
total : 5000  current step :  3723
total : 5000  current step :  3724
total : 5000  current step :  3725
total : 5000  current step :  3726
total : 5000  current step :  3727
total : 5000  current step :  3728
total : 5000  current step :  3729
total : 5000  current step :  3730
total : 5000  current step :  3731
total : 5000  current step :  3732
total : 5000  current step :  3733
total : 5000  current step :  3734
total : 5000  current step :  3735
total : 5000  current step :  3736
total : 5000  current step :  3737
total : 5000  current step :  3738
total : 5000  current step :  3739
total : 5000  current step :  3740
total : 5000  current step :  3741
total : 5000  current step :  3742
total : 5000  current step :  3743
total : 5000  current step :  3744
total : 5000  current step :  3745
total : 5000  current step :  3746
total : 5000  current step :  3747
total : 5000  current step :  3748
total : 5000  current step :  3749
total : 5000  current step :  3750
total : 5000  current step :  3751
total : 5000  current step :  3752
total : 5000  current step :  3753
total : 5000  current step :  3754
total : 5000  current step :  3755
total : 5000  current step :  3756
total : 5000  current step :  3757
total : 5000  current step :  3758
total : 5000  current step :  3759
total : 5000  current step :  3760
total : 5000  current step :  3761
total : 5000  current step :  3762
total : 5000  current step :  3763
total : 5000  current step :  3764
total : 5000  current step :  3765
total : 5000  current step :  3766
total : 5000  current step :  3767
total : 5000  current step :  3768
total : 5000  current step :  3769
total : 5000  current step :  3770
total : 5000  current step :  3771
total : 5000  current step :  3772
total : 5000  current step :  3773
total : 5000  current step :  3774
total : 5000  current step :  3775
total : 5000  current step :  3776
total : 5000  current step :  3777
total : 5000  current step :  3778
total : 5000  current step :  3779
total : 5000  current step :  3780
total : 5000  current step :  3781
total : 5000  current step :  3782
total : 5000  current step :  3783
total : 5000  current step :  3784
total : 5000  current step :  3785
total : 5000  current step :  3786
total : 5000  current step :  3787
total : 5000  current step :  3788
total : 5000  current step :  3789
total : 5000  current step :  3790
total : 5000  current step :  3791
total : 5000  current step :  3792
total : 5000  current step :  3793
total : 5000  current step :  3794
total : 5000  current step :  3795
total : 5000  current step :  3796
total : 5000  current step :  3797
total : 5000  current step :  3798
total : 5000  current step :  3799
total : 5000  current step :  3800
total : 5000  current step :  3801
total : 5000  current step :  3802
total : 5000  current step :  3803
total : 5000  current step :  3804
total : 5000  current step :  3805
total : 5000  current step :  3806
total : 5000  current step :  3807
total : 5000  current step :  3808
total : 5000  current step :  3809
total : 5000  current step :  3810
total : 5000  current step :  3811
total : 5000  current step :  3812
total : 5000  current step :  3813
total : 5000  current step :  3814
total : 5000  current step :  3815
total : 5000  current step :  3816
total : 5000  current step :  3817
total : 5000  current step :  3818
total : 5000  current step :  3819
total : 5000  current step :  3820
total : 5000  current step :  3821
total : 5000  current step :  3822
total : 5000  current step :  3823
total : 5000  current step :  3824
total : 5000  current step :  3825
total : 5000  current step :  3826
total : 5000  current step :  3827
total : 5000  current step :  3828
total : 5000  current step :  3829
total : 5000  current step :  3830
total : 5000  current step :  3831
total : 5000  current step :  3832
total : 5000  current step :  3833
total : 5000  current step :  3834
total : 5000  current step :  3835
total : 5000  current step :  3836
total : 5000  current step :  3837
total : 5000  current step :  3838
total : 5000  current step :  3839
total : 5000  current step :  3840
total : 5000  current step :  3841
total : 5000  current step :  3842
total : 5000  current step :  3843
total : 5000  current step :  3844
total : 5000  current step :  3845
total : 5000  current step :  3846
total : 5000  current step :  3847
total : 5000  current step :  3848
total : 5000  current step :  3849
total : 5000  current step :  3850
total : 5000  current step :  3851
total : 5000  current step :  3852
total : 5000  current step :  3853
total : 5000  current step :  3854
total : 5000  current step :  3855
total : 5000  current step :  3856
total : 5000  current step :  3857
total : 5000  current step :  3858
total : 5000  current step :  3859
total : 5000  current step :  3860
total : 5000  current step :  3861
total : 5000  current step :  3862
total : 5000  current step :  3863
total : 5000  current step :  3864
total : 5000  current step :  3865
total : 5000  current step :  3866
total : 5000  current step :  3867
total : 5000  current step :  3868
total : 5000  current step :  3869
total : 5000  current step :  3870
total : 5000  current step :  3871
total : 5000  current step :  3872
total : 5000  current step :  3873
total : 5000  current step :  3874
total : 5000  current step :  3875
total : 5000  current step :  3876
total : 5000  current step :  3877
total : 5000  current step :  3878
total : 5000  current step :  3879
total : 5000  current step :  3880
total : 5000  current step :  3881
total : 5000  current step :  3882
total : 5000  current step :  3883
total : 5000  current step :  3884
total : 5000  current step :  3885
total : 5000  current step :  3886
total : 5000  current step :  3887
total : 5000  current step :  3888
total : 5000  current step :  3889
total : 5000  current step :  3890
total : 5000  current step :  3891
total : 5000  current step :  3892
total : 5000  current step :  3893
total : 5000  current step :  3894
total : 5000  current step :  3895
total : 5000  current step :  3896
total : 5000  current step :  3897
total : 5000  current step :  3898
total : 5000  current step :  3899
total : 5000  current step :  3900
total : 5000  current step :  3901
total : 5000  current step :  3902
total : 5000  current step :  3903
total : 5000  current step :  3904
total : 5000  current step :  3905
total : 5000  current step :  3906
total : 5000  current step :  3907
total : 5000  current step :  3908
total : 5000  current step :  3909
total : 5000  current step :  3910
total : 5000  current step :  3911
total : 5000  current step :  3912
total : 5000  current step :  3913
total : 5000  current step :  3914
total : 5000  current step :  3915
total : 5000  current step :  3916
total : 5000  current step :  3917
total : 5000  current step :  3918
total : 5000  current step :  3919
total : 5000  current step :  3920
total : 5000  current step :  3921
total : 5000  current step :  3922
total : 5000  current step :  3923
total : 5000  current step :  3924
total : 5000  current step :  3925
total : 5000  current step :  3926
total : 5000  current step :  3927
total : 5000  current step :  3928
total : 5000  current step :  3929
total : 5000  current step :  3930
total : 5000  current step :  3931
total : 5000  current step :  3932
total : 5000  current step :  3933
total : 5000  current step :  3934
total : 5000  current step :  3935
total : 5000  current step :  3936
total : 5000  current step :  3937
total : 5000  current step :  3938
total : 5000  current step :  3939
total : 5000  current step :  3940
total : 5000  current step :  3941
total : 5000  current step :  3942
total : 5000  current step :  3943
total : 5000  current step :  3944
total : 5000  current step :  3945
total : 5000  current step :  3946
total : 5000  current step :  3947
total : 5000  current step :  3948
total : 5000  current step :  3949
total : 5000  current step :  3950
total : 5000  current step :  3951
total : 5000  current step :  3952
total : 5000  current step :  3953
total : 5000  current step :  3954
total : 5000  current step :  3955
total : 5000  current step :  3956
total : 5000  current step :  3957
total : 5000  current step :  3958
total : 5000  current step :  3959
total : 5000  current step :  3960
total : 5000  current step :  3961
total : 5000  current step :  3962
total : 5000  current step :  3963
total : 5000  current step :  3964
total : 5000  current step :  3965
total : 5000  current step :  3966
total : 5000  current step :  3967
total : 5000  current step :  3968
total : 5000  current step :  3969
total : 5000  current step :  3970
total : 5000  current step :  3971
total : 5000  current step :  3972
total : 5000  current step :  3973
total : 5000  current step :  3974
total : 5000  current step :  3975
total : 5000  current step :  3976
total : 5000  current step :  3977
total : 5000  current step :  3978
total : 5000  current step :  3979
total : 5000  current step :  3980
total : 5000  current step :  3981
total : 5000  current step :  3982
total : 5000  current step :  3983
total : 5000  current step :  3984
total : 5000  current step :  3985
total : 5000  current step :  3986
total : 5000  current step :  3987
total : 5000  current step :  3988
total : 5000  current step :  3989
total : 5000  current step :  3990
total : 5000  current step :  3991
total : 5000  current step :  3992
total : 5000  current step :  3993
total : 5000  current step :  3994
total : 5000  current step :  3995
total : 5000  current step :  3996
total : 5000  current step :  3997
total : 5000  current step :  3998
total : 5000  current step :  3999
total : 5000  current step :  4000
total : 5000  current step :  4001
total : 5000  current step :  4002
total : 5000  current step :  4003
total : 5000  current step :  4004
total : 5000  current step :  4005
total : 5000  current step :  4006
total : 5000  current step :  4007
total : 5000  current step :  4008
total : 5000  current step :  4009
total : 5000  current step :  4010
total : 5000  current step :  4011
total : 5000  current step :  4012
total : 5000  current step :  4013
total : 5000  current step :  4014
total : 5000  current step :  4015
total : 5000  current step :  4016
total : 5000  current step :  4017
total : 5000  current step :  4018
total : 5000  current step :  4019
total : 5000  current step :  4020
total : 5000  current step :  4021
total : 5000  current step :  4022
total : 5000  current step :  4023
total : 5000  current step :  4024
total : 5000  current step :  4025
total : 5000  current step :  4026
total : 5000  current step :  4027
total : 5000  current step :  4028
total : 5000  current step :  4029
total : 5000  current step :  4030
total : 5000  current step :  4031
total : 5000  current step :  4032
total : 5000  current step :  4033
total : 5000  current step :  4034
total : 5000  current step :  4035
total : 5000  current step :  4036
total : 5000  current step :  4037
total : 5000  current step :  4038
total : 5000  current step :  4039
total : 5000  current step :  4040
total : 5000  current step :  4041
total : 5000  current step :  4042
total : 5000  current step :  4043
total : 5000  current step :  4044
total : 5000  current step :  4045
total : 5000  current step :  4046
total : 5000  current step :  4047
total : 5000  current step :  4048
total : 5000  current step :  4049
total : 5000  current step :  4050
total : 5000  current step :  4051
total : 5000  current step :  4052
total : 5000  current step :  4053
total : 5000  current step :  4054
total : 5000  current step :  4055
total : 5000  current step :  4056
total : 5000  current step :  4057
total : 5000  current step :  4058
total : 5000  current step :  4059
total : 5000  current step :  4060
total : 5000  current step :  4061
total : 5000  current step :  4062
total : 5000  current step :  4063
total : 5000  current step :  4064
total : 5000  current step :  4065
total : 5000  current step :  4066
total : 5000  current step :  4067
total : 5000  current step :  4068
total : 5000  current step :  4069
total : 5000  current step :  4070
total : 5000  current step :  4071
total : 5000  current step :  4072
total : 5000  current step :  4073
total : 5000  current step :  4074
total : 5000  current step :  4075
total : 5000  current step :  4076
total : 5000  current step :  4077
total : 5000  current step :  4078
total : 5000  current step :  4079
total : 5000  current step :  4080
total : 5000  current step :  4081
total : 5000  current step :  4082
total : 5000  current step :  4083
total : 5000  current step :  4084
total : 5000  current step :  4085
total : 5000  current step :  4086
total : 5000  current step :  4087
total : 5000  current step :  4088
total : 5000  current step :  4089
total : 5000  current step :  4090
total : 5000  current step :  4091
total : 5000  current step :  4092
total : 5000  current step :  4093
total : 5000  current step :  4094
total : 5000  current step :  4095
total : 5000  current step :  4096
total : 5000  current step :  4097
total : 5000  current step :  4098
total : 5000  current step :  4099
total : 5000  current step :  4100
total : 5000  current step :  4101
total : 5000  current step :  4102
total : 5000  current step :  4103
total : 5000  current step :  4104
total : 5000  current step :  4105
total : 5000  current step :  4106
total : 5000  current step :  4107
total : 5000  current step :  4108
total : 5000  current step :  4109
total : 5000  current step :  4110
total : 5000  current step :  4111
total : 5000  current step :  4112
total : 5000  current step :  4113
total : 5000  current step :  4114
total : 5000  current step :  4115
total : 5000  current step :  4116
total : 5000  current step :  4117
total : 5000  current step :  4118
total : 5000  current step :  4119
total : 5000  current step :  4120
total : 5000  current step :  4121
total : 5000  current step :  4122
total : 5000  current step :  4123
total : 5000  current step :  4124
total : 5000  current step :  4125
total : 5000  current step :  4126
total : 5000  current step :  4127
total : 5000  current step :  4128
total : 5000  current step :  4129
total : 5000  current step :  4130
total : 5000  current step :  4131
total : 5000  current step :  4132
total : 5000  current step :  4133
total : 5000  current step :  4134
total : 5000  current step :  4135
total : 5000  current step :  4136
total : 5000  current step :  4137
total : 5000  current step :  4138
total : 5000  current step :  4139
total : 5000  current step :  4140
total : 5000  current step :  4141
total : 5000  current step :  4142
total : 5000  current step :  4143
total : 5000  current step :  4144
total : 5000  current step :  4145
total : 5000  current step :  4146
total : 5000  current step :  4147
total : 5000  current step :  4148
total : 5000  current step :  4149
total : 5000  current step :  4150
total : 5000  current step :  4151
total : 5000  current step :  4152
total : 5000  current step :  4153
total : 5000  current step :  4154
total : 5000  current step :  4155
total : 5000  current step :  4156
total : 5000  current step :  4157
total : 5000  current step :  4158
total : 5000  current step :  4159
total : 5000  current step :  4160
total : 5000  current step :  4161
total : 5000  current step :  4162
total : 5000  current step :  4163
total : 5000  current step :  4164
total : 5000  current step :  4165
total : 5000  current step :  4166
total : 5000  current step :  4167
total : 5000  current step :  4168
total : 5000  current step :  4169
total : 5000  current step :  4170
total : 5000  current step :  4171
total : 5000  current step :  4172
total : 5000  current step :  4173
total : 5000  current step :  4174
total : 5000  current step :  4175
total : 5000  current step :  4176
total : 5000  current step :  4177
total : 5000  current step :  4178
total : 5000  current step :  4179
total : 5000  current step :  4180
total : 5000  current step :  4181
total : 5000  current step :  4182
total : 5000  current step :  4183
total : 5000  current step :  4184
total : 5000  current step :  4185
total : 5000  current step :  4186
total : 5000  current step :  4187
total : 5000  current step :  4188
total : 5000  current step :  4189
total : 5000  current step :  4190
total : 5000  current step :  4191
total : 5000  current step :  4192
total : 5000  current step :  4193
total : 5000  current step :  4194
total : 5000  current step :  4195
total : 5000  current step :  4196
total : 5000  current step :  4197
total : 5000  current step :  4198
total : 5000  current step :  4199
total : 5000  current step :  4200
total : 5000  current step :  4201
total : 5000  current step :  4202
total : 5000  current step :  4203
total : 5000  current step :  4204
total : 5000  current step :  4205
total : 5000  current step :  4206
total : 5000  current step :  4207
total : 5000  current step :  4208
total : 5000  current step :  4209
total : 5000  current step :  4210
total : 5000  current step :  4211
total : 5000  current step :  4212
total : 5000  current step :  4213
total : 5000  current step :  4214
total : 5000  current step :  4215
total : 5000  current step :  4216
total : 5000  current step :  4217
total : 5000  current step :  4218
total : 5000  current step :  4219
total : 5000  current step :  4220
total : 5000  current step :  4221
total : 5000  current step :  4222
total : 5000  current step :  4223
total : 5000  current step :  4224
total : 5000  current step :  4225
total : 5000  current step :  4226
total : 5000  current step :  4227
total : 5000  current step :  4228
total : 5000  current step :  4229
total : 5000  current step :  4230
total : 5000  current step :  4231
total : 5000  current step :  4232
total : 5000  current step :  4233
total : 5000  current step :  4234
total : 5000  current step :  4235
total : 5000  current step :  4236
total : 5000  current step :  4237
total : 5000  current step :  4238
total : 5000  current step :  4239
total : 5000  current step :  4240
total : 5000  current step :  4241
total : 5000  current step :  4242
total : 5000  current step :  4243
total : 5000  current step :  4244
total : 5000  current step :  4245
total : 5000  current step :  4246
total : 5000  current step :  4247
total : 5000  current step :  4248
total : 5000  current step :  4249
total : 5000  current step :  4250
total : 5000  current step :  4251
total : 5000  current step :  4252
total : 5000  current step :  4253
total : 5000  current step :  4254
total : 5000  current step :  4255
total : 5000  current step :  4256
total : 5000  current step :  4257
total : 5000  current step :  4258
total : 5000  current step :  4259
total : 5000  current step :  4260
total : 5000  current step :  4261
total : 5000  current step :  4262
total : 5000  current step :  4263
total : 5000  current step :  4264
total : 5000  current step :  4265
total : 5000  current step :  4266
total : 5000  current step :  4267
total : 5000  current step :  4268
total : 5000  current step :  4269
total : 5000  current step :  4270
total : 5000  current step :  4271
total : 5000  current step :  4272
total : 5000  current step :  4273
total : 5000  current step :  4274
total : 5000  current step :  4275
total : 5000  current step :  4276
total : 5000  current step :  4277
total : 5000  current step :  4278
total : 5000  current step :  4279
total : 5000  current step :  4280
total : 5000  current step :  4281
total : 5000  current step :  4282
total : 5000  current step :  4283
total : 5000  current step :  4284
total : 5000  current step :  4285
total : 5000  current step :  4286
total : 5000  current step :  4287
total : 5000  current step :  4288
total : 5000  current step :  4289
total : 5000  current step :  4290
total : 5000  current step :  4291
total : 5000  current step :  4292
total : 5000  current step :  4293
total : 5000  current step :  4294
total : 5000  current step :  4295
total : 5000  current step :  4296
total : 5000  current step :  4297
total : 5000  current step :  4298
total : 5000  current step :  4299
total : 5000  current step :  4300
total : 5000  current step :  4301
total : 5000  current step :  4302
total : 5000  current step :  4303
total : 5000  current step :  4304
total : 5000  current step :  4305
total : 5000  current step :  4306
total : 5000  current step :  4307
total : 5000  current step :  4308
total : 5000  current step :  4309
total : 5000  current step :  4310
total : 5000  current step :  4311
total : 5000  current step :  4312
total : 5000  current step :  4313
total : 5000  current step :  4314
total : 5000  current step :  4315
total : 5000  current step :  4316
total : 5000  current step :  4317
total : 5000  current step :  4318
total : 5000  current step :  4319
total : 5000  current step :  4320
total : 5000  current step :  4321
total : 5000  current step :  4322
total : 5000  current step :  4323
total : 5000  current step :  4324
total : 5000  current step :  4325
total : 5000  current step :  4326
total : 5000  current step :  4327
total : 5000  current step :  4328
total : 5000  current step :  4329
total : 5000  current step :  4330
total : 5000  current step :  4331
total : 5000  current step :  4332
total : 5000  current step :  4333
total : 5000  current step :  4334
total : 5000  current step :  4335
total : 5000  current step :  4336
total : 5000  current step :  4337
total : 5000  current step :  4338
total : 5000  current step :  4339
total : 5000  current step :  4340
total : 5000  current step :  4341
total : 5000  current step :  4342
total : 5000  current step :  4343
total : 5000  current step :  4344
total : 5000  current step :  4345
total : 5000  current step :  4346
total : 5000  current step :  4347
total : 5000  current step :  4348
total : 5000  current step :  4349
total : 5000  current step :  4350
total : 5000  current step :  4351
total : 5000  current step :  4352
total : 5000  current step :  4353
total : 5000  current step :  4354
total : 5000  current step :  4355
total : 5000  current step :  4356
total : 5000  current step :  4357
total : 5000  current step :  4358
total : 5000  current step :  4359
total : 5000  current step :  4360
total : 5000  current step :  4361
total : 5000  current step :  4362
total : 5000  current step :  4363
total : 5000  current step :  4364
total : 5000  current step :  4365
total : 5000  current step :  4366
total : 5000  current step :  4367
total : 5000  current step :  4368
total : 5000  current step :  4369
total : 5000  current step :  4370
total : 5000  current step :  4371
total : 5000  current step :  4372
total : 5000  current step :  4373
total : 5000  current step :  4374
total : 5000  current step :  4375
total : 5000  current step :  4376
total : 5000  current step :  4377
total : 5000  current step :  4378
total : 5000  current step :  4379
total : 5000  current step :  4380
total : 5000  current step :  4381
total : 5000  current step :  4382
total : 5000  current step :  4383
total : 5000  current step :  4384
total : 5000  current step :  4385
total : 5000  current step :  4386
total : 5000  current step :  4387
total : 5000  current step :  4388
total : 5000  current step :  4389
total : 5000  current step :  4390
total : 5000  current step :  4391
total : 5000  current step :  4392
total : 5000  current step :  4393
total : 5000  current step :  4394
total : 5000  current step :  4395
total : 5000  current step :  4396
total : 5000  current step :  4397
total : 5000  current step :  4398
total : 5000  current step :  4399
total : 5000  current step :  4400
total : 5000  current step :  4401
total : 5000  current step :  4402
total : 5000  current step :  4403
total : 5000  current step :  4404
total : 5000  current step :  4405
total : 5000  current step :  4406
total : 5000  current step :  4407
total : 5000  current step :  4408
total : 5000  current step :  4409
total : 5000  current step :  4410
total : 5000  current step :  4411
total : 5000  current step :  4412
total : 5000  current step :  4413
total : 5000  current step :  4414
total : 5000  current step :  4415
total : 5000  current step :  4416
total : 5000  current step :  4417
total : 5000  current step :  4418
total : 5000  current step :  4419
total : 5000  current step :  4420
total : 5000  current step :  4421
total : 5000  current step :  4422
total : 5000  current step :  4423
total : 5000  current step :  4424
total : 5000  current step :  4425
total : 5000  current step :  4426
total : 5000  current step :  4427
total : 5000  current step :  4428
total : 5000  current step :  4429
total : 5000  current step :  4430
total : 5000  current step :  4431
total : 5000  current step :  4432
total : 5000  current step :  4433
total : 5000  current step :  4434
total : 5000  current step :  4435
total : 5000  current step :  4436
total : 5000  current step :  4437
total : 5000  current step :  4438
total : 5000  current step :  4439
total : 5000  current step :  4440
total : 5000  current step :  4441
total : 5000  current step :  4442
total : 5000  current step :  4443
total : 5000  current step :  4444
total : 5000  current step :  4445
total : 5000  current step :  4446
total : 5000  current step :  4447
total : 5000  current step :  4448
total : 5000  current step :  4449
total : 5000  current step :  4450
total : 5000  current step :  4451
total : 5000  current step :  4452
total : 5000  current step :  4453
total : 5000  current step :  4454
total : 5000  current step :  4455
total : 5000  current step :  4456
total : 5000  current step :  4457
total : 5000  current step :  4458
total : 5000  current step :  4459
total : 5000  current step :  4460
total : 5000  current step :  4461
total : 5000  current step :  4462
total : 5000  current step :  4463
total : 5000  current step :  4464
total : 5000  current step :  4465
total : 5000  current step :  4466
total : 5000  current step :  4467
total : 5000  current step :  4468
total : 5000  current step :  4469
total : 5000  current step :  4470
total : 5000  current step :  4471
total : 5000  current step :  4472
total : 5000  current step :  4473
total : 5000  current step :  4474
total : 5000  current step :  4475
total : 5000  current step :  4476
total : 5000  current step :  4477
total : 5000  current step :  4478
total : 5000  current step :  4479
total : 5000  current step :  4480
total : 5000  current step :  4481
total : 5000  current step :  4482
total : 5000  current step :  4483
total : 5000  current step :  4484
total : 5000  current step :  4485
total : 5000  current step :  4486
total : 5000  current step :  4487
total : 5000  current step :  4488
total : 5000  current step :  4489
total : 5000  current step :  4490
total : 5000  current step :  4491
total : 5000  current step :  4492
total : 5000  current step :  4493
total : 5000  current step :  4494
total : 5000  current step :  4495
total : 5000  current step :  4496
total : 5000  current step :  4497
total : 5000  current step :  4498
total : 5000  current step :  4499
total : 5000  current step :  4500
total : 5000  current step :  4501
total : 5000  current step :  4502
total : 5000  current step :  4503
total : 5000  current step :  4504
total : 5000  current step :  4505
total : 5000  current step :  4506
total : 5000  current step :  4507
total : 5000  current step :  4508
total : 5000  current step :  4509
total : 5000  current step :  4510
total : 5000  current step :  4511
total : 5000  current step :  4512
total : 5000  current step :  4513
total : 5000  current step :  4514
total : 5000  current step :  4515
total : 5000  current step :  4516
total : 5000  current step :  4517
total : 5000  current step :  4518
total : 5000  current step :  4519
total : 5000  current step :  4520
total : 5000  current step :  4521
total : 5000  current step :  4522
total : 5000  current step :  4523
total : 5000  current step :  4524
total : 5000  current step :  4525
total : 5000  current step :  4526
total : 5000  current step :  4527
total : 5000  current step :  4528
total : 5000  current step :  4529
total : 5000  current step :  4530
total : 5000  current step :  4531
total : 5000  current step :  4532
total : 5000  current step :  4533
total : 5000  current step :  4534
total : 5000  current step :  4535
total : 5000  current step :  4536
total : 5000  current step :  4537
total : 5000  current step :  4538
total : 5000  current step :  4539
total : 5000  current step :  4540
total : 5000  current step :  4541
total : 5000  current step :  4542
total : 5000  current step :  4543
total : 5000  current step :  4544
total : 5000  current step :  4545
total : 5000  current step :  4546
total : 5000  current step :  4547
total : 5000  current step :  4548
total : 5000  current step :  4549
total : 5000  current step :  4550
total : 5000  current step :  4551
total : 5000  current step :  4552
total : 5000  current step :  4553
total : 5000  current step :  4554
total : 5000  current step :  4555
total : 5000  current step :  4556
total : 5000  current step :  4557
total : 5000  current step :  4558
total : 5000  current step :  4559
total : 5000  current step :  4560
total : 5000  current step :  4561
total : 5000  current step :  4562
total : 5000  current step :  4563
total : 5000  current step :  4564
total : 5000  current step :  4565
total : 5000  current step :  4566
total : 5000  current step :  4567
total : 5000  current step :  4568
total : 5000  current step :  4569
total : 5000  current step :  4570
total : 5000  current step :  4571
total : 5000  current step :  4572
total : 5000  current step :  4573
total : 5000  current step :  4574
total : 5000  current step :  4575
total : 5000  current step :  4576
total : 5000  current step :  4577
total : 5000  current step :  4578
total : 5000  current step :  4579
total : 5000  current step :  4580
total : 5000  current step :  4581
total : 5000  current step :  4582
total : 5000  current step :  4583
total : 5000  current step :  4584
total : 5000  current step :  4585
total : 5000  current step :  4586
total : 5000  current step :  4587
total : 5000  current step :  4588
total : 5000  current step :  4589
total : 5000  current step :  4590
total : 5000  current step :  4591
total : 5000  current step :  4592
total : 5000  current step :  4593
total : 5000  current step :  4594
total : 5000  current step :  4595
total : 5000  current step :  4596
total : 5000  current step :  4597
total : 5000  current step :  4598
total : 5000  current step :  4599
total : 5000  current step :  4600
total : 5000  current step :  4601
total : 5000  current step :  4602
total : 5000  current step :  4603
total : 5000  current step :  4604
total : 5000  current step :  4605
total : 5000  current step :  4606
total : 5000  current step :  4607
total : 5000  current step :  4608
total : 5000  current step :  4609
total : 5000  current step :  4610
total : 5000  current step :  4611
total : 5000  current step :  4612
total : 5000  current step :  4613
total : 5000  current step :  4614
total : 5000  current step :  4615
total : 5000  current step :  4616
total : 5000  current step :  4617
total : 5000  current step :  4618
total : 5000  current step :  4619
total : 5000  current step :  4620
total : 5000  current step :  4621
total : 5000  current step :  4622
total : 5000  current step :  4623
total : 5000  current step :  4624
total : 5000  current step :  4625
total : 5000  current step :  4626
total : 5000  current step :  4627
total : 5000  current step :  4628
total : 5000  current step :  4629
total : 5000  current step :  4630
total : 5000  current step :  4631
total : 5000  current step :  4632
total : 5000  current step :  4633
total : 5000  current step :  4634
total : 5000  current step :  4635
total : 5000  current step :  4636
total : 5000  current step :  4637
total : 5000  current step :  4638
total : 5000  current step :  4639
total : 5000  current step :  4640
total : 5000  current step :  4641
total : 5000  current step :  4642
total : 5000  current step :  4643
total : 5000  current step :  4644
total : 5000  current step :  4645
total : 5000  current step :  4646
total : 5000  current step :  4647
total : 5000  current step :  4648
total : 5000  current step :  4649
total : 5000  current step :  4650
total : 5000  current step :  4651
total : 5000  current step :  4652
total : 5000  current step :  4653
total : 5000  current step :  4654
total : 5000  current step :  4655
total : 5000  current step :  4656
total : 5000  current step :  4657
total : 5000  current step :  4658
total : 5000  current step :  4659
total : 5000  current step :  4660
total : 5000  current step :  4661
total : 5000  current step :  4662
total : 5000  current step :  4663
total : 5000  current step :  4664
total : 5000  current step :  4665
total : 5000  current step :  4666
total : 5000  current step :  4667
total : 5000  current step :  4668
total : 5000  current step :  4669
total : 5000  current step :  4670
total : 5000  current step :  4671
total : 5000  current step :  4672
total : 5000  current step :  4673
total : 5000  current step :  4674
total : 5000  current step :  4675
total : 5000  current step :  4676
total : 5000  current step :  4677
total : 5000  current step :  4678
total : 5000  current step :  4679
total : 5000  current step :  4680
total : 5000  current step :  4681
total : 5000  current step :  4682
total : 5000  current step :  4683
total : 5000  current step :  4684
total : 5000  current step :  4685
total : 5000  current step :  4686
total : 5000  current step :  4687
total : 5000  current step :  4688
total : 5000  current step :  4689
total : 5000  current step :  4690
total : 5000  current step :  4691
total : 5000  current step :  4692
total : 5000  current step :  4693
total : 5000  current step :  4694
total : 5000  current step :  4695
total : 5000  current step :  4696
total : 5000  current step :  4697
total : 5000  current step :  4698
total : 5000  current step :  4699
total : 5000  current step :  4700
total : 5000  current step :  4701
total : 5000  current step :  4702
total : 5000  current step :  4703
total : 5000  current step :  4704
total : 5000  current step :  4705
total : 5000  current step :  4706
total : 5000  current step :  4707
total : 5000  current step :  4708
total : 5000  current step :  4709
total : 5000  current step :  4710
total : 5000  current step :  4711
total : 5000  current step :  4712
total : 5000  current step :  4713
total : 5000  current step :  4714
total : 5000  current step :  4715
total : 5000  current step :  4716
total : 5000  current step :  4717
total : 5000  current step :  4718
total : 5000  current step :  4719
total : 5000  current step :  4720
total : 5000  current step :  4721
total : 5000  current step :  4722
total : 5000  current step :  4723
total : 5000  current step :  4724
total : 5000  current step :  4725
total : 5000  current step :  4726
total : 5000  current step :  4727
total : 5000  current step :  4728
total : 5000  current step :  4729
total : 5000  current step :  4730
total : 5000  current step :  4731
total : 5000  current step :  4732
total : 5000  current step :  4733
total : 5000  current step :  4734
total : 5000  current step :  4735
total : 5000  current step :  4736
total : 5000  current step :  4737
total : 5000  current step :  4738
total : 5000  current step :  4739
total : 5000  current step :  4740
total : 5000  current step :  4741
total : 5000  current step :  4742
total : 5000  current step :  4743
total : 5000  current step :  4744
total : 5000  current step :  4745
total : 5000  current step :  4746
total : 5000  current step :  4747
total : 5000  current step :  4748
total : 5000  current step :  4749
total : 5000  current step :  4750
total : 5000  current step :  4751
total : 5000  current step :  4752
total : 5000  current step :  4753
total : 5000  current step :  4754
total : 5000  current step :  4755
total : 5000  current step :  4756
total : 5000  current step :  4757
total : 5000  current step :  4758
total : 5000  current step :  4759
total : 5000  current step :  4760
total : 5000  current step :  4761
total : 5000  current step :  4762
total : 5000  current step :  4763
total : 5000  current step :  4764
total : 5000  current step :  4765
total : 5000  current step :  4766
total : 5000  current step :  4767
total : 5000  current step :  4768
total : 5000  current step :  4769
total : 5000  current step :  4770
total : 5000  current step :  4771
total : 5000  current step :  4772
total : 5000  current step :  4773
total : 5000  current step :  4774
total : 5000  current step :  4775
total : 5000  current step :  4776
total : 5000  current step :  4777
total : 5000  current step :  4778
total : 5000  current step :  4779
total : 5000  current step :  4780
total : 5000  current step :  4781
total : 5000  current step :  4782
total : 5000  current step :  4783
total : 5000  current step :  4784
total : 5000  current step :  4785
total : 5000  current step :  4786
total : 5000  current step :  4787
total : 5000  current step :  4788
total : 5000  current step :  4789
total : 5000  current step :  4790
total : 5000  current step :  4791
total : 5000  current step :  4792
total : 5000  current step :  4793
total : 5000  current step :  4794
total : 5000  current step :  4795
total : 5000  current step :  4796
total : 5000  current step :  4797
total : 5000  current step :  4798
total : 5000  current step :  4799
total : 5000  current step :  4800
total : 5000  current step :  4801
total : 5000  current step :  4802
total : 5000  current step :  4803
total : 5000  current step :  4804
total : 5000  current step :  4805
total : 5000  current step :  4806
total : 5000  current step :  4807
total : 5000  current step :  4808
total : 5000  current step :  4809
total : 5000  current step :  4810
total : 5000  current step :  4811
total : 5000  current step :  4812
total : 5000  current step :  4813
total : 5000  current step :  4814
total : 5000  current step :  4815
total : 5000  current step :  4816
total : 5000  current step :  4817
total : 5000  current step :  4818
total : 5000  current step :  4819
total : 5000  current step :  4820
total : 5000  current step :  4821
total : 5000  current step :  4822
total : 5000  current step :  4823
total : 5000  current step :  4824
total : 5000  current step :  4825
total : 5000  current step :  4826
total : 5000  current step :  4827
total : 5000  current step :  4828
total : 5000  current step :  4829
total : 5000  current step :  4830
total : 5000  current step :  4831
total : 5000  current step :  4832
total : 5000  current step :  4833
total : 5000  current step :  4834
total : 5000  current step :  4835
total : 5000  current step :  4836
total : 5000  current step :  4837
total : 5000  current step :  4838
total : 5000  current step :  4839
total : 5000  current step :  4840
total : 5000  current step :  4841
total : 5000  current step :  4842
total : 5000  current step :  4843
total : 5000  current step :  4844
total : 5000  current step :  4845
total : 5000  current step :  4846
total : 5000  current step :  4847
total : 5000  current step :  4848
total : 5000  current step :  4849
total : 5000  current step :  4850
total : 5000  current step :  4851
total : 5000  current step :  4852
total : 5000  current step :  4853
total : 5000  current step :  4854
total : 5000  current step :  4855
total : 5000  current step :  4856
total : 5000  current step :  4857
total : 5000  current step :  4858
total : 5000  current step :  4859
total : 5000  current step :  4860
total : 5000  current step :  4861
total : 5000  current step :  4862
total : 5000  current step :  4863
total : 5000  current step :  4864
total : 5000  current step :  4865
total : 5000  current step :  4866
total : 5000  current step :  4867
total : 5000  current step :  4868
total : 5000  current step :  4869
total : 5000  current step :  4870
total : 5000  current step :  4871
total : 5000  current step :  4872
total : 5000  current step :  4873
total : 5000  current step :  4874
total : 5000  current step :  4875
total : 5000  current step :  4876
total : 5000  current step :  4877
total : 5000  current step :  4878
total : 5000  current step :  4879
total : 5000  current step :  4880
total : 5000  current step :  4881
total : 5000  current step :  4882
total : 5000  current step :  4883
total : 5000  current step :  4884
total : 5000  current step :  4885
total : 5000  current step :  4886
total : 5000  current step :  4887
total : 5000  current step :  4888
total : 5000  current step :  4889
total : 5000  current step :  4890
total : 5000  current step :  4891
total : 5000  current step :  4892
total : 5000  current step :  4893
total : 5000  current step :  4894
total : 5000  current step :  4895
total : 5000  current step :  4896
total : 5000  current step :  4897
total : 5000  current step :  4898
total : 5000  current step :  4899
total : 5000  current step :  4900
total : 5000  current step :  4901
total : 5000  current step :  4902
total : 5000  current step :  4903
total : 5000  current step :  4904
total : 5000  current step :  4905
total : 5000  current step :  4906
total : 5000  current step :  4907
total : 5000  current step :  4908
total : 5000  current step :  4909
total : 5000  current step :  4910
total : 5000  current step :  4911
total : 5000  current step :  4912
total : 5000  current step :  4913
total : 5000  current step :  4914
total : 5000  current step :  4915
total : 5000  current step :  4916
total : 5000  current step :  4917
total : 5000  current step :  4918
total : 5000  current step :  4919
total : 5000  current step :  4920
total : 5000  current step :  4921
total : 5000  current step :  4922
total : 5000  current step :  4923
total : 5000  current step :  4924
total : 5000  current step :  4925
total : 5000  current step :  4926
total : 5000  current step :  4927
total : 5000  current step :  4928
total : 5000  current step :  4929
total : 5000  current step :  4930
total : 5000  current step :  4931
total : 5000  current step :  4932
total : 5000  current step :  4933
total : 5000  current step :  4934
total : 5000  current step :  4935
total : 5000  current step :  4936
total : 5000  current step :  4937
total : 5000  current step :  4938
total : 5000  current step :  4939
total : 5000  current step :  4940
total : 5000  current step :  4941
total : 5000  current step :  4942
total : 5000  current step :  4943
total : 5000  current step :  4944
total : 5000  current step :  4945
total : 5000  current step :  4946
total : 5000  current step :  4947
total : 5000  current step :  4948
total : 5000  current step :  4949
total : 5000  current step :  4950
total : 5000  current step :  4951
total : 5000  current step :  4952
total : 5000  current step :  4953
total : 5000  current step :  4954
total : 5000  current step :  4955
total : 5000  current step :  4956
total : 5000  current step :  4957
total : 5000  current step :  4958
total : 5000  current step :  4959
total : 5000  current step :  4960
total : 5000  current step :  4961
total : 5000  current step :  4962
total : 5000  current step :  4963
total : 5000  current step :  4964
total : 5000  current step :  4965
total : 5000  current step :  4966
total : 5000  current step :  4967
total : 5000  current step :  4968
total : 5000  current step :  4969
total : 5000  current step :  4970
total : 5000  current step :  4971
total : 5000  current step :  4972
total : 5000  current step :  4973
total : 5000  current step :  4974
total : 5000  current step :  4975
total : 5000  current step :  4976
total : 5000  current step :  4977
total : 5000  current step :  4978
total : 5000  current step :  4979
total : 5000  current step :  4980
total : 5000  current step :  4981
total : 5000  current step :  4982
total : 5000  current step :  4983
total : 5000  current step :  4984
total : 5000  current step :  4985
total : 5000  current step :  4986
total : 5000  current step :  4987
total : 5000  current step :  4988
total : 5000  current step :  4989
total : 5000  current step :  4990
total : 5000  current step :  4991
total : 5000  current step :  4992
total : 5000  current step :  4993
total : 5000  current step :  4994
total : 5000  current step :  4995
total : 5000  current step :  4996
total : 5000  current step :  4997
total : 5000  current step :  4998
total : 5000  current step :  4999
soft_pseudo_label
tensor([[0.0016, 0.0016, 0.0017,  ..., 0.0016, 0.0017, 0.0017],
        [0.0010, 0.0010, 0.0011,  ..., 0.0010, 0.0010, 0.0009],
        [0.0010, 0.0010, 0.0010,  ..., 0.0011, 0.0011, 0.0011],
        ...,
        [0.0002, 0.0001, 0.0001,  ..., 0.0002, 0.0002, 0.0001],
        [0.0037, 0.0034, 0.0033,  ..., 0.0031, 0.0030, 0.0028],
        [0.0017, 0.0018, 0.0018,  ..., 0.0018, 0.0018, 0.0020]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4,
        5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4,
        4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4,
        5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4,
        4, 5, 4, 4, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5]
original label
tensor([4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5,
        5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 7, 5, 4, 4, 4, 4, 5, 5, 5, 4,
        7, 5, 4, 4, 4, 4, 5, 5, 9, 4, 4, 5, 5, 5, 4, 5, 7, 5, 4, 5, 5, 5, 4, 4,
        4, 9, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 0, 4, 5, 5, 5, 4, 4, 3, 5, 4, 4,
        5, 4, 4, 4, 3, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4,
        9, 5, 2, 4, 5, 5, 4, 5])
soft_pseudo_label
tensor([[9.8075e-05, 9.8796e-05, 8.7657e-05,  ..., 9.6555e-05, 8.6340e-05,
         1.0496e-04],
        [2.8987e-03, 2.4940e-03, 2.6927e-03,  ..., 2.5037e-03, 2.5807e-03,
         2.7297e-03],
        [2.6202e-03, 2.9582e-03, 2.6350e-03,  ..., 2.6337e-03, 2.6817e-03,
         2.8918e-03],
        ...,
        [2.5276e-03, 2.6827e-03, 2.6078e-03,  ..., 2.6129e-03, 2.5674e-03,
         2.6827e-03],
        [6.2211e-03, 6.6062e-03, 6.4500e-03,  ..., 5.8958e-03, 6.5772e-03,
         6.1637e-03],
        [2.7518e-03, 2.5425e-03, 2.7384e-03,  ..., 2.5351e-03, 2.6258e-03,
         2.4885e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4,
        5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5,
        5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5,
        4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4,
        5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4,
        4, 5, 5, 4, 4, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5]
original label
tensor([4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 6, 4,
        6, 5, 4, 5, 5, 4, 5, 5, 1, 5, 4, 5, 4, 1, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5,
        5, 5, 4, 4, 4, 6, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 3,
        6, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 3, 5, 5, 4, 2, 5, 4,
        3, 4, 4, 5, 4, 5, 2, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4,
        4, 5, 5, 4, 4, 5, 4, 5])
soft_pseudo_label
tensor([[0.0009, 0.0010, 0.0010,  ..., 0.0009, 0.0010, 0.0009],
        [0.0037, 0.0034, 0.0036,  ..., 0.0035, 0.0032, 0.0035],
        [0.0023, 0.0024, 0.0025,  ..., 0.0022, 0.0024, 0.0025],
        ...,
        [0.0031, 0.0027, 0.0026,  ..., 0.0025, 0.0026, 0.0026],
        [0.0010, 0.0009, 0.0009,  ..., 0.0010, 0.0010, 0.0009],
        [0.0022, 0.0021, 0.0021,  ..., 0.0020, 0.0021, 0.0020]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4,
        4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5,
        5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4,
        4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4,
        4, 5, 4, 4, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5]
original label
tensor([5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 0, 9, 4, 5, 5, 4,
        1, 5, 5, 5, 4, 5, 9, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 1, 0, 4, 5,
        5, 4, 4, 5, 5, 0, 4, 0, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5,
        4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 0, 1, 5, 5, 5, 4, 5, 8, 5, 4, 5, 5,
        4, 5, 4, 7, 5, 5, 4, 5])
soft_pseudo_label
tensor([[0.0019, 0.0020, 0.0020,  ..., 0.0022, 0.0020, 0.0020],
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0024, 0.0023, 0.0024,  ..., 0.0023, 0.0025, 0.0023],
        ...,
        [0.0006, 0.0006, 0.0006,  ..., 0.0006, 0.0006, 0.0007],
        [0.0024, 0.0022, 0.0021,  ..., 0.0022, 0.0023, 0.0025],
        [0.0021, 0.0020, 0.0022,  ..., 0.0020, 0.0020, 0.0021]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5,
        4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4,
        4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5,
        5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5,
        5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5,
        4, 5, 5, 5, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5]
original label
tensor([4, 7, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 1, 5,
        4, 5, 4, 5, 9, 5, 6, 9, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 3, 6, 4, 5, 4, 4,
        4, 5, 5, 9, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5,
        5, 4, 5, 5, 4, 4, 9, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 7, 4, 4, 4, 5, 4, 5,
        4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 0, 5, 4, 0, 4, 9, 5, 5, 5, 9, 5,
        4, 5, 5, 5, 4, 4, 5, 5])
soft_pseudo_label
tensor([[0.0007, 0.0006, 0.0006,  ..., 0.0006, 0.0006, 0.0007],
        [0.0010, 0.0012, 0.0012,  ..., 0.0013, 0.0013, 0.0012],
        [0.0009, 0.0010, 0.0011,  ..., 0.0010, 0.0011, 0.0010],
        ...,
        [0.0056, 0.0057, 0.0060,  ..., 0.0057, 0.0059, 0.0059],
        [0.0013, 0.0013, 0.0012,  ..., 0.0012, 0.0013, 0.0012],
        [0.0004, 0.0005, 0.0004,  ..., 0.0004, 0.0004, 0.0005]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4,
        5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5,
        5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5,
        5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5,
        5, 5, 4, 4, 5, 5, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4]
original label
tensor([5, 4, 5, 5, 5, 0, 4, 4, 3, 5, 4, 4, 5, 5, 1, 4, 4, 4, 4, 3, 5, 8, 5, 7,
        5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 1, 4, 5, 4,
        3, 4, 5, 4, 0, 5, 5, 1, 5, 5, 5, 4, 5, 5, 5, 4, 2, 4, 5, 4, 4, 4, 4, 4,
        5, 4, 4, 0, 4, 5, 7, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5,
        5, 2, 7, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 9, 5,
        6, 5, 4, 4, 5, 5, 4, 4])
soft_pseudo_label
tensor([[0.0189, 0.0194, 0.0186,  ..., 0.0181, 0.0181, 0.0185],
        [0.0022, 0.0020, 0.0022,  ..., 0.0022, 0.0023, 0.0021],
        [0.0022, 0.0026, 0.0024,  ..., 0.0024, 0.0025, 0.0023],
        ...,
        [0.0018, 0.0018, 0.0020,  ..., 0.0016, 0.0019, 0.0017],
        [0.0025, 0.0027, 0.0026,  ..., 0.0025, 0.0027, 0.0026],
        [0.0025, 0.0022, 0.0023,  ..., 0.0022, 0.0019, 0.0021]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4,
        4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5,
        4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4,
        4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4,
        4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5,
        4, 5, 4, 4, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4]
original label
tensor([5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4,
        4, 3, 5, 4, 4, 8, 4, 4, 5, 5, 0, 4, 5, 1, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,
        4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4,
        4, 5, 4, 5, 5, 5, 9, 5, 9, 5, 4, 3, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 9,
        4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 7, 5, 5, 5,
        5, 5, 4, 5, 4, 5, 4, 4])
soft_pseudo_label
tensor([[0.0012, 0.0014, 0.0015,  ..., 0.0013, 0.0014, 0.0014],
        [0.0020, 0.0021, 0.0019,  ..., 0.0019, 0.0019, 0.0022],
        [0.0020, 0.0021, 0.0023,  ..., 0.0020, 0.0021, 0.0019],
        ...,
        [0.0026, 0.0028, 0.0029,  ..., 0.0026, 0.0026, 0.0027],
        [0.0008, 0.0008, 0.0008,  ..., 0.0008, 0.0009, 0.0009],
        [0.0008, 0.0008, 0.0010,  ..., 0.0007, 0.0009, 0.0008]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4,
        5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5,
        5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4,
        4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5,
        4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4,
        4, 4, 4, 4, 4, 5, 4, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4]
original label
tensor([4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 9, 4, 5, 4, 5, 5, 6, 5, 5, 4,
        5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 0, 5, 4, 4, 5, 5, 4, 4, 4, 5,
        5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 9, 4, 4, 4, 5, 4,
        4, 5, 5, 4, 5, 7, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4,
        4, 6, 1, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4,
        4, 4, 4, 4, 4, 5, 7, 4])
soft_pseudo_label
tensor([[0.0031, 0.0033, 0.0034,  ..., 0.0031, 0.0033, 0.0032],
        [0.0034, 0.0036, 0.0037,  ..., 0.0035, 0.0036, 0.0033],
        [0.0033, 0.0035, 0.0037,  ..., 0.0034, 0.0035, 0.0036],
        ...,
        [0.0014, 0.0014, 0.0013,  ..., 0.0013, 0.0013, 0.0013],
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0021, 0.0020, 0.0020,  ..., 0.0020, 0.0021, 0.0020]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4,
        4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4,
        4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5,
        5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5,
        5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5,
        4, 4, 4, 4, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5]
original label
tensor([4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 2, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4,
        4, 5, 4, 2, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4,
        4, 4, 4, 5, 5, 4, 4, 1, 5, 5, 5, 4, 4, 4, 2, 5, 4, 5, 5, 5, 5, 4, 4, 5,
        5, 5, 5, 5, 8, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 9, 5, 7, 5, 5, 5, 4,
        5, 4, 8, 4, 5, 5, 5, 5, 4, 5, 1, 5, 0, 6, 4, 5, 7, 4, 4, 4, 4, 5, 4, 5,
        4, 4, 4, 4, 5, 5, 5, 5])
soft_pseudo_label
tensor([[0.0040, 0.0045, 0.0045,  ..., 0.0042, 0.0047, 0.0042],
        [0.0019, 0.0019, 0.0019,  ..., 0.0020, 0.0020, 0.0020],
        [0.0048, 0.0054, 0.0055,  ..., 0.0051, 0.0053, 0.0057],
        ...,
        [0.0010, 0.0011, 0.0011,  ..., 0.0010, 0.0010, 0.0012],
        [0.0029, 0.0030, 0.0033,  ..., 0.0034, 0.0031, 0.0031],
        [0.0010, 0.0009, 0.0008,  ..., 0.0009, 0.0009, 0.0009]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4,
        5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5,
        5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5,
        4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5,
        4, 5, 4, 4, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5]
original label
tensor([4, 5, 5, 9, 5, 4, 4, 5, 5, 7, 7, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 3, 1, 4,
        5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 6, 2, 1, 5, 4, 5, 4, 3, 4, 4, 4, 2, 5,
        5, 5, 4, 4, 5, 4, 5, 0, 4, 5, 5, 9, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5,
        6, 4, 4, 4, 5, 6, 9, 4, 4, 0, 3, 4, 5, 9, 5, 4, 1, 5, 5, 5, 5, 4, 4, 5,
        9, 8, 5, 4, 5, 4, 5, 7, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5,
        4, 5, 4, 4, 4, 5, 4, 5])
soft_pseudo_label
tensor([[0.0014, 0.0013, 0.0015,  ..., 0.0015, 0.0015, 0.0014],
        [0.0015, 0.0016, 0.0015,  ..., 0.0015, 0.0015, 0.0017],
        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],
        ...,
        [0.0053, 0.0048, 0.0051,  ..., 0.0054, 0.0050, 0.0054],
        [0.0013, 0.0015, 0.0015,  ..., 0.0014, 0.0015, 0.0016],
        [0.0025, 0.0024, 0.0023,  ..., 0.0022, 0.0023, 0.0024]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5,
        5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4,
        4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5,
        4, 5, 5, 4, 4, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4]
original label
tensor([4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 7, 5, 8, 5, 9, 4, 4, 8, 4, 5, 4, 5, 5, 5,
        5, 5, 4, 2, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 3, 0, 5, 4, 4, 5, 4, 4,
        4, 4, 1, 5, 4, 5, 5, 5, 4, 3, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 6, 4, 6,
        4, 4, 5, 5, 4, 4, 4, 5, 0, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5,
        4, 4, 4, 5, 7, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 1, 5, 5, 5,
        4, 5, 5, 4, 4, 4, 4, 4])
soft_pseudo_label
tensor([[1.7698e-03, 1.8950e-03, 1.8394e-03,  ..., 2.0015e-03, 1.7099e-03,
         1.8047e-03],
        [3.0133e-03, 3.1795e-03, 3.1112e-03,  ..., 2.9370e-03, 3.0809e-03,
         3.0044e-03],
        [7.1119e-03, 7.1782e-03, 7.6002e-03,  ..., 7.0980e-03, 7.5743e-03,
         8.0431e-03],
        ...,
        [3.4298e-05, 3.4399e-05, 2.9423e-05,  ..., 3.3898e-05, 3.5352e-05,
         3.4131e-05],
        [2.1911e-03, 2.5897e-03, 2.9247e-03,  ..., 2.3018e-03, 2.9394e-03,
         2.5979e-03],
        [1.4904e-03, 1.2949e-03, 1.3012e-03,  ..., 1.0995e-03, 1.3037e-03,
         1.2680e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5,
        5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5,
        5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5,
        5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4,
        5, 4, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5]
original label
tensor([4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 6, 4, 5, 5, 5, 8, 5, 5, 5, 5, 4, 4,
        4, 5, 1, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 1, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5,
        5, 5, 4, 5, 8, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5,
        9, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 7, 4, 4, 7, 4, 5, 4, 5, 5, 3,
        5, 4, 5, 5, 4, 4, 4, 5])
soft_pseudo_label
tensor([[0.0022, 0.0025, 0.0027,  ..., 0.0025, 0.0026, 0.0025],
        [0.0049, 0.0051, 0.0052,  ..., 0.0048, 0.0051, 0.0053],
        [0.0041, 0.0037, 0.0035,  ..., 0.0041, 0.0038, 0.0038],
        ...,
        [0.0048, 0.0047, 0.0046,  ..., 0.0049, 0.0048, 0.0049],
        [0.0014, 0.0015, 0.0014,  ..., 0.0014, 0.0014, 0.0014],
        [0.0019, 0.0020, 0.0019,  ..., 0.0020, 0.0019, 0.0022]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5,
        4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4,
        4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4,
        4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4,
        4, 5, 4, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5]
original label
tensor([1, 9, 4, 5, 4, 4, 5, 3, 4, 5, 1, 4, 4, 9, 5, 4, 4, 5, 1, 5, 5, 5, 5, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5,
        4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 9, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4,
        5, 4, 5, 9, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4,
        4, 4, 5, 4, 3, 6, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4,
        4, 5, 4, 5, 4, 5, 4, 5])
soft_pseudo_label
tensor([[0.0028, 0.0030, 0.0029,  ..., 0.0029, 0.0029, 0.0029],
        [0.0032, 0.0036, 0.0032,  ..., 0.0033, 0.0033, 0.0034],
        [0.0035, 0.0037, 0.0035,  ..., 0.0033, 0.0032, 0.0031],
        ...,
        [0.0017, 0.0015, 0.0016,  ..., 0.0017, 0.0015, 0.0016],
        [0.0026, 0.0022, 0.0022,  ..., 0.0023, 0.0019, 0.0019],
        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5,
        5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4,
        5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4,
        4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4,
        4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5,
        5, 5, 4, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5]
original label
tensor([5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 6, 4, 5, 4, 4, 4, 5, 5, 4, 4, 7, 5,
        2, 5, 5, 5, 4, 4, 5, 6, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 8, 4, 4,
        8, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 6, 5, 5, 4, 1, 5, 5, 4, 4, 5,
        4, 5, 5, 4, 4, 1, 4, 4, 4, 5, 4, 4, 5, 5, 2, 5, 4, 4, 5, 4, 5, 4, 4, 4,
        7, 5, 5, 5, 5, 4, 4, 4, 8, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5,
        5, 5, 4, 5, 5, 9, 4, 3])
soft_pseudo_label
tensor([[0.0014, 0.0013, 0.0015,  ..., 0.0015, 0.0013, 0.0014],
        [0.0008, 0.0007, 0.0008,  ..., 0.0007, 0.0008, 0.0007],
        [0.0023, 0.0027, 0.0026,  ..., 0.0026, 0.0026, 0.0024],
        ...,
        [0.0019, 0.0020, 0.0018,  ..., 0.0019, 0.0018, 0.0020],
        [0.0021, 0.0022, 0.0021,  ..., 0.0021, 0.0022, 0.0021],
        [0.0031, 0.0030, 0.0034,  ..., 0.0033, 0.0032, 0.0033]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4,
        4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4,
        5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5,
        4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4,
        4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5,
        4, 4, 5, 4, 5, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4]
original label
tensor([5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4,
        4, 4, 4, 1, 5, 5, 4, 5, 5, 4, 4, 4, 5, 7, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4,
        4, 4, 4, 4, 4, 5, 5, 5, 4, 0, 8, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4,
        4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 0, 5, 5, 4, 4,
        4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5,
        4, 4, 5, 4, 5, 5, 5, 4])
soft_pseudo_label
tensor([[0.0018, 0.0020, 0.0017,  ..., 0.0019, 0.0017, 0.0020],
        [0.0009, 0.0010, 0.0010,  ..., 0.0009, 0.0009, 0.0009],
        [0.0012, 0.0013, 0.0013,  ..., 0.0012, 0.0012, 0.0013],
        ...,
        [0.0016, 0.0016, 0.0014,  ..., 0.0017, 0.0015, 0.0016],
        [0.0014, 0.0015, 0.0014,  ..., 0.0015, 0.0017, 0.0014],
        [0.0023, 0.0023, 0.0024,  ..., 0.0022, 0.0023, 0.0021]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5,
        5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4,
        5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4,
        5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5,
        4, 4, 4, 4, 5, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4]
original label
tensor([5, 5, 5, 5, 4, 4, 7, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5,
        5, 4, 1, 9, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 8, 4, 5, 5, 4,
        6, 5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 7, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4,
        5, 4, 4, 4, 4, 4, 5, 4, 5, 3, 5, 4, 4, 5, 5, 4, 8, 5, 5, 5, 4, 2, 4, 5,
        4, 4, 5, 4, 4, 4, 5, 4, 7, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 7, 5, 5, 4, 5,
        4, 4, 4, 4, 5, 4, 4, 4])
soft_pseudo_label
tensor([[0.0012, 0.0013, 0.0013,  ..., 0.0013, 0.0013, 0.0014],
        [0.0033, 0.0030, 0.0030,  ..., 0.0029, 0.0029, 0.0030],
        [0.0026, 0.0027, 0.0023,  ..., 0.0025, 0.0022, 0.0026],
        ...,
        [0.0022, 0.0024, 0.0026,  ..., 0.0023, 0.0024, 0.0023],
        [0.0030, 0.0027, 0.0029,  ..., 0.0029, 0.0036, 0.0029],
        [0.0017, 0.0019, 0.0020,  ..., 0.0018, 0.0017, 0.0019]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4,
        5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4,
        5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4,
        4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5,
        5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4,
        4, 4, 5, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5]
original label
tensor([5, 4, 4, 5, 5, 4, 4, 0, 4, 5, 5, 5, 5, 4, 4, 5, 6, 5, 4, 5, 5, 5, 8, 4,
        1, 5, 7, 4, 5, 5, 5, 5, 5, 5, 2, 4, 5, 5, 4, 6, 5, 5, 5, 5, 4, 5, 4, 4,
        0, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 1, 4,
        4, 3, 4, 5, 5, 5, 4, 4, 5, 4, 1, 5, 4, 4, 8, 5, 5, 4, 5, 5, 5, 4, 5, 5,
        4, 4, 5, 2, 4, 4, 5, 4, 5, 4, 5, 7, 6, 4, 5, 4, 8, 5, 4, 5, 5, 7, 4, 4,
        4, 4, 5, 0, 5, 5, 7, 5])
soft_pseudo_label
tensor([[4.9071e-05, 4.4723e-05, 4.5162e-05,  ..., 4.6098e-05, 4.4462e-05,
         5.2543e-05],
        [3.4431e-03, 3.6260e-03, 3.3404e-03,  ..., 3.0909e-03, 3.0864e-03,
         2.9350e-03],
        [2.1379e-03, 2.3809e-03, 2.1731e-03,  ..., 1.9988e-03, 2.2138e-03,
         1.7957e-03],
        ...,
        [5.1392e-05, 4.9182e-05, 4.6157e-05,  ..., 4.7114e-05, 4.6747e-05,
         5.3596e-05],
        [2.4841e-03, 2.5111e-03, 2.3262e-03,  ..., 2.4160e-03, 2.6986e-03,
         2.1506e-03],
        [2.4221e-03, 2.5859e-03, 2.4615e-03,  ..., 2.3927e-03, 2.4471e-03,
         2.3533e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5,
        4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4,
        5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4,
        4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4,
        5, 5, 5, 4, 4, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4]
original label
tensor([4, 4, 4, 4, 5, 2, 4, 1, 4, 5, 0, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 8,
        4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4,
        0, 5, 5, 4, 4, 5, 5, 6, 5, 4, 4, 4, 5, 5, 4, 5, 5, 2, 5, 4, 5, 4, 5, 5,
        4, 5, 5, 5, 4, 2, 5, 4, 2, 4, 4, 5, 5, 4, 4, 4, 4, 5, 7, 3, 4, 4, 5, 4,
        4, 0, 5, 4, 4, 4, 5, 5, 4, 4, 3, 4, 4, 4, 4, 4, 5, 3, 4, 5, 4, 6, 4, 4,
        5, 5, 5, 4, 4, 4, 4, 9])
soft_pseudo_label
tensor([[0.0036, 0.0032, 0.0031,  ..., 0.0031, 0.0031, 0.0031],
        [0.0057, 0.0054, 0.0060,  ..., 0.0050, 0.0046, 0.0053],
        [0.0019, 0.0020, 0.0020,  ..., 0.0018, 0.0020, 0.0021],
        ...,
        [0.0013, 0.0012, 0.0014,  ..., 0.0013, 0.0013, 0.0013],
        [0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0020, 0.0019],
        [0.0033, 0.0036, 0.0033,  ..., 0.0035, 0.0032, 0.0037]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5,
        4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4,
        5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5,
        5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4,
        5, 4, 4, 4, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5]
original label
tensor([5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 0, 4, 4, 0, 4, 4, 4, 5,
        4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 1,
        5, 5, 4, 8, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5,
        5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 6, 9, 8, 2,
        5, 4, 4, 4, 4, 0, 5, 5])
soft_pseudo_label
tensor([[0.0005, 0.0005, 0.0004,  ..., 0.0005, 0.0005, 0.0005],
        [0.0019, 0.0020, 0.0020,  ..., 0.0018, 0.0020, 0.0018],
        [0.0014, 0.0015, 0.0014,  ..., 0.0015, 0.0015, 0.0013],
        ...,
        [0.0013, 0.0014, 0.0015,  ..., 0.0013, 0.0015, 0.0013],
        [0.0046, 0.0053, 0.0055,  ..., 0.0045, 0.0054, 0.0057],
        [0.0014, 0.0014, 0.0015,  ..., 0.0014, 0.0014, 0.0014]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4,
        5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4,
        5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5,
        5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5,
        5, 5, 5, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5]
original label
tensor([4, 4, 4, 4, 0, 4, 8, 5, 1, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 7, 4, 4, 4,
        5, 5, 5, 4, 4, 4, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 5, 5, 5, 0,
        5, 4, 6, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 8, 4, 4, 4, 5, 4, 5, 5,
        5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 0, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 4, 3, 5, 4, 5, 5, 5, 5, 5, 4, 5, 0, 4, 5, 8, 4, 2, 5, 5,
        5, 5, 5, 5, 5, 5, 4, 5])
soft_pseudo_label
tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],
        [0.0013, 0.0013, 0.0013,  ..., 0.0013, 0.0013, 0.0013],
        [0.0018, 0.0019, 0.0019,  ..., 0.0017, 0.0020, 0.0018],
        ...,
        [0.0023, 0.0025, 0.0023,  ..., 0.0022, 0.0025, 0.0025],
        [0.0013, 0.0011, 0.0012,  ..., 0.0012, 0.0012, 0.0012],
        [0.0021, 0.0023, 0.0023,  ..., 0.0022, 0.0023, 0.0022]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4,
        4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5,
        4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4,
        5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4,
        4, 4, 5, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5]
original label
tensor([5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 9, 4,
        5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 8, 5, 4, 7, 5, 4, 5, 5, 5, 5, 4, 4,
        4, 4, 5, 5, 4, 5, 5, 8, 5, 9, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5,
        4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4,
        4, 5, 5, 3, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 8, 8, 5, 3, 5, 6, 4, 5, 4,
        4, 4, 4, 5, 4, 5, 4, 5])
soft_pseudo_label
tensor([[1.6500e-03, 1.5867e-03, 1.5752e-03,  ..., 1.5060e-03, 1.4964e-03,
         1.7123e-03],
        [2.4570e-03, 2.3617e-03, 2.3814e-03,  ..., 2.6218e-03, 2.4763e-03,
         2.4083e-03],
        [1.9384e-04, 1.7563e-04, 1.8352e-04,  ..., 1.9308e-04, 1.6957e-04,
         1.8388e-04],
        ...,
        [2.9234e-05, 2.4331e-05, 2.4047e-05,  ..., 2.5424e-05, 2.4212e-05,
         2.6180e-05],
        [6.9164e-03, 6.5356e-03, 6.3967e-03,  ..., 6.2241e-03, 6.6417e-03,
         6.3562e-03],
        [2.3651e-03, 2.3398e-03, 2.2185e-03,  ..., 2.2491e-03, 2.2957e-03,
         2.4401e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4,
        5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5,
        5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5,
        5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4,
        5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5,
        5, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4]
original label
tensor([5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 6, 4, 5, 5, 4, 3, 2, 5, 4, 5, 4, 4, 4,
        5, 4, 2, 5, 8, 5, 5, 5, 5, 5, 4, 5, 4, 7, 4, 6, 4, 4, 5, 5, 4, 4, 4, 5,
        5, 4, 4, 6, 5, 5, 4, 4, 3, 5, 1, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5,
        5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 8, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4,
        5, 4, 5, 4, 4, 1, 4, 1, 4, 5, 5, 4, 1, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5,
        3, 7, 4, 4, 4, 9, 4, 4])
soft_pseudo_label
tensor([[0.0015, 0.0016, 0.0015,  ..., 0.0016, 0.0014, 0.0016],
        [0.0053, 0.0051, 0.0051,  ..., 0.0055, 0.0050, 0.0057],
        [0.0035, 0.0035, 0.0035,  ..., 0.0034, 0.0037, 0.0036],
        ...,
        [0.0018, 0.0018, 0.0018,  ..., 0.0016, 0.0018, 0.0018],
        [0.0012, 0.0012, 0.0013,  ..., 0.0013, 0.0012, 0.0012],
        [0.0024, 0.0032, 0.0030,  ..., 0.0030, 0.0027, 0.0030]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5,
        5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4,
        5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5,
        5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5,
        4, 5, 4, 5, 5, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5]
original label
tensor([5, 4, 4, 7, 4, 4, 6, 4, 4, 4, 4, 5, 3, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5,
        4, 4, 4, 5, 4, 5, 1, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 2,
        5, 5, 5, 5, 4, 9, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 0, 4, 4, 5, 8, 5, 5, 0, 5, 4, 4, 5,
        5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 2, 3, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5,
        4, 5, 4, 5, 3, 4, 5, 5])
soft_pseudo_label
tensor([[0.0006, 0.0006, 0.0006,  ..., 0.0006, 0.0005, 0.0005],
        [0.0035, 0.0030, 0.0029,  ..., 0.0029, 0.0032, 0.0030],
        [0.0011, 0.0010, 0.0011,  ..., 0.0011, 0.0011, 0.0010],
        ...,
        [0.0037, 0.0041, 0.0036,  ..., 0.0037, 0.0036, 0.0036],
        [0.0018, 0.0017, 0.0017,  ..., 0.0017, 0.0018, 0.0017],
        [0.0014, 0.0013, 0.0013,  ..., 0.0013, 0.0013, 0.0013]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4,
        5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5,
        4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5,
        5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4,
        5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4,
        5, 4, 4, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5]
original label
tensor([5, 4, 4, 2, 5, 4, 5, 5, 4, 5, 5, 8, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4,
        5, 1, 5, 8, 4, 5, 5, 5, 1, 4, 4, 4, 4, 5, 4, 5, 4, 5, 2, 4, 5, 7, 5, 5,
        4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 8, 5, 4, 4, 8, 4,
        3, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 9, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4,
        5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 2, 6, 6, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4,
        5, 4, 4, 5, 5, 8, 0, 5])
soft_pseudo_label
tensor([[5.6036e-03, 5.5036e-03, 4.9603e-03,  ..., 5.2692e-03, 4.8774e-03,
         4.7977e-03],
        [2.2367e-03, 2.4401e-03, 2.1148e-03,  ..., 2.3420e-03, 2.2842e-03,
         2.2012e-03],
        [1.5366e-03, 1.4393e-03, 1.4612e-03,  ..., 1.3494e-03, 1.4281e-03,
         1.5010e-03],
        ...,
        [3.4441e-03, 3.4282e-03, 3.5086e-03,  ..., 3.2513e-03, 3.5154e-03,
         3.3775e-03],
        [3.5623e-05, 3.5903e-05, 3.8782e-05,  ..., 3.3432e-05, 3.7589e-05,
         3.3596e-05],
        [3.0499e-03, 3.3175e-03, 3.2841e-03,  ..., 3.3702e-03, 3.1074e-03,
         3.4283e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4,
        5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4,
        5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4,
        5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5,
        5, 5, 4, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5]
original label
tensor([4, 4, 5, 4, 4, 5, 4, 5, 5, 8, 4, 2, 1, 4, 4, 5, 5, 5, 4, 2, 4, 8, 5, 4,
        4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 3, 5, 4, 4, 5, 5, 4, 4, 4,
        8, 4, 5, 5, 5, 4, 3, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 8, 5, 4, 5, 5, 4, 4,
        5, 4, 5, 4, 3, 4, 3, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 8, 8, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5,
        5, 5, 4, 5, 5, 5, 4, 5])
soft_pseudo_label
tensor([[8.9928e-04, 8.9315e-04, 9.1746e-04,  ..., 8.7630e-04, 9.3830e-04,
         9.2782e-04],
        [1.3579e-03, 1.3213e-03, 1.3861e-03,  ..., 1.2970e-03, 1.3963e-03,
         1.3773e-03],
        [1.3170e-03, 1.2246e-03, 1.1731e-03,  ..., 1.2342e-03, 1.2610e-03,
         1.2505e-03],
        ...,
        [5.0845e-05, 3.5565e-05, 4.4175e-05,  ..., 4.2235e-05, 3.7896e-05,
         4.3448e-05],
        [2.8555e-03, 2.9182e-03, 2.9977e-03,  ..., 2.6884e-03, 2.8044e-03,
         2.6306e-03],
        [4.2158e-03, 4.0385e-03, 4.2138e-03,  ..., 4.0504e-03, 4.0702e-03,
         4.4268e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5,
        5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5,
        5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5,
        5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4,
        5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4,
        4, 5, 5, 5, 5, 4, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4]
original label
tensor([4, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 0,
        5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 9, 4, 2, 4, 4, 4, 4, 5, 4, 5,
        4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5,
        5, 4, 4, 4, 5, 4, 4, 6, 4, 5, 4, 7, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4,
        5, 5, 4, 5, 5, 4, 0, 5, 4, 4, 7, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 3, 9, 4,
        4, 4, 5, 5, 5, 4, 5, 4])
soft_pseudo_label
tensor([[0.0047, 0.0048, 0.0047,  ..., 0.0042, 0.0042, 0.0042],
        [0.0022, 0.0025, 0.0025,  ..., 0.0026, 0.0024, 0.0028],
        [0.0067, 0.0073, 0.0064,  ..., 0.0071, 0.0066, 0.0070],
        ...,
        [0.0032, 0.0035, 0.0033,  ..., 0.0032, 0.0033, 0.0034],
        [0.0034, 0.0037, 0.0035,  ..., 0.0035, 0.0036, 0.0036],
        [0.0016, 0.0013, 0.0015,  ..., 0.0015, 0.0014, 0.0013]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5,
        5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4,
        4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4,
        5, 5, 5, 5, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4]
original label
tensor([4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 0, 5, 5, 5, 4, 5, 4, 5,
        5, 4, 4, 9, 4, 5, 5, 7, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 8, 5, 4, 4, 4,
        4, 3, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 9, 4, 4, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 4, 5, 5, 5, 8, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 3,
        4, 3, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 0,
        5, 5, 5, 5, 4, 5, 5, 4])
soft_pseudo_label
tensor([[1.6313e-03, 1.7188e-03, 1.6988e-03,  ..., 1.5896e-03, 1.6586e-03,
         1.7925e-03],
        [1.0676e-03, 1.1203e-03, 1.1366e-03,  ..., 1.0899e-03, 9.7678e-04,
         1.2328e-03],
        [2.9104e-04, 2.9204e-04, 2.8850e-04,  ..., 2.8361e-04, 2.7461e-04,
         3.2043e-04],
        ...,
        [1.1702e-03, 1.0801e-03, 1.0686e-03,  ..., 1.0759e-03, 1.0950e-03,
         1.0865e-03],
        [8.6219e-05, 8.2755e-05, 8.4883e-05,  ..., 9.7699e-05, 8.3812e-05,
         9.2410e-05],
        [1.1729e-03, 1.0675e-03, 1.0659e-03,  ..., 1.0597e-03, 1.1280e-03,
         1.1062e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4,
        5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5,
        5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4,
        4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5,
        4, 5, 4, 5, 5, 5, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4]
original label
tensor([5, 5, 5, 4, 5, 5, 5, 4, 4, 8, 6, 4, 3, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 6,
        5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 2, 4, 7, 4, 5, 1, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 5, 5, 4, 4, 5, 4, 4, 4, 4, 3, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5,
        5, 5, 4, 5, 7, 5, 5, 5, 4, 7, 4, 4, 4, 5, 2, 6, 5, 4, 4, 4, 5, 5, 5, 4,
        9, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 3, 4, 5, 5, 4, 5, 5, 5,
        5, 4, 4, 5, 5, 1, 5, 4])
soft_pseudo_label
tensor([[0.0024, 0.0026, 0.0024,  ..., 0.0024, 0.0025, 0.0027],
        [0.0019, 0.0021, 0.0019,  ..., 0.0018, 0.0019, 0.0020],
        [0.0014, 0.0014, 0.0014,  ..., 0.0014, 0.0015, 0.0016],
        ...,
        [0.0043, 0.0042, 0.0041,  ..., 0.0041, 0.0043, 0.0042],
        [0.0011, 0.0012, 0.0012,  ..., 0.0012, 0.0011, 0.0012],
        [0.0019, 0.0022, 0.0021,  ..., 0.0020, 0.0020, 0.0022]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4,
        4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4,
        4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4,
        5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4,
        4, 4, 5, 5, 5, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5]
original label
tensor([5, 4, 4, 5, 4, 9, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 6, 5, 4, 4, 4, 4,
        7, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4,
        4, 4, 4, 1, 5, 5, 5, 5, 1, 4, 4, 5, 4, 4, 5, 7, 4, 5, 4, 5, 4, 5, 4, 5,
        4, 0, 4, 5, 5, 5, 5, 3, 4, 4, 4, 5, 5, 5, 6, 4, 4, 5, 4, 5, 5, 4, 5, 4,
        5, 5, 5, 4, 5, 4, 5, 4, 7, 4, 4, 5, 0, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4,
        4, 4, 5, 5, 5, 9, 5, 5])
soft_pseudo_label
tensor([[0.0011, 0.0012, 0.0012,  ..., 0.0011, 0.0012, 0.0011],
        [0.0025, 0.0029, 0.0027,  ..., 0.0027, 0.0025, 0.0029],
        [0.0011, 0.0012, 0.0012,  ..., 0.0012, 0.0013, 0.0012],
        ...,
        [0.0019, 0.0018, 0.0019,  ..., 0.0017, 0.0019, 0.0016],
        [0.0025, 0.0024, 0.0025,  ..., 0.0024, 0.0024, 0.0024],
        [0.0026, 0.0025, 0.0024,  ..., 0.0024, 0.0027, 0.0025]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5,
        5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4,
        4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4,
        5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5,
        5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5,
        4, 4, 4, 4, 5, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4]
original label
tensor([5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 6, 5, 7, 1, 5, 0, 4, 4, 5, 5, 4, 4,
        5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 6,
        5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 8, 4, 5, 7, 4, 5, 5, 4, 4, 5, 4, 4,
        5, 5, 4, 7, 1, 5, 4, 4, 5, 4, 2, 4, 5, 4, 5, 5, 5, 9, 5, 5, 4, 5, 4, 5,
        5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 1, 4, 9, 0, 5, 4, 4, 5, 6,
        4, 4, 4, 4, 5, 6, 4, 4])
soft_pseudo_label
tensor([[0.0015, 0.0015, 0.0016,  ..., 0.0015, 0.0014, 0.0015],
        [0.0014, 0.0013, 0.0013,  ..., 0.0014, 0.0014, 0.0012],
        [0.0023, 0.0023, 0.0022,  ..., 0.0021, 0.0020, 0.0021],
        ...,
        [0.0010, 0.0009, 0.0010,  ..., 0.0009, 0.0010, 0.0010],
        [0.0014, 0.0014, 0.0016,  ..., 0.0017, 0.0014, 0.0016],
        [0.0014, 0.0017, 0.0016,  ..., 0.0015, 0.0013, 0.0015]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5,
        4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4,
        5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4,
        5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4,
        5, 5, 4, 4, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5]
original label
tensor([4, 6, 1, 5, 5, 4, 5, 4, 1, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5,
        4, 4, 5, 4, 4, 4, 5, 2, 4, 4, 5, 0, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4,
        5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 1, 4, 4, 1, 4, 4, 4,
        5, 4, 4, 5, 5, 6, 4, 5, 5, 5, 5, 8, 5, 5, 4, 4, 4, 5, 4, 5, 4, 0, 5, 4,
        5, 5, 5, 4, 4, 4, 4, 4, 7, 4, 1, 5, 5, 5, 1, 5, 5, 4, 5, 5, 6, 4, 5, 4,
        5, 5, 4, 4, 4, 4, 4, 5])
soft_pseudo_label
tensor([[0.0050, 0.0060, 0.0054,  ..., 0.0056, 0.0052, 0.0057],
        [0.0045, 0.0046, 0.0049,  ..., 0.0043, 0.0049, 0.0048],
        [0.0023, 0.0023, 0.0024,  ..., 0.0024, 0.0024, 0.0027],
        ...,
        [0.0031, 0.0032, 0.0032,  ..., 0.0030, 0.0032, 0.0032],
        [0.0030, 0.0031, 0.0032,  ..., 0.0031, 0.0030, 0.0032],
        [0.0012, 0.0013, 0.0012,  ..., 0.0012, 0.0012, 0.0013]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4,
        5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5,
        5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4,
        5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5,
        4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4,
        4, 5, 4, 5, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4]
original label
tensor([5, 4, 4, 4, 6, 5, 4, 3, 4, 5, 4, 5, 3, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4,
        5, 5, 4, 9, 7, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 0, 5, 5, 5, 4, 4, 5,
        5, 7, 6, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 7, 5, 5, 4, 4, 5, 5, 4,
        5, 4, 4, 4, 4, 5, 4, 5, 3, 5, 4, 5, 5, 5, 5, 4, 4, 3, 0, 4, 5, 4, 4, 5,
        9, 1, 4, 5, 5, 4, 5, 0, 4, 4, 5, 7, 5, 5, 4, 4, 0, 4, 4, 5, 4, 4, 4, 4,
        4, 5, 4, 5, 4, 5, 5, 4])
soft_pseudo_label
tensor([[0.0036, 0.0036, 0.0035,  ..., 0.0034, 0.0036, 0.0034],
        [0.0034, 0.0036, 0.0037,  ..., 0.0035, 0.0034, 0.0035],
        [0.0028, 0.0030, 0.0027,  ..., 0.0031, 0.0028, 0.0028],
        ...,
        [0.0016, 0.0014, 0.0016,  ..., 0.0014, 0.0016, 0.0012],
        [0.0041, 0.0042, 0.0042,  ..., 0.0043, 0.0039, 0.0039],
        [0.0053, 0.0062, 0.0060,  ..., 0.0058, 0.0063, 0.0057]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5,
        5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5,
        5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4,
        5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5,
        5, 5, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5]
original label
tensor([4, 4, 4, 4, 7, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 8, 5,
        5, 5, 1, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5,
        4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 1, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 0, 4, 4, 4, 5, 4, 5, 4,
        5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 8, 5, 4, 4, 4, 4, 4, 4, 5,
        5, 5, 5, 0, 4, 4, 4, 5])
soft_pseudo_label
tensor([[1.6891e-03, 1.6581e-03, 1.6024e-03,  ..., 1.4697e-03, 1.6212e-03,
         1.5637e-03],
        [4.6713e-03, 5.5976e-03, 5.1216e-03,  ..., 5.4095e-03, 5.1316e-03,
         5.3426e-03],
        [1.1617e-03, 1.0765e-03, 1.0865e-03,  ..., 1.1260e-03, 1.0839e-03,
         1.0892e-03],
        ...,
        [3.9063e-03, 3.6172e-03, 3.1914e-03,  ..., 3.4222e-03, 3.6358e-03,
         3.5084e-03],
        [3.9965e-04, 3.7709e-04, 3.8246e-04,  ..., 3.8622e-04, 3.7215e-04,
         3.6123e-04],
        [9.2654e-05, 8.7980e-05, 8.6532e-05,  ..., 8.9803e-05, 8.7894e-05,
         1.0057e-04]], device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5,
        4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5,
        4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4,
        4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4,
        4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4,
        4, 4, 5, 5, 5, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5]
original label
tensor([5, 4, 5, 5, 4, 4, 4, 5, 6, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 3, 5, 8, 5,
        4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 6, 5, 4, 4, 4, 4, 4, 4, 2, 4, 5,
        4, 5, 5, 4, 4, 7, 5, 5, 1, 5, 5, 2, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4,
        4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 3, 4, 4, 5,
        0, 4, 5, 4, 4, 5, 4, 5, 5, 7, 5, 5, 5, 3, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4,
        5, 4, 5, 5, 5, 4, 5, 1])
soft_pseudo_label
tensor([[0.0042, 0.0044, 0.0043,  ..., 0.0042, 0.0044, 0.0042],
        [0.0029, 0.0028, 0.0026,  ..., 0.0024, 0.0027, 0.0026],
        [0.0012, 0.0012, 0.0012,  ..., 0.0011, 0.0011, 0.0012],
        ...,
        [0.0027, 0.0023, 0.0022,  ..., 0.0022, 0.0022, 0.0022],
        [0.0030, 0.0035, 0.0031,  ..., 0.0032, 0.0030, 0.0036],
        [0.0016, 0.0017, 0.0015,  ..., 0.0015, 0.0015, 0.0014]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4,
        5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5,
        5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5,
        5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4,
        4, 5, 5, 4, 4, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5]
original label
tensor([5, 5, 0, 4, 4, 5, 4, 0, 4, 4, 4, 4, 4, 6, 4, 4, 5, 4, 4, 4, 2, 4, 5, 4,
        4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 0, 5, 5,
        5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 7, 4, 5, 4, 4, 4, 4, 4, 0, 0, 4, 5, 5, 6,
        0, 4, 4, 5, 5, 5, 6, 4, 5, 5, 7, 5, 4, 7, 5, 5, 5, 4, 5, 4, 4, 4, 5, 2,
        0, 5, 4, 5, 5, 5, 1, 5, 0, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4,
        4, 5, 5, 4, 4, 5, 5, 4])
soft_pseudo_label
tensor([[0.0011, 0.0010, 0.0010,  ..., 0.0009, 0.0010, 0.0010],
        [0.0015, 0.0017, 0.0016,  ..., 0.0015, 0.0016, 0.0016],
        [0.0020, 0.0019, 0.0020,  ..., 0.0018, 0.0018, 0.0019],
        ...,
        [0.0013, 0.0012, 0.0010,  ..., 0.0012, 0.0011, 0.0010],
        [0.0022, 0.0022, 0.0024,  ..., 0.0021, 0.0021, 0.0022],
        [0.0025, 0.0025, 0.0025,  ..., 0.0027, 0.0024, 0.0025]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4,
        5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4,
        4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5,
        5, 5, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5]
original label
tensor([4, 5, 4, 4, 9, 4, 4, 4, 5, 4, 5, 5, 4, 0, 4, 4, 5, 3, 5, 4, 5, 5, 4, 4,
        6, 5, 4, 5, 6, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 9, 4, 5, 4, 4, 4, 4, 6, 4, 4, 4,
        4, 5, 5, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4,
        4, 5, 5, 4, 7, 4, 4, 4, 4, 4, 4, 1, 5, 5, 4, 4, 5, 4, 5, 5, 4, 9, 4, 5,
        5, 5, 4, 4, 4, 4, 4, 5])
soft_pseudo_label
tensor([[0.0016, 0.0014, 0.0016,  ..., 0.0015, 0.0017, 0.0016],
        [0.0010, 0.0012, 0.0011,  ..., 0.0010, 0.0010, 0.0010],
        [0.0007, 0.0007, 0.0007,  ..., 0.0006, 0.0007, 0.0006],
        ...,
        [0.0021, 0.0022, 0.0020,  ..., 0.0023, 0.0023, 0.0022],
        [0.0009, 0.0009, 0.0009,  ..., 0.0008, 0.0009, 0.0009],
        [0.0004, 0.0004, 0.0004,  ..., 0.0003, 0.0004, 0.0004]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5,
        4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5,
        4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4,
        4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5,
        5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 4, 5, 4, 5, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5]
original label
tensor([4, 5, 5, 5, 4, 5, 5, 4, 4, 8, 5, 4, 5, 8, 4, 5, 9, 4, 5, 4, 4, 4, 4, 5,
        4, 5, 5, 2, 5, 5, 5, 4, 4, 4, 4, 4, 7, 0, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5,
        5, 5, 5, 4, 4, 0, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4,
        4, 4, 4, 5, 5, 4, 4, 6, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5,
        5, 4, 4, 4, 4, 5, 4, 5, 5, 1, 5, 9, 4, 4, 4, 5, 5, 5, 4, 5, 0, 5, 4, 5,
        4, 4, 5, 4, 5, 4, 4, 5])
soft_pseudo_label
tensor([[8.3236e-03, 8.6383e-03, 9.0374e-03,  ..., 8.0478e-03, 8.3971e-03,
         8.6383e-03],
        [3.2715e-03, 3.7571e-03, 3.5589e-03,  ..., 4.2309e-03, 3.3085e-03,
         3.9965e-03],
        [7.1121e-04, 8.3352e-04, 7.6787e-04,  ..., 7.5486e-04, 7.5486e-04,
         7.4316e-04],
        ...,
        [2.6689e-03, 2.8417e-03, 2.9369e-03,  ..., 2.8970e-03, 2.6390e-03,
         2.7328e-03],
        [7.5033e-06, 7.5621e-06, 7.1248e-06,  ..., 7.2441e-06, 6.6540e-06,
         7.7869e-06],
        [1.7448e-03, 1.7923e-03, 1.6682e-03,  ..., 1.6090e-03, 1.7645e-03,
         1.7491e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4,
        4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4,
        5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5,
        5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4,
        4, 5, 5, 4, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5]
original label
tensor([5, 4, 5, 3, 4, 5, 5, 5, 5, 4, 5, 4, 5, 7, 3, 4, 4, 4, 5, 4, 4, 5, 5, 4,
        4, 3, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 9, 5, 4, 5, 4, 4, 5, 4,
        4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 2, 5, 4, 5, 5, 4,
        5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 8, 5, 5, 4, 4,
        5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 6, 5, 4, 4, 1, 5, 4, 7, 4, 5, 4,
        4, 4, 5, 4, 2, 4, 4, 5])
soft_pseudo_label
tensor([[1.7513e-03, 2.0575e-03, 1.7754e-03,  ..., 1.9001e-03, 1.8945e-03,
         1.9508e-03],
        [1.6172e-03, 1.4489e-03, 1.4993e-03,  ..., 1.4589e-03, 1.6148e-03,
         1.5052e-03],
        [3.8050e-05, 3.6771e-05, 3.5122e-05,  ..., 3.7662e-05, 3.7496e-05,
         4.1202e-05],
        ...,
        [1.9278e-03, 1.9707e-03, 1.8878e-03,  ..., 1.9515e-03, 2.0663e-03,
         2.0263e-03],
        [5.5012e-03, 6.0316e-03, 5.6138e-03,  ..., 5.5362e-03, 5.2123e-03,
         5.7835e-03],
        [6.6387e-03, 6.5903e-03, 5.8529e-03,  ..., 5.6397e-03, 6.1159e-03,
         5.8358e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5,
        4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5,
        4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4,
        5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4,
        4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4,
        5, 4, 5, 5, 4, 4, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4]
original label
tensor([4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 7, 4, 4, 5, 5, 4, 5, 5,
        4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 1, 5, 4, 5, 4, 4, 5, 4, 5,
        4, 4, 4, 2, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 1, 9, 5, 4, 5, 5, 4,
        5, 5, 5, 4, 5, 4, 3, 5, 8, 5, 4, 4, 4, 9, 4, 5, 5, 5, 5, 4, 6, 5, 5, 5,
        4, 4, 5, 5, 4, 4, 5, 4, 7, 4, 5, 4, 4, 5, 5, 4, 9, 5, 5, 4, 4, 4, 5, 3,
        5, 4, 9, 5, 4, 5, 5, 5])
soft_pseudo_label
tensor([[0.0014, 0.0015, 0.0015,  ..., 0.0015, 0.0015, 0.0014],
        [0.0019, 0.0020, 0.0021,  ..., 0.0019, 0.0020, 0.0020],
        [0.0014, 0.0014, 0.0014,  ..., 0.0013, 0.0014, 0.0014],
        ...,
        [0.0019, 0.0019, 0.0019,  ..., 0.0018, 0.0019, 0.0017],
        [0.0033, 0.0034, 0.0033,  ..., 0.0030, 0.0032, 0.0033],
        [0.0028, 0.0028, 0.0027,  ..., 0.0029, 0.0027, 0.0026]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4,
        5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5,
        5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4,
        4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4,
        5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5,
        4, 4, 5, 4, 5, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5]
original label
tensor([6, 4, 5, 5, 5, 5, 5, 5, 4, 9, 2, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 3, 5, 2,
        5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 3,
        5, 6, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4,
        4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4,
        5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5,
        4, 4, 5, 4, 5, 4, 4, 5])
soft_pseudo_label
tensor([[0.0004, 0.0004, 0.0003,  ..., 0.0004, 0.0004, 0.0004],
        [0.0015, 0.0017, 0.0016,  ..., 0.0016, 0.0017, 0.0016],
        [0.0032, 0.0034, 0.0034,  ..., 0.0030, 0.0031, 0.0035],
        ...,
        [0.0046, 0.0049, 0.0043,  ..., 0.0048, 0.0047, 0.0051],
        [0.0029, 0.0030, 0.0027,  ..., 0.0026, 0.0028, 0.0028],
        [0.0021, 0.0026, 0.0024,  ..., 0.0026, 0.0024, 0.0025]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4,
        4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5,
        4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5,
        4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5,
        5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5,
        5, 4, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5]
original label
tensor([5, 5, 4, 5, 9, 2, 5, 9, 4, 5, 4, 4, 2, 4, 5, 5, 4, 4, 5, 4, 5, 7, 4, 4,
        4, 5, 4, 5, 4, 1, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5,
        4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 2, 5, 4, 4, 5, 5, 4, 5, 5, 5, 2, 4, 5,
        4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4,
        5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 2, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5,
        5, 4, 8, 5, 9, 4, 5, 5])
soft_pseudo_label
tensor([[0.0009, 0.0009, 0.0008,  ..., 0.0008, 0.0009, 0.0009],
        [0.0020, 0.0022, 0.0021,  ..., 0.0022, 0.0018, 0.0022],
        [0.0018, 0.0016, 0.0017,  ..., 0.0016, 0.0018, 0.0016],
        ...,
        [0.0014, 0.0014, 0.0015,  ..., 0.0014, 0.0015, 0.0012],
        [0.0018, 0.0016, 0.0015,  ..., 0.0015, 0.0014, 0.0015],
        [0.0045, 0.0053, 0.0051,  ..., 0.0049, 0.0045, 0.0050]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4,
        4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4,
        5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5,
        5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4,
        4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4,
        5, 5, 5, 4, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5]
original label
tensor([4, 5, 4, 5, 4, 5, 5, 5, 5, 7, 0, 4, 5, 5, 4, 5, 4, 5, 4, 5, 2, 4, 5, 4,
        4, 4, 5, 5, 4, 4, 5, 5, 2, 5, 4, 5, 4, 5, 5, 5, 4, 4, 9, 5, 5, 4, 6, 4,
        5, 7, 5, 9, 5, 5, 4, 3, 9, 4, 4, 4, 5, 5, 5, 5, 4, 7, 5, 4, 5, 5, 5, 5,
        0, 6, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 0, 4, 6, 5, 4, 4, 4, 4, 4, 5, 5, 4,
        4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 6, 4, 5, 4, 4, 2, 5, 5, 4,
        5, 5, 5, 4, 4, 5, 4, 5])
soft_pseudo_label
tensor([[1.3169e-03, 1.2826e-03, 1.3943e-03,  ..., 1.3370e-03, 1.4100e-03,
         1.3143e-03],
        [2.9281e-03, 2.9917e-03, 3.0418e-03,  ..., 3.0867e-03, 3.1661e-03,
         3.0108e-03],
        [1.4482e-03, 1.6616e-03, 1.5507e-03,  ..., 1.6374e-03, 1.6148e-03,
         1.6066e-03],
        ...,
        [4.6962e-03, 4.7701e-03, 4.7100e-03,  ..., 4.6870e-03, 4.4593e-03,
         4.4354e-03],
        [2.7258e-05, 2.7850e-05, 2.4601e-05,  ..., 2.6035e-05, 2.6626e-05,
         2.5933e-05],
        [1.7569e-03, 2.2850e-03, 2.0966e-03,  ..., 2.0356e-03, 1.9884e-03,
         2.2204e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5,
        4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5,
        5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5,
        4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4,
        5, 4, 4, 4, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5]
original label
tensor([4, 3, 5, 0, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 8, 4, 5, 5, 5, 4, 4, 4, 5,
        4, 0, 4, 5, 7, 4, 4, 1, 1, 8, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 9, 5,
        4, 4, 4, 5, 4, 4, 5, 1, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5,
        5, 5, 4, 5, 4, 4, 5, 4, 2, 5, 5, 4, 5, 4, 6, 5, 8, 5, 5, 4, 5, 5, 4, 5,
        4, 4, 4, 5, 4, 4, 5, 2, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 8,
        5, 4, 4, 4, 5, 5, 4, 5])
soft_pseudo_label
tensor([[0.0015, 0.0014, 0.0015,  ..., 0.0016, 0.0017, 0.0015],
        [0.0040, 0.0041, 0.0039,  ..., 0.0039, 0.0035, 0.0038],
        [0.0032, 0.0032, 0.0033,  ..., 0.0031, 0.0033, 0.0030],
        ...,
        [0.0019, 0.0018, 0.0019,  ..., 0.0018, 0.0020, 0.0018],
        [0.0020, 0.0015, 0.0018,  ..., 0.0019, 0.0017, 0.0015],
        [0.0029, 0.0034, 0.0032,  ..., 0.0032, 0.0030, 0.0036]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5,
        4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4,
        4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4,
        4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4,
        4, 4, 5, 4, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5]
original label
tensor([4, 4, 4, 4, 5, 2, 5, 4, 5, 5, 4, 4, 5, 5, 7, 4, 3, 4, 5, 4, 5, 4, 5, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 8, 5, 4,
        4, 5, 5, 4, 5, 5, 5, 5, 5, 8, 4, 4, 5, 7, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4,
        4, 4, 4, 5, 4, 5, 1, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 1, 4, 5, 4,
        4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 9, 4, 0, 4, 4,
        5, 4, 5, 4, 4, 4, 4, 5])
soft_pseudo_label
tensor([[3.0425e-03, 3.0248e-03, 3.3481e-03,  ..., 3.0381e-03, 3.3221e-03,
         3.1870e-03],
        [1.8411e-03, 2.0342e-03, 1.8651e-03,  ..., 1.8526e-03, 1.8219e-03,
         1.8845e-03],
        [3.5648e-03, 3.9726e-03, 3.4628e-03,  ..., 3.6915e-03, 3.6701e-03,
         3.3778e-03],
        ...,
        [4.9989e-05, 4.7723e-05, 5.2967e-05,  ..., 4.7957e-05, 4.8145e-05,
         4.9334e-05],
        [1.1019e-03, 1.1873e-03, 1.2179e-03,  ..., 1.1792e-03, 1.0763e-03,
         1.1661e-03],
        [2.4742e-03, 2.5987e-03, 2.4526e-03,  ..., 2.8402e-03, 2.2939e-03,
         2.9555e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4,
        4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4,
        4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5,
        5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5,
        4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4,
        5, 4, 5, 5, 5, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5]
original label
tensor([5, 5, 5, 4, 4, 5, 3, 4, 4, 4, 4, 5, 8, 4, 4, 5, 5, 5, 6, 5, 4, 4, 4, 4,
        4, 4, 4, 5, 5, 5, 5, 3, 4, 5, 5, 4, 5, 5, 5, 7, 4, 4, 5, 4, 4, 0, 0, 5,
        4, 4, 4, 4, 7, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 6, 5, 5, 5, 5, 4, 5, 5,
        5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 3, 5, 5, 5, 5, 5, 4, 2, 5, 4, 7, 4, 4, 5,
        4, 5, 5, 2, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 2, 5, 5, 4, 4, 5, 2, 4, 4,
        5, 4, 5, 5, 5, 2, 5, 4])
soft_pseudo_label
tensor([[0.0012, 0.0011, 0.0012,  ..., 0.0011, 0.0011, 0.0009],
        [0.0022, 0.0021, 0.0022,  ..., 0.0023, 0.0023, 0.0021],
        [0.0001, 0.0002, 0.0001,  ..., 0.0001, 0.0001, 0.0001],
        ...,
        [0.0017, 0.0017, 0.0017,  ..., 0.0017, 0.0016, 0.0015],
        [0.0018, 0.0020, 0.0019,  ..., 0.0019, 0.0019, 0.0019],
        [0.0042, 0.0039, 0.0044,  ..., 0.0042, 0.0038, 0.0040]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5,
        4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5,
        5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4,
        5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4,
        4, 4, 5, 4, 5, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4]
original label
tensor([4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 1, 4, 5, 4, 4, 4, 4, 5, 5, 5, 7,
        4, 5, 5, 9, 4, 4, 4, 5, 5, 2, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 3, 5, 5,
        5, 4, 0, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 0, 4, 5, 4, 5, 4, 5, 4, 5, 3,
        5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 7, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5,
        5, 5, 5, 5, 5, 5, 6, 0, 4, 5, 9, 5, 4, 5, 5, 5, 4, 5, 5, 5, 9, 4, 5, 4,
        4, 5, 5, 4, 5, 5, 5, 4])
soft_pseudo_label
tensor([[0.0020, 0.0021, 0.0021,  ..., 0.0019, 0.0020, 0.0019],
        [0.0019, 0.0018, 0.0022,  ..., 0.0019, 0.0018, 0.0019],
        [0.0013, 0.0017, 0.0015,  ..., 0.0016, 0.0014, 0.0016],
        ...,
        [0.0029, 0.0030, 0.0029,  ..., 0.0027, 0.0030, 0.0028],
        [0.0019, 0.0021, 0.0024,  ..., 0.0021, 0.0020, 0.0023],
        [0.0013, 0.0015, 0.0014,  ..., 0.0014, 0.0014, 0.0014]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5,
        4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5,
        5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4,
        4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4,
        5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5,
        5, 4, 4, 4, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5]
original label
tensor([2, 5, 5, 5, 4, 8, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 3, 4, 1,
        5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 6, 4, 4, 4, 5,
        5, 5, 5, 9, 5, 4, 4, 4, 4, 5, 4, 5, 6, 2, 5, 4, 4, 4, 8, 5, 4, 5, 4, 4,
        4, 4, 5, 4, 5, 3, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 7, 4,
        5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 9, 5, 4, 5, 4, 5, 4, 5, 4, 5,
        3, 4, 1, 4, 4, 6, 4, 5])
soft_pseudo_label
tensor([[0.0023, 0.0024, 0.0025,  ..., 0.0024, 0.0025, 0.0025],
        [0.0029, 0.0030, 0.0033,  ..., 0.0029, 0.0033, 0.0027],
        [0.0024, 0.0023, 0.0026,  ..., 0.0024, 0.0023, 0.0024],
        ...,
        [0.0029, 0.0029, 0.0031,  ..., 0.0029, 0.0028, 0.0027],
        [0.0043, 0.0039, 0.0042,  ..., 0.0038, 0.0042, 0.0040],
        [0.0015, 0.0017, 0.0017,  ..., 0.0015, 0.0014, 0.0017]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5,
        4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4,
        4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5,
        4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5,
        4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5,
        4, 5, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5]
original label
tensor([5, 4, 5, 0, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5,
        4, 5, 4, 5, 4, 4, 4, 1, 4, 5, 4, 5, 7, 4, 3, 4, 5, 4, 5, 4, 4, 4, 5, 4,
        4, 5, 4, 4, 3, 4, 5, 5, 4, 2, 5, 4, 1, 4, 5, 5, 5, 4, 4, 4, 4, 6, 4, 4,
        4, 2, 4, 4, 4, 4, 5, 6, 6, 4, 4, 5, 5, 6, 4, 5, 5, 4, 5, 4, 4, 6, 3, 4,
        4, 3, 4, 5, 4, 4, 4, 5, 5, 5, 4, 0, 4, 5, 5, 4, 5, 6, 5, 5, 5, 5, 4, 5,
        4, 5, 5, 4, 2, 4, 5, 5])
soft_pseudo_label
tensor([[0.0013, 0.0013, 0.0014,  ..., 0.0013, 0.0015, 0.0013],
        [0.0018, 0.0018, 0.0017,  ..., 0.0016, 0.0018, 0.0019],
        [0.0019, 0.0021, 0.0021,  ..., 0.0020, 0.0024, 0.0023],
        ...,
        [0.0024, 0.0021, 0.0023,  ..., 0.0024, 0.0028, 0.0020],
        [0.0018, 0.0021, 0.0020,  ..., 0.0018, 0.0018, 0.0020],
        [0.0012, 0.0011, 0.0012,  ..., 0.0013, 0.0012, 0.0011]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5,
        5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4,
        4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5,
        4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5,
        5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4,
        4, 4, 4, 5, 5, 4, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4]
original label
tensor([4, 4, 4, 5, 4, 5, 5, 8, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5,
        5, 5, 9, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 7, 5, 9, 5, 5, 5, 4, 5, 0,
        4, 4, 4, 5, 5, 5, 3, 9, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 3, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5,
        0, 4, 5, 5, 4, 2, 0, 8, 4, 5, 6, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4,
        4, 4, 4, 3, 5, 4, 5, 4])
soft_pseudo_label
tensor([[0.0027, 0.0025, 0.0025,  ..., 0.0025, 0.0026, 0.0023],
        [0.0024, 0.0027, 0.0026,  ..., 0.0024, 0.0023, 0.0026],
        [0.0030, 0.0031, 0.0033,  ..., 0.0032, 0.0027, 0.0041],
        ...,
        [0.0026, 0.0026, 0.0024,  ..., 0.0024, 0.0024, 0.0025],
        [0.0015, 0.0015, 0.0016,  ..., 0.0015, 0.0016, 0.0018],
        [0.0002, 0.0002, 0.0002,  ..., 0.0001, 0.0001, 0.0002]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4,
        4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4,
        5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4,
        4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4,
        4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4,
        5, 4, 4, 5, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4]
original label
tensor([4, 5, 4, 5, 5, 5, 5, 4, 5, 1, 4, 8, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4,
        3, 5, 5, 4, 5, 4, 4, 4, 4, 5, 0, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4,
        5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 0,
        4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4,
        5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 3, 4, 4, 4, 4, 5, 4, 5, 4, 4, 3, 5, 4,
        5, 4, 4, 5, 4, 5, 5, 4])
soft_pseudo_label
tensor([[0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],
        [0.0029, 0.0029, 0.0031,  ..., 0.0031, 0.0032, 0.0027],
        [0.0051, 0.0056, 0.0054,  ..., 0.0051, 0.0051, 0.0052],
        ...,
        [0.0039, 0.0045, 0.0046,  ..., 0.0043, 0.0040, 0.0049],
        [0.0012, 0.0011, 0.0012,  ..., 0.0011, 0.0012, 0.0012],
        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4,
        5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5,
        5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5,
        4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4,
        4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4,
        4, 5, 5, 5, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5]
original label
tensor([5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 7, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 9,
        5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 2, 4, 5, 6, 4, 5, 4, 5, 5,
        5, 5, 4, 4, 5, 5, 8, 1, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 9, 5, 5, 8, 5,
        4, 5, 4, 9, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 0, 2, 4, 4, 4, 4,
        5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4,
        4, 5, 4, 5, 4, 5, 4, 1])
soft_pseudo_label
tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],
        [0.0014, 0.0012, 0.0015,  ..., 0.0013, 0.0014, 0.0014],
        [0.0021, 0.0025, 0.0023,  ..., 0.0023, 0.0021, 0.0025],
        ...,
        [0.0022, 0.0025, 0.0024,  ..., 0.0025, 0.0024, 0.0025],
        [0.0028, 0.0028, 0.0027,  ..., 0.0025, 0.0028, 0.0027],
        [0.0050, 0.0047, 0.0053,  ..., 0.0048, 0.0056, 0.0047]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5,
        5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4,
        5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5,
        4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5]
original label
tensor([5, 4, 5, 5, 4, 4, 3, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 7, 5, 4, 4, 6, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 9, 5, 4, 4, 5, 3,
        5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 9, 4, 5, 5, 4, 4, 4, 2, 4,
        5, 5, 5, 4, 4, 4, 5, 5, 9, 4, 5, 5, 5, 5, 6, 4, 5, 7, 5, 5, 4, 4, 5, 5,
        4, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 2, 5, 5, 5, 5, 2, 5, 5, 5, 5,
        5, 5, 7, 5, 5, 5, 5, 5])
soft_pseudo_label
tensor([[5.1548e-05, 4.5358e-05, 4.4917e-05,  ..., 5.6947e-05, 4.3365e-05,
         6.0620e-05],
        [1.7063e-03, 2.2503e-03, 1.9292e-03,  ..., 1.8694e-03, 1.8868e-03,
         1.8431e-03],
        [4.6214e-03, 4.5298e-03, 5.3075e-03,  ..., 4.9399e-03, 4.5099e-03,
         4.4033e-03],
        ...,
        [3.3402e-03, 3.7702e-03, 3.4278e-03,  ..., 3.4061e-03, 3.4045e-03,
         3.2965e-03],
        [2.5092e-03, 2.8614e-03, 2.9500e-03,  ..., 2.6321e-03, 2.4636e-03,
         2.7370e-03],
        [2.4685e-03, 2.4090e-03, 2.4818e-03,  ..., 2.4649e-03, 2.4891e-03,
         2.3032e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4,
        4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5,
        5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5,
        5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4,
        4, 5, 4, 4, 5, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4]
original label
tensor([4, 5, 5, 5, 8, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 8, 4,
        4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 2, 5, 4, 9, 5, 5, 5, 4, 5, 4, 5, 5, 5,
        5, 4, 5, 4, 1, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5,
        5, 4, 5, 5, 4, 2, 5, 5, 4, 3, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5,
        0, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 9, 4,
        4, 5, 4, 4, 5, 8, 4, 4])
soft_pseudo_label
tensor([[0.0045, 0.0044, 0.0038,  ..., 0.0044, 0.0046, 0.0043],
        [0.0021, 0.0021, 0.0024,  ..., 0.0023, 0.0021, 0.0026],
        [0.0009, 0.0008, 0.0009,  ..., 0.0009, 0.0009, 0.0008],
        ...,
        [0.0062, 0.0062, 0.0056,  ..., 0.0054, 0.0060, 0.0056],
        [0.0018, 0.0020, 0.0018,  ..., 0.0016, 0.0017, 0.0017],
        [0.0021, 0.0023, 0.0023,  ..., 0.0022, 0.0023, 0.0023]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4,
        5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5,
        5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4,
        4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4,
        5, 4, 4, 5, 4, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5]
original label
tensor([4, 1, 0, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 7, 5, 4, 5, 4, 4, 7, 4,
        5, 5, 0, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 7, 5, 5,
        5, 5, 9, 2, 5, 5, 4, 5, 9, 5, 7, 0, 3, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 7, 5, 4,
        4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 1, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4,
        5, 8, 4, 5, 4, 5, 5, 0])
soft_pseudo_label
tensor([[2.9508e-03, 2.7938e-03, 2.9278e-03,  ..., 2.7397e-03, 2.8754e-03,
         3.0954e-03],
        [2.1011e-04, 2.1011e-04, 2.3189e-04,  ..., 2.1933e-04, 2.0088e-04,
         2.2235e-04],
        [2.2190e-03, 2.2794e-03, 2.2463e-03,  ..., 2.1782e-03, 2.2125e-03,
         2.1434e-03],
        ...,
        [8.1071e-04, 8.4425e-04, 8.0559e-04,  ..., 7.9000e-04, 8.9826e-04,
         8.1071e-04],
        [7.1506e-05, 7.7090e-05, 6.6261e-05,  ..., 7.0811e-05, 7.9227e-05,
         7.7241e-05],
        [1.8793e-03, 1.8161e-03, 1.7372e-03,  ..., 1.5174e-03, 1.7906e-03,
         1.6763e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5,
        5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4,
        4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5,
        5, 4, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5]
original label
tensor([5, 4, 5, 5, 5, 5, 8, 7, 4, 5, 4, 6, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5,
        5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 0, 4, 2, 5, 4, 4, 4, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5,
        5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 3, 5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5,
        1, 4, 5, 3, 4, 5, 4, 5])
soft_pseudo_label
tensor([[0.0051, 0.0053, 0.0047,  ..., 0.0046, 0.0047, 0.0044],
        [0.0022, 0.0021, 0.0022,  ..., 0.0022, 0.0021, 0.0021],
        [0.0014, 0.0013, 0.0014,  ..., 0.0014, 0.0014, 0.0014],
        ...,
        [0.0008, 0.0008, 0.0009,  ..., 0.0008, 0.0008, 0.0008],
        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],
        [0.0022, 0.0023, 0.0023,  ..., 0.0023, 0.0025, 0.0023]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4,
        5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5,
        4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5,
        4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4,
        4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5,
        4, 5, 5, 4, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4]
original label
tensor([1, 4, 3, 5, 5, 4, 4, 5, 4, 5, 9, 5, 5, 4, 4, 5, 3, 4, 8, 5, 5, 4, 4, 4,
        5, 5, 5, 4, 5, 2, 4, 4, 5, 5, 4, 4, 4, 4, 3, 4, 4, 5, 5, 4, 5, 5, 4, 5,
        4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 9, 3, 2, 5, 4, 4, 5, 5,
        4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 3,
        4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 7, 5, 5,
        4, 5, 5, 4, 4, 5, 5, 6])
soft_pseudo_label
tensor([[0.0017, 0.0019, 0.0018,  ..., 0.0020, 0.0017, 0.0017],
        [0.0102, 0.0128, 0.0117,  ..., 0.0116, 0.0116, 0.0119],
        [0.0024, 0.0022, 0.0022,  ..., 0.0020, 0.0021, 0.0022],
        ...,
        [0.0015, 0.0016, 0.0016,  ..., 0.0014, 0.0015, 0.0015],
        [0.0015, 0.0016, 0.0017,  ..., 0.0016, 0.0016, 0.0016],
        [0.0018, 0.0020, 0.0018,  ..., 0.0017, 0.0017, 0.0019]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5,
        4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5,
        5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5,
        4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5,
        5, 4, 5, 5, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4]
original label
tensor([5, 5, 5, 5, 9, 4, 4, 5, 4, 8, 5, 1, 5, 5, 1, 5, 4, 5, 4, 4, 7, 5, 5, 5,
        4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5,
        5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5,
        4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 7, 4, 5, 5, 4, 4, 4, 5, 5, 4,
        5, 4, 5, 5, 6, 5, 5, 9])
soft_pseudo_label
tensor([[0.0014, 0.0015, 0.0015,  ..., 0.0014, 0.0015, 0.0014],
        [0.0042, 0.0040, 0.0040,  ..., 0.0038, 0.0038, 0.0038],
        [0.0027, 0.0027, 0.0022,  ..., 0.0024, 0.0024, 0.0027],
        ...,
        [0.0054, 0.0042, 0.0055,  ..., 0.0052, 0.0047, 0.0054],
        [0.0016, 0.0016, 0.0018,  ..., 0.0017, 0.0017, 0.0017],
        [0.0035, 0.0036, 0.0036,  ..., 0.0036, 0.0035, 0.0037]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4,
        5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5,
        5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4,
        5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5,
        5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5]
original label
tensor([5, 1, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 9, 5, 3, 5, 5, 5, 5, 4,
        5, 5, 4, 5, 5, 7, 5, 5, 7, 5, 5, 5, 5, 5, 5, 4, 4, 5, 6, 4, 5, 5, 4, 5,
        4, 8, 5, 4, 4, 5, 5, 3, 4, 5, 4, 5, 4, 2, 5, 0, 5, 5, 4, 5, 1, 5, 5, 4,
        5, 5, 4, 5, 4, 7, 8, 1, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 9, 4, 5, 5, 5,
        5, 4, 5, 3, 4, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 4, 5])
soft_pseudo_label
tensor([[0.0038, 0.0037, 0.0036,  ..., 0.0038, 0.0035, 0.0035],
        [0.0017, 0.0017, 0.0019,  ..., 0.0018, 0.0016, 0.0019],
        [0.0026, 0.0024, 0.0028,  ..., 0.0025, 0.0025, 0.0024],
        ...,
        [0.0017, 0.0015, 0.0015,  ..., 0.0016, 0.0016, 0.0015],
        [0.0007, 0.0007, 0.0007,  ..., 0.0007, 0.0007, 0.0007],
        [0.0061, 0.0072, 0.0069,  ..., 0.0059, 0.0056, 0.0074]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4,
        4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5,
        5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4,
        5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5,
        4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4,
        5, 5, 5, 5, 4, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4]
original label
tensor([5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 1, 5, 4, 9, 5, 5, 5, 5, 4, 5, 4, 7, 4,
        4, 5, 4, 4, 4, 5, 4, 9, 4, 4, 4, 4, 4, 4, 4, 9, 8, 5, 5, 5, 4, 5, 5, 5,
        5, 4, 5, 4, 4, 5, 3, 4, 5, 5, 5, 5, 0, 5, 4, 3, 5, 4, 5, 5, 7, 5, 5, 4,
        5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 3, 5, 4, 4, 5, 4, 4, 4,
        4, 8, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4,
        5, 5, 5, 0, 4, 4, 4, 4])
soft_pseudo_label
tensor([[0.0007, 0.0007, 0.0007,  ..., 0.0008, 0.0007, 0.0007],
        [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0022, 0.0020],
        [0.0035, 0.0036, 0.0036,  ..., 0.0034, 0.0035, 0.0037],
        ...,
        [0.0021, 0.0021, 0.0019,  ..., 0.0020, 0.0021, 0.0021],
        [0.0023, 0.0020, 0.0021,  ..., 0.0022, 0.0024, 0.0019],
        [0.0026, 0.0033, 0.0028,  ..., 0.0031, 0.0027, 0.0028]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4,
        4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5,
        4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5,
        5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5,
        5, 4, 4, 4, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5]
original label
tensor([5, 4, 5, 4, 5, 4, 4, 3, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4,
        4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4,
        5, 4, 5, 7, 4, 5, 4, 4, 4, 9, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5,
        4, 4, 5, 6, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 0, 5, 5, 4, 4, 5, 6, 4, 4, 5,
        5, 4, 4, 5, 4, 4, 5, 5, 5, 0, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 8, 5, 5,
        5, 4, 4, 4, 9, 5, 4, 5])
soft_pseudo_label
tensor([[4.0175e-05, 3.9015e-05, 3.9707e-05,  ..., 3.7011e-05, 3.8186e-05,
         4.8037e-05],
        [2.6972e-03, 2.7781e-03, 2.7727e-03,  ..., 2.6302e-03, 2.5178e-03,
         2.5612e-03],
        [1.7783e-03, 1.9041e-03, 1.8582e-03,  ..., 1.7844e-03, 1.8267e-03,
         1.8019e-03],
        ...,
        [5.0884e-03, 4.3360e-03, 4.6466e-03,  ..., 4.9716e-03, 4.1720e-03,
         5.6629e-03],
        [1.0110e-04, 9.8565e-05, 9.3410e-05,  ..., 1.0012e-04, 1.0169e-04,
         9.6847e-05],
        [1.9080e-03, 2.1620e-03, 1.9985e-03,  ..., 1.8747e-03, 2.0015e-03,
         2.0670e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4,
        4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5,
        4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5,
        5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4,
        4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5,
        4, 4, 4, 4, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5]
original label
tensor([4, 5, 5, 1, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 0, 8, 5, 5, 4, 0,
        4, 5, 5, 4, 5, 2, 4, 0, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 6, 5, 5, 5, 4, 5,
        5, 5, 3, 4, 4, 4, 4, 5, 1, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 4, 5, 9,
        4, 4, 5, 5, 4, 4, 4, 4, 5, 3, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 1, 5,
        4, 0, 4, 4, 5, 4, 5, 5])
soft_pseudo_label
tensor([[0.0024, 0.0025, 0.0025,  ..., 0.0024, 0.0025, 0.0024],
        [0.0002, 0.0001, 0.0002,  ..., 0.0001, 0.0001, 0.0001],
        [0.0023, 0.0023, 0.0023,  ..., 0.0020, 0.0022, 0.0023],
        ...,
        [0.0016, 0.0016, 0.0015,  ..., 0.0016, 0.0018, 0.0015],
        [0.0018, 0.0018, 0.0018,  ..., 0.0017, 0.0018, 0.0019],
        [0.0036, 0.0041, 0.0037,  ..., 0.0036, 0.0040, 0.0038]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4,
        4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4,
        5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4,
        4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4,
        5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4,
        5, 4, 4, 4, 5, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5]
original label
tensor([4, 5, 4, 9, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4,
        4, 5, 4, 5, 5, 4, 5, 8, 4, 5, 8, 5, 5, 8, 4, 5, 4, 5, 4, 2, 4, 3, 5, 4,
        5, 5, 5, 4, 4, 5, 5, 2, 5, 4, 4, 2, 9, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4,
        4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 2, 5, 4, 4, 4, 5, 5, 4,
        5, 5, 7, 5, 4, 4, 4, 5, 0, 5, 4, 4, 4, 4, 0, 9, 4, 4, 4, 5, 4, 4, 5, 4,
        5, 4, 4, 4, 5, 5, 5, 5])
soft_pseudo_label
tensor([[5.3129e-03, 5.3807e-03, 5.5191e-03,  ..., 5.5083e-03, 5.2973e-03,
         5.2254e-03],
        [5.7658e-05, 5.3901e-05, 5.2962e-05,  ..., 6.4072e-05, 5.6654e-05,
         6.3947e-05],
        [9.2150e-04, 1.0609e-03, 1.0779e-03,  ..., 1.0779e-03, 1.0108e-03,
         1.1173e-03],
        ...,
        [2.6364e-03, 2.8167e-03, 2.8291e-03,  ..., 2.5317e-03, 2.6956e-03,
         2.7602e-03],
        [1.1320e-03, 1.2032e-03, 1.1582e-03,  ..., 1.0029e-03, 1.1921e-03,
         1.0603e-03],
        [1.4317e-03, 1.2518e-03, 1.3417e-03,  ..., 1.3515e-03, 1.2777e-03,
         1.3221e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5,
        5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5,
        4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5,
        5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5,
        4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4,
        5, 4, 4, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5]
original label
tensor([5, 4, 5, 5, 4, 8, 5, 5, 4, 4, 6, 5, 5, 5, 9, 3, 4, 5, 4, 5, 5, 5, 2, 5,
        5, 4, 4, 4, 0, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 9, 4, 4, 5,
        7, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 4, 5,
        5, 2, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 8, 4, 5,
        9, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 6, 4, 5, 4,
        3, 4, 4, 2, 5, 5, 4, 5])
soft_pseudo_label
tensor([[0.0040, 0.0046, 0.0043,  ..., 0.0044, 0.0039, 0.0051],
        [0.0023, 0.0023, 0.0023,  ..., 0.0022, 0.0021, 0.0023],
        [0.0004, 0.0004, 0.0003,  ..., 0.0004, 0.0004, 0.0004],
        ...,
        [0.0054, 0.0056, 0.0054,  ..., 0.0053, 0.0056, 0.0053],
        [0.0013, 0.0013, 0.0012,  ..., 0.0011, 0.0012, 0.0012],
        [0.0031, 0.0031, 0.0032,  ..., 0.0035, 0.0031, 0.0036]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4,
        4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4,
        5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4,
        5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5,
        4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4,
        4, 4, 5, 4, 5, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4]
original label
tensor([5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4,
        4, 5, 4, 4, 5, 4, 5, 5, 6, 5, 5, 4, 4, 5, 4, 5, 4, 7, 4, 5, 5, 8, 4, 4,
        5, 5, 4, 4, 4, 5, 4, 4, 9, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4,
        5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 6, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5,
        4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4,
        4, 4, 5, 4, 5, 4, 6, 4])
soft_pseudo_label
tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0009, 0.0010, 0.0010],
        [0.0024, 0.0025, 0.0026,  ..., 0.0024, 0.0024, 0.0025],
        [0.0015, 0.0017, 0.0015,  ..., 0.0013, 0.0015, 0.0015],
        ...,
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0017, 0.0017, 0.0018,  ..., 0.0016, 0.0016, 0.0018],
        [0.0016, 0.0016, 0.0017,  ..., 0.0015, 0.0016, 0.0017]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4,
        4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5,
        5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4,
        4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5,
        4, 5, 5, 4, 4, 5, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5]
original label
tensor([6, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 9, 6, 4, 5, 5, 4,
        4, 0, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 0, 5, 4, 4, 5, 4, 5, 4, 5,
        5, 4, 7, 4, 6, 0, 5, 4, 5, 5, 5, 4, 4, 6, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4,
        4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 7, 5, 5, 4, 3, 5, 4, 4, 5, 4, 4, 5, 4, 2,
        4, 5, 5, 7, 4, 5, 4, 4, 5, 0, 5, 4, 5, 4, 5, 4, 5, 5, 3, 4, 5, 5, 4, 5,
        4, 5, 5, 4, 4, 5, 5, 5])
soft_pseudo_label
tensor([[1.4058e-03, 1.3732e-03, 1.3799e-03,  ..., 1.3155e-03, 1.3685e-03,
         1.4244e-03],
        [1.2346e-03, 1.2274e-03, 1.3766e-03,  ..., 1.2262e-03, 1.2154e-03,
         1.3752e-03],
        [4.4657e-03, 5.2581e-03, 4.8605e-03,  ..., 4.8593e-03, 4.7887e-03,
         4.7712e-03],
        ...,
        [1.2934e-03, 1.2335e-03, 1.2112e-03,  ..., 1.2356e-03, 1.2206e-03,
         1.2287e-03],
        [1.9226e-03, 2.0296e-03, 2.0178e-03,  ..., 1.9826e-03, 1.9519e-03,
         2.2971e-03],
        [1.7901e-05, 1.5705e-05, 1.4783e-05,  ..., 1.8361e-05, 1.5614e-05,
         1.7555e-05]], device='cuda:0')
hard_pseudo_label
tensor([5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5,
        4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5,
        4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5,
        4, 5, 4, 5, 5, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 4]
original label
tensor([8, 8, 5, 5, 4, 4, 6, 5, 5, 8, 5, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5,
        4, 5, 5, 5, 4, 4, 5, 5, 3, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 9, 5,
        5, 4, 4, 5, 4, 5, 9, 4, 5, 4, 4, 4, 6, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 4, 8, 5, 4, 5, 4, 5, 5, 5,
        4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5,
        4, 5, 9, 5, 0, 4, 4, 5])
soft_pseudo_label
tensor([[0.0015, 0.0015, 0.0015,  ..., 0.0015, 0.0015, 0.0015],
        [0.0008, 0.0008, 0.0007,  ..., 0.0007, 0.0007, 0.0007],
        [0.0018, 0.0019, 0.0021,  ..., 0.0020, 0.0020, 0.0020],
        ...,
        [0.0062, 0.0068, 0.0063,  ..., 0.0064, 0.0065, 0.0061],
        [0.0026, 0.0026, 0.0028,  ..., 0.0027, 0.0028, 0.0027],
        [0.0016, 0.0016, 0.0016,  ..., 0.0016, 0.0016, 0.0017]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4,
        4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4,
        5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5,
        5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4,
        5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4,
        5, 5, 4, 4, 5, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5]
original label
tensor([5, 7, 4, 5, 4, 4, 5, 4, 4, 5, 4, 2, 5, 0, 5, 5, 1, 8, 4, 9, 5, 5, 4, 4,
        4, 5, 8, 4, 5, 5, 8, 5, 5, 4, 2, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4,
        6, 3, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 2, 4, 4, 3, 4, 2, 5,
        5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 1, 5, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5,
        5, 5, 4, 0, 5, 4, 4, 5, 1, 5, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 7, 2, 0,
        5, 6, 4, 4, 5, 4, 5, 5])
soft_pseudo_label
tensor([[1.2191e-03, 1.2335e-03, 1.2191e-03,  ..., 1.1799e-03, 1.1903e-03,
         1.0896e-03],
        [2.6426e-03, 2.6086e-03, 2.5707e-03,  ..., 2.2725e-03, 2.5482e-03,
         2.3122e-03],
        [2.9545e-04, 2.8137e-04, 2.8636e-04,  ..., 2.8358e-04, 3.0587e-04,
         3.1236e-04],
        ...,
        [1.5022e-05, 1.2490e-05, 1.3718e-05,  ..., 1.3961e-05, 1.3853e-05,
         1.6579e-05],
        [2.3190e-03, 2.3483e-03, 2.3374e-03,  ..., 2.2078e-03, 2.2719e-03,
         2.5214e-03],
        [6.2505e-03, 6.3987e-03, 6.0759e-03,  ..., 5.6248e-03, 6.0789e-03,
         6.1897e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4,
        5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5,
        5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5,
        5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4,
        5, 4, 4, 5, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5]
original label
tensor([4, 4, 4, 4, 5, 5, 5, 4, 5, 7, 4, 5, 5, 4, 5, 4, 5, 5, 8, 4, 5, 4, 7, 4,
        5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 8, 5, 5, 4, 5,
        5, 1, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 1, 4, 5, 4, 4, 4,
        5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 0, 4, 4, 4, 4, 4, 4, 6, 4, 5, 4, 4, 4, 1,
        4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 0, 4, 5, 4, 4, 6, 2, 4,
        5, 4, 4, 5, 4, 4, 5, 5])
soft_pseudo_label
tensor([[0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],
        [0.0018, 0.0018, 0.0018,  ..., 0.0017, 0.0020, 0.0017],
        [0.0017, 0.0018, 0.0018,  ..., 0.0018, 0.0017, 0.0019],
        ...,
        [0.0026, 0.0024, 0.0022,  ..., 0.0023, 0.0024, 0.0022],
        [0.0021, 0.0023, 0.0023,  ..., 0.0021, 0.0022, 0.0021],
        [0.0015, 0.0015, 0.0016,  ..., 0.0014, 0.0016, 0.0014]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4,
        4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5,
        4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4,
        4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5,
        4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4,
        4, 5, 5, 5, 4, 4, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 5, 4]
original label
tensor([4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 8, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4,
        4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 8, 5, 4, 0, 4, 4, 8, 5, 5, 4, 4, 5, 5, 5,
        4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 0, 5, 5, 4, 5, 4, 4, 5, 4,
        4, 4, 5, 4, 2, 5, 5, 5, 5, 5, 4, 5, 4, 4, 1, 4, 4, 9, 4, 6, 5, 4, 4, 5,
        4, 4, 4, 4, 4, 4, 2, 5, 7, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 2, 5, 5, 4,
        4, 5, 5, 5, 4, 4, 5, 4])
soft_pseudo_label
tensor([[0.0038, 0.0039, 0.0042,  ..., 0.0039, 0.0041, 0.0038],
        [0.0067, 0.0072, 0.0063,  ..., 0.0072, 0.0063, 0.0068],
        [0.0008, 0.0009, 0.0008,  ..., 0.0008, 0.0007, 0.0008],
        ...,
        [0.0027, 0.0026, 0.0027,  ..., 0.0027, 0.0025, 0.0025],
        [0.0031, 0.0031, 0.0035,  ..., 0.0030, 0.0032, 0.0031],
        [0.0015, 0.0015, 0.0014,  ..., 0.0015, 0.0014, 0.0014]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5,
        5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4,
        5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4,
        5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4,
        5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5,
        4, 5, 4, 5, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5]
original label
tensor([4, 5, 5, 4, 5, 4, 7, 5, 4, 5, 4, 4, 4, 5, 4, 4, 0, 5, 0, 5, 4, 4, 4, 5,
        4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 1, 5, 4, 4, 5, 5, 4, 5, 4, 8, 4, 4, 5, 4,
        5, 2, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 3, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 5,
        5, 4, 4, 5, 4, 5, 5, 4, 2, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4,
        8, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 2, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5,
        4, 5, 4, 5, 4, 4, 5, 0])
soft_pseudo_label
tensor([[9.5758e-05, 9.4320e-05, 8.5419e-05,  ..., 1.0410e-04, 8.6384e-05,
         1.0502e-04],
        [2.1665e-03, 2.4028e-03, 2.0517e-03,  ..., 2.2418e-03, 2.2017e-03,
         2.3169e-03],
        [2.5703e-03, 2.6125e-03, 2.6898e-03,  ..., 2.4758e-03, 2.5849e-03,
         3.0525e-03],
        ...,
        [2.5202e-03, 2.5636e-03, 2.4570e-03,  ..., 2.4224e-03, 2.3583e-03,
         2.3388e-03],
        [1.9082e-03, 1.7357e-03, 1.7374e-03,  ..., 1.4524e-03, 1.7987e-03,
         1.6652e-03],
        [1.2654e-03, 1.2169e-03, 1.2271e-03,  ..., 1.2294e-03, 1.2397e-03,
         1.1348e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5,
        5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4,
        5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4,
        4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5,
        4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        5, 5, 4, 4, 5, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4]
original label
tensor([4, 5, 4, 4, 5, 5, 5, 4, 6, 4, 4, 5, 5, 4, 4, 4, 7, 5, 5, 5, 0, 4, 5, 3,
        5, 4, 1, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4,
        8, 4, 5, 4, 5, 4, 8, 5, 5, 4, 3, 5, 4, 5, 4, 4, 5, 5, 4, 5, 0, 5, 5, 4,
        4, 4, 5, 4, 4, 2, 8, 4, 5, 5, 6, 6, 4, 5, 4, 4, 5, 5, 4, 6, 5, 5, 4, 5,
        5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4,
        5, 5, 4, 7, 5, 5, 5, 4])
soft_pseudo_label
tensor([[0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0024, 0.0022],
        [0.0014, 0.0014, 0.0015,  ..., 0.0015, 0.0014, 0.0014],
        [0.0035, 0.0035, 0.0035,  ..., 0.0034, 0.0033, 0.0031],
        ...,
        [0.0050, 0.0053, 0.0045,  ..., 0.0043, 0.0042, 0.0039],
        [0.0057, 0.0053, 0.0054,  ..., 0.0056, 0.0057, 0.0055],
        [0.0012, 0.0014, 0.0013,  ..., 0.0013, 0.0014, 0.0013]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4,
        5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4,
        4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4,
        4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4,
        5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5,
        5, 5, 4, 4, 4, 4, 5, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5]
original label
tensor([4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4,
        5, 5, 5, 4, 4, 5, 1, 6, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 9,
        4, 4, 5, 7, 5, 3, 9, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4,
        4, 3, 5, 4, 5, 5, 4, 5, 0, 5, 5, 5, 5, 4, 0, 5, 4, 5, 4, 4, 5, 5, 4, 4,
        5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 7, 4, 5,
        5, 5, 4, 4, 4, 4, 5, 5])
soft_pseudo_label
tensor([[0.0026, 0.0027, 0.0024,  ..., 0.0024, 0.0025, 0.0027],
        [0.0013, 0.0013, 0.0014,  ..., 0.0013, 0.0013, 0.0014],
        [0.0009, 0.0009, 0.0009,  ..., 0.0010, 0.0009, 0.0010],
        ...,
        [0.0030, 0.0029, 0.0028,  ..., 0.0033, 0.0031, 0.0033],
        [0.0027, 0.0026, 0.0026,  ..., 0.0028, 0.0029, 0.0029],
        [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0005]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5,
        4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5,
        5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4,
        4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5,
        5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5,
        4, 4, 5, 4, 4, 4, 4, 4], device='cuda:0')
hard_pseudo_label
[5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4]
original label
tensor([5, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 8, 1, 5, 5, 4, 3, 4, 5, 1, 4, 5, 2, 5,
        4, 8, 5, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 0, 5, 4, 9, 5, 5, 5,
        5, 4, 5, 0, 1, 5, 9, 4, 3, 5, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 4,
        4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 3, 4, 7, 5, 4, 5, 8, 5, 2, 4, 5, 5, 4, 4,
        5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 5, 9, 4, 5, 4, 5, 5, 5, 5,
        4, 4, 5, 4, 4, 4, 4, 4])
soft_pseudo_label
tensor([[0.0020, 0.0021, 0.0023,  ..., 0.0020, 0.0019, 0.0022],
        [0.0021, 0.0020, 0.0019,  ..., 0.0022, 0.0020, 0.0021],
        [0.0008, 0.0008, 0.0008,  ..., 0.0008, 0.0008, 0.0008],
        ...,
        [0.0022, 0.0031, 0.0031,  ..., 0.0029, 0.0026, 0.0031],
        [0.0014, 0.0013, 0.0013,  ..., 0.0012, 0.0013, 0.0011],
        [0.0019, 0.0020, 0.0019,  ..., 0.0017, 0.0018, 0.0019]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4,
        5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4,
        4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5,
        4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4,
        5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5,
        4, 5, 5, 5, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 4, 5]
original label
tensor([4, 4, 5, 4, 4, 5, 6, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4,
        7, 4, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4,
        4, 4, 4, 4, 5, 4, 4, 4, 5, 0, 4, 3, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 0,
        4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 4,
        0, 5, 5, 5, 5, 4, 5, 5, 4, 8, 5, 7, 4, 5, 5, 5, 5, 5, 9, 4, 5, 4, 4, 5,
        4, 5, 5, 5, 5, 5, 4, 5])
soft_pseudo_label
tensor([[3.9127e-03, 4.3638e-03, 4.1084e-03,  ..., 4.2100e-03, 4.3638e-03,
         4.4760e-03],
        [1.0205e-03, 1.1252e-03, 9.6741e-04,  ..., 1.0797e-03, 1.0432e-03,
         9.5567e-04],
        [1.9479e-03, 2.0634e-03, 1.9622e-03,  ..., 1.8833e-03, 2.0776e-03,
         1.8174e-03],
        ...,
        [3.9943e-05, 3.8526e-05, 3.7781e-05,  ..., 4.3528e-05, 3.9826e-05,
         4.3784e-05],
        [2.0116e-03, 1.9631e-03, 1.9346e-03,  ..., 1.9195e-03, 1.9403e-03,
         1.9650e-03],
        [2.5719e-03, 2.9805e-03, 2.7144e-03,  ..., 2.7856e-03, 2.6926e-03,
         2.9522e-03]], device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4,
        4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5,
        5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4,
        5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5,
        4, 5, 5, 4, 5, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 5, 5, 4, 5]
original label
tensor([5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 4, 1, 5, 5, 5, 4, 4, 4,
        5, 4, 2, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5,
        5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 2, 4, 5, 5, 5,
        5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 8, 5, 5, 0, 9, 5, 8, 4, 5, 4, 4, 4, 5, 4,
        5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 0, 4, 5, 4, 4, 5, 4, 4, 5,
        6, 5, 5, 4, 5, 5, 9, 5])
soft_pseudo_label
tensor([[2.9252e-03, 3.1230e-03, 3.4082e-03,  ..., 2.9024e-03, 3.0078e-03,
         3.4619e-03],
        [3.2485e-03, 3.8714e-03, 3.5220e-03,  ..., 3.9113e-03, 3.3722e-03,
         3.8795e-03],
        [3.2344e-05, 3.0001e-05, 3.1533e-05,  ..., 3.6615e-05, 2.8965e-05,
         3.2854e-05],
        ...,
        [2.5483e-03, 2.6935e-03, 2.5696e-03,  ..., 2.5558e-03, 2.6094e-03,
         2.6700e-03],
        [5.3333e-03, 5.1970e-03, 5.1415e-03,  ..., 4.7714e-03, 4.9277e-03,
         4.8418e-03],
        [1.1431e-03, 1.2204e-03, 1.1510e-03,  ..., 1.1645e-03, 1.1342e-03,
         1.1015e-03]], device='cuda:0')
hard_pseudo_label
tensor([4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4,
        4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5,
        5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4,
        4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5,
        4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4,
        4, 5, 4, 4, 4, 5, 5, 4], device='cuda:0')
hard_pseudo_label
[4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 5, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4]
original label
tensor([4, 4, 5, 5, 4, 4, 4, 5, 5, 4, 3, 7, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 4,
        4, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 4, 5, 0, 4, 5, 5,
        5, 3, 4, 5, 4, 4, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 2, 5, 5, 4, 5, 5,
        4, 5, 5, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 4, 5, 5, 0, 5, 4, 4, 4, 4, 4, 8,
        4, 5, 6, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 4, 6, 4, 4, 5, 5, 4, 4, 5, 5, 4,
        4, 5, 4, 4, 4, 5, 5, 4])
soft_pseudo_label
tensor([[0.0030, 0.0032, 0.0034,  ..., 0.0036, 0.0036, 0.0035],
        [0.0024, 0.0026, 0.0024,  ..., 0.0025, 0.0022, 0.0027],
        [0.0011, 0.0012, 0.0012,  ..., 0.0012, 0.0011, 0.0012],
        ...,
        [0.0029, 0.0032, 0.0028,  ..., 0.0027, 0.0031, 0.0028],
        [0.0045, 0.0046, 0.0044,  ..., 0.0033, 0.0036, 0.0031],
        [0.0026, 0.0026, 0.0026,  ..., 0.0026, 0.0024, 0.0024]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4,
        4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4,
        5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5,
        4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4,
        5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4]
original label
tensor([4, 5, 5, 5, 4, 6, 5, 4, 5, 4, 4, 5, 5, 5, 5, 5, 9, 4, 4, 5, 6, 0, 5, 4,
        4, 0, 2, 4, 0, 3, 5, 4, 4, 5, 5, 5, 3, 4, 5, 5, 5, 4, 1, 5, 5, 5, 4, 4,
        5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5,
        4, 5, 4, 5, 4, 4, 8, 5, 4, 9, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4,
        5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 3, 5, 4, 5, 5, 8, 4, 5,
        4, 4, 4, 5, 4, 5, 4, 4])
soft_pseudo_label
tensor([[0.0062, 0.0072, 0.0074,  ..., 0.0074, 0.0082, 0.0073],
        [0.0045, 0.0047, 0.0048,  ..., 0.0045, 0.0043, 0.0049],
        [0.0023, 0.0024, 0.0025,  ..., 0.0023, 0.0023, 0.0024],
        ...,
        [0.0019, 0.0019, 0.0020,  ..., 0.0019, 0.0022, 0.0019],
        [0.0081, 0.0090, 0.0092,  ..., 0.0091, 0.0100, 0.0095],
        [0.0008, 0.0009, 0.0009,  ..., 0.0008, 0.0008, 0.0008]],
       device='cuda:0')
hard_pseudo_label
tensor([4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4,
        4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5,
        5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4,
        5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5,
        5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4,
        5, 5, 5, 5, 4, 4, 4, 5], device='cuda:0')
hard_pseudo_label
[4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 5]
original label
tensor([4, 5, 5, 5, 3, 4, 5, 8, 5, 5, 4, 4, 5, 7, 4, 5, 4, 5, 1, 4, 5, 4, 4, 8,
        4, 4, 4, 5, 2, 4, 2, 3, 4, 5, 5, 5, 4, 5, 9, 4, 5, 1, 4, 5, 5, 4, 5, 5,
        5, 5, 4, 5, 4, 5, 5, 8, 4, 5, 4, 5, 5, 5, 4, 4, 5, 2, 4, 5, 4, 4, 7, 7,
        5, 4, 2, 4, 5, 4, 5, 4, 4, 4, 4, 4, 7, 9, 4, 5, 5, 4, 4, 4, 0, 4, 5, 5,
        5, 4, 5, 4, 4, 5, 5, 5, 9, 4, 5, 4, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4,
        5, 5, 5, 5, 8, 4, 4, 5])
soft_pseudo_label
tensor([[0.0020, 0.0025, 0.0023,  ..., 0.0023, 0.0023, 0.0024],
        [0.0028, 0.0023, 0.0023,  ..., 0.0020, 0.0024, 0.0022],
        [0.0007, 0.0006, 0.0007,  ..., 0.0007, 0.0007, 0.0007],
        ...,
        [0.0011, 0.0011, 0.0012,  ..., 0.0011, 0.0011, 0.0011],
        [0.0048, 0.0045, 0.0047,  ..., 0.0047, 0.0050, 0.0047],
        [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004]],
       device='cuda:0')
hard_pseudo_label
tensor([5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5,
        5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5,
        4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5,
        4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4,
        4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4,
        5, 4, 5, 4, 4, 5, 4, 5], device='cuda:0')
hard_pseudo_label
[5, 5, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5]
original label
tensor([5, 5, 4, 4, 5, 5, 4, 5, 5, 1, 4, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 5,
        5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 8, 5, 4, 4, 5, 4, 4, 5,
        4, 4, 5, 4, 9, 0, 4, 4, 4, 5, 5, 5, 5, 4, 9, 5, 4, 4, 5, 5, 4, 5, 6, 6,
        4, 5, 4, 5, 4, 9, 4, 4, 4, 4, 5, 5, 5, 9, 5, 4, 5, 5, 4, 4, 3, 4, 5, 4,
        4, 5, 4, 5, 4, 5, 4, 2, 5, 5, 4, 4, 5, 2, 4, 7, 5, 5, 2, 5, 4, 5, 5, 4,
        5, 4, 5, 4, 4, 5, 4, 5])
[INFO] main.py:340 > [2-2] Set environment for the current task
[INFO] finetune.py:104 > Apply before_task
[INFO] finetune.py:146 > Reset the optimizer and scheduler states
[INFO] finetune.py:152 > Increasing the head of fc 10 -> 10
[INFO] main.py:348 > [2-3] Start to train under online
[INFO] main.py:363 > Train over streamed data once
batch_size : 128 stream_batch_size : 44 memory_batch_size : 42
[INFO] rainbow_memory.py:119 > Streamed samples: 800
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 9984
[INFO] rainbow_memory.py:127 > Train samples: 11284
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 1/1 | train_loss 1.9088 | train_acc 0.6130 | test_loss 1.1046 | test_acc 0.9660 | lr 0.0050
[INFO] finetune.py:169 > Update memory over 10 classes by uncertainty
uncertainty
[INFO] finetune.py:679 > Compute uncertainty by vr_randaug!
[WARNING] finetune.py:639 > Fill the unused slots by breaking the equilibrium.
[WARNING] finetune.py:650 > Duplicated samples in memory: 4
[INFO] finetune.py:223 > Memory statistic
[INFO] finetune.py:225 > 
truck         61
frog          55
bird          52
automobile    51
dog           50
deer          50
ship          50
airplane      48
cat           47
horse         36
Name: klass, dtype: int64
[INFO] main.py:379 > Train over memory
batch_size : 64 stream_batch_size : 22 memory_batch_size : 21
[INFO] rainbow_memory.py:119 > Streamed samples: 0
[INFO] rainbow_memory.py:120 > In-memory samples: 500
[INFO] rainbow_memory.py:121 > Pseudo samples: 0
[INFO] rainbow_memory.py:127 > Train samples: 500
[INFO] rainbow_memory.py:128 > Test samples: 2000
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 1/256 | train_loss 1.8005 | train_acc 0.2860 | test_loss 0.8248 | test_acc 0.9510 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 2/256 | train_loss 1.1483 | train_acc 0.7500 | test_loss 0.7307 | test_acc 0.7710 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 3/256 | train_loss 1.1659 | train_acc 0.6680 | test_loss 0.6511 | test_acc 0.7805 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 4/256 | train_loss 0.8524 | train_acc 0.7920 | test_loss 0.7901 | test_acc 0.7285 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 5/256 | train_loss 0.8190 | train_acc 0.7760 | test_loss 0.9497 | test_acc 0.6900 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 6/256 | train_loss 0.8092 | train_acc 0.8100 | test_loss 1.1031 | test_acc 0.6170 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 7/256 | train_loss 0.8427 | train_acc 0.7200 | test_loss 0.6452 | test_acc 0.7820 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 8/256 | train_loss 0.6198 | train_acc 0.8320 | test_loss 0.5453 | test_acc 0.8175 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 9/256 | train_loss 0.6321 | train_acc 0.8120 | test_loss 0.7510 | test_acc 0.7525 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 10/256 | train_loss 0.9082 | train_acc 0.7460 | test_loss 1.0605 | test_acc 0.6540 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 11/256 | train_loss 0.8837 | train_acc 0.7880 | test_loss 1.0292 | test_acc 0.6295 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 12/256 | train_loss 0.8820 | train_acc 0.7540 | test_loss 0.5835 | test_acc 0.8085 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 13/256 | train_loss 0.7014 | train_acc 0.8120 | test_loss 0.6507 | test_acc 0.7825 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 14/256 | train_loss 0.9192 | train_acc 0.7780 | test_loss 0.5752 | test_acc 0.8180 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 15/256 | train_loss 0.8367 | train_acc 0.7800 | test_loss 0.5635 | test_acc 0.8160 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 16/256 | train_loss 0.7448 | train_acc 0.7720 | test_loss 0.5638 | test_acc 0.8140 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 17/256 | train_loss 0.5211 | train_acc 0.8560 | test_loss 0.5849 | test_acc 0.8080 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 18/256 | train_loss 0.8655 | train_acc 0.7140 | test_loss 0.8044 | test_acc 0.7500 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 19/256 | train_loss 0.9526 | train_acc 0.7520 | test_loss 0.8177 | test_acc 0.7435 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 20/256 | train_loss 0.7967 | train_acc 0.7900 | test_loss 0.8569 | test_acc 0.7395 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 21/256 | train_loss 0.8716 | train_acc 0.7580 | test_loss 0.8522 | test_acc 0.7190 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 22/256 | train_loss 0.6973 | train_acc 0.8160 | test_loss 0.6201 | test_acc 0.8055 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 23/256 | train_loss 0.6930 | train_acc 0.8060 | test_loss 0.7258 | test_acc 0.7655 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 24/256 | train_loss 0.7194 | train_acc 0.8060 | test_loss 0.9281 | test_acc 0.7245 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 25/256 | train_loss 0.6935 | train_acc 0.8080 | test_loss 0.7988 | test_acc 0.7440 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 26/256 | train_loss 0.5440 | train_acc 0.8300 | test_loss 0.6396 | test_acc 0.7930 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 27/256 | train_loss 0.8109 | train_acc 0.7540 | test_loss 0.6110 | test_acc 0.8030 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 28/256 | train_loss 1.0756 | train_acc 0.6740 | test_loss 0.7367 | test_acc 0.7635 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 29/256 | train_loss 0.6381 | train_acc 0.8400 | test_loss 0.7314 | test_acc 0.7660 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 30/256 | train_loss 0.6826 | train_acc 0.8240 | test_loss 0.5953 | test_acc 0.8175 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 31/256 | train_loss 0.6573 | train_acc 0.8020 | test_loss 0.5930 | test_acc 0.8150 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 32/256 | train_loss 0.6931 | train_acc 0.7900 | test_loss 0.6344 | test_acc 0.8010 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 33/256 | train_loss 0.6963 | train_acc 0.8220 | test_loss 0.8928 | test_acc 0.7090 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 34/256 | train_loss 0.5770 | train_acc 0.8480 | test_loss 0.9440 | test_acc 0.7175 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 35/256 | train_loss 1.0673 | train_acc 0.6760 | test_loss 1.5111 | test_acc 0.5145 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 36/256 | train_loss 0.7850 | train_acc 0.7900 | test_loss 1.4622 | test_acc 0.5535 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 37/256 | train_loss 0.7825 | train_acc 0.8000 | test_loss 1.1494 | test_acc 0.6320 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 38/256 | train_loss 0.8845 | train_acc 0.7420 | test_loss 1.1442 | test_acc 0.6205 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 39/256 | train_loss 0.8275 | train_acc 0.7940 | test_loss 0.9528 | test_acc 0.6955 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 40/256 | train_loss 0.8668 | train_acc 0.7640 | test_loss 1.2796 | test_acc 0.5910 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 41/256 | train_loss 0.7644 | train_acc 0.8100 | test_loss 0.7413 | test_acc 0.7785 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 42/256 | train_loss 0.8293 | train_acc 0.7440 | test_loss 0.8937 | test_acc 0.7140 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 43/256 | train_loss 0.7251 | train_acc 0.8320 | test_loss 0.8138 | test_acc 0.7255 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 44/256 | train_loss 0.7433 | train_acc 0.8420 | test_loss 0.8085 | test_acc 0.7535 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 45/256 | train_loss 0.6602 | train_acc 0.8380 | test_loss 0.9207 | test_acc 0.7095 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 46/256 | train_loss 0.7412 | train_acc 0.7700 | test_loss 0.6094 | test_acc 0.8050 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 47/256 | train_loss 0.7228 | train_acc 0.7960 | test_loss 0.7518 | test_acc 0.7580 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 48/256 | train_loss 0.8669 | train_acc 0.6800 | test_loss 0.5417 | test_acc 0.8295 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 49/256 | train_loss 0.6623 | train_acc 0.8380 | test_loss 0.8867 | test_acc 0.7250 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 50/256 | train_loss 0.8591 | train_acc 0.7380 | test_loss 0.6324 | test_acc 0.7870 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 51/256 | train_loss 0.7508 | train_acc 0.8140 | test_loss 0.5280 | test_acc 0.8335 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 52/256 | train_loss 0.7119 | train_acc 0.7880 | test_loss 0.5532 | test_acc 0.8195 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 53/256 | train_loss 0.6188 | train_acc 0.8800 | test_loss 0.5595 | test_acc 0.8140 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 54/256 | train_loss 0.5333 | train_acc 0.8680 | test_loss 0.6911 | test_acc 0.7735 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 55/256 | train_loss 0.9251 | train_acc 0.7080 | test_loss 0.7615 | test_acc 0.7550 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 56/256 | train_loss 0.4651 | train_acc 0.8960 | test_loss 0.5958 | test_acc 0.8085 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 57/256 | train_loss 0.5322 | train_acc 0.8800 | test_loss 0.5849 | test_acc 0.8115 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 58/256 | train_loss 0.7215 | train_acc 0.7920 | test_loss 0.6282 | test_acc 0.7985 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 59/256 | train_loss 0.7473 | train_acc 0.8700 | test_loss 0.6402 | test_acc 0.7905 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 60/256 | train_loss 0.5751 | train_acc 0.8840 | test_loss 0.6217 | test_acc 0.7955 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 61/256 | train_loss 0.5833 | train_acc 0.8280 | test_loss 0.5778 | test_acc 0.8145 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 62/256 | train_loss 0.4954 | train_acc 0.8680 | test_loss 0.5779 | test_acc 0.8130 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 63/256 | train_loss 0.7418 | train_acc 0.8220 | test_loss 0.5453 | test_acc 0.8285 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 64/256 | train_loss 0.8296 | train_acc 0.7460 | test_loss 0.5858 | test_acc 0.8115 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 65/256 | train_loss 0.6043 | train_acc 0.8380 | test_loss 0.7193 | test_acc 0.7825 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 66/256 | train_loss 0.7429 | train_acc 0.8100 | test_loss 0.7198 | test_acc 0.7665 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 67/256 | train_loss 0.9315 | train_acc 0.7880 | test_loss 0.8735 | test_acc 0.7125 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 68/256 | train_loss 0.8281 | train_acc 0.7740 | test_loss 0.9378 | test_acc 0.6970 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 69/256 | train_loss 0.9092 | train_acc 0.7140 | test_loss 0.7041 | test_acc 0.7590 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 70/256 | train_loss 0.6741 | train_acc 0.8600 | test_loss 0.8551 | test_acc 0.6980 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 71/256 | train_loss 0.6265 | train_acc 0.8020 | test_loss 1.4578 | test_acc 0.5430 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 72/256 | train_loss 0.7590 | train_acc 0.8040 | test_loss 0.6795 | test_acc 0.7830 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 73/256 | train_loss 0.9464 | train_acc 0.7840 | test_loss 0.6140 | test_acc 0.7965 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 74/256 | train_loss 0.7062 | train_acc 0.8380 | test_loss 0.8732 | test_acc 0.7325 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 75/256 | train_loss 0.8150 | train_acc 0.8100 | test_loss 0.8930 | test_acc 0.7100 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 76/256 | train_loss 0.9690 | train_acc 0.7440 | test_loss 0.7839 | test_acc 0.7450 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 77/256 | train_loss 0.7374 | train_acc 0.8100 | test_loss 1.1216 | test_acc 0.6480 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 78/256 | train_loss 0.7314 | train_acc 0.7680 | test_loss 0.8170 | test_acc 0.7455 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 79/256 | train_loss 0.7315 | train_acc 0.7780 | test_loss 0.7041 | test_acc 0.7735 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 80/256 | train_loss 0.7601 | train_acc 0.7740 | test_loss 1.4147 | test_acc 0.5310 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 81/256 | train_loss 0.9065 | train_acc 0.7040 | test_loss 0.8151 | test_acc 0.7460 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 82/256 | train_loss 0.5699 | train_acc 0.8760 | test_loss 0.9955 | test_acc 0.6930 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 83/256 | train_loss 0.7880 | train_acc 0.7720 | test_loss 0.7733 | test_acc 0.7430 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 84/256 | train_loss 0.7121 | train_acc 0.8200 | test_loss 0.6878 | test_acc 0.7710 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 85/256 | train_loss 0.7805 | train_acc 0.7400 | test_loss 0.7440 | test_acc 0.7560 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 86/256 | train_loss 0.9044 | train_acc 0.7260 | test_loss 0.9630 | test_acc 0.6865 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 87/256 | train_loss 0.6774 | train_acc 0.8200 | test_loss 0.7972 | test_acc 0.7620 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 88/256 | train_loss 0.6699 | train_acc 0.7600 | test_loss 0.5075 | test_acc 0.8505 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 89/256 | train_loss 0.6121 | train_acc 0.8020 | test_loss 0.6091 | test_acc 0.8130 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 90/256 | train_loss 0.4724 | train_acc 0.8940 | test_loss 0.7491 | test_acc 0.7700 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 91/256 | train_loss 0.6364 | train_acc 0.8440 | test_loss 0.7167 | test_acc 0.7720 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 92/256 | train_loss 0.4803 | train_acc 0.8060 | test_loss 1.0823 | test_acc 0.6575 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 93/256 | train_loss 0.7042 | train_acc 0.8300 | test_loss 0.6519 | test_acc 0.7875 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 94/256 | train_loss 0.8103 | train_acc 0.7480 | test_loss 0.7511 | test_acc 0.7450 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 95/256 | train_loss 0.6674 | train_acc 0.8200 | test_loss 0.7207 | test_acc 0.7705 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 96/256 | train_loss 0.8063 | train_acc 0.7960 | test_loss 0.8630 | test_acc 0.7265 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 97/256 | train_loss 0.5228 | train_acc 0.8740 | test_loss 0.8823 | test_acc 0.7110 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 98/256 | train_loss 0.7243 | train_acc 0.7740 | test_loss 0.5385 | test_acc 0.8315 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 99/256 | train_loss 0.6889 | train_acc 0.8520 | test_loss 0.7397 | test_acc 0.7565 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 100/256 | train_loss 0.6863 | train_acc 0.8220 | test_loss 0.5484 | test_acc 0.8235 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 101/256 | train_loss 0.9583 | train_acc 0.7180 | test_loss 0.5835 | test_acc 0.8225 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 102/256 | train_loss 0.7877 | train_acc 0.7840 | test_loss 0.7185 | test_acc 0.7640 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 103/256 | train_loss 0.6343 | train_acc 0.7640 | test_loss 0.6039 | test_acc 0.8040 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 104/256 | train_loss 0.6584 | train_acc 0.8180 | test_loss 0.7759 | test_acc 0.7485 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 105/256 | train_loss 0.5282 | train_acc 0.8980 | test_loss 0.7732 | test_acc 0.7415 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 106/256 | train_loss 0.3728 | train_acc 0.8980 | test_loss 0.6765 | test_acc 0.7885 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 107/256 | train_loss 0.5687 | train_acc 0.8580 | test_loss 0.6280 | test_acc 0.8040 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 108/256 | train_loss 0.5535 | train_acc 0.8500 | test_loss 0.9438 | test_acc 0.7030 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 109/256 | train_loss 0.6600 | train_acc 0.8080 | test_loss 0.6465 | test_acc 0.7890 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 110/256 | train_loss 0.5087 | train_acc 0.9040 | test_loss 0.5524 | test_acc 0.8210 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 111/256 | train_loss 0.7454 | train_acc 0.7760 | test_loss 0.7015 | test_acc 0.7705 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 112/256 | train_loss 0.6273 | train_acc 0.7820 | test_loss 0.7127 | test_acc 0.7685 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 113/256 | train_loss 0.5400 | train_acc 0.8340 | test_loss 0.6527 | test_acc 0.7910 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 114/256 | train_loss 0.6797 | train_acc 0.8280 | test_loss 0.6013 | test_acc 0.8095 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 115/256 | train_loss 0.5795 | train_acc 0.8500 | test_loss 0.6323 | test_acc 0.7975 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 116/256 | train_loss 0.6789 | train_acc 0.7920 | test_loss 0.7159 | test_acc 0.7690 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 117/256 | train_loss 0.6217 | train_acc 0.8320 | test_loss 0.6842 | test_acc 0.7900 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 118/256 | train_loss 0.4119 | train_acc 0.8900 | test_loss 0.6773 | test_acc 0.7890 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 119/256 | train_loss 0.4955 | train_acc 0.8520 | test_loss 0.6386 | test_acc 0.8000 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 120/256 | train_loss 0.5761 | train_acc 0.8080 | test_loss 0.6775 | test_acc 0.7875 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 121/256 | train_loss 0.4565 | train_acc 0.8540 | test_loss 0.6587 | test_acc 0.7945 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 122/256 | train_loss 0.7146 | train_acc 0.7940 | test_loss 0.6727 | test_acc 0.7890 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 123/256 | train_loss 0.6685 | train_acc 0.8300 | test_loss 0.6979 | test_acc 0.7795 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 124/256 | train_loss 0.6717 | train_acc 0.8220 | test_loss 0.6712 | test_acc 0.7900 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 125/256 | train_loss 0.3436 | train_acc 0.9200 | test_loss 0.6131 | test_acc 0.8095 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 126/256 | train_loss 0.6053 | train_acc 0.8500 | test_loss 0.6670 | test_acc 0.7930 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 127/256 | train_loss 0.5791 | train_acc 0.8540 | test_loss 0.6334 | test_acc 0.8010 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 128/256 | train_loss 0.4153 | train_acc 0.8680 | test_loss 0.6552 | test_acc 0.7955 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 129/256 | train_loss 0.6465 | train_acc 0.8060 | test_loss 0.5377 | test_acc 0.8330 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 130/256 | train_loss 0.6727 | train_acc 0.8040 | test_loss 0.8201 | test_acc 0.7645 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 131/256 | train_loss 0.6144 | train_acc 0.7640 | test_loss 0.8934 | test_acc 0.7230 | lr 0.0500
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 132/256 | train_loss 0.7500 | train_acc 0.8320 | test_loss 0.9070 | test_acc 0.7045 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 133/256 | train_loss 0.5717 | train_acc 0.8580 | test_loss 0.7160 | test_acc 0.7830 | lr 0.0499
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 134/256 | train_loss 1.0379 | train_acc 0.7000 | test_loss 0.5920 | test_acc 0.8190 | lr 0.0498
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 135/256 | train_loss 0.8162 | train_acc 0.8000 | test_loss 1.2096 | test_acc 0.6180 | lr 0.0497
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 136/256 | train_loss 0.5769 | train_acc 0.8140 | test_loss 1.4404 | test_acc 0.5845 | lr 0.0496
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 137/256 | train_loss 0.7989 | train_acc 0.8100 | test_loss 0.9494 | test_acc 0.7210 | lr 0.0495
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 138/256 | train_loss 0.7217 | train_acc 0.8560 | test_loss 0.4843 | test_acc 0.8545 | lr 0.0494
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 139/256 | train_loss 0.6906 | train_acc 0.8280 | test_loss 1.2802 | test_acc 0.6150 | lr 0.0493
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 140/256 | train_loss 0.6506 | train_acc 0.7900 | test_loss 0.7430 | test_acc 0.7725 | lr 0.0491
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 141/256 | train_loss 0.7273 | train_acc 0.8420 | test_loss 0.7510 | test_acc 0.7690 | lr 0.0489
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 142/256 | train_loss 0.6285 | train_acc 0.8220 | test_loss 0.9707 | test_acc 0.7060 | lr 0.0488
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 143/256 | train_loss 0.8008 | train_acc 0.7520 | test_loss 0.9750 | test_acc 0.6970 | lr 0.0486
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 144/256 | train_loss 0.6789 | train_acc 0.8240 | test_loss 0.9138 | test_acc 0.7030 | lr 0.0483
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 145/256 | train_loss 0.6756 | train_acc 0.8120 | test_loss 0.8669 | test_acc 0.7370 | lr 0.0481
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 146/256 | train_loss 0.9216 | train_acc 0.7360 | test_loss 0.8942 | test_acc 0.7300 | lr 0.0479
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 147/256 | train_loss 0.8155 | train_acc 0.7980 | test_loss 0.9412 | test_acc 0.6880 | lr 0.0476
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 148/256 | train_loss 0.6902 | train_acc 0.8020 | test_loss 0.7831 | test_acc 0.7545 | lr 0.0474
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 149/256 | train_loss 0.8302 | train_acc 0.7960 | test_loss 0.6646 | test_acc 0.8000 | lr 0.0471
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 150/256 | train_loss 0.6382 | train_acc 0.8520 | test_loss 0.7319 | test_acc 0.7715 | lr 0.0468
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 151/256 | train_loss 0.4582 | train_acc 0.8900 | test_loss 0.8687 | test_acc 0.7365 | lr 0.0465
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 152/256 | train_loss 0.8046 | train_acc 0.7920 | test_loss 1.1258 | test_acc 0.6355 | lr 0.0462
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 153/256 | train_loss 0.7145 | train_acc 0.7760 | test_loss 1.0522 | test_acc 0.6665 | lr 0.0458
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 154/256 | train_loss 0.5937 | train_acc 0.8640 | test_loss 0.8335 | test_acc 0.7425 | lr 0.0455
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 155/256 | train_loss 0.6038 | train_acc 0.8260 | test_loss 0.8232 | test_acc 0.7640 | lr 0.0451
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 156/256 | train_loss 0.5166 | train_acc 0.8660 | test_loss 1.2307 | test_acc 0.6545 | lr 0.0448
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 157/256 | train_loss 0.6872 | train_acc 0.7940 | test_loss 1.2871 | test_acc 0.6305 | lr 0.0444
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 158/256 | train_loss 0.6087 | train_acc 0.8300 | test_loss 1.0312 | test_acc 0.7010 | lr 0.0440
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 159/256 | train_loss 0.7650 | train_acc 0.7900 | test_loss 0.4607 | test_acc 0.8565 | lr 0.0436
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 160/256 | train_loss 0.8561 | train_acc 0.7340 | test_loss 0.9692 | test_acc 0.6870 | lr 0.0432
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 161/256 | train_loss 0.7853 | train_acc 0.7900 | test_loss 1.3927 | test_acc 0.6015 | lr 0.0428
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 162/256 | train_loss 0.7215 | train_acc 0.8040 | test_loss 0.8605 | test_acc 0.7170 | lr 0.0423
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 163/256 | train_loss 0.5448 | train_acc 0.8660 | test_loss 0.9357 | test_acc 0.7300 | lr 0.0419
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 164/256 | train_loss 0.6941 | train_acc 0.8480 | test_loss 1.0924 | test_acc 0.6715 | lr 0.0414
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 165/256 | train_loss 0.7969 | train_acc 0.7540 | test_loss 0.7604 | test_acc 0.7535 | lr 0.0410
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 166/256 | train_loss 0.5750 | train_acc 0.8460 | test_loss 0.9458 | test_acc 0.7265 | lr 0.0405
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 167/256 | train_loss 0.8327 | train_acc 0.6980 | test_loss 1.1142 | test_acc 0.6410 | lr 0.0400
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 168/256 | train_loss 0.6660 | train_acc 0.8300 | test_loss 0.8040 | test_acc 0.7545 | lr 0.0395
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 169/256 | train_loss 0.8656 | train_acc 0.7700 | test_loss 0.7092 | test_acc 0.7885 | lr 0.0390
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 170/256 | train_loss 0.5960 | train_acc 0.8560 | test_loss 0.8696 | test_acc 0.7275 | lr 0.0385
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 171/256 | train_loss 0.8887 | train_acc 0.7300 | test_loss 0.7424 | test_acc 0.7620 | lr 0.0380
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 172/256 | train_loss 0.8735 | train_acc 0.7620 | test_loss 0.7959 | test_acc 0.7415 | lr 0.0374
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 173/256 | train_loss 0.4298 | train_acc 0.9020 | test_loss 0.9991 | test_acc 0.6965 | lr 0.0369
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 174/256 | train_loss 0.5486 | train_acc 0.8240 | test_loss 0.5818 | test_acc 0.8210 | lr 0.0364
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 175/256 | train_loss 0.7287 | train_acc 0.7960 | test_loss 0.5927 | test_acc 0.8175 | lr 0.0358
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 176/256 | train_loss 0.6629 | train_acc 0.8540 | test_loss 0.5896 | test_acc 0.8165 | lr 0.0353
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 177/256 | train_loss 0.7709 | train_acc 0.7600 | test_loss 0.7784 | test_acc 0.7555 | lr 0.0347
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 178/256 | train_loss 0.7457 | train_acc 0.8060 | test_loss 0.5910 | test_acc 0.8145 | lr 0.0342
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 179/256 | train_loss 0.6618 | train_acc 0.8700 | test_loss 0.7455 | test_acc 0.7605 | lr 0.0336
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 180/256 | train_loss 0.6011 | train_acc 0.8520 | test_loss 1.0396 | test_acc 0.6790 | lr 0.0330
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 181/256 | train_loss 0.4679 | train_acc 0.9080 | test_loss 0.9255 | test_acc 0.7315 | lr 0.0324
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 182/256 | train_loss 0.5789 | train_acc 0.8580 | test_loss 1.1686 | test_acc 0.6530 | lr 0.0319
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 183/256 | train_loss 0.4595 | train_acc 0.8880 | test_loss 0.6888 | test_acc 0.7985 | lr 0.0313
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 184/256 | train_loss 0.5959 | train_acc 0.8720 | test_loss 0.9147 | test_acc 0.7155 | lr 0.0307
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 185/256 | train_loss 0.6526 | train_acc 0.8760 | test_loss 0.8288 | test_acc 0.7310 | lr 0.0301
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 186/256 | train_loss 0.3975 | train_acc 0.9160 | test_loss 0.8036 | test_acc 0.7505 | lr 0.0295
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 187/256 | train_loss 0.4698 | train_acc 0.8700 | test_loss 1.0844 | test_acc 0.6660 | lr 0.0289
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 188/256 | train_loss 0.8947 | train_acc 0.7000 | test_loss 0.9327 | test_acc 0.6965 | lr 0.0283
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 189/256 | train_loss 0.8172 | train_acc 0.8200 | test_loss 0.9654 | test_acc 0.6885 | lr 0.0277
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 190/256 | train_loss 0.4336 | train_acc 0.8720 | test_loss 1.2091 | test_acc 0.6400 | lr 0.0271
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 191/256 | train_loss 0.7716 | train_acc 0.8320 | test_loss 0.9890 | test_acc 0.6905 | lr 0.0265
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 192/256 | train_loss 0.6320 | train_acc 0.8440 | test_loss 0.6588 | test_acc 0.7880 | lr 0.0259
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 193/256 | train_loss 0.5273 | train_acc 0.8220 | test_loss 0.5583 | test_acc 0.8220 | lr 0.0253
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 194/256 | train_loss 0.4454 | train_acc 0.9020 | test_loss 0.7454 | test_acc 0.7695 | lr 0.0246
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 195/256 | train_loss 0.6987 | train_acc 0.7260 | test_loss 0.6694 | test_acc 0.7850 | lr 0.0240
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 196/256 | train_loss 0.5523 | train_acc 0.8280 | test_loss 0.8932 | test_acc 0.7150 | lr 0.0234
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 197/256 | train_loss 0.7042 | train_acc 0.8480 | test_loss 0.8104 | test_acc 0.7375 | lr 0.0228
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 198/256 | train_loss 0.5430 | train_acc 0.8840 | test_loss 0.5136 | test_acc 0.8395 | lr 0.0222
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 199/256 | train_loss 0.6082 | train_acc 0.8180 | test_loss 0.7212 | test_acc 0.7745 | lr 0.0216
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 200/256 | train_loss 0.5435 | train_acc 0.8200 | test_loss 0.7140 | test_acc 0.7775 | lr 0.0210
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 201/256 | train_loss 0.6883 | train_acc 0.8100 | test_loss 0.7969 | test_acc 0.7465 | lr 0.0204
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 202/256 | train_loss 0.5196 | train_acc 0.8300 | test_loss 0.8203 | test_acc 0.7510 | lr 0.0198
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 203/256 | train_loss 0.5592 | train_acc 0.8800 | test_loss 0.6138 | test_acc 0.8045 | lr 0.0192
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 204/256 | train_loss 0.6533 | train_acc 0.7520 | test_loss 0.7633 | test_acc 0.7580 | lr 0.0186
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 205/256 | train_loss 0.4975 | train_acc 0.8640 | test_loss 0.6089 | test_acc 0.8100 | lr 0.0181
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 206/256 | train_loss 0.6303 | train_acc 0.8260 | test_loss 0.7724 | test_acc 0.7620 | lr 0.0175
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 207/256 | train_loss 0.6914 | train_acc 0.8660 | test_loss 0.7344 | test_acc 0.7765 | lr 0.0169
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 208/256 | train_loss 0.6425 | train_acc 0.8040 | test_loss 0.6020 | test_acc 0.8125 | lr 0.0163
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 209/256 | train_loss 0.4943 | train_acc 0.8840 | test_loss 0.6861 | test_acc 0.7905 | lr 0.0158
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 210/256 | train_loss 0.5351 | train_acc 0.8780 | test_loss 0.5926 | test_acc 0.8325 | lr 0.0152
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 211/256 | train_loss 0.4180 | train_acc 0.8880 | test_loss 0.5454 | test_acc 0.8370 | lr 0.0147
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 212/256 | train_loss 0.6913 | train_acc 0.7680 | test_loss 0.6466 | test_acc 0.8030 | lr 0.0141
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 213/256 | train_loss 0.5386 | train_acc 0.8360 | test_loss 0.7314 | test_acc 0.7770 | lr 0.0136
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 214/256 | train_loss 0.5430 | train_acc 0.8340 | test_loss 0.8163 | test_acc 0.7550 | lr 0.0131
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 215/256 | train_loss 0.6115 | train_acc 0.8280 | test_loss 0.8497 | test_acc 0.7560 | lr 0.0125
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 216/256 | train_loss 0.4953 | train_acc 0.9000 | test_loss 0.6711 | test_acc 0.7985 | lr 0.0120
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 217/256 | train_loss 0.5460 | train_acc 0.8740 | test_loss 0.6873 | test_acc 0.7940 | lr 0.0115
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 218/256 | train_loss 0.5476 | train_acc 0.9160 | test_loss 0.6707 | test_acc 0.7945 | lr 0.0110
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 219/256 | train_loss 0.6765 | train_acc 0.7460 | test_loss 0.8204 | test_acc 0.7555 | lr 0.0105
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 220/256 | train_loss 0.4249 | train_acc 0.8680 | test_loss 0.7320 | test_acc 0.7835 | lr 0.0100
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 221/256 | train_loss 0.7455 | train_acc 0.7560 | test_loss 0.7770 | test_acc 0.7635 | lr 0.0095
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 222/256 | train_loss 0.4575 | train_acc 0.8920 | test_loss 0.6971 | test_acc 0.7845 | lr 0.0091
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 223/256 | train_loss 0.5155 | train_acc 0.8280 | test_loss 0.7789 | test_acc 0.7570 | lr 0.0086
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 224/256 | train_loss 0.4937 | train_acc 0.8980 | test_loss 0.8564 | test_acc 0.7480 | lr 0.0082
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 225/256 | train_loss 0.5891 | train_acc 0.8600 | test_loss 0.8079 | test_acc 0.7530 | lr 0.0077
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 226/256 | train_loss 0.5188 | train_acc 0.8700 | test_loss 0.7673 | test_acc 0.7675 | lr 0.0073
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 227/256 | train_loss 0.7193 | train_acc 0.7940 | test_loss 0.7645 | test_acc 0.7680 | lr 0.0069
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 228/256 | train_loss 0.5133 | train_acc 0.9040 | test_loss 0.6819 | test_acc 0.7920 | lr 0.0065
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 229/256 | train_loss 0.6564 | train_acc 0.8860 | test_loss 0.8541 | test_acc 0.7335 | lr 0.0061
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 230/256 | train_loss 0.4936 | train_acc 0.9040 | test_loss 0.7734 | test_acc 0.7605 | lr 0.0057
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 231/256 | train_loss 0.5365 | train_acc 0.8620 | test_loss 0.7895 | test_acc 0.7585 | lr 0.0054
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 232/256 | train_loss 0.4005 | train_acc 0.9220 | test_loss 0.7489 | test_acc 0.7700 | lr 0.0050
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 233/256 | train_loss 0.4699 | train_acc 0.8640 | test_loss 0.7460 | test_acc 0.7675 | lr 0.0047
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 234/256 | train_loss 0.7093 | train_acc 0.7940 | test_loss 0.7589 | test_acc 0.7690 | lr 0.0043
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 235/256 | train_loss 0.8007 | train_acc 0.7400 | test_loss 0.7352 | test_acc 0.7755 | lr 0.0040
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 236/256 | train_loss 0.6167 | train_acc 0.8660 | test_loss 0.7112 | test_acc 0.7825 | lr 0.0037
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 237/256 | train_loss 0.5873 | train_acc 0.8340 | test_loss 0.6767 | test_acc 0.7935 | lr 0.0034
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 238/256 | train_loss 0.5216 | train_acc 0.8720 | test_loss 0.7124 | test_acc 0.7830 | lr 0.0031
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 239/256 | train_loss 0.4454 | train_acc 0.8600 | test_loss 0.7741 | test_acc 0.7650 | lr 0.0029
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 240/256 | train_loss 0.5692 | train_acc 0.8160 | test_loss 0.7599 | test_acc 0.7670 | lr 0.0026
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 241/256 | train_loss 0.1990 | train_acc 0.9200 | test_loss 0.8079 | test_acc 0.7535 | lr 0.0024
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 242/256 | train_loss 0.4510 | train_acc 0.8720 | test_loss 0.7304 | test_acc 0.7775 | lr 0.0022
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 243/256 | train_loss 0.4296 | train_acc 0.8440 | test_loss 0.7314 | test_acc 0.7785 | lr 0.0019
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 244/256 | train_loss 0.5445 | train_acc 0.8100 | test_loss 0.7056 | test_acc 0.7855 | lr 0.0017
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 245/256 | train_loss 0.5130 | train_acc 0.8680 | test_loss 0.6843 | test_acc 0.7940 | lr 0.0016
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 246/256 | train_loss 0.4673 | train_acc 0.8620 | test_loss 0.7267 | test_acc 0.7780 | lr 0.0014
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 247/256 | train_loss 0.4551 | train_acc 0.8600 | test_loss 0.7671 | test_acc 0.7695 | lr 0.0012
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 248/256 | train_loss 0.5446 | train_acc 0.7980 | test_loss 0.6536 | test_acc 0.8015 | lr 0.0011
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 249/256 | train_loss 0.6662 | train_acc 0.8280 | test_loss 0.7395 | test_acc 0.7715 | lr 0.0010
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 250/256 | train_loss 0.5182 | train_acc 0.8920 | test_loss 0.7099 | test_acc 0.7810 | lr 0.0009
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 251/256 | train_loss 0.6492 | train_acc 0.8180 | test_loss 0.7194 | test_acc 0.7800 | lr 0.0008
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 252/256 | train_loss 0.5568 | train_acc 0.8900 | test_loss 0.7752 | test_acc 0.7585 | lr 0.0007
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 253/256 | train_loss 0.3721 | train_acc 0.9140 | test_loss 0.7194 | test_acc 0.7790 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 254/256 | train_loss 0.3734 | train_acc 0.9140 | test_loss 0.7376 | test_acc 0.7735 | lr 0.0006
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 255/256 | train_loss 0.6318 | train_acc 0.8360 | test_loss 0.7133 | test_acc 0.7830 | lr 0.0005
[INFO] rainbow_memory.py:183 > Task 4 | Epoch 256/256 | train_loss 0.4782 | train_acc 0.8500 | test_loss 0.7600 | test_acc 0.7660 | lr 0.0005
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (4)
[INFO] main.py:389 > [2-4] Update the information for the current task
[INFO] finetune.py:157 > Apply after_task
[WARNING] finetune.py:230 > Already updated the memory during this iter (4)
[INFO] main.py:396 > [2-5] Report task result
[INFO] main.py:423 > ======== Summary =======
[INFO] main.py:425 > A_last 0.951 | A_avg 0.9054 | F_last 0.2356250062584877 | I_last -0.951
